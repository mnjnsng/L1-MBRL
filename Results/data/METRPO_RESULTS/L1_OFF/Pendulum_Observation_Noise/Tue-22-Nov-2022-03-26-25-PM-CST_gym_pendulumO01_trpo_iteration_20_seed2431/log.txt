Logging to experiments/gym_pendulumO01/Pendulum_Observation_Noise/Tue-22-Nov-2022-03-26-25-PM-CST_gym_pendulumO01_trpo_iteration_20_seed2431
Print configuration .....
{'env_name': 'gym_pendulumO01', 'random_seeds': [3214, 2431, 2531, 2231], 'save_variables': False, 'model_save_dir': '/tmp/pendulumO01_models/', 'restore_variables': False, 'start_onpol_iter': 0, 'onpol_iters': 40, 'num_path_random': 25, 'num_path_onpol': 25, 'env_horizon': 200, 'max_train_data': 200000, 'max_val_data': 100000, 'discard_ratio': 0.0, 'dynamics': {'pre_training': {'mode': 'intrinsic_reward', 'itr': 0, 'policy_itr': 20}, 'model': 'nn', 'ensemble': True, 'ensemble_model_count': 5, 'enable_particle_ensemble': True, 'particles': 5, 'obs_var': 1.0, 'intrinsic_reward_coeff': 1.0, 'ita': 1.0, 'mode': 'random', 'val': True, 'n_layers': 4, 'hidden_size': 1000, 'activation': 'relu', 'batch_size': 1000, 'learning_rate': 0.001, 'reg_coeff': 0.0, 'epochs': 200, 'kfac_params': {'learning_rate': 0.1, 'damping': 0.001, 'momentum': 0.9, 'kl_clip': 0.0001, 'cov_ema_decay': 0.99}}, 'policy': {'network_shape': [64, 64], 'init_logstd': 0.0, 'activation': 'tanh', 'reinitialize_every_itr': False}, 'trpo': {'horizon': 200, 'gamma': 0.99, 'step_size': 0.01, 'iterations': 20, 'batch_size': 50000, 'gae': 0.95, 'visualization': False, 'visualize_iterations': [0]}, 'algo': 'trpo'}
Generating random rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating random rollouts.
Creating normalization for training data.
Done creating normalization for training data.
Particle ensemble enabled? True
An ensemble of 5 dynamics model <class 'model.dynamics.NNDynamicsModel'> initialized
Train dynamics model with intrinsic reward only? False
Pre-training enabled. Using only intrinsic reward.
Pre-training dynamics model for 0 iterations...
Done pre-training dynamics model.
Using external reward only.
itr #0 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3932162821292877
Validation loss = 0.21747225522994995
Validation loss = 0.20668348670005798
Validation loss = 0.20218776166439056
Validation loss = 0.20198364555835724
Validation loss = 0.2018267661333084
Validation loss = 0.20320352911949158
Validation loss = 0.1988392323255539
Validation loss = 0.19717484712600708
Validation loss = 0.19211366772651672
Validation loss = 0.18912829458713531
Validation loss = 0.1865411102771759
Validation loss = 0.1843731701374054
Validation loss = 0.18514908850193024
Validation loss = 0.20398612320423126
Validation loss = 0.18260207772254944
Validation loss = 0.1792450100183487
Validation loss = 0.17823882400989532
Validation loss = 0.17828142642974854
Validation loss = 0.17207853496074677
Validation loss = 0.17371927201747894
Validation loss = 0.17175260186195374
Validation loss = 0.17040668427944183
Validation loss = 0.17398014664649963
Validation loss = 0.17748627066612244
Validation loss = 0.1804899126291275
Validation loss = 0.16884322464466095
Validation loss = 0.17080041766166687
Validation loss = 0.1671980321407318
Validation loss = 0.17849892377853394
Validation loss = 0.16733363270759583
Validation loss = 0.17495261132717133
Validation loss = 0.1667674332857132
Validation loss = 0.17049376666545868
Validation loss = 0.1702030897140503
Validation loss = 0.1686471551656723
Validation loss = 0.17469696700572968
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.40245720744132996
Validation loss = 0.21573562920093536
Validation loss = 0.21020551025867462
Validation loss = 0.2047111988067627
Validation loss = 0.20223866403102875
Validation loss = 0.19979581236839294
Validation loss = 0.19894731044769287
Validation loss = 0.19493737816810608
Validation loss = 0.19266970455646515
Validation loss = 0.19255201518535614
Validation loss = 0.20904557406902313
Validation loss = 0.1860436201095581
Validation loss = 0.1801968216896057
Validation loss = 0.18147894740104675
Validation loss = 0.17950160801410675
Validation loss = 0.1814529150724411
Validation loss = 0.18453668057918549
Validation loss = 0.1772751808166504
Validation loss = 0.17535662651062012
Validation loss = 0.17288590967655182
Validation loss = 0.170751690864563
Validation loss = 0.1797676384449005
Validation loss = 0.1720615178346634
Validation loss = 0.16824813187122345
Validation loss = 0.17807531356811523
Validation loss = 0.1707925647497177
Validation loss = 0.17759281396865845
Validation loss = 0.17362569272518158
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3696781396865845
Validation loss = 0.21613605320453644
Validation loss = 0.2081967145204544
Validation loss = 0.20399515330791473
Validation loss = 0.20059971511363983
Validation loss = 0.19925081729888916
Validation loss = 0.1986553817987442
Validation loss = 0.19623878598213196
Validation loss = 0.19304662942886353
Validation loss = 0.19018174707889557
Validation loss = 0.18961845338344574
Validation loss = 0.18523354828357697
Validation loss = 0.1825968623161316
Validation loss = 0.19167767465114594
Validation loss = 0.1796654313802719
Validation loss = 0.18132825195789337
Validation loss = 0.17790713906288147
Validation loss = 0.18045194447040558
Validation loss = 0.18089072406291962
Validation loss = 0.17551861703395844
Validation loss = 0.18828491866588593
Validation loss = 0.1796937882900238
Validation loss = 0.17510344088077545
Validation loss = 0.1725529134273529
Validation loss = 0.17248302698135376
Validation loss = 0.172597736120224
Validation loss = 0.17303083837032318
Validation loss = 0.17306400835514069
Validation loss = 0.1683274209499359
Validation loss = 0.16938768327236176
Validation loss = 0.17795759439468384
Validation loss = 0.16696880757808685
Validation loss = 0.17397716641426086
Validation loss = 0.16843654215335846
Validation loss = 0.1721460372209549
Validation loss = 0.16907283663749695
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.33230164647102356
Validation loss = 0.2162979543209076
Validation loss = 0.20473062992095947
Validation loss = 0.20239953696727753
Validation loss = 0.20110511779785156
Validation loss = 0.2014273703098297
Validation loss = 0.1981116533279419
Validation loss = 0.20014169812202454
Validation loss = 0.1961313933134079
Validation loss = 0.19321738183498383
Validation loss = 0.19477131962776184
Validation loss = 0.18743541836738586
Validation loss = 0.18254581093788147
Validation loss = 0.18285737931728363
Validation loss = 0.1782626211643219
Validation loss = 0.17598077654838562
Validation loss = 0.17709869146347046
Validation loss = 0.17650732398033142
Validation loss = 0.17798826098442078
Validation loss = 0.18100197613239288
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4088279604911804
Validation loss = 0.2138195037841797
Validation loss = 0.20461009442806244
Validation loss = 0.20383980870246887
Validation loss = 0.2015366554260254
Validation loss = 0.19982926547527313
Validation loss = 0.19868479669094086
Validation loss = 0.2006935030221939
Validation loss = 0.1952870637178421
Validation loss = 0.19541291892528534
Validation loss = 0.19497181475162506
Validation loss = 0.1909339874982834
Validation loss = 0.1857893019914627
Validation loss = 0.18475157022476196
Validation loss = 0.18097089231014252
Validation loss = 0.1728493869304657
Validation loss = 0.1763508915901184
Validation loss = 0.17993056774139404
Validation loss = 0.17611654102802277
Validation loss = 0.17731423676013947
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 0        |
| MaximumReturn | 199      |
| MinimumReturn | 112      |
| TotalSamples  | 6666     |
----------------------------
itr #1 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.35786566138267517
Validation loss = 0.24476175010204315
Validation loss = 0.2359144687652588
Validation loss = 0.22632785141468048
Validation loss = 0.2274516075849533
Validation loss = 0.2237245887517929
Validation loss = 0.2272629588842392
Validation loss = 0.22712254524230957
Validation loss = 0.22348521649837494
Validation loss = 0.22230416536331177
Validation loss = 0.2279822677373886
Validation loss = 0.22354991734027863
Validation loss = 0.22767795622348785
Validation loss = 0.22113193571567535
Validation loss = 0.22290973365306854
Validation loss = 0.22295862436294556
Validation loss = 0.22019082307815552
Validation loss = 0.22642232477664948
Validation loss = 0.22478164732456207
Validation loss = 0.22144685685634613
Validation loss = 0.2239699810743332
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.32893964648246765
Validation loss = 0.2394837886095047
Validation loss = 0.2274450808763504
Validation loss = 0.23198439180850983
Validation loss = 0.22762000560760498
Validation loss = 0.22677229344844818
Validation loss = 0.2259594351053238
Validation loss = 0.22527194023132324
Validation loss = 0.22543013095855713
Validation loss = 0.22294867038726807
Validation loss = 0.22305454313755035
Validation loss = 0.22435379028320312
Validation loss = 0.22953224182128906
Validation loss = 0.23152752220630646
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3557499647140503
Validation loss = 0.24006867408752441
Validation loss = 0.22763574123382568
Validation loss = 0.22706453502178192
Validation loss = 0.2243104726076126
Validation loss = 0.2305181622505188
Validation loss = 0.22808998823165894
Validation loss = 0.22969649732112885
Validation loss = 0.22326867282390594
Validation loss = 0.22869567573070526
Validation loss = 0.222231924533844
Validation loss = 0.2232017070055008
Validation loss = 0.22098584473133087
Validation loss = 0.22106236219406128
Validation loss = 0.22565054893493652
Validation loss = 0.2240934818983078
Validation loss = 0.22213053703308105
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3086383640766144
Validation loss = 0.24777185916900635
Validation loss = 0.2373446375131607
Validation loss = 0.2339673638343811
Validation loss = 0.23144935071468353
Validation loss = 0.22657226026058197
Validation loss = 0.2280128449201584
Validation loss = 0.22724227607250214
Validation loss = 0.23418118059635162
Validation loss = 0.2276756316423416
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.31745731830596924
Validation loss = 0.24057109653949738
Validation loss = 0.24231594800949097
Validation loss = 0.22937029600143433
Validation loss = 0.23333917558193207
Validation loss = 0.2265452891588211
Validation loss = 0.23111337423324585
Validation loss = 0.22965212166309357
Validation loss = 0.22917896509170532
Validation loss = 0.22941331565380096
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 1        |
| MaximumReturn | 199      |
| MinimumReturn | 117      |
| TotalSamples  | 9999     |
----------------------------
itr #2 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.2691299319267273
Validation loss = 0.2504808306694031
Validation loss = 0.24756240844726562
Validation loss = 0.24918468296527863
Validation loss = 0.24843783676624298
Validation loss = 0.24674811959266663
Validation loss = 0.24863465130329132
Validation loss = 0.25324326753616333
Validation loss = 0.24911364912986755
Validation loss = 0.24669256806373596
Validation loss = 0.2475525587797165
Validation loss = 0.24676401913166046
Validation loss = 0.25121235847473145
Validation loss = 0.25324252247810364
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.2702717185020447
Validation loss = 0.2474844753742218
Validation loss = 0.24839219450950623
Validation loss = 0.25160080194473267
Validation loss = 0.252930223941803
Validation loss = 0.25036659836769104
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.2719588875770569
Validation loss = 0.25539669394493103
Validation loss = 0.2527076005935669
Validation loss = 0.24942061305046082
Validation loss = 0.24777427315711975
Validation loss = 0.24824638664722443
Validation loss = 0.247806578874588
Validation loss = 0.25002071261405945
Validation loss = 0.24680569767951965
Validation loss = 0.24957609176635742
Validation loss = 0.2502177357673645
Validation loss = 0.24968168139457703
Validation loss = 0.2475021630525589
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.26669588685035706
Validation loss = 0.254029244184494
Validation loss = 0.255622923374176
Validation loss = 0.2473963499069214
Validation loss = 0.2500357925891876
Validation loss = 0.2483743131160736
Validation loss = 0.2508590817451477
Validation loss = 0.24851477146148682
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.27362748980522156
Validation loss = 0.2580108642578125
Validation loss = 0.24982181191444397
Validation loss = 0.25155407190322876
Validation loss = 0.2521229088306427
Validation loss = 0.25008100271224976
Validation loss = 0.24846675992012024
Validation loss = 0.25147634744644165
Validation loss = 0.25257548689842224
Validation loss = 0.2523638606071472
Validation loss = 0.2484126091003418
Validation loss = 0.25687482953071594
Validation loss = 0.2485775500535965
Validation loss = 0.24763374030590057
Validation loss = 0.2560364902019501
Validation loss = 0.25322574377059937
Validation loss = 0.24845413863658905
Validation loss = 0.24891798198223114
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 180      |
| Iteration     | 2        |
| MaximumReturn | 199      |
| MinimumReturn | 125      |
| TotalSamples  | 13332    |
----------------------------
itr #3 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.28667527437210083
Validation loss = 0.27932778000831604
Validation loss = 0.27917835116386414
Validation loss = 0.2866836190223694
Validation loss = 0.2826118469238281
Validation loss = 0.2782144844532013
Validation loss = 0.2809739112854004
Validation loss = 0.28197506070137024
Validation loss = 0.2818252146244049
Validation loss = 0.28045201301574707
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.2857271432876587
Validation loss = 0.27948060631752014
Validation loss = 0.2878384292125702
Validation loss = 0.28412070870399475
Validation loss = 0.2809354066848755
Validation loss = 0.2779519259929657
Validation loss = 0.2767667770385742
Validation loss = 0.2780883014202118
Validation loss = 0.28403040766716003
Validation loss = 0.2798745632171631
Validation loss = 0.28011056780815125
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.2892988622188568
Validation loss = 0.279228538274765
Validation loss = 0.28085488080978394
Validation loss = 0.2787418067455292
Validation loss = 0.28173398971557617
Validation loss = 0.28046923875808716
Validation loss = 0.2816147804260254
Validation loss = 0.2799204885959625
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.2883698344230652
Validation loss = 0.2792626917362213
Validation loss = 0.2835236191749573
Validation loss = 0.28065839409828186
Validation loss = 0.2790997624397278
Validation loss = 0.2764703333377838
Validation loss = 0.28192418813705444
Validation loss = 0.2787732779979706
Validation loss = 0.28577134013175964
Validation loss = 0.28088629245758057
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.2896767258644104
Validation loss = 0.2849549949169159
Validation loss = 0.2810911238193512
Validation loss = 0.27821674942970276
Validation loss = 0.28004345297813416
Validation loss = 0.27983856201171875
Validation loss = 0.28187546133995056
Validation loss = 0.280114084482193
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 3        |
| MaximumReturn | 198      |
| MinimumReturn | 120      |
| TotalSamples  | 16665    |
----------------------------
itr #4 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.30472245812416077
Validation loss = 0.3016715943813324
Validation loss = 0.3005918264389038
Validation loss = 0.30052298307418823
Validation loss = 0.2998974025249481
Validation loss = 0.2978946566581726
Validation loss = 0.2991621792316437
Validation loss = 0.29747045040130615
Validation loss = 0.30232229828834534
Validation loss = 0.29828429222106934
Validation loss = 0.30257371068000793
Validation loss = 0.2990540862083435
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.2998667061328888
Validation loss = 0.29827237129211426
Validation loss = 0.29839131236076355
Validation loss = 0.3010139465332031
Validation loss = 0.30133822560310364
Validation loss = 0.29809248447418213
Validation loss = 0.30023401975631714
Validation loss = 0.3019863963127136
Validation loss = 0.29976069927215576
Validation loss = 0.29923057556152344
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3007338047027588
Validation loss = 0.3008120656013489
Validation loss = 0.3020709753036499
Validation loss = 0.29831764101982117
Validation loss = 0.29886844754219055
Validation loss = 0.2981860935688019
Validation loss = 0.2986590266227722
Validation loss = 0.3010095953941345
Validation loss = 0.30122193694114685
Validation loss = 0.30228227376937866
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3014407455921173
Validation loss = 0.3016030192375183
Validation loss = 0.2995032072067261
Validation loss = 0.30077555775642395
Validation loss = 0.29794639348983765
Validation loss = 0.3016456365585327
Validation loss = 0.29729780554771423
Validation loss = 0.30605489015579224
Validation loss = 0.29852402210235596
Validation loss = 0.2974843978881836
Validation loss = 0.297718346118927
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.30047956109046936
Validation loss = 0.2970811128616333
Validation loss = 0.29865092039108276
Validation loss = 0.2994036078453064
Validation loss = 0.302334725856781
Validation loss = 0.2999459505081177
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 4        |
| MaximumReturn | 199      |
| MinimumReturn | 128      |
| TotalSamples  | 19998    |
----------------------------
itr #5 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3088875412940979
Validation loss = 0.3087157905101776
Validation loss = 0.3055267333984375
Validation loss = 0.30883175134658813
Validation loss = 0.30732962489128113
Validation loss = 0.30587631464004517
Validation loss = 0.30646026134490967
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3141688108444214
Validation loss = 0.3037875294685364
Validation loss = 0.3042440712451935
Validation loss = 0.30567315220832825
Validation loss = 0.30460312962532043
Validation loss = 0.30709612369537354
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.30540257692337036
Validation loss = 0.3068225085735321
Validation loss = 0.30712810158729553
Validation loss = 0.3077293932437897
Validation loss = 0.30617189407348633
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3053756654262543
Validation loss = 0.3025619089603424
Validation loss = 0.3030676245689392
Validation loss = 0.3044356107711792
Validation loss = 0.3073415756225586
Validation loss = 0.3034001290798187
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.30739083886146545
Validation loss = 0.3052382171154022
Validation loss = 0.3085402846336365
Validation loss = 0.3043947219848633
Validation loss = 0.303903192281723
Validation loss = 0.3047596216201782
Validation loss = 0.3050521910190582
Validation loss = 0.3050231635570526
Validation loss = 0.3049366772174835
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 5        |
| MaximumReturn | 199      |
| MinimumReturn | 124      |
| TotalSamples  | 23331    |
----------------------------
itr #6 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.31972238421440125
Validation loss = 0.31712716817855835
Validation loss = 0.31486523151397705
Validation loss = 0.31407541036605835
Validation loss = 0.3151595890522003
Validation loss = 0.3165503740310669
Validation loss = 0.31755849719047546
Validation loss = 0.3189089298248291
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.31667372584342957
Validation loss = 0.31767040491104126
Validation loss = 0.3164466619491577
Validation loss = 0.31924203038215637
Validation loss = 0.3158002197742462
Validation loss = 0.31617870926856995
Validation loss = 0.3174690008163452
Validation loss = 0.3153111934661865
Validation loss = 0.31603339314460754
Validation loss = 0.3162328004837036
Validation loss = 0.31466805934906006
Validation loss = 0.3215589225292206
Validation loss = 0.31613826751708984
Validation loss = 0.31745240092277527
Validation loss = 0.3182027041912079
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.31478917598724365
Validation loss = 0.31709933280944824
Validation loss = 0.31597888469696045
Validation loss = 0.31611695885658264
Validation loss = 0.31508728861808777
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.31713369488716125
Validation loss = 0.3146459758281708
Validation loss = 0.3161983788013458
Validation loss = 0.3153141736984253
Validation loss = 0.31624680757522583
Validation loss = 0.317376971244812
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3143233358860016
Validation loss = 0.3185345232486725
Validation loss = 0.31492841243743896
Validation loss = 0.3145378828048706
Validation loss = 0.3178268373012543
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 6        |
| MaximumReturn | 198      |
| MinimumReturn | 129      |
| TotalSamples  | 26664    |
----------------------------
itr #7 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3246614336967468
Validation loss = 0.32460424304008484
Validation loss = 0.32508179545402527
Validation loss = 0.32371658086776733
Validation loss = 0.3232806324958801
Validation loss = 0.32223305106163025
Validation loss = 0.32405561208724976
Validation loss = 0.3236408233642578
Validation loss = 0.32414624094963074
Validation loss = 0.32615944743156433
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3248898386955261
Validation loss = 0.3227858245372772
Validation loss = 0.32642439007759094
Validation loss = 0.32267826795578003
Validation loss = 0.3226281404495239
Validation loss = 0.3234155774116516
Validation loss = 0.322774738073349
Validation loss = 0.32540956139564514
Validation loss = 0.3233833312988281
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.32314980030059814
Validation loss = 0.32127171754837036
Validation loss = 0.3211117386817932
Validation loss = 0.321584552526474
Validation loss = 0.3217969238758087
Validation loss = 0.32174554467201233
Validation loss = 0.3228614628314972
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3216339349746704
Validation loss = 0.3223539888858795
Validation loss = 0.32120248675346375
Validation loss = 0.3217398524284363
Validation loss = 0.32391536235809326
Validation loss = 0.32056576013565063
Validation loss = 0.3215455710887909
Validation loss = 0.3214368224143982
Validation loss = 0.32255345582962036
Validation loss = 0.32223600149154663
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3221922516822815
Validation loss = 0.3198879063129425
Validation loss = 0.3198035657405853
Validation loss = 0.3206911087036133
Validation loss = 0.32170364260673523
Validation loss = 0.3242548704147339
Validation loss = 0.32106029987335205
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 178      |
| Iteration     | 7        |
| MaximumReturn | 199      |
| MinimumReturn | 126      |
| TotalSamples  | 29997    |
----------------------------
itr #8 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3326023519039154
Validation loss = 0.33447372913360596
Validation loss = 0.33180421590805054
Validation loss = 0.3311385214328766
Validation loss = 0.3374917805194855
Validation loss = 0.33141231536865234
Validation loss = 0.33104997873306274
Validation loss = 0.33340463042259216
Validation loss = 0.3326270878314972
Validation loss = 0.33282917737960815
Validation loss = 0.33336004614830017
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.330569863319397
Validation loss = 0.3319343626499176
Validation loss = 0.33100518584251404
Validation loss = 0.33311906456947327
Validation loss = 0.3306773900985718
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.32911375164985657
Validation loss = 0.3319152891635895
Validation loss = 0.33325472474098206
Validation loss = 0.32751283049583435
Validation loss = 0.329142302274704
Validation loss = 0.3292028307914734
Validation loss = 0.3323620557785034
Validation loss = 0.32933393120765686
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.32954341173171997
Validation loss = 0.3285893201828003
Validation loss = 0.3296515643596649
Validation loss = 0.3309958875179291
Validation loss = 0.3292336165904999
Validation loss = 0.3312543034553528
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3290618062019348
Validation loss = 0.3292810320854187
Validation loss = 0.3280791640281677
Validation loss = 0.32946014404296875
Validation loss = 0.3279098570346832
Validation loss = 0.33153167366981506
Validation loss = 0.32799407839775085
Validation loss = 0.32958918809890747
Validation loss = 0.32951977849006653
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 8        |
| MaximumReturn | 199      |
| MinimumReturn | 125      |
| TotalSamples  | 33330    |
----------------------------
itr #9 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.33863866329193115
Validation loss = 0.33827394247055054
Validation loss = 0.3366199731826782
Validation loss = 0.34157589077949524
Validation loss = 0.3390718698501587
Validation loss = 0.34119343757629395
Validation loss = 0.3398289680480957
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3332393169403076
Validation loss = 0.3340372145175934
Validation loss = 0.336762934923172
Validation loss = 0.3366902768611908
Validation loss = 0.3357892632484436
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.33690527081489563
Validation loss = 0.33364593982696533
Validation loss = 0.33756402134895325
Validation loss = 0.3342088460922241
Validation loss = 0.3321972191333771
Validation loss = 0.33518269658088684
Validation loss = 0.33475756645202637
Validation loss = 0.3341946005821228
Validation loss = 0.33582761883735657
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.33455967903137207
Validation loss = 0.3336864709854126
Validation loss = 0.33533450961112976
Validation loss = 0.3358364999294281
Validation loss = 0.33556047081947327
Validation loss = 0.3339844346046448
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3404834568500519
Validation loss = 0.33420050144195557
Validation loss = 0.33444076776504517
Validation loss = 0.3373485803604126
Validation loss = 0.3338238000869751
Validation loss = 0.3376995921134949
Validation loss = 0.33570173382759094
Validation loss = 0.3357960283756256
Validation loss = 0.335000216960907
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 9        |
| MaximumReturn | 199      |
| MinimumReturn | 122      |
| TotalSamples  | 36663    |
----------------------------
itr #10 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3427906930446625
Validation loss = 0.34202849864959717
Validation loss = 0.3431653678417206
Validation loss = 0.3442603349685669
Validation loss = 0.3455740213394165
Validation loss = 0.34266382455825806
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.34083226323127747
Validation loss = 0.3384287655353546
Validation loss = 0.3391103148460388
Validation loss = 0.3397805988788605
Validation loss = 0.34067848324775696
Validation loss = 0.34119150042533875
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.34552156925201416
Validation loss = 0.33831679821014404
Validation loss = 0.3379195034503937
Validation loss = 0.3392693102359772
Validation loss = 0.3468151092529297
Validation loss = 0.3402095139026642
Validation loss = 0.34136077761650085
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3405447006225586
Validation loss = 0.33985504508018494
Validation loss = 0.33796703815460205
Validation loss = 0.33828216791152954
Validation loss = 0.3382784128189087
Validation loss = 0.33915266394615173
Validation loss = 0.3407335877418518
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.34278571605682373
Validation loss = 0.3407323360443115
Validation loss = 0.3397374153137207
Validation loss = 0.34095504879951477
Validation loss = 0.34225916862487793
Validation loss = 0.3413984179496765
Validation loss = 0.3410741090774536
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 10       |
| MaximumReturn | 198      |
| MinimumReturn | 123      |
| TotalSamples  | 39996    |
----------------------------
itr #11 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.34827715158462524
Validation loss = 0.34755244851112366
Validation loss = 0.3480226993560791
Validation loss = 0.3464590907096863
Validation loss = 0.34838247299194336
Validation loss = 0.34963926672935486
Validation loss = 0.34993186593055725
Validation loss = 0.34837234020233154
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.34544897079467773
Validation loss = 0.3482030928134918
Validation loss = 0.3471173644065857
Validation loss = 0.3455856740474701
Validation loss = 0.3468303084373474
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3459893465042114
Validation loss = 0.3433115482330322
Validation loss = 0.34782853722572327
Validation loss = 0.3464295268058777
Validation loss = 0.3452598452568054
Validation loss = 0.3452373147010803
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.34497708082199097
Validation loss = 0.3459056317806244
Validation loss = 0.3461853861808777
Validation loss = 0.34778696298599243
Validation loss = 0.34505364298820496
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.34715455770492554
Validation loss = 0.3454698622226715
Validation loss = 0.3433043658733368
Validation loss = 0.34613096714019775
Validation loss = 0.34685754776000977
Validation loss = 0.3470912575721741
Validation loss = 0.34626278281211853
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 11       |
| MaximumReturn | 198      |
| MinimumReturn | 122      |
| TotalSamples  | 43329    |
----------------------------
itr #12 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3577779233455658
Validation loss = 0.3593781888484955
Validation loss = 0.3596769869327545
Validation loss = 0.36070361733436584
Validation loss = 0.3634636104106903
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.355731725692749
Validation loss = 0.35397931933403015
Validation loss = 0.355697900056839
Validation loss = 0.3555322587490082
Validation loss = 0.3568875789642334
Validation loss = 0.35966119170188904
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3571738600730896
Validation loss = 0.3565550744533539
Validation loss = 0.3560459017753601
Validation loss = 0.357143372297287
Validation loss = 0.3572695255279541
Validation loss = 0.3561100363731384
Validation loss = 0.3559727370738983
Validation loss = 0.3575124144554138
Validation loss = 0.3571709096431732
Validation loss = 0.35711514949798584
Validation loss = 0.3628048002719879
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.35484859347343445
Validation loss = 0.3560901880264282
Validation loss = 0.353861004114151
Validation loss = 0.35369181632995605
Validation loss = 0.35709771513938904
Validation loss = 0.3564085066318512
Validation loss = 0.35664716362953186
Validation loss = 0.3555060029029846
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.35896775126457214
Validation loss = 0.3548734486103058
Validation loss = 0.35527756810188293
Validation loss = 0.35528913140296936
Validation loss = 0.35768207907676697
Validation loss = 0.3576638698577881
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 12       |
| MaximumReturn | 198      |
| MinimumReturn | 127      |
| TotalSamples  | 46662    |
----------------------------
itr #13 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.37214890122413635
Validation loss = 0.37180328369140625
Validation loss = 0.3712014853954315
Validation loss = 0.37092599272727966
Validation loss = 0.3737308382987976
Validation loss = 0.3709467947483063
Validation loss = 0.3719695508480072
Validation loss = 0.37095630168914795
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3683595359325409
Validation loss = 0.369642049074173
Validation loss = 0.3661358952522278
Validation loss = 0.3673354387283325
Validation loss = 0.368078351020813
Validation loss = 0.3717459440231323
Validation loss = 0.37243324518203735
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.36971643567085266
Validation loss = 0.3689538836479187
Validation loss = 0.3708365857601166
Validation loss = 0.3694986402988434
Validation loss = 0.3713732957839966
Validation loss = 0.37012553215026855
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3654409945011139
Validation loss = 0.37026312947273254
Validation loss = 0.3661409318447113
Validation loss = 0.3705582022666931
Validation loss = 0.3664310574531555
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3686617612838745
Validation loss = 0.36801964044570923
Validation loss = 0.36804842948913574
Validation loss = 0.36762428283691406
Validation loss = 0.3708582818508148
Validation loss = 0.36855772137641907
Validation loss = 0.36971205472946167
Validation loss = 0.3693387508392334
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 13       |
| MaximumReturn | 196      |
| MinimumReturn | 135      |
| TotalSamples  | 49995    |
----------------------------
itr #14 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.38209110498428345
Validation loss = 0.3815127909183502
Validation loss = 0.3807089924812317
Validation loss = 0.38500604033470154
Validation loss = 0.38542383909225464
Validation loss = 0.3845294117927551
Validation loss = 0.38536182045936584
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.380121648311615
Validation loss = 0.3776746690273285
Validation loss = 0.37999045848846436
Validation loss = 0.3795701563358307
Validation loss = 0.38014641404151917
Validation loss = 0.381031334400177
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3816481828689575
Validation loss = 0.37844496965408325
Validation loss = 0.38319018483161926
Validation loss = 0.38114234805107117
Validation loss = 0.3796607255935669
Validation loss = 0.3804200291633606
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.37793785333633423
Validation loss = 0.3796311914920807
Validation loss = 0.3790265917778015
Validation loss = 0.378305584192276
Validation loss = 0.3786313235759735
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3812117278575897
Validation loss = 0.3808903992176056
Validation loss = 0.38314637541770935
Validation loss = 0.3802013099193573
Validation loss = 0.38189324736595154
Validation loss = 0.38027822971343994
Validation loss = 0.38097241520881653
Validation loss = 0.3823023736476898
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 14       |
| MaximumReturn | 196      |
| MinimumReturn | 124      |
| TotalSamples  | 53328    |
----------------------------
itr #15 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.38737738132476807
Validation loss = 0.3875552713871002
Validation loss = 0.3896428942680359
Validation loss = 0.38709551095962524
Validation loss = 0.3908577561378479
Validation loss = 0.38972708582878113
Validation loss = 0.390041321516037
Validation loss = 0.3906313478946686
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.38445642590522766
Validation loss = 0.3851064145565033
Validation loss = 0.3859347105026245
Validation loss = 0.38473084568977356
Validation loss = 0.38677817583084106
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.38750359416007996
Validation loss = 0.38622209429740906
Validation loss = 0.38494840264320374
Validation loss = 0.38531583547592163
Validation loss = 0.38340938091278076
Validation loss = 0.38734182715415955
Validation loss = 0.386235773563385
Validation loss = 0.3876611888408661
Validation loss = 0.38848018646240234
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.38453567028045654
Validation loss = 0.38406604528427124
Validation loss = 0.38577064871788025
Validation loss = 0.3826823830604553
Validation loss = 0.38592055439949036
Validation loss = 0.38558271527290344
Validation loss = 0.3852847218513489
Validation loss = 0.38589251041412354
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.38557153940200806
Validation loss = 0.3851141333580017
Validation loss = 0.388996422290802
Validation loss = 0.38527950644493103
Validation loss = 0.38557323813438416
Validation loss = 0.3885246515274048
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 15       |
| MaximumReturn | 197      |
| MinimumReturn | 113      |
| TotalSamples  | 56661    |
----------------------------
itr #16 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.39367130398750305
Validation loss = 0.39650779962539673
Validation loss = 0.3940412402153015
Validation loss = 0.3949223458766937
Validation loss = 0.39777010679244995
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.39144665002822876
Validation loss = 0.38865426182746887
Validation loss = 0.38920119404792786
Validation loss = 0.38973164558410645
Validation loss = 0.3892688751220703
Validation loss = 0.3909408152103424
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.39148443937301636
Validation loss = 0.3928472399711609
Validation loss = 0.3916146159172058
Validation loss = 0.3905341923236847
Validation loss = 0.39539510011672974
Validation loss = 0.3921041488647461
Validation loss = 0.393709659576416
Validation loss = 0.39532679319381714
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3902721107006073
Validation loss = 0.3904033601284027
Validation loss = 0.38984695076942444
Validation loss = 0.39022132754325867
Validation loss = 0.39230912923812866
Validation loss = 0.39040544629096985
Validation loss = 0.39108434319496155
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3887731432914734
Validation loss = 0.3886318504810333
Validation loss = 0.39124980568885803
Validation loss = 0.391439288854599
Validation loss = 0.39200061559677124
Validation loss = 0.3922404944896698
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 16       |
| MaximumReturn | 198      |
| MinimumReturn | 136      |
| TotalSamples  | 59994    |
----------------------------
itr #17 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.39759454131126404
Validation loss = 0.39907512068748474
Validation loss = 0.39810910820961
Validation loss = 0.39704379439353943
Validation loss = 0.3980783224105835
Validation loss = 0.3978428244590759
Validation loss = 0.40281447768211365
Validation loss = 0.39911559224128723
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3923172354698181
Validation loss = 0.39387625455856323
Validation loss = 0.3941725790500641
Validation loss = 0.39550307393074036
Validation loss = 0.39447733759880066
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3940941393375397
Validation loss = 0.39432820677757263
Validation loss = 0.3965345025062561
Validation loss = 0.39589497447013855
Validation loss = 0.39523330330848694
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3934697210788727
Validation loss = 0.3927909731864929
Validation loss = 0.39575520157814026
Validation loss = 0.39260780811309814
Validation loss = 0.3943431079387665
Validation loss = 0.3934813439846039
Validation loss = 0.39495617151260376
Validation loss = 0.3960338234901428
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3933255076408386
Validation loss = 0.3958379328250885
Validation loss = 0.39530202746391296
Validation loss = 0.39611655473709106
Validation loss = 0.3955269753932953
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 17       |
| MaximumReturn | 198      |
| MinimumReturn | 119      |
| TotalSamples  | 63327    |
----------------------------
itr #18 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.401691734790802
Validation loss = 0.40318551659584045
Validation loss = 0.40431562066078186
Validation loss = 0.40211668610572815
Validation loss = 0.40467679500579834
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.39471888542175293
Validation loss = 0.39679256081581116
Validation loss = 0.3981001675128937
Validation loss = 0.39798006415367126
Validation loss = 0.3957202732563019
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4004034399986267
Validation loss = 0.39929255843162537
Validation loss = 0.3966056704521179
Validation loss = 0.4003220498561859
Validation loss = 0.4004691243171692
Validation loss = 0.4013921022415161
Validation loss = 0.40034353733062744
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.39723673462867737
Validation loss = 0.3983542323112488
Validation loss = 0.396866112947464
Validation loss = 0.39617088437080383
Validation loss = 0.39830711483955383
Validation loss = 0.4004283845424652
Validation loss = 0.3988427221775055
Validation loss = 0.3992069959640503
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3966170847415924
Validation loss = 0.3960813879966736
Validation loss = 0.3951628506183624
Validation loss = 0.3991398513317108
Validation loss = 0.3982299566268921
Validation loss = 0.3985823392868042
Validation loss = 0.3997018039226532
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 18       |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 66660    |
----------------------------
itr #19 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.40433892607688904
Validation loss = 0.40267258882522583
Validation loss = 0.4088125228881836
Validation loss = 0.4059688150882721
Validation loss = 0.40436697006225586
Validation loss = 0.4051973223686218
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.39898747205734253
Validation loss = 0.39776650071144104
Validation loss = 0.39829444885253906
Validation loss = 0.3990298807621002
Validation loss = 0.39950254559516907
Validation loss = 0.4018203616142273
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4012817442417145
Validation loss = 0.40242570638656616
Validation loss = 0.40320682525634766
Validation loss = 0.4020441472530365
Validation loss = 0.40278956294059753
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4006485044956207
Validation loss = 0.40112370252609253
Validation loss = 0.4018864631652832
Validation loss = 0.4018170237541199
Validation loss = 0.4024011492729187
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4010211229324341
Validation loss = 0.40067189931869507
Validation loss = 0.3995620608329773
Validation loss = 0.40279287099838257
Validation loss = 0.402302086353302
Validation loss = 0.40449681878089905
Validation loss = 0.40324896574020386
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 19       |
| MaximumReturn | 199      |
| MinimumReturn | 119      |
| TotalSamples  | 69993    |
----------------------------
itr #20 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4085264801979065
Validation loss = 0.4077000916004181
Validation loss = 0.407676637172699
Validation loss = 0.4083143472671509
Validation loss = 0.4088527262210846
Validation loss = 0.4109926223754883
Validation loss = 0.4085674285888672
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.402491956949234
Validation loss = 0.40054839849472046
Validation loss = 0.40259647369384766
Validation loss = 0.4021528959274292
Validation loss = 0.40547990798950195
Validation loss = 0.40389809012413025
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.403268963098526
Validation loss = 0.403409868478775
Validation loss = 0.4050223231315613
Validation loss = 0.40491983294487
Validation loss = 0.4052390158176422
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.40202829241752625
Validation loss = 0.4050463140010834
Validation loss = 0.40370747447013855
Validation loss = 0.40640756487846375
Validation loss = 0.40473800897598267
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.40521541237831116
Validation loss = 0.404450386762619
Validation loss = 0.4027392566204071
Validation loss = 0.40608853101730347
Validation loss = 0.40530186891555786
Validation loss = 0.40532079339027405
Validation loss = 0.40735581517219543
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 20       |
| MaximumReturn | 198      |
| MinimumReturn | 127      |
| TotalSamples  | 73326    |
----------------------------
itr #21 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4116263687610626
Validation loss = 0.4109937846660614
Validation loss = 0.41385629773139954
Validation loss = 0.4148617088794708
Validation loss = 0.41191163659095764
Validation loss = 0.41613972187042236
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.40418726205825806
Validation loss = 0.4048808813095093
Validation loss = 0.40458613634109497
Validation loss = 0.40881115198135376
Validation loss = 0.40599900484085083
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4072977304458618
Validation loss = 0.40661856532096863
Validation loss = 0.4102759063243866
Validation loss = 0.40889617800712585
Validation loss = 0.4094482362270355
Validation loss = 0.41047632694244385
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.40961554646492004
Validation loss = 0.407127320766449
Validation loss = 0.40814679861068726
Validation loss = 0.40805327892303467
Validation loss = 0.41020625829696655
Validation loss = 0.40933361649513245
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4075111746788025
Validation loss = 0.4105023741722107
Validation loss = 0.40790411829948425
Validation loss = 0.4100362956523895
Validation loss = 0.4097212553024292
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 21       |
| MaximumReturn | 198      |
| MinimumReturn | 127      |
| TotalSamples  | 76659    |
----------------------------
itr #22 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.41750726103782654
Validation loss = 0.41582679748535156
Validation loss = 0.41949528455734253
Validation loss = 0.4162525534629822
Validation loss = 0.4186108708381653
Validation loss = 0.4181719720363617
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.40845829248428345
Validation loss = 0.4082888662815094
Validation loss = 0.41042324900627136
Validation loss = 0.412030965089798
Validation loss = 0.41147732734680176
Validation loss = 0.4119606018066406
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4100586473941803
Validation loss = 0.4120299816131592
Validation loss = 0.41191554069519043
Validation loss = 0.4127897620201111
Validation loss = 0.41293099522590637
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.41290369629859924
Validation loss = 0.4115082621574402
Validation loss = 0.41210106015205383
Validation loss = 0.4097772538661957
Validation loss = 0.41385960578918457
Validation loss = 0.41556021571159363
Validation loss = 0.4138694703578949
Validation loss = 0.414661169052124
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4124505817890167
Validation loss = 0.41185262799263
Validation loss = 0.4150294363498688
Validation loss = 0.41507235169410706
Validation loss = 0.41298678517341614
Validation loss = 0.4146670401096344
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 22       |
| MaximumReturn | 198      |
| MinimumReturn | 109      |
| TotalSamples  | 79992    |
----------------------------
itr #23 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.41929736733436584
Validation loss = 0.4222397804260254
Validation loss = 0.42039862275123596
Validation loss = 0.42046141624450684
Validation loss = 0.4231477379798889
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.41096892952919006
Validation loss = 0.4121604859828949
Validation loss = 0.41398364305496216
Validation loss = 0.4151136875152588
Validation loss = 0.4165366291999817
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4134463667869568
Validation loss = 0.41398972272872925
Validation loss = 0.4152129292488098
Validation loss = 0.4137329161167145
Validation loss = 0.4156349301338196
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.41731739044189453
Validation loss = 0.4166359007358551
Validation loss = 0.41658300161361694
Validation loss = 0.4160660207271576
Validation loss = 0.41725462675094604
Validation loss = 0.41917872428894043
Validation loss = 0.4184238314628601
Validation loss = 0.4217258095741272
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4163435399532318
Validation loss = 0.41484159231185913
Validation loss = 0.41672706604003906
Validation loss = 0.4178956151008606
Validation loss = 0.41920948028564453
Validation loss = 0.41798916459083557
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 23       |
| MaximumReturn | 198      |
| MinimumReturn | 119      |
| TotalSamples  | 83325    |
----------------------------
itr #24 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4232896566390991
Validation loss = 0.42284345626831055
Validation loss = 0.4206070899963379
Validation loss = 0.42390161752700806
Validation loss = 0.42382967472076416
Validation loss = 0.4261276423931122
Validation loss = 0.42577528953552246
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4159944951534271
Validation loss = 0.41643840074539185
Validation loss = 0.4181792438030243
Validation loss = 0.4163784384727478
Validation loss = 0.4177154302597046
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.417764812707901
Validation loss = 0.4159895181655884
Validation loss = 0.4163070321083069
Validation loss = 0.41903606057167053
Validation loss = 0.4163901209831238
Validation loss = 0.41804730892181396
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.420498251914978
Validation loss = 0.4224008321762085
Validation loss = 0.42188766598701477
Validation loss = 0.42009302973747253
Validation loss = 0.4240947365760803
Validation loss = 0.4246213138103485
Validation loss = 0.42367470264434814
Validation loss = 0.42465731501579285
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.41574886441230774
Validation loss = 0.4183412790298462
Validation loss = 0.4189017117023468
Validation loss = 0.42022058367729187
Validation loss = 0.42176494002342224
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 24       |
| MaximumReturn | 198      |
| MinimumReturn | 113      |
| TotalSamples  | 86658    |
----------------------------
itr #25 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.42715415358543396
Validation loss = 0.42829567193984985
Validation loss = 0.42664846777915955
Validation loss = 0.42838239669799805
Validation loss = 0.42940738797187805
Validation loss = 0.42848286032676697
Validation loss = 0.4324836730957031
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.41744324564933777
Validation loss = 0.41780680418014526
Validation loss = 0.417476624250412
Validation loss = 0.4193266034126282
Validation loss = 0.42049098014831543
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4178487956523895
Validation loss = 0.4196251332759857
Validation loss = 0.4174076318740845
Validation loss = 0.41940468549728394
Validation loss = 0.42073383927345276
Validation loss = 0.42327243089675903
Validation loss = 0.42036929726600647
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.42613485455513
Validation loss = 0.4254687428474426
Validation loss = 0.4278697669506073
Validation loss = 0.4258509874343872
Validation loss = 0.42773616313934326
Validation loss = 0.42659807205200195
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4197195768356323
Validation loss = 0.4206640422344208
Validation loss = 0.42213931679725647
Validation loss = 0.42485466599464417
Validation loss = 0.4216419458389282
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 25       |
| MaximumReturn | 197      |
| MinimumReturn | 123      |
| TotalSamples  | 89991    |
----------------------------
itr #26 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4279608726501465
Validation loss = 0.42856258153915405
Validation loss = 0.43046900629997253
Validation loss = 0.4321257770061493
Validation loss = 0.4304420053958893
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4224562644958496
Validation loss = 0.4183938801288605
Validation loss = 0.4215908348560333
Validation loss = 0.41983985900878906
Validation loss = 0.41992923617362976
Validation loss = 0.4220981299877167
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4210171103477478
Validation loss = 0.4213671088218689
Validation loss = 0.42177197337150574
Validation loss = 0.4245259463787079
Validation loss = 0.42448553442955017
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.426069051027298
Validation loss = 0.4257323741912842
Validation loss = 0.42673832178115845
Validation loss = 0.4277719259262085
Validation loss = 0.42663711309432983
Validation loss = 0.4296347200870514
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4242401421070099
Validation loss = 0.4223608672618866
Validation loss = 0.42449119687080383
Validation loss = 0.4238620698451996
Validation loss = 0.42417508363723755
Validation loss = 0.42409634590148926
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 26       |
| MaximumReturn | 197      |
| MinimumReturn | 64.2     |
| TotalSamples  | 93324    |
----------------------------
itr #27 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.43179354071617126
Validation loss = 0.43316367268562317
Validation loss = 0.43377307057380676
Validation loss = 0.4343700706958771
Validation loss = 0.4339340925216675
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.42271187901496887
Validation loss = 0.42226943373680115
Validation loss = 0.4219537377357483
Validation loss = 0.42278778553009033
Validation loss = 0.42188817262649536
Validation loss = 0.425549179315567
Validation loss = 0.4271720349788666
Validation loss = 0.42790424823760986
Validation loss = 0.42633911967277527
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.42372944951057434
Validation loss = 0.4246067404747009
Validation loss = 0.42697960138320923
Validation loss = 0.42539694905281067
Validation loss = 0.4241548478603363
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4299951493740082
Validation loss = 0.43013501167297363
Validation loss = 0.42915672063827515
Validation loss = 0.42908602952957153
Validation loss = 0.431825190782547
Validation loss = 0.43182751536369324
Validation loss = 0.43128883838653564
Validation loss = 0.4344501197338104
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4265575110912323
Validation loss = 0.4258667826652527
Validation loss = 0.4248356521129608
Validation loss = 0.4273948669433594
Validation loss = 0.42619094252586365
Validation loss = 0.4274832606315613
Validation loss = 0.4274178743362427
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 163      |
| Iteration     | 27       |
| MaximumReturn | 198      |
| MinimumReturn | 104      |
| TotalSamples  | 96657    |
----------------------------
itr #28 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.43070435523986816
Validation loss = 0.43283167481422424
Validation loss = 0.43380728363990784
Validation loss = 0.43355023860931396
Validation loss = 0.4332716763019562
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4286764860153198
Validation loss = 0.4262821674346924
Validation loss = 0.4261544644832611
Validation loss = 0.4267137944698334
Validation loss = 0.4304286539554596
Validation loss = 0.42817986011505127
Validation loss = 0.4296075105667114
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.42504653334617615
Validation loss = 0.42476487159729004
Validation loss = 0.4237574636936188
Validation loss = 0.4246680438518524
Validation loss = 0.4261529743671417
Validation loss = 0.42698583006858826
Validation loss = 0.42711448669433594
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.43022313714027405
Validation loss = 0.43203625082969666
Validation loss = 0.43319353461265564
Validation loss = 0.432279109954834
Validation loss = 0.43218132853507996
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.42844095826148987
Validation loss = 0.42659714818000793
Validation loss = 0.4292181432247162
Validation loss = 0.4301605522632599
Validation loss = 0.42990097403526306
Validation loss = 0.43160557746887207
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 28       |
| MaximumReturn | 198      |
| MinimumReturn | 120      |
| TotalSamples  | 99990    |
----------------------------
itr #29 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4335818588733673
Validation loss = 0.4315752387046814
Validation loss = 0.43323856592178345
Validation loss = 0.43520882725715637
Validation loss = 0.4331629276275635
Validation loss = 0.4355699419975281
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.42694559693336487
Validation loss = 0.42822369933128357
Validation loss = 0.4293915033340454
Validation loss = 0.4285929203033447
Validation loss = 0.43101823329925537
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.42572373151779175
Validation loss = 0.4268760681152344
Validation loss = 0.4254751205444336
Validation loss = 0.4266931414604187
Validation loss = 0.4277363717556
Validation loss = 0.4305572509765625
Validation loss = 0.42728856205940247
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.43255099654197693
Validation loss = 0.4327797293663025
Validation loss = 0.4319356083869934
Validation loss = 0.4319630563259125
Validation loss = 0.4354914724826813
Validation loss = 0.4356566369533539
Validation loss = 0.4332474172115326
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4286271035671234
Validation loss = 0.4282892942428589
Validation loss = 0.43065959215164185
Validation loss = 0.42996498942375183
Validation loss = 0.42950066924095154
Validation loss = 0.43148717284202576
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 29       |
| MaximumReturn | 198      |
| MinimumReturn | 120      |
| TotalSamples  | 103323   |
----------------------------
itr #30 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.43366551399230957
Validation loss = 0.43251776695251465
Validation loss = 0.43524453043937683
Validation loss = 0.4349459707736969
Validation loss = 0.4344615638256073
Validation loss = 0.43385693430900574
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4284684658050537
Validation loss = 0.42824676632881165
Validation loss = 0.43063366413116455
Validation loss = 0.4310581386089325
Validation loss = 0.43141263723373413
Validation loss = 0.43381059169769287
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.42790380120277405
Validation loss = 0.42596757411956787
Validation loss = 0.4288143813610077
Validation loss = 0.4305992126464844
Validation loss = 0.4320541322231293
Validation loss = 0.4315369129180908
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4338107705116272
Validation loss = 0.4347364902496338
Validation loss = 0.43540531396865845
Validation loss = 0.43640655279159546
Validation loss = 0.43498072028160095
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4300125241279602
Validation loss = 0.43017253279685974
Validation loss = 0.42973098158836365
Validation loss = 0.429999440908432
Validation loss = 0.433457612991333
Validation loss = 0.43120697140693665
Validation loss = 0.4326298236846924
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 30       |
| MaximumReturn | 198      |
| MinimumReturn | 122      |
| TotalSamples  | 106656   |
----------------------------
itr #31 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.43751680850982666
Validation loss = 0.4361211359500885
Validation loss = 0.43736040592193604
Validation loss = 0.43714213371276855
Validation loss = 0.43847617506980896
Validation loss = 0.4393986165523529
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.43472033739089966
Validation loss = 0.43275439739227295
Validation loss = 0.43024587631225586
Validation loss = 0.43443921208381653
Validation loss = 0.4322429299354553
Validation loss = 0.43234360218048096
Validation loss = 0.4352032542228699
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4306313991546631
Validation loss = 0.4300941526889801
Validation loss = 0.4305931627750397
Validation loss = 0.43216103315353394
Validation loss = 0.43260136246681213
Validation loss = 0.43121251463890076
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.43768468499183655
Validation loss = 0.4350280165672302
Validation loss = 0.43730682134628296
Validation loss = 0.43615829944610596
Validation loss = 0.4373093843460083
Validation loss = 0.4373602271080017
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.43207672238349915
Validation loss = 0.43369826674461365
Validation loss = 0.43237221240997314
Validation loss = 0.4332367479801178
Validation loss = 0.4348362386226654
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 31       |
| MaximumReturn | 198      |
| MinimumReturn | 122      |
| TotalSamples  | 109989   |
----------------------------
itr #32 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4349778890609741
Validation loss = 0.43404436111450195
Validation loss = 0.4381534457206726
Validation loss = 0.43536561727523804
Validation loss = 0.4402141869068146
Validation loss = 0.4404103457927704
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.43183645606040955
Validation loss = 0.4311063587665558
Validation loss = 0.4298330843448639
Validation loss = 0.4329124391078949
Validation loss = 0.4340570271015167
Validation loss = 0.43408432602882385
Validation loss = 0.4340231418609619
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4296470284461975
Validation loss = 0.430755615234375
Validation loss = 0.4318222105503082
Validation loss = 0.43280819058418274
Validation loss = 0.43128126859664917
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4347127676010132
Validation loss = 0.435239315032959
Validation loss = 0.43602555990219116
Validation loss = 0.4359162151813507
Validation loss = 0.43790945410728455
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.43359169363975525
Validation loss = 0.43030568957328796
Validation loss = 0.4329785704612732
Validation loss = 0.43239298462867737
Validation loss = 0.4326576888561249
Validation loss = 0.4349942207336426
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 177      |
| Iteration     | 32       |
| MaximumReturn | 198      |
| MinimumReturn | 122      |
| TotalSamples  | 113322   |
----------------------------
itr #33 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4410759508609772
Validation loss = 0.4372546970844269
Validation loss = 0.43802085518836975
Validation loss = 0.4379759430885315
Validation loss = 0.4385771155357361
Validation loss = 0.43797510862350464
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4336722195148468
Validation loss = 0.43206319212913513
Validation loss = 0.4339040219783783
Validation loss = 0.43418583273887634
Validation loss = 0.4352802336215973
Validation loss = 0.4364226162433624
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.43128564953804016
Validation loss = 0.4298277497291565
Validation loss = 0.43100762367248535
Validation loss = 0.43220657110214233
Validation loss = 0.4326820373535156
Validation loss = 0.43266481161117554
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.43679141998291016
Validation loss = 0.4354389011859894
Validation loss = 0.43448925018310547
Validation loss = 0.43762773275375366
Validation loss = 0.4375276565551758
Validation loss = 0.43741559982299805
Validation loss = 0.43604645133018494
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.43475309014320374
Validation loss = 0.43457022309303284
Validation loss = 0.4346778988838196
Validation loss = 0.43646588921546936
Validation loss = 0.43463796377182007
Validation loss = 0.43589240312576294
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 33       |
| MaximumReturn | 198      |
| MinimumReturn | 128      |
| TotalSamples  | 116655   |
----------------------------
itr #34 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.43952471017837524
Validation loss = 0.4375099837779999
Validation loss = 0.4394533038139343
Validation loss = 0.4373572766780853
Validation loss = 0.4383913278579712
Validation loss = 0.43896472454071045
Validation loss = 0.44086480140686035
Validation loss = 0.4414510428905487
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.43503397703170776
Validation loss = 0.434587687253952
Validation loss = 0.43426504731178284
Validation loss = 0.43396463990211487
Validation loss = 0.43550509214401245
Validation loss = 0.4384844899177551
Validation loss = 0.43659210205078125
Validation loss = 0.44018658995628357
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.43464794754981995
Validation loss = 0.4311755299568176
Validation loss = 0.431763231754303
Validation loss = 0.43185633420944214
Validation loss = 0.43252772092819214
Validation loss = 0.4332420527935028
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4393506646156311
Validation loss = 0.43674570322036743
Validation loss = 0.43859222531318665
Validation loss = 0.4377569854259491
Validation loss = 0.43821075558662415
Validation loss = 0.4371807873249054
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4340282380580902
Validation loss = 0.43733590841293335
Validation loss = 0.43662703037261963
Validation loss = 0.43348029255867004
Validation loss = 0.4341285228729248
Validation loss = 0.4358236491680145
Validation loss = 0.4378850758075714
Validation loss = 0.43515121936798096
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 34       |
| MaximumReturn | 198      |
| MinimumReturn | 116      |
| TotalSamples  | 119988   |
----------------------------
itr #35 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.43825435638427734
Validation loss = 0.439290851354599
Validation loss = 0.4409545063972473
Validation loss = 0.4398247003555298
Validation loss = 0.4411761164665222
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4373130798339844
Validation loss = 0.436955988407135
Validation loss = 0.4379979372024536
Validation loss = 0.43627092242240906
Validation loss = 0.4385165870189667
Validation loss = 0.44126930832862854
Validation loss = 0.437664657831192
Validation loss = 0.43910086154937744
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.434127539396286
Validation loss = 0.43335139751434326
Validation loss = 0.43269383907318115
Validation loss = 0.4341641962528229
Validation loss = 0.4339846968650818
Validation loss = 0.4330064058303833
Validation loss = 0.4390603303909302
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4373578429222107
Validation loss = 0.43716490268707275
Validation loss = 0.4389021694660187
Validation loss = 0.43752747774124146
Validation loss = 0.43786466121673584
Validation loss = 0.43818753957748413
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.43549492955207825
Validation loss = 0.4359111785888672
Validation loss = 0.43640074133872986
Validation loss = 0.43732017278671265
Validation loss = 0.4358822703361511
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 160      |
| Iteration     | 35       |
| MaximumReturn | 198      |
| MinimumReturn | 115      |
| TotalSamples  | 123321   |
----------------------------
itr #36 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4430150091648102
Validation loss = 0.44186392426490784
Validation loss = 0.4408482015132904
Validation loss = 0.4424975514411926
Validation loss = 0.4414512515068054
Validation loss = 0.4433312714099884
Validation loss = 0.4432753026485443
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4386216402053833
Validation loss = 0.44002655148506165
Validation loss = 0.43809130787849426
Validation loss = 0.4392119348049164
Validation loss = 0.4377109706401825
Validation loss = 0.4391006827354431
Validation loss = 0.4410065710544586
Validation loss = 0.43845704197883606
Validation loss = 0.4409261643886566
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4336785078048706
Validation loss = 0.4344867467880249
Validation loss = 0.4365237355232239
Validation loss = 0.43512290716171265
Validation loss = 0.43437907099723816
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4393569529056549
Validation loss = 0.4386753439903259
Validation loss = 0.43851059675216675
Validation loss = 0.4393857717514038
Validation loss = 0.4387504458427429
Validation loss = 0.43808040022850037
Validation loss = 0.437540203332901
Validation loss = 0.4423198401927948
Validation loss = 0.44026726484298706
Validation loss = 0.44114845991134644
Validation loss = 0.4401768147945404
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.43600034713745117
Validation loss = 0.43916311860084534
Validation loss = 0.4378059506416321
Validation loss = 0.43733131885528564
Validation loss = 0.4381812810897827
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 36       |
| MaximumReturn | 198      |
| MinimumReturn | 110      |
| TotalSamples  | 126654   |
----------------------------
itr #37 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4417247176170349
Validation loss = 0.4418794810771942
Validation loss = 0.4434175193309784
Validation loss = 0.44612956047058105
Validation loss = 0.44193315505981445
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.44029006361961365
Validation loss = 0.43971383571624756
Validation loss = 0.4405103325843811
Validation loss = 0.4426693022251129
Validation loss = 0.4413147568702698
Validation loss = 0.44219475984573364
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.43493011593818665
Validation loss = 0.4345199763774872
Validation loss = 0.43556928634643555
Validation loss = 0.4372599720954895
Validation loss = 0.4359287917613983
Validation loss = 0.43508732318878174
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.44045794010162354
Validation loss = 0.4426294267177582
Validation loss = 0.44080281257629395
Validation loss = 0.4408251941204071
Validation loss = 0.44301652908325195
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4376872181892395
Validation loss = 0.4358559250831604
Validation loss = 0.43664830923080444
Validation loss = 0.43773484230041504
Validation loss = 0.440002977848053
Validation loss = 0.44000744819641113
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 164      |
| Iteration     | 37       |
| MaximumReturn | 197      |
| MinimumReturn | 125      |
| TotalSamples  | 129987   |
----------------------------
itr #38 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.44546636939048767
Validation loss = 0.4421362578868866
Validation loss = 0.4417342245578766
Validation loss = 0.44424352049827576
Validation loss = 0.4434167742729187
Validation loss = 0.4460129737854004
Validation loss = 0.4445478022098541
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.441689133644104
Validation loss = 0.44039684534072876
Validation loss = 0.4402317404747009
Validation loss = 0.44207963347435
Validation loss = 0.439949095249176
Validation loss = 0.44266751408576965
Validation loss = 0.44186657667160034
Validation loss = 0.4429370164871216
Validation loss = 0.4436710476875305
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4355100691318512
Validation loss = 0.43540385365486145
Validation loss = 0.4350316524505615
Validation loss = 0.4360760450363159
Validation loss = 0.4338696300983429
Validation loss = 0.4379309415817261
Validation loss = 0.4375550448894501
Validation loss = 0.43922656774520874
Validation loss = 0.43781086802482605
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4416932165622711
Validation loss = 0.44041186571121216
Validation loss = 0.44285550713539124
Validation loss = 0.44054028391838074
Validation loss = 0.4419543743133545
Validation loss = 0.44342538714408875
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.43809032440185547
Validation loss = 0.4379512667655945
Validation loss = 0.4369655251502991
Validation loss = 0.4388534724712372
Validation loss = 0.438865602016449
Validation loss = 0.43900275230407715
Validation loss = 0.44260114431381226
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 178      |
| Iteration     | 38       |
| MaximumReturn | 198      |
| MinimumReturn | 133      |
| TotalSamples  | 133320   |
----------------------------
itr #39 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4469274580478668
Validation loss = 0.44537922739982605
Validation loss = 0.44622811675071716
Validation loss = 0.44562384486198425
Validation loss = 0.44690218567848206
Validation loss = 0.446975976228714
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4432533383369446
Validation loss = 0.44438931345939636
Validation loss = 0.4454069137573242
Validation loss = 0.4440024793148041
Validation loss = 0.44516485929489136
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4391283690929413
Validation loss = 0.43863168358802795
Validation loss = 0.43965551257133484
Validation loss = 0.44089338183403015
Validation loss = 0.4394945502281189
Validation loss = 0.43986696004867554
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4430537223815918
Validation loss = 0.4420582950115204
Validation loss = 0.4419904947280884
Validation loss = 0.44264745712280273
Validation loss = 0.4447591304779053
Validation loss = 0.4428445100784302
Validation loss = 0.4433581531047821
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4413056969642639
Validation loss = 0.44079864025115967
Validation loss = 0.44143813848495483
Validation loss = 0.4416837990283966
Validation loss = 0.4410265386104584
Validation loss = 0.44116395711898804
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 39       |
| MaximumReturn | 198      |
| MinimumReturn | 118      |
| TotalSamples  | 136653   |
----------------------------
