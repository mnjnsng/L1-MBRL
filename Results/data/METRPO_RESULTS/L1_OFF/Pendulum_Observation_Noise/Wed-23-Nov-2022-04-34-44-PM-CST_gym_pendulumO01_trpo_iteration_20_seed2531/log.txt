Logging to experiments/gym_pendulumO01/Pendulum_Observation_Noise/Wed-23-Nov-2022-04-34-44-PM-CST_gym_pendulumO01_trpo_iteration_20_seed2531
Print configuration .....
{'env_name': 'gym_pendulumO01', 'random_seeds': [2531, 2231], 'save_variables': False, 'model_save_dir': '/tmp/pendulumO01_models/', 'restore_variables': False, 'start_onpol_iter': 0, 'onpol_iters': 40, 'num_path_random': 25, 'num_path_onpol': 25, 'env_horizon': 200, 'max_train_data': 200000, 'max_val_data': 100000, 'discard_ratio': 0.0, 'dynamics': {'pre_training': {'mode': 'intrinsic_reward', 'itr': 0, 'policy_itr': 20}, 'model': 'nn', 'ensemble': True, 'ensemble_model_count': 5, 'enable_particle_ensemble': True, 'particles': 5, 'obs_var': 1.0, 'intrinsic_reward_coeff': 1.0, 'ita': 1.0, 'mode': 'random', 'val': True, 'n_layers': 4, 'hidden_size': 1000, 'activation': 'relu', 'batch_size': 1000, 'learning_rate': 0.001, 'reg_coeff': 0.0, 'epochs': 200, 'kfac_params': {'learning_rate': 0.1, 'damping': 0.001, 'momentum': 0.9, 'kl_clip': 0.0001, 'cov_ema_decay': 0.99}}, 'policy': {'network_shape': [64, 64], 'init_logstd': 0.0, 'activation': 'tanh', 'reinitialize_every_itr': False}, 'trpo': {'horizon': 200, 'gamma': 0.99, 'step_size': 0.01, 'iterations': 20, 'batch_size': 50000, 'gae': 0.95, 'visualization': False, 'visualize_iterations': [0]}, 'algo': 'trpo'}
Generating random rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating random rollouts.
Creating normalization for training data.
Done creating normalization for training data.
Particle ensemble enabled? True
An ensemble of 5 dynamics model <class 'model.dynamics.NNDynamicsModel'> initialized
Train dynamics model with intrinsic reward only? False
Pre-training enabled. Using only intrinsic reward.
Pre-training dynamics model for 0 iterations...
Done pre-training dynamics model.
Using external reward only.
itr #0 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.37018001079559326
Validation loss = 0.2526509463787079
Validation loss = 0.2425529956817627
Validation loss = 0.22933565080165863
Validation loss = 0.22724348306655884
Validation loss = 0.21959902346134186
Validation loss = 0.21925972402095795
Validation loss = 0.21359826624393463
Validation loss = 0.20982962846755981
Validation loss = 0.2051662802696228
Validation loss = 0.20422771573066711
Validation loss = 0.2150643765926361
Validation loss = 0.20260721445083618
Validation loss = 0.19945064187049866
Validation loss = 0.19921280443668365
Validation loss = 0.1963716298341751
Validation loss = 0.19350329041481018
Validation loss = 0.19249019026756287
Validation loss = 0.19737611711025238
Validation loss = 0.1928592324256897
Validation loss = 0.18843843042850494
Validation loss = 0.193802610039711
Validation loss = 0.19040948152542114
Validation loss = 0.18906855583190918
Validation loss = 0.18817484378814697
Validation loss = 0.19955657422542572
Validation loss = 0.18813051283359528
Validation loss = 0.18777325749397278
Validation loss = 0.19379381835460663
Validation loss = 0.19116990268230438
Validation loss = 0.19009020924568176
Validation loss = 0.18554042279720306
Validation loss = 0.20256367325782776
Validation loss = 0.19174964725971222
Validation loss = 0.19269829988479614
Validation loss = 0.19272932410240173
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.41116636991500854
Validation loss = 0.24974314868450165
Validation loss = 0.23847338557243347
Validation loss = 0.2309083193540573
Validation loss = 0.22637991607189178
Validation loss = 0.21925309300422668
Validation loss = 0.2244471162557602
Validation loss = 0.22237658500671387
Validation loss = 0.21189652383327484
Validation loss = 0.20471757650375366
Validation loss = 0.20505328476428986
Validation loss = 0.20650798082351685
Validation loss = 0.20141880214214325
Validation loss = 0.200974240899086
Validation loss = 0.20229345560073853
Validation loss = 0.20769673585891724
Validation loss = 0.20108532905578613
Validation loss = 0.20176391303539276
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4012739956378937
Validation loss = 0.24786366522312164
Validation loss = 0.23474988341331482
Validation loss = 0.23010019958019257
Validation loss = 0.2263435423374176
Validation loss = 0.2199748307466507
Validation loss = 0.2143758088350296
Validation loss = 0.20939336717128754
Validation loss = 0.20737802982330322
Validation loss = 0.2111193984746933
Validation loss = 0.20650199055671692
Validation loss = 0.20136471092700958
Validation loss = 0.1981515884399414
Validation loss = 0.19764329493045807
Validation loss = 0.200377956032753
Validation loss = 0.20826736092567444
Validation loss = 0.19561420381069183
Validation loss = 0.20945583283901215
Validation loss = 0.19326652586460114
Validation loss = 0.19367879629135132
Validation loss = 0.19356679916381836
Validation loss = 0.19533662497997284
Validation loss = 0.1874568909406662
Validation loss = 0.18733948469161987
Validation loss = 0.18682046234607697
Validation loss = 0.18942150473594666
Validation loss = 0.19585497677326202
Validation loss = 0.19039969146251678
Validation loss = 0.18828549981117249
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.37501952052116394
Validation loss = 0.2467019408941269
Validation loss = 0.2354746311903
Validation loss = 0.2301902025938034
Validation loss = 0.22435198724269867
Validation loss = 0.22150442004203796
Validation loss = 0.21598699688911438
Validation loss = 0.21773841977119446
Validation loss = 0.20456533133983612
Validation loss = 0.20282478630542755
Validation loss = 0.2060050666332245
Validation loss = 0.20026180148124695
Validation loss = 0.20201155543327332
Validation loss = 0.19960707426071167
Validation loss = 0.19921158254146576
Validation loss = 0.1987667828798294
Validation loss = 0.20014211535453796
Validation loss = 0.19894926249980927
Validation loss = 0.20098529756069183
Validation loss = 0.18964724242687225
Validation loss = 0.1887134164571762
Validation loss = 0.1901073455810547
Validation loss = 0.20434802770614624
Validation loss = 0.19479252398014069
Validation loss = 0.18988798558712006
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4309030771255493
Validation loss = 0.2526451349258423
Validation loss = 0.2377336174249649
Validation loss = 0.22922155261039734
Validation loss = 0.22455240786075592
Validation loss = 0.22211605310440063
Validation loss = 0.2150062471628189
Validation loss = 0.20925991237163544
Validation loss = 0.20654752850532532
Validation loss = 0.2097029983997345
Validation loss = 0.21921701729297638
Validation loss = 0.20449961721897125
Validation loss = 0.2011922150850296
Validation loss = 0.19864030182361603
Validation loss = 0.2004927098751068
Validation loss = 0.19612158834934235
Validation loss = 0.20250272750854492
Validation loss = 0.1940900832414627
Validation loss = 0.19618187844753265
Validation loss = 0.19723717868328094
Validation loss = 0.19263692200183868
Validation loss = 0.19533133506774902
Validation loss = 0.19084249436855316
Validation loss = 0.19835585355758667
Validation loss = 0.19363780319690704
Validation loss = 0.19181054830551147
Validation loss = 0.19279927015304565
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 0        |
| MaximumReturn | 198      |
| MinimumReturn | 86.2     |
| TotalSamples  | 6666     |
----------------------------
itr #1 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.42913421988487244
Validation loss = 0.2862267792224884
Validation loss = 0.27871060371398926
Validation loss = 0.27304956316947937
Validation loss = 0.27177777886390686
Validation loss = 0.2681882679462433
Validation loss = 0.2669930160045624
Validation loss = 0.26648882031440735
Validation loss = 0.26643267273902893
Validation loss = 0.26695266366004944
Validation loss = 0.2693709433078766
Validation loss = 0.2664072811603546
Validation loss = 0.2737898528575897
Validation loss = 0.2767583429813385
Validation loss = 0.26562806963920593
Validation loss = 0.2716269791126251
Validation loss = 0.26796475052833557
Validation loss = 0.26835688948631287
Validation loss = 0.27012357115745544
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.406585693359375
Validation loss = 0.2890186309814453
Validation loss = 0.2852870523929596
Validation loss = 0.2746933400630951
Validation loss = 0.27750149369239807
Validation loss = 0.2772464156150818
Validation loss = 0.2759438455104828
Validation loss = 0.27131348848342896
Validation loss = 0.2738681435585022
Validation loss = 0.27359625697135925
Validation loss = 0.27868178486824036
Validation loss = 0.27647915482521057
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3973780870437622
Validation loss = 0.28338226675987244
Validation loss = 0.2761419713497162
Validation loss = 0.2699739634990692
Validation loss = 0.2684151232242584
Validation loss = 0.27188241481781006
Validation loss = 0.2725989520549774
Validation loss = 0.27177751064300537
Validation loss = 0.27371445298194885
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.409099817276001
Validation loss = 0.2874354124069214
Validation loss = 0.2747124135494232
Validation loss = 0.274995356798172
Validation loss = 0.27428165078163147
Validation loss = 0.26920536160469055
Validation loss = 0.26786959171295166
Validation loss = 0.27669641375541687
Validation loss = 0.266998291015625
Validation loss = 0.26860496401786804
Validation loss = 0.2674131691455841
Validation loss = 0.2682390511035919
Validation loss = 0.26709792017936707
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4302487373352051
Validation loss = 0.29346999526023865
Validation loss = 0.2761332094669342
Validation loss = 0.27483269572257996
Validation loss = 0.27243393659591675
Validation loss = 0.2756102979183197
Validation loss = 0.2649439871311188
Validation loss = 0.2739658057689667
Validation loss = 0.26647353172302246
Validation loss = 0.2698548138141632
Validation loss = 0.2654968500137329
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 1        |
| MaximumReturn | 199      |
| MinimumReturn | 120      |
| TotalSamples  | 9999     |
----------------------------
itr #2 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3030843138694763
Validation loss = 0.28160661458969116
Validation loss = 0.280190646648407
Validation loss = 0.28210994601249695
Validation loss = 0.27991166710853577
Validation loss = 0.2773750126361847
Validation loss = 0.28260430693626404
Validation loss = 0.281202495098114
Validation loss = 0.27845144271850586
Validation loss = 0.28080958127975464
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.300952285528183
Validation loss = 0.290152370929718
Validation loss = 0.28198739886283875
Validation loss = 0.28599637746810913
Validation loss = 0.2865729033946991
Validation loss = 0.281540185213089
Validation loss = 0.2801346778869629
Validation loss = 0.2821565568447113
Validation loss = 0.28846022486686707
Validation loss = 0.27937227487564087
Validation loss = 0.27756357192993164
Validation loss = 0.2813655436038971
Validation loss = 0.2782212495803833
Validation loss = 0.28068116307258606
Validation loss = 0.28102293610572815
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.29944658279418945
Validation loss = 0.2862814962863922
Validation loss = 0.29118943214416504
Validation loss = 0.2852208912372589
Validation loss = 0.28285640478134155
Validation loss = 0.28373485803604126
Validation loss = 0.2831607460975647
Validation loss = 0.2792084217071533
Validation loss = 0.28078359365463257
Validation loss = 0.2804597318172455
Validation loss = 0.2841545641422272
Validation loss = 0.28204768896102905
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3108856678009033
Validation loss = 0.28046277165412903
Validation loss = 0.2807697653770447
Validation loss = 0.28178560733795166
Validation loss = 0.29470235109329224
Validation loss = 0.2775052785873413
Validation loss = 0.28646159172058105
Validation loss = 0.28178292512893677
Validation loss = 0.2812255024909973
Validation loss = 0.2927623689174652
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.30124431848526
Validation loss = 0.2831394076347351
Validation loss = 0.28388622403144836
Validation loss = 0.29080715775489807
Validation loss = 0.28149041533470154
Validation loss = 0.28662973642349243
Validation loss = 0.2864537537097931
Validation loss = 0.27880167961120605
Validation loss = 0.2846447825431824
Validation loss = 0.284357488155365
Validation loss = 0.2846125662326813
Validation loss = 0.28444188833236694
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 2        |
| MaximumReturn | 199      |
| MinimumReturn | 112      |
| TotalSamples  | 13332    |
----------------------------
itr #3 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3175812065601349
Validation loss = 0.30968722701072693
Validation loss = 0.3045840859413147
Validation loss = 0.31055501103401184
Validation loss = 0.3072570264339447
Validation loss = 0.3069993257522583
Validation loss = 0.30686020851135254
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.31147173047065735
Validation loss = 0.30351150035858154
Validation loss = 0.30982449650764465
Validation loss = 0.31243762373924255
Validation loss = 0.30569547414779663
Validation loss = 0.3042665719985962
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.32308831810951233
Validation loss = 0.316827654838562
Validation loss = 0.3090915381908417
Validation loss = 0.30818045139312744
Validation loss = 0.30541273951530457
Validation loss = 0.3046856224536896
Validation loss = 0.3062972128391266
Validation loss = 0.3057434856891632
Validation loss = 0.30664417147636414
Validation loss = 0.3085179030895233
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.315293550491333
Validation loss = 0.3082827627658844
Validation loss = 0.3070143759250641
Validation loss = 0.3104185163974762
Validation loss = 0.3043096363544464
Validation loss = 0.3070109784603119
Validation loss = 0.3193700909614563
Validation loss = 0.3066381812095642
Validation loss = 0.3060883581638336
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3226744830608368
Validation loss = 0.3095583915710449
Validation loss = 0.30466583371162415
Validation loss = 0.30339959263801575
Validation loss = 0.30372026562690735
Validation loss = 0.3137778639793396
Validation loss = 0.3070692718029022
Validation loss = 0.30321645736694336
Validation loss = 0.30586332082748413
Validation loss = 0.3020641505718231
Validation loss = 0.30721360445022583
Validation loss = 0.310529500246048
Validation loss = 0.30936768651008606
Validation loss = 0.30554163455963135
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 3        |
| MaximumReturn | 198      |
| MinimumReturn | 121      |
| TotalSamples  | 16665    |
----------------------------
itr #4 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3257245123386383
Validation loss = 0.3191360831260681
Validation loss = 0.3169819116592407
Validation loss = 0.32424047589302063
Validation loss = 0.32355260848999023
Validation loss = 0.3213002681732178
Validation loss = 0.31962156295776367
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.32702770829200745
Validation loss = 0.313692569732666
Validation loss = 0.3209089934825897
Validation loss = 0.31509149074554443
Validation loss = 0.32193827629089355
Validation loss = 0.3282523453235626
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.32725995779037476
Validation loss = 0.31686466932296753
Validation loss = 0.3213197886943817
Validation loss = 0.3201780915260315
Validation loss = 0.3187718391418457
Validation loss = 0.32071125507354736
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.32330411672592163
Validation loss = 0.3222421109676361
Validation loss = 0.3168565034866333
Validation loss = 0.32064032554626465
Validation loss = 0.31909608840942383
Validation loss = 0.31712913513183594
Validation loss = 0.320411741733551
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.317379891872406
Validation loss = 0.31797629594802856
Validation loss = 0.31970512866973877
Validation loss = 0.3195461928844452
Validation loss = 0.3160502016544342
Validation loss = 0.32202088832855225
Validation loss = 0.3177300691604614
Validation loss = 0.3184969425201416
Validation loss = 0.3185451030731201
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 161      |
| Iteration     | 4        |
| MaximumReturn | 199      |
| MinimumReturn | 125      |
| TotalSamples  | 19998    |
----------------------------
itr #5 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.326463520526886
Validation loss = 0.32049599289894104
Validation loss = 0.32308825850486755
Validation loss = 0.3263840675354004
Validation loss = 0.3214998245239258
Validation loss = 0.32991939783096313
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.32364100217819214
Validation loss = 0.32397565245628357
Validation loss = 0.3200145959854126
Validation loss = 0.32096582651138306
Validation loss = 0.32403573393821716
Validation loss = 0.32253965735435486
Validation loss = 0.3188740015029907
Validation loss = 0.3207068145275116
Validation loss = 0.32178202271461487
Validation loss = 0.32201287150382996
Validation loss = 0.32178664207458496
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3227575719356537
Validation loss = 0.32883208990097046
Validation loss = 0.3221682608127594
Validation loss = 0.3232681155204773
Validation loss = 0.3221558928489685
Validation loss = 0.32386189699172974
Validation loss = 0.32508423924446106
Validation loss = 0.32174983620643616
Validation loss = 0.3247355818748474
Validation loss = 0.3206769824028015
Validation loss = 0.3255535662174225
Validation loss = 0.3243608772754669
Validation loss = 0.3219946622848511
Validation loss = 0.3253629505634308
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.32135406136512756
Validation loss = 0.3225260078907013
Validation loss = 0.3188563883304596
Validation loss = 0.32275205850601196
Validation loss = 0.32383280992507935
Validation loss = 0.32052385807037354
Validation loss = 0.3268979787826538
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.32373327016830444
Validation loss = 0.32391640543937683
Validation loss = 0.3234630525112152
Validation loss = 0.32429319620132446
Validation loss = 0.32065537571907043
Validation loss = 0.32456374168395996
Validation loss = 0.3211129307746887
Validation loss = 0.3257528841495514
Validation loss = 0.3225890100002289
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 5        |
| MaximumReturn | 198      |
| MinimumReturn | 125      |
| TotalSamples  | 23331    |
----------------------------
itr #6 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3352218270301819
Validation loss = 0.333345890045166
Validation loss = 0.3310626745223999
Validation loss = 0.33386942744255066
Validation loss = 0.33580276370048523
Validation loss = 0.33139273524284363
Validation loss = 0.33349207043647766
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.33293265104293823
Validation loss = 0.3299195468425751
Validation loss = 0.3365285396575928
Validation loss = 0.33165881037712097
Validation loss = 0.3288745880126953
Validation loss = 0.33419069647789
Validation loss = 0.3293319344520569
Validation loss = 0.3329656720161438
Validation loss = 0.33373185992240906
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.33373546600341797
Validation loss = 0.33022254705429077
Validation loss = 0.3364426791667938
Validation loss = 0.33320024609565735
Validation loss = 0.3354737460613251
Validation loss = 0.33213135600090027
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3354857265949249
Validation loss = 0.3331771492958069
Validation loss = 0.3327115774154663
Validation loss = 0.33067676424980164
Validation loss = 0.333276629447937
Validation loss = 0.3315151035785675
Validation loss = 0.33343660831451416
Validation loss = 0.32991477847099304
Validation loss = 0.33514949679374695
Validation loss = 0.34118831157684326
Validation loss = 0.33210307359695435
Validation loss = 0.33178555965423584
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.33090612292289734
Validation loss = 0.33272072672843933
Validation loss = 0.33279892802238464
Validation loss = 0.33087486028671265
Validation loss = 0.33656102418899536
Validation loss = 0.33784934878349304
Validation loss = 0.338736355304718
Validation loss = 0.3369215726852417
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 6        |
| MaximumReturn | 198      |
| MinimumReturn | 125      |
| TotalSamples  | 26664    |
----------------------------
itr #7 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3390496075153351
Validation loss = 0.34501534700393677
Validation loss = 0.3392198383808136
Validation loss = 0.34267234802246094
Validation loss = 0.34502890706062317
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.34159529209136963
Validation loss = 0.34198474884033203
Validation loss = 0.34028446674346924
Validation loss = 0.3416496515274048
Validation loss = 0.34300071001052856
Validation loss = 0.34436148405075073
Validation loss = 0.3392527401447296
Validation loss = 0.3414371609687805
Validation loss = 0.3433245122432709
Validation loss = 0.34405726194381714
Validation loss = 0.3420262038707733
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.34318920969963074
Validation loss = 0.34113404154777527
Validation loss = 0.3411165773868561
Validation loss = 0.34090009331703186
Validation loss = 0.3397734761238098
Validation loss = 0.34152621030807495
Validation loss = 0.3439265787601471
Validation loss = 0.3443703353404999
Validation loss = 0.3430892825126648
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.33989065885543823
Validation loss = 0.342495322227478
Validation loss = 0.34066545963287354
Validation loss = 0.3399518132209778
Validation loss = 0.34018027782440186
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3413001298904419
Validation loss = 0.34141427278518677
Validation loss = 0.3418973684310913
Validation loss = 0.34257060289382935
Validation loss = 0.3422616422176361
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 7        |
| MaximumReturn | 198      |
| MinimumReturn | 123      |
| TotalSamples  | 29997    |
----------------------------
itr #8 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.35590094327926636
Validation loss = 0.35360103845596313
Validation loss = 0.3525069057941437
Validation loss = 0.3506438434123993
Validation loss = 0.3542843163013458
Validation loss = 0.35689446330070496
Validation loss = 0.35287415981292725
Validation loss = 0.352713406085968
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.35547909140586853
Validation loss = 0.3542185425758362
Validation loss = 0.35468733310699463
Validation loss = 0.3544793426990509
Validation loss = 0.3530615568161011
Validation loss = 0.35240843892097473
Validation loss = 0.3552826941013336
Validation loss = 0.35435715317726135
Validation loss = 0.3559083640575409
Validation loss = 0.35594475269317627
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3526875078678131
Validation loss = 0.3520340919494629
Validation loss = 0.35574406385421753
Validation loss = 0.35370370745658875
Validation loss = 0.35372063517570496
Validation loss = 0.3564940392971039
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.35442081093788147
Validation loss = 0.35350003838539124
Validation loss = 0.350555956363678
Validation loss = 0.35209426283836365
Validation loss = 0.3497301936149597
Validation loss = 0.34949254989624023
Validation loss = 0.35192617774009705
Validation loss = 0.35295572876930237
Validation loss = 0.3544579744338989
Validation loss = 0.3534696400165558
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.35257384181022644
Validation loss = 0.3513404130935669
Validation loss = 0.352517694234848
Validation loss = 0.3552968204021454
Validation loss = 0.35260745882987976
Validation loss = 0.35440880060195923
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 8        |
| MaximumReturn | 197      |
| MinimumReturn | 120      |
| TotalSamples  | 33330    |
----------------------------
itr #9 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3608682453632355
Validation loss = 0.3630889058113098
Validation loss = 0.3640480041503906
Validation loss = 0.362045556306839
Validation loss = 0.3623669445514679
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3620401918888092
Validation loss = 0.3638644814491272
Validation loss = 0.3639456033706665
Validation loss = 0.36506539583206177
Validation loss = 0.3647191524505615
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.36477693915367126
Validation loss = 0.36433127522468567
Validation loss = 0.36228057742118835
Validation loss = 0.36346301436424255
Validation loss = 0.36490631103515625
Validation loss = 0.3650732636451721
Validation loss = 0.3678674101829529
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.36183059215545654
Validation loss = 0.36158156394958496
Validation loss = 0.3633826971054077
Validation loss = 0.36617735028266907
Validation loss = 0.36600160598754883
Validation loss = 0.362973690032959
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.36426690220832825
Validation loss = 0.36322543025016785
Validation loss = 0.36317867040634155
Validation loss = 0.36244478821754456
Validation loss = 0.3652874827384949
Validation loss = 0.3656742572784424
Validation loss = 0.36549705266952515
Validation loss = 0.36604204773902893
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 160      |
| Iteration     | 9        |
| MaximumReturn | 199      |
| MinimumReturn | 119      |
| TotalSamples  | 36663    |
----------------------------
itr #10 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.36646169424057007
Validation loss = 0.3676641881465912
Validation loss = 0.3661278486251831
Validation loss = 0.3674101233482361
Validation loss = 0.3661268949508667
Validation loss = 0.3660689890384674
Validation loss = 0.36485856771469116
Validation loss = 0.3669423758983612
Validation loss = 0.36749693751335144
Validation loss = 0.3675895631313324
Validation loss = 0.3679744005203247
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3695601224899292
Validation loss = 0.3685983419418335
Validation loss = 0.3709527552127838
Validation loss = 0.37148571014404297
Validation loss = 0.3710061311721802
Validation loss = 0.37013375759124756
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.36874091625213623
Validation loss = 0.37421801686286926
Validation loss = 0.3694022595882416
Validation loss = 0.36847302317619324
Validation loss = 0.3702388405799866
Validation loss = 0.37044215202331543
Validation loss = 0.3705655634403229
Validation loss = 0.37213289737701416
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.36637505888938904
Validation loss = 0.3702293038368225
Validation loss = 0.3674263060092926
Validation loss = 0.36686405539512634
Validation loss = 0.3691648244857788
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3692411184310913
Validation loss = 0.3683014214038849
Validation loss = 0.36943358182907104
Validation loss = 0.368967205286026
Validation loss = 0.36908796429634094
Validation loss = 0.36875855922698975
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 164      |
| Iteration     | 10       |
| MaximumReturn | 196      |
| MinimumReturn | 109      |
| TotalSamples  | 39996    |
----------------------------
itr #11 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.37451398372650146
Validation loss = 0.3719654977321625
Validation loss = 0.3738380968570709
Validation loss = 0.37392061948776245
Validation loss = 0.3749609589576721
Validation loss = 0.3740192651748657
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.374688059091568
Validation loss = 0.37380552291870117
Validation loss = 0.3715434670448303
Validation loss = 0.3751082420349121
Validation loss = 0.3795456886291504
Validation loss = 0.37618815898895264
Validation loss = 0.3786294162273407
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3794938325881958
Validation loss = 0.37744805216789246
Validation loss = 0.3754273056983948
Validation loss = 0.3797531723976135
Validation loss = 0.3746531903743744
Validation loss = 0.37739306688308716
Validation loss = 0.3791813850402832
Validation loss = 0.37784913182258606
Validation loss = 0.3810425400733948
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.37245821952819824
Validation loss = 0.37308016419410706
Validation loss = 0.37298521399497986
Validation loss = 0.3753073811531067
Validation loss = 0.3714962899684906
Validation loss = 0.37286683917045593
Validation loss = 0.37340790033340454
Validation loss = 0.37269455194473267
Validation loss = 0.376228004693985
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.37524211406707764
Validation loss = 0.3751673996448517
Validation loss = 0.38092631101608276
Validation loss = 0.3768600821495056
Validation loss = 0.37505191564559937
Validation loss = 0.3759858310222626
Validation loss = 0.3749808669090271
Validation loss = 0.3771251142024994
Validation loss = 0.37581777572631836
Validation loss = 0.37760478258132935
Validation loss = 0.37604793906211853
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 11       |
| MaximumReturn | 196      |
| MinimumReturn | 124      |
| TotalSamples  | 43329    |
----------------------------
itr #12 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.37872928380966187
Validation loss = 0.37813878059387207
Validation loss = 0.37950530648231506
Validation loss = 0.3792985677719116
Validation loss = 0.37973302602767944
Validation loss = 0.3798999786376953
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3804153800010681
Validation loss = 0.3813919425010681
Validation loss = 0.38136905431747437
Validation loss = 0.38428306579589844
Validation loss = 0.38227584958076477
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3826582729816437
Validation loss = 0.3833249807357788
Validation loss = 0.3817809522151947
Validation loss = 0.3866904675960541
Validation loss = 0.38312652707099915
Validation loss = 0.3857371509075165
Validation loss = 0.38493919372558594
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3796917498111725
Validation loss = 0.3788529634475708
Validation loss = 0.37823018431663513
Validation loss = 0.3805345296859741
Validation loss = 0.38180750608444214
Validation loss = 0.3803337514400482
Validation loss = 0.3817000687122345
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3852982819080353
Validation loss = 0.3826749920845032
Validation loss = 0.38373643159866333
Validation loss = 0.382126122713089
Validation loss = 0.38446009159088135
Validation loss = 0.3843182921409607
Validation loss = 0.3846529424190521
Validation loss = 0.3841463327407837
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 12       |
| MaximumReturn | 198      |
| MinimumReturn | 113      |
| TotalSamples  | 46662    |
----------------------------
itr #13 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3864138722419739
Validation loss = 0.3879707455635071
Validation loss = 0.38848093152046204
Validation loss = 0.38928964734077454
Validation loss = 0.39021167159080505
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3876861035823822
Validation loss = 0.3876916766166687
Validation loss = 0.38937142491340637
Validation loss = 0.38986384868621826
Validation loss = 0.38980981707572937
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3936820328235626
Validation loss = 0.3911330997943878
Validation loss = 0.39207175374031067
Validation loss = 0.3962803781032562
Validation loss = 0.3928182125091553
Validation loss = 0.3948635458946228
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.38694635033607483
Validation loss = 0.38765949010849
Validation loss = 0.3884985148906708
Validation loss = 0.38856393098831177
Validation loss = 0.38903310894966125
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.39387258887290955
Validation loss = 0.394788533449173
Validation loss = 0.3931269347667694
Validation loss = 0.39340540766716003
Validation loss = 0.39655646681785583
Validation loss = 0.3950199484825134
Validation loss = 0.3970421850681305
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 13       |
| MaximumReturn | 195      |
| MinimumReturn | 106      |
| TotalSamples  | 49995    |
----------------------------
itr #14 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3938902020454407
Validation loss = 0.3952886462211609
Validation loss = 0.3948687016963959
Validation loss = 0.3951742649078369
Validation loss = 0.3940944969654083
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3980967700481415
Validation loss = 0.39587607979774475
Validation loss = 0.3963938057422638
Validation loss = 0.39703476428985596
Validation loss = 0.3979739844799042
Validation loss = 0.39849263429641724
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.400023877620697
Validation loss = 0.400980144739151
Validation loss = 0.4004861116409302
Validation loss = 0.3990117013454437
Validation loss = 0.4011618494987488
Validation loss = 0.4010121822357178
Validation loss = 0.4040161073207855
Validation loss = 0.40201857686042786
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3959127366542816
Validation loss = 0.39465537667274475
Validation loss = 0.3931000828742981
Validation loss = 0.39692017436027527
Validation loss = 0.3943028748035431
Validation loss = 0.39653831720352173
Validation loss = 0.39852291345596313
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4023023247718811
Validation loss = 0.4027211368083954
Validation loss = 0.4007023870944977
Validation loss = 0.4032687246799469
Validation loss = 0.4028846323490143
Validation loss = 0.4048205614089966
Validation loss = 0.40437790751457214
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 14       |
| MaximumReturn | 198      |
| MinimumReturn | 126      |
| TotalSamples  | 53328    |
----------------------------
itr #15 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3993472754955292
Validation loss = 0.399353951215744
Validation loss = 0.3988356590270996
Validation loss = 0.3994438946247101
Validation loss = 0.40215638279914856
Validation loss = 0.39931708574295044
Validation loss = 0.40314552187919617
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.401912659406662
Validation loss = 0.4007527530193329
Validation loss = 0.39931508898735046
Validation loss = 0.4030308723449707
Validation loss = 0.40378570556640625
Validation loss = 0.40424850583076477
Validation loss = 0.4060298502445221
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4070231318473816
Validation loss = 0.40814876556396484
Validation loss = 0.4063853919506073
Validation loss = 0.4088042974472046
Validation loss = 0.4090224504470825
Validation loss = 0.409091979265213
Validation loss = 0.41091230511665344
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4004122316837311
Validation loss = 0.3997208774089813
Validation loss = 0.4011079668998718
Validation loss = 0.3998177945613861
Validation loss = 0.4013097882270813
Validation loss = 0.4008803367614746
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.40841925144195557
Validation loss = 0.4085121750831604
Validation loss = 0.40826845169067383
Validation loss = 0.4088098108768463
Validation loss = 0.4116446375846863
Validation loss = 0.41241341829299927
Validation loss = 0.4100046455860138
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 15       |
| MaximumReturn | 197      |
| MinimumReturn | 127      |
| TotalSamples  | 56661    |
----------------------------
itr #16 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.40509942173957825
Validation loss = 0.4054037928581238
Validation loss = 0.4063645303249359
Validation loss = 0.40781641006469727
Validation loss = 0.4073977470397949
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4072025716304779
Validation loss = 0.4080985486507416
Validation loss = 0.407471239566803
Validation loss = 0.4079658091068268
Validation loss = 0.40888527035713196
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.41282445192337036
Validation loss = 0.414075642824173
Validation loss = 0.41241195797920227
Validation loss = 0.4123553931713104
Validation loss = 0.41584062576293945
Validation loss = 0.4149380624294281
Validation loss = 0.41649559140205383
Validation loss = 0.4185071587562561
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4058711528778076
Validation loss = 0.41008609533309937
Validation loss = 0.4063267409801483
Validation loss = 0.4068787693977356
Validation loss = 0.41014692187309265
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4169515073299408
Validation loss = 0.4162582755088806
Validation loss = 0.4149418771266937
Validation loss = 0.4185478389263153
Validation loss = 0.41661301255226135
Validation loss = 0.42459461092948914
Validation loss = 0.41749829053878784
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 16       |
| MaximumReturn | 197      |
| MinimumReturn | 123      |
| TotalSamples  | 59994    |
----------------------------
itr #17 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.40778619050979614
Validation loss = 0.4109050929546356
Validation loss = 0.4095595180988312
Validation loss = 0.41107961535453796
Validation loss = 0.4093855321407318
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.41278886795043945
Validation loss = 0.4114817976951599
Validation loss = 0.41306570172309875
Validation loss = 0.41334354877471924
Validation loss = 0.4148944914340973
Validation loss = 0.4146685302257538
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.41875016689300537
Validation loss = 0.4192991256713867
Validation loss = 0.42061731219291687
Validation loss = 0.42386898398399353
Validation loss = 0.42182886600494385
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.41009414196014404
Validation loss = 0.4105697274208069
Validation loss = 0.4129921793937683
Validation loss = 0.4112488925457001
Validation loss = 0.4132777154445648
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.42234838008880615
Validation loss = 0.4233579635620117
Validation loss = 0.4221706688404083
Validation loss = 0.423774391412735
Validation loss = 0.42597901821136475
Validation loss = 0.42582806944847107
Validation loss = 0.4263497292995453
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 17       |
| MaximumReturn | 199      |
| MinimumReturn | 124      |
| TotalSamples  | 63327    |
----------------------------
itr #18 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4134337604045868
Validation loss = 0.41361337900161743
Validation loss = 0.41243472695350647
Validation loss = 0.41484954953193665
Validation loss = 0.41608908772468567
Validation loss = 0.4149188697338104
Validation loss = 0.4144824743270874
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.41688936948776245
Validation loss = 0.415012925863266
Validation loss = 0.41422587633132935
Validation loss = 0.41594359278678894
Validation loss = 0.4197103679180145
Validation loss = 0.42092546820640564
Validation loss = 0.4172212481498718
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4222880005836487
Validation loss = 0.4229055345058441
Validation loss = 0.4251958727836609
Validation loss = 0.42394769191741943
Validation loss = 0.42705217003822327
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4138419032096863
Validation loss = 0.4142721891403198
Validation loss = 0.41330626606941223
Validation loss = 0.4153739809989929
Validation loss = 0.4145416021347046
Validation loss = 0.4172283411026001
Validation loss = 0.41512158513069153
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4249931275844574
Validation loss = 0.4262132942676544
Validation loss = 0.42732489109039307
Validation loss = 0.4299314022064209
Validation loss = 0.4277690649032593
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 18       |
| MaximumReturn | 199      |
| MinimumReturn | 124      |
| TotalSamples  | 66660    |
----------------------------
itr #19 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4207037687301636
Validation loss = 0.4192357063293457
Validation loss = 0.4179633557796478
Validation loss = 0.4178169369697571
Validation loss = 0.4212408661842346
Validation loss = 0.4188176989555359
Validation loss = 0.4210822582244873
Validation loss = 0.421078622341156
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4188815653324127
Validation loss = 0.4199911952018738
Validation loss = 0.42035841941833496
Validation loss = 0.42400991916656494
Validation loss = 0.4224920868873596
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.42732954025268555
Validation loss = 0.42542871832847595
Validation loss = 0.42551618814468384
Validation loss = 0.42645543813705444
Validation loss = 0.42871323227882385
Validation loss = 0.4280293583869934
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.41850316524505615
Validation loss = 0.4189477562904358
Validation loss = 0.4197620153427124
Validation loss = 0.41845977306365967
Validation loss = 0.420698881149292
Validation loss = 0.41913944482803345
Validation loss = 0.4218935966491699
Validation loss = 0.4211398959159851
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4312185049057007
Validation loss = 0.4288279712200165
Validation loss = 0.43037307262420654
Validation loss = 0.43029603362083435
Validation loss = 0.4332994520664215
Validation loss = 0.4322265386581421
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 19       |
| MaximumReturn | 198      |
| MinimumReturn | 127      |
| TotalSamples  | 69993    |
----------------------------
itr #20 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.42235079407691956
Validation loss = 0.42120349407196045
Validation loss = 0.42031052708625793
Validation loss = 0.42222708463668823
Validation loss = 0.4232651889324188
Validation loss = 0.4246816635131836
Validation loss = 0.42297041416168213
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.42200151085853577
Validation loss = 0.42066237330436707
Validation loss = 0.4225926399230957
Validation loss = 0.422321081161499
Validation loss = 0.4225851893424988
Validation loss = 0.4232422113418579
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.42972898483276367
Validation loss = 0.4298965334892273
Validation loss = 0.42823997139930725
Validation loss = 0.42840656638145447
Validation loss = 0.42966026067733765
Validation loss = 0.42911553382873535
Validation loss = 0.43163609504699707
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4255925416946411
Validation loss = 0.4227456748485565
Validation loss = 0.42354246973991394
Validation loss = 0.4216829240322113
Validation loss = 0.4245022237300873
Validation loss = 0.4235309064388275
Validation loss = 0.42759430408477783
Validation loss = 0.42566075921058655
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.431397408246994
Validation loss = 0.43171554803848267
Validation loss = 0.43302062153816223
Validation loss = 0.43113088607788086
Validation loss = 0.4324592053890228
Validation loss = 0.4354124367237091
Validation loss = 0.43348804116249084
Validation loss = 0.43653178215026855
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 164      |
| Iteration     | 20       |
| MaximumReturn | 198      |
| MinimumReturn | 100      |
| TotalSamples  | 73326    |
----------------------------
itr #21 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4249192476272583
Validation loss = 0.4271375834941864
Validation loss = 0.4240749776363373
Validation loss = 0.42800459265708923
Validation loss = 0.4256802499294281
Validation loss = 0.42714834213256836
Validation loss = 0.42658039927482605
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.42513251304626465
Validation loss = 0.42473796010017395
Validation loss = 0.42615416646003723
Validation loss = 0.42591559886932373
Validation loss = 0.4273449778556824
Validation loss = 0.42721542716026306
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.43596407771110535
Validation loss = 0.4312365651130676
Validation loss = 0.4337688684463501
Validation loss = 0.4346083402633667
Validation loss = 0.43479618430137634
Validation loss = 0.4348515570163727
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4246380031108856
Validation loss = 0.428227961063385
Validation loss = 0.42675721645355225
Validation loss = 0.42825210094451904
Validation loss = 0.4287407696247101
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.439205527305603
Validation loss = 0.43581077456474304
Validation loss = 0.4363483786582947
Validation loss = 0.4375210702419281
Validation loss = 0.4402485191822052
Validation loss = 0.4387226700782776
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 21       |
| MaximumReturn | 198      |
| MinimumReturn | 111      |
| TotalSamples  | 76659    |
----------------------------
itr #22 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4287615418434143
Validation loss = 0.4279053807258606
Validation loss = 0.42780575156211853
Validation loss = 0.4293632507324219
Validation loss = 0.429867684841156
Validation loss = 0.4290664494037628
Validation loss = 0.43032169342041016
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.42849069833755493
Validation loss = 0.4272163212299347
Validation loss = 0.42793896794319153
Validation loss = 0.42900776863098145
Validation loss = 0.43169915676116943
Validation loss = 0.42936888337135315
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4337138235569
Validation loss = 0.43465957045555115
Validation loss = 0.43535688519477844
Validation loss = 0.4350026249885559
Validation loss = 0.43715712428092957
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.42633217573165894
Validation loss = 0.4261613190174103
Validation loss = 0.42848366498947144
Validation loss = 0.4280647933483124
Validation loss = 0.4322303831577301
Validation loss = 0.43047991394996643
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.43742841482162476
Validation loss = 0.4368036687374115
Validation loss = 0.4392045736312866
Validation loss = 0.4409824311733246
Validation loss = 0.43890872597694397
Validation loss = 0.44144323468208313
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 22       |
| MaximumReturn | 199      |
| MinimumReturn | 113      |
| TotalSamples  | 79992    |
----------------------------
itr #23 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.42964667081832886
Validation loss = 0.4281504154205322
Validation loss = 0.43180981278419495
Validation loss = 0.43089255690574646
Validation loss = 0.4316694140434265
Validation loss = 0.43399110436439514
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4301007390022278
Validation loss = 0.42749372124671936
Validation loss = 0.4289860725402832
Validation loss = 0.4290987551212311
Validation loss = 0.43099188804626465
Validation loss = 0.43284329771995544
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.43395453691482544
Validation loss = 0.43292751908302307
Validation loss = 0.4361196458339691
Validation loss = 0.4365676939487457
Validation loss = 0.4404762387275696
Validation loss = 0.4391798973083496
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4271140694618225
Validation loss = 0.4286176264286041
Validation loss = 0.42928701639175415
Validation loss = 0.4302964210510254
Validation loss = 0.4350402355194092
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4380321502685547
Validation loss = 0.4376424252986908
Validation loss = 0.4385751783847809
Validation loss = 0.4403948187828064
Validation loss = 0.44133681058883667
Validation loss = 0.44103938341140747
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 23       |
| MaximumReturn | 199      |
| MinimumReturn | 113      |
| TotalSamples  | 83325    |
----------------------------
itr #24 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.43310117721557617
Validation loss = 0.431639701128006
Validation loss = 0.43342265486717224
Validation loss = 0.43385329842567444
Validation loss = 0.43555477261543274
Validation loss = 0.43678027391433716
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.43207740783691406
Validation loss = 0.4329383075237274
Validation loss = 0.4328596591949463
Validation loss = 0.43487879633903503
Validation loss = 0.4332520067691803
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.43787881731987
Validation loss = 0.4371144771575928
Validation loss = 0.441995769739151
Validation loss = 0.4393159747123718
Validation loss = 0.43921154737472534
Validation loss = 0.4415501058101654
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4289771616458893
Validation loss = 0.4310385584831238
Validation loss = 0.4295616149902344
Validation loss = 0.4351990520954132
Validation loss = 0.4324810206890106
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.44142624735832214
Validation loss = 0.443397581577301
Validation loss = 0.4420510232448578
Validation loss = 0.4416576623916626
Validation loss = 0.44321176409721375
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 24       |
| MaximumReturn | 198      |
| MinimumReturn | 112      |
| TotalSamples  | 86658    |
----------------------------
itr #25 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.43189045786857605
Validation loss = 0.43549567461013794
Validation loss = 0.437393456697464
Validation loss = 0.43677395582199097
Validation loss = 0.43774813413619995
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.43349140882492065
Validation loss = 0.43393173813819885
Validation loss = 0.4372902810573578
Validation loss = 0.4353253245353699
Validation loss = 0.4359065592288971
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.43948328495025635
Validation loss = 0.4421601891517639
Validation loss = 0.43908384442329407
Validation loss = 0.44087788462638855
Validation loss = 0.443606972694397
Validation loss = 0.44256454706192017
Validation loss = 0.44003915786743164
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.43326812982559204
Validation loss = 0.4343138039112091
Validation loss = 0.43189993500709534
Validation loss = 0.4324912130832672
Validation loss = 0.4337606132030487
Validation loss = 0.43386486172676086
Validation loss = 0.4347967207431793
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.44278809428215027
Validation loss = 0.4415789842605591
Validation loss = 0.44229018688201904
Validation loss = 0.4436919093132019
Validation loss = 0.443141907453537
Validation loss = 0.4433465003967285
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 25       |
| MaximumReturn | 199      |
| MinimumReturn | 125      |
| TotalSamples  | 89991    |
----------------------------
itr #26 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4347195625305176
Validation loss = 0.432783842086792
Validation loss = 0.43375253677368164
Validation loss = 0.4346484839916229
Validation loss = 0.43679550290107727
Validation loss = 0.4348839223384857
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4360947012901306
Validation loss = 0.43432313203811646
Validation loss = 0.43336957693099976
Validation loss = 0.433358758687973
Validation loss = 0.4353073239326477
Validation loss = 0.43543291091918945
Validation loss = 0.4384915232658386
Validation loss = 0.43744969367980957
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4410867393016815
Validation loss = 0.44119882583618164
Validation loss = 0.44152989983558655
Validation loss = 0.44492924213409424
Validation loss = 0.44290390610694885
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4351359009742737
Validation loss = 0.43365049362182617
Validation loss = 0.43338602781295776
Validation loss = 0.4341121315956116
Validation loss = 0.435376912355423
Validation loss = 0.43463242053985596
Validation loss = 0.4369767904281616
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.44529587030410767
Validation loss = 0.44306743144989014
Validation loss = 0.4434144198894501
Validation loss = 0.44417473673820496
Validation loss = 0.44409510493278503
Validation loss = 0.44505929946899414
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 159      |
| Iteration     | 26       |
| MaximumReturn | 199      |
| MinimumReturn | 120      |
| TotalSamples  | 93324    |
----------------------------
itr #27 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4350247085094452
Validation loss = 0.4345381259918213
Validation loss = 0.43583717942237854
Validation loss = 0.4348260164260864
Validation loss = 0.43770575523376465
Validation loss = 0.43718117475509644
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.43669846653938293
Validation loss = 0.4364141523838043
Validation loss = 0.43774110078811646
Validation loss = 0.436334490776062
Validation loss = 0.4386434555053711
Validation loss = 0.43685153126716614
Validation loss = 0.44028186798095703
Validation loss = 0.4384184777736664
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.44233500957489014
Validation loss = 0.44229504466056824
Validation loss = 0.441951185464859
Validation loss = 0.4419574737548828
Validation loss = 0.44279545545578003
Validation loss = 0.4424391984939575
Validation loss = 0.445665180683136
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.43553659319877625
Validation loss = 0.4345606565475464
Validation loss = 0.44104450941085815
Validation loss = 0.43568819761276245
Validation loss = 0.43897154927253723
Validation loss = 0.43911468982696533
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.44269299507141113
Validation loss = 0.4436764121055603
Validation loss = 0.4428839087486267
Validation loss = 0.44482365250587463
Validation loss = 0.44576209783554077
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 27       |
| MaximumReturn | 199      |
| MinimumReturn | 121      |
| TotalSamples  | 96657    |
----------------------------
itr #28 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.43602266907691956
Validation loss = 0.4355798065662384
Validation loss = 0.43727490305900574
Validation loss = 0.4373021125793457
Validation loss = 0.439089298248291
Validation loss = 0.4383820593357086
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4381667673587799
Validation loss = 0.4377639591693878
Validation loss = 0.4383885860443115
Validation loss = 0.4413401782512665
Validation loss = 0.44130435585975647
Validation loss = 0.44170084595680237
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4441755712032318
Validation loss = 0.44405221939086914
Validation loss = 0.44480887055397034
Validation loss = 0.4423287808895111
Validation loss = 0.44696393609046936
Validation loss = 0.44581469893455505
Validation loss = 0.4459961950778961
Validation loss = 0.4483760893344879
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.43966856598854065
Validation loss = 0.4377630650997162
Validation loss = 0.4376707375049591
Validation loss = 0.43756893277168274
Validation loss = 0.43999695777893066
Validation loss = 0.44146767258644104
Validation loss = 0.4398063123226166
Validation loss = 0.44111180305480957
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.443297415971756
Validation loss = 0.4443000853061676
Validation loss = 0.4425595700740814
Validation loss = 0.44428932666778564
Validation loss = 0.4476102590560913
Validation loss = 0.445528507232666
Validation loss = 0.4479685127735138
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 28       |
| MaximumReturn | 199      |
| MinimumReturn | 132      |
| TotalSamples  | 99990    |
----------------------------
itr #29 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.43411168456077576
Validation loss = 0.43558064103126526
Validation loss = 0.4364390969276428
Validation loss = 0.4361157715320587
Validation loss = 0.43721896409988403
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4376693665981293
Validation loss = 0.4366118311882019
Validation loss = 0.4375624358654022
Validation loss = 0.4367738366127014
Validation loss = 0.4385916590690613
Validation loss = 0.44081130623817444
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4442007541656494
Validation loss = 0.44437432289123535
Validation loss = 0.44363608956336975
Validation loss = 0.44524914026260376
Validation loss = 0.4471639394760132
Validation loss = 0.448490709066391
Validation loss = 0.44703397154808044
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4374901056289673
Validation loss = 0.43886521458625793
Validation loss = 0.4387187659740448
Validation loss = 0.4421825408935547
Validation loss = 0.43988826870918274
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.44404301047325134
Validation loss = 0.44523119926452637
Validation loss = 0.44464942812919617
Validation loss = 0.44877082109451294
Validation loss = 0.4443432688713074
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 29       |
| MaximumReturn | 198      |
| MinimumReturn | 129      |
| TotalSamples  | 103323   |
----------------------------
itr #30 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4397689998149872
Validation loss = 0.437577486038208
Validation loss = 0.4385848045349121
Validation loss = 0.43726813793182373
Validation loss = 0.4367603659629822
Validation loss = 0.4392009377479553
Validation loss = 0.43978583812713623
Validation loss = 0.43983587622642517
Validation loss = 0.44003382325172424
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4399723410606384
Validation loss = 0.44029784202575684
Validation loss = 0.44150835275650024
Validation loss = 0.4399230480194092
Validation loss = 0.4417889416217804
Validation loss = 0.4406684339046478
Validation loss = 0.4417283535003662
Validation loss = 0.4436938762664795
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.44645965099334717
Validation loss = 0.4453746974468231
Validation loss = 0.4468213617801666
Validation loss = 0.4489588737487793
Validation loss = 0.44769197702407837
Validation loss = 0.4473661482334137
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.43974950909614563
Validation loss = 0.4403705596923828
Validation loss = 0.44049009680747986
Validation loss = 0.440485417842865
Validation loss = 0.43955937027931213
Validation loss = 0.44050630927085876
Validation loss = 0.4398288428783417
Validation loss = 0.4415604770183563
Validation loss = 0.44217023253440857
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.44417470693588257
Validation loss = 0.4446135461330414
Validation loss = 0.4434567987918854
Validation loss = 0.44308334589004517
Validation loss = 0.44297823309898376
Validation loss = 0.44703468680381775
Validation loss = 0.44610774517059326
Validation loss = 0.4457400143146515
Validation loss = 0.44899094104766846
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 30       |
| MaximumReturn | 199      |
| MinimumReturn | 137      |
| TotalSamples  | 106656   |
----------------------------
itr #31 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4397590160369873
Validation loss = 0.43894028663635254
Validation loss = 0.44078347086906433
Validation loss = 0.44099727272987366
Validation loss = 0.44010496139526367
Validation loss = 0.43994706869125366
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.44090428948402405
Validation loss = 0.43927451968193054
Validation loss = 0.44166645407676697
Validation loss = 0.4421568810939789
Validation loss = 0.4440058171749115
Validation loss = 0.4433099329471588
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.44760578870773315
Validation loss = 0.4456263482570648
Validation loss = 0.4449598789215088
Validation loss = 0.447125643491745
Validation loss = 0.44624200463294983
Validation loss = 0.44895678758621216
Validation loss = 0.449091374874115
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.43980419635772705
Validation loss = 0.44047749042510986
Validation loss = 0.4413844347000122
Validation loss = 0.44025981426239014
Validation loss = 0.4422691762447357
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4460212290287018
Validation loss = 0.44640761613845825
Validation loss = 0.4468483030796051
Validation loss = 0.44570812582969666
Validation loss = 0.446851521730423
Validation loss = 0.44767922163009644
Validation loss = 0.452632337808609
Validation loss = 0.44762200117111206
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 163      |
| Iteration     | 31       |
| MaximumReturn | 198      |
| MinimumReturn | 126      |
| TotalSamples  | 109989   |
----------------------------
itr #32 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4392800033092499
Validation loss = 0.43832775950431824
Validation loss = 0.4371201992034912
Validation loss = 0.43951430916786194
Validation loss = 0.43897950649261475
Validation loss = 0.4426004886627197
Validation loss = 0.43834903836250305
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4413403570652008
Validation loss = 0.4392331838607788
Validation loss = 0.4406808912754059
Validation loss = 0.440401166677475
Validation loss = 0.4422931969165802
Validation loss = 0.44107261300086975
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.44818195700645447
Validation loss = 0.446005254983902
Validation loss = 0.4472369849681854
Validation loss = 0.4489993155002594
Validation loss = 0.4463258683681488
Validation loss = 0.447634756565094
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.441427081823349
Validation loss = 0.4391956031322479
Validation loss = 0.44073939323425293
Validation loss = 0.4390561282634735
Validation loss = 0.44127321243286133
Validation loss = 0.4410201609134674
Validation loss = 0.44149884581565857
Validation loss = 0.4410066306591034
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.44681626558303833
Validation loss = 0.44946369528770447
Validation loss = 0.4461262822151184
Validation loss = 0.4474368989467621
Validation loss = 0.45084068179130554
Validation loss = 0.4473371207714081
Validation loss = 0.44995853304862976
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 32       |
| MaximumReturn | 199      |
| MinimumReturn | 119      |
| TotalSamples  | 113322   |
----------------------------
itr #33 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.44243964552879333
Validation loss = 0.440680593252182
Validation loss = 0.4399492144584656
Validation loss = 0.44035911560058594
Validation loss = 0.44284868240356445
Validation loss = 0.441912978887558
Validation loss = 0.44192197918891907
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4429451525211334
Validation loss = 0.44068294763565063
Validation loss = 0.44244787096977234
Validation loss = 0.4409763216972351
Validation loss = 0.44281306862831116
Validation loss = 0.44366952776908875
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.44779327511787415
Validation loss = 0.44656285643577576
Validation loss = 0.45127588510513306
Validation loss = 0.44966572523117065
Validation loss = 0.4515698254108429
Validation loss = 0.4483852684497833
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4403613209724426
Validation loss = 0.4423690438270569
Validation loss = 0.44018998742103577
Validation loss = 0.44191163778305054
Validation loss = 0.44537368416786194
Validation loss = 0.4447486698627472
Validation loss = 0.4435100853443146
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4514062702655792
Validation loss = 0.4470326602458954
Validation loss = 0.44824954867362976
Validation loss = 0.4500366747379303
Validation loss = 0.4486517906188965
Validation loss = 0.4528908133506775
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 33       |
| MaximumReturn | 198      |
| MinimumReturn | 126      |
| TotalSamples  | 116655   |
----------------------------
itr #34 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.44105851650238037
Validation loss = 0.438812792301178
Validation loss = 0.44093436002731323
Validation loss = 0.4419896900653839
Validation loss = 0.4422394633293152
Validation loss = 0.44315335154533386
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.44078299403190613
Validation loss = 0.4420153796672821
Validation loss = 0.4404733180999756
Validation loss = 0.44256502389907837
Validation loss = 0.445659875869751
Validation loss = 0.44304707646369934
Validation loss = 0.4447973370552063
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4469960629940033
Validation loss = 0.44715315103530884
Validation loss = 0.4492635726928711
Validation loss = 0.4473018944263458
Validation loss = 0.44936203956604004
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4400627315044403
Validation loss = 0.4420585334300995
Validation loss = 0.4442598819732666
Validation loss = 0.4427729845046997
Validation loss = 0.44216886162757874
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.44804054498672485
Validation loss = 0.4481121003627777
Validation loss = 0.4484274387359619
Validation loss = 0.4515678584575653
Validation loss = 0.4507678151130676
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 34       |
| MaximumReturn | 199      |
| MinimumReturn | 130      |
| TotalSamples  | 119988   |
----------------------------
itr #35 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4408198595046997
Validation loss = 0.43977242708206177
Validation loss = 0.4433801472187042
Validation loss = 0.4424666166305542
Validation loss = 0.44106581807136536
Validation loss = 0.4417865574359894
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4436931312084198
Validation loss = 0.4465411305427551
Validation loss = 0.44281205534935
Validation loss = 0.44319379329681396
Validation loss = 0.4425560534000397
Validation loss = 0.4441111385822296
Validation loss = 0.44666311144828796
Validation loss = 0.4445323050022125
Validation loss = 0.44445356726646423
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4481810927391052
Validation loss = 0.44822248816490173
Validation loss = 0.44802385568618774
Validation loss = 0.44852691888809204
Validation loss = 0.44762876629829407
Validation loss = 0.4498462677001953
Validation loss = 0.4481130540370941
Validation loss = 0.45168831944465637
Validation loss = 0.45206400752067566
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.44334930181503296
Validation loss = 0.44156837463378906
Validation loss = 0.4429630637168884
Validation loss = 0.44164469838142395
Validation loss = 0.44197168946266174
Validation loss = 0.4437955617904663
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.44502994418144226
Validation loss = 0.4489363133907318
Validation loss = 0.4458128809928894
Validation loss = 0.44978874921798706
Validation loss = 0.449954092502594
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 35       |
| MaximumReturn | 199      |
| MinimumReturn | 124      |
| TotalSamples  | 123321   |
----------------------------
itr #36 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4438773989677429
Validation loss = 0.44192034006118774
Validation loss = 0.4406786561012268
Validation loss = 0.44091957807540894
Validation loss = 0.44110408425331116
Validation loss = 0.4433327615261078
Validation loss = 0.4422498643398285
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.44314253330230713
Validation loss = 0.44435760378837585
Validation loss = 0.4463035762310028
Validation loss = 0.4442610740661621
Validation loss = 0.4452582597732544
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.44954800605773926
Validation loss = 0.4502391219139099
Validation loss = 0.452004075050354
Validation loss = 0.44951972365379333
Validation loss = 0.4508986175060272
Validation loss = 0.4509020447731018
Validation loss = 0.4539110064506531
Validation loss = 0.45104724168777466
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4436837434768677
Validation loss = 0.4415103793144226
Validation loss = 0.4423221945762634
Validation loss = 0.44270509481430054
Validation loss = 0.44490912556648254
Validation loss = 0.4434075653553009
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.44866013526916504
Validation loss = 0.44592946767807007
Validation loss = 0.4453658163547516
Validation loss = 0.4473307430744171
Validation loss = 0.4483505189418793
Validation loss = 0.45006996393203735
Validation loss = 0.452157199382782
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 36       |
| MaximumReturn | 198      |
| MinimumReturn | 129      |
| TotalSamples  | 126654   |
----------------------------
itr #37 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.44039493799209595
Validation loss = 0.441398024559021
Validation loss = 0.4417448341846466
Validation loss = 0.44190818071365356
Validation loss = 0.44279760122299194
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4443648159503937
Validation loss = 0.44155409932136536
Validation loss = 0.4433172643184662
Validation loss = 0.444021075963974
Validation loss = 0.4444518983364105
Validation loss = 0.4438322186470032
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4490981996059418
Validation loss = 0.4489794373512268
Validation loss = 0.4510501027107239
Validation loss = 0.4504565894603729
Validation loss = 0.4524472653865814
Validation loss = 0.45125600695610046
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.441869854927063
Validation loss = 0.44064944982528687
Validation loss = 0.44445112347602844
Validation loss = 0.44200044870376587
Validation loss = 0.443756103515625
Validation loss = 0.4445549249649048
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4482461214065552
Validation loss = 0.4480743408203125
Validation loss = 0.4479658603668213
Validation loss = 0.4476357400417328
Validation loss = 0.449547678232193
Validation loss = 0.4511641561985016
Validation loss = 0.45110565423965454
Validation loss = 0.44999995827674866
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 37       |
| MaximumReturn | 199      |
| MinimumReturn | 119      |
| TotalSamples  | 129987   |
----------------------------
itr #38 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.44220882654190063
Validation loss = 0.43931061029434204
Validation loss = 0.4391406178474426
Validation loss = 0.44086089730262756
Validation loss = 0.4421379864215851
Validation loss = 0.442596435546875
Validation loss = 0.44167014956474304
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4429640471935272
Validation loss = 0.44350665807724
Validation loss = 0.44438496232032776
Validation loss = 0.44188597798347473
Validation loss = 0.44285285472869873
Validation loss = 0.4421313405036926
Validation loss = 0.445261150598526
Validation loss = 0.44440126419067383
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.45070406794548035
Validation loss = 0.449810266494751
Validation loss = 0.45054396986961365
Validation loss = 0.4491457939147949
Validation loss = 0.4490753710269928
Validation loss = 0.454867959022522
Validation loss = 0.45076778531074524
Validation loss = 0.4557254910469055
Validation loss = 0.45316869020462036
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.44351640343666077
Validation loss = 0.4429471492767334
Validation loss = 0.4414331912994385
Validation loss = 0.4410105347633362
Validation loss = 0.44209083914756775
Validation loss = 0.4429491460323334
Validation loss = 0.44227707386016846
Validation loss = 0.4428322911262512
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.44635581970214844
Validation loss = 0.4476519823074341
Validation loss = 0.44787243008613586
Validation loss = 0.4461340010166168
Validation loss = 0.44752055406570435
Validation loss = 0.44962453842163086
Validation loss = 0.45005515217781067
Validation loss = 0.4496469795703888
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 38       |
| MaximumReturn | 198      |
| MinimumReturn | 115      |
| TotalSamples  | 133320   |
----------------------------
itr #39 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4402577579021454
Validation loss = 0.44114425778388977
Validation loss = 0.4444893002510071
Validation loss = 0.4430312216281891
Validation loss = 0.4417911171913147
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4439873695373535
Validation loss = 0.4446879029273987
Validation loss = 0.4446287751197815
Validation loss = 0.4450548589229584
Validation loss = 0.44535133242607117
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4557037651538849
Validation loss = 0.45321235060691833
Validation loss = 0.45019203424453735
Validation loss = 0.4531739354133606
Validation loss = 0.4534390866756439
Validation loss = 0.45283395051956177
Validation loss = 0.4548527002334595
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4436018168926239
Validation loss = 0.44129911065101624
Validation loss = 0.4433918297290802
Validation loss = 0.4446931779384613
Validation loss = 0.4427246153354645
Validation loss = 0.44339287281036377
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.44978857040405273
Validation loss = 0.45160576701164246
Validation loss = 0.4492426812648773
Validation loss = 0.45001211762428284
Validation loss = 0.4510654807090759
Validation loss = 0.452536940574646
Validation loss = 0.4512743055820465
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 164      |
| Iteration     | 39       |
| MaximumReturn | 199      |
| MinimumReturn | 121      |
| TotalSamples  | 136653   |
----------------------------
