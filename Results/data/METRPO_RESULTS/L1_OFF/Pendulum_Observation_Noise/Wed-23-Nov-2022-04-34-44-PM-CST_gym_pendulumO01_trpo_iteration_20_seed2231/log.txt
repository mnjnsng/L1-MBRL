Logging to experiments/gym_pendulumO01/Pendulum_Observation_Noise/Wed-23-Nov-2022-04-34-44-PM-CST_gym_pendulumO01_trpo_iteration_20_seed2231
Print configuration .....
{'env_name': 'gym_pendulumO01', 'random_seeds': [2531, 2231], 'save_variables': False, 'model_save_dir': '/tmp/pendulumO01_models/', 'restore_variables': False, 'start_onpol_iter': 0, 'onpol_iters': 40, 'num_path_random': 25, 'num_path_onpol': 25, 'env_horizon': 200, 'max_train_data': 200000, 'max_val_data': 100000, 'discard_ratio': 0.0, 'dynamics': {'pre_training': {'mode': 'intrinsic_reward', 'itr': 0, 'policy_itr': 20}, 'model': 'nn', 'ensemble': True, 'ensemble_model_count': 5, 'enable_particle_ensemble': True, 'particles': 5, 'obs_var': 1.0, 'intrinsic_reward_coeff': 1.0, 'ita': 1.0, 'mode': 'random', 'val': True, 'n_layers': 4, 'hidden_size': 1000, 'activation': 'relu', 'batch_size': 1000, 'learning_rate': 0.001, 'reg_coeff': 0.0, 'epochs': 200, 'kfac_params': {'learning_rate': 0.1, 'damping': 0.001, 'momentum': 0.9, 'kl_clip': 0.0001, 'cov_ema_decay': 0.99}}, 'policy': {'network_shape': [64, 64], 'init_logstd': 0.0, 'activation': 'tanh', 'reinitialize_every_itr': False}, 'trpo': {'horizon': 200, 'gamma': 0.99, 'step_size': 0.01, 'iterations': 20, 'batch_size': 50000, 'gae': 0.95, 'visualization': False, 'visualize_iterations': [0]}, 'algo': 'trpo'}
Generating random rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating random rollouts.
Creating normalization for training data.
Done creating normalization for training data.
Particle ensemble enabled? True
An ensemble of 5 dynamics model <class 'model.dynamics.NNDynamicsModel'> initialized
Train dynamics model with intrinsic reward only? False
Pre-training enabled. Using only intrinsic reward.
Pre-training dynamics model for 0 iterations...
Done pre-training dynamics model.
Using external reward only.
itr #0 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.35980290174484253
Validation loss = 0.20108915865421295
Validation loss = 0.19162589311599731
Validation loss = 0.19335953891277313
Validation loss = 0.18441154062747955
Validation loss = 0.18290302157402039
Validation loss = 0.18259146809577942
Validation loss = 0.17846830189228058
Validation loss = 0.17574967443943024
Validation loss = 0.17472922801971436
Validation loss = 0.17158479988574982
Validation loss = 0.16714325547218323
Validation loss = 0.17123419046401978
Validation loss = 0.1635560542345047
Validation loss = 0.16513553261756897
Validation loss = 0.15963010489940643
Validation loss = 0.16454161703586578
Validation loss = 0.16519956290721893
Validation loss = 0.1609344482421875
Validation loss = 0.16094782948493958
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3381178081035614
Validation loss = 0.20100373029708862
Validation loss = 0.1898844987154007
Validation loss = 0.1869172900915146
Validation loss = 0.187794491648674
Validation loss = 0.1831573247909546
Validation loss = 0.18234004080295563
Validation loss = 0.17989715933799744
Validation loss = 0.17603757977485657
Validation loss = 0.1763589084148407
Validation loss = 0.16623711585998535
Validation loss = 0.16968050599098206
Validation loss = 0.17011021077632904
Validation loss = 0.16313405334949493
Validation loss = 0.18298277258872986
Validation loss = 0.1667136400938034
Validation loss = 0.15899845957756042
Validation loss = 0.1632530391216278
Validation loss = 0.160146102309227
Validation loss = 0.16402414441108704
Validation loss = 0.1612022966146469
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3539242446422577
Validation loss = 0.20025065541267395
Validation loss = 0.19301439821720123
Validation loss = 0.1866268664598465
Validation loss = 0.18523775041103363
Validation loss = 0.1835312843322754
Validation loss = 0.18204621970653534
Validation loss = 0.18156567215919495
Validation loss = 0.176039919257164
Validation loss = 0.17371439933776855
Validation loss = 0.1684076189994812
Validation loss = 0.2059238851070404
Validation loss = 0.1676572859287262
Validation loss = 0.17692777514457703
Validation loss = 0.16039887070655823
Validation loss = 0.15874679386615753
Validation loss = 0.1732383370399475
Validation loss = 0.16106154024600983
Validation loss = 0.1607263833284378
Validation loss = 0.17920951545238495
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3388712704181671
Validation loss = 0.20259808003902435
Validation loss = 0.19101431965827942
Validation loss = 0.18599247932434082
Validation loss = 0.18496741354465485
Validation loss = 0.18408364057540894
Validation loss = 0.18158310651779175
Validation loss = 0.18472211062908173
Validation loss = 0.17889589071273804
Validation loss = 0.17319278419017792
Validation loss = 0.17173656821250916
Validation loss = 0.17660565674304962
Validation loss = 0.17485441267490387
Validation loss = 0.1637190878391266
Validation loss = 0.17031924426555634
Validation loss = 0.1757582575082779
Validation loss = 0.17232784628868103
Validation loss = 0.15931521356105804
Validation loss = 0.17136332392692566
Validation loss = 0.15893104672431946
Validation loss = 0.15925395488739014
Validation loss = 0.16075527667999268
Validation loss = 0.16347935795783997
Validation loss = 0.15945422649383545
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.34778180718421936
Validation loss = 0.19933319091796875
Validation loss = 0.18899932503700256
Validation loss = 0.18715329468250275
Validation loss = 0.18628408014774323
Validation loss = 0.18564388155937195
Validation loss = 0.18192443251609802
Validation loss = 0.1859656125307083
Validation loss = 0.17973779141902924
Validation loss = 0.18337422609329224
Validation loss = 0.18053863942623138
Validation loss = 0.16922251880168915
Validation loss = 0.167388454079628
Validation loss = 0.16682612895965576
Validation loss = 0.17023561894893646
Validation loss = 0.17177926003932953
Validation loss = 0.16212661564350128
Validation loss = 0.17032113671302795
Validation loss = 0.1670898050069809
Validation loss = 0.15834932029247284
Validation loss = 0.15694071352481842
Validation loss = 0.15640467405319214
Validation loss = 0.1609295755624771
Validation loss = 0.16136454045772552
Validation loss = 0.16117706894874573
Validation loss = 0.1560693234205246
Validation loss = 0.15551097691059113
Validation loss = 0.15508025884628296
Validation loss = 0.15738888084888458
Validation loss = 0.160525843501091
Validation loss = 0.1533527374267578
Validation loss = 0.1681530624628067
Validation loss = 0.1574903130531311
Validation loss = 0.15305951237678528
Validation loss = 0.15607960522174835
Validation loss = 0.1573043018579483
Validation loss = 0.15419244766235352
Validation loss = 0.15461748838424683
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 0        |
| MaximumReturn | 198      |
| MinimumReturn | 115      |
| TotalSamples  | 6666     |
----------------------------
itr #1 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.2999690771102905
Validation loss = 0.23452477157115936
Validation loss = 0.22779691219329834
Validation loss = 0.23112119734287262
Validation loss = 0.21956448256969452
Validation loss = 0.21985135972499847
Validation loss = 0.22821491956710815
Validation loss = 0.21563564240932465
Validation loss = 0.21592454612255096
Validation loss = 0.22073747217655182
Validation loss = 0.21621352434158325
Validation loss = 0.2142016738653183
Validation loss = 0.21396265923976898
Validation loss = 0.21577303111553192
Validation loss = 0.2150699645280838
Validation loss = 0.21284037828445435
Validation loss = 0.21517758071422577
Validation loss = 0.21490478515625
Validation loss = 0.21334554255008698
Validation loss = 0.21608507633209229
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3026181161403656
Validation loss = 0.23101991415023804
Validation loss = 0.2250211238861084
Validation loss = 0.22160442173480988
Validation loss = 0.22409772872924805
Validation loss = 0.21887154877185822
Validation loss = 0.21996335685253143
Validation loss = 0.21654914319515228
Validation loss = 0.21560700237751007
Validation loss = 0.2151222974061966
Validation loss = 0.23106174170970917
Validation loss = 0.21316634118556976
Validation loss = 0.21518945693969727
Validation loss = 0.21435503661632538
Validation loss = 0.21614821255207062
Validation loss = 0.2122824639081955
Validation loss = 0.21318627893924713
Validation loss = 0.21452416479587555
Validation loss = 0.21542541682720184
Validation loss = 0.2091549187898636
Validation loss = 0.21135996282100677
Validation loss = 0.2118348628282547
Validation loss = 0.21054767072200775
Validation loss = 0.20945946872234344
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3061926066875458
Validation loss = 0.23191864788532257
Validation loss = 0.22806961834430695
Validation loss = 0.22230751812458038
Validation loss = 0.22070805728435516
Validation loss = 0.22173966467380524
Validation loss = 0.2246728539466858
Validation loss = 0.224655881524086
Validation loss = 0.21844477951526642
Validation loss = 0.21371550858020782
Validation loss = 0.21867747604846954
Validation loss = 0.21685343980789185
Validation loss = 0.2151869684457779
Validation loss = 0.21447373926639557
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3019445240497589
Validation loss = 0.22982876002788544
Validation loss = 0.2235174924135208
Validation loss = 0.22087053954601288
Validation loss = 0.21593905985355377
Validation loss = 0.21758075058460236
Validation loss = 0.21754658222198486
Validation loss = 0.2144547700881958
Validation loss = 0.22991983592510223
Validation loss = 0.2169518619775772
Validation loss = 0.215390145778656
Validation loss = 0.21452146768569946
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3290986120700836
Validation loss = 0.23103685677051544
Validation loss = 0.2223987728357315
Validation loss = 0.2172185182571411
Validation loss = 0.21455883979797363
Validation loss = 0.21584080159664154
Validation loss = 0.21784533560276031
Validation loss = 0.21420349180698395
Validation loss = 0.21429593861103058
Validation loss = 0.21690768003463745
Validation loss = 0.2239076942205429
Validation loss = 0.21669608354568481
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 1        |
| MaximumReturn | 199      |
| MinimumReturn | 123      |
| TotalSamples  | 9999     |
----------------------------
itr #2 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.2625301778316498
Validation loss = 0.24738559126853943
Validation loss = 0.2468077689409256
Validation loss = 0.24871186912059784
Validation loss = 0.24406075477600098
Validation loss = 0.2498163878917694
Validation loss = 0.2448943853378296
Validation loss = 0.24914059042930603
Validation loss = 0.24986152350902557
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.27126550674438477
Validation loss = 0.2468845397233963
Validation loss = 0.2463844269514084
Validation loss = 0.25218909978866577
Validation loss = 0.24859969317913055
Validation loss = 0.244024395942688
Validation loss = 0.24415746331214905
Validation loss = 0.24540016055107117
Validation loss = 0.24506846070289612
Validation loss = 0.2509596347808838
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.26781797409057617
Validation loss = 0.25039950013160706
Validation loss = 0.2456199824810028
Validation loss = 0.24599488079547882
Validation loss = 0.24604368209838867
Validation loss = 0.24643318355083466
Validation loss = 0.24673405289649963
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.2732510566711426
Validation loss = 0.2512717545032501
Validation loss = 0.24691589176654816
Validation loss = 0.24960848689079285
Validation loss = 0.2493116855621338
Validation loss = 0.24424418807029724
Validation loss = 0.24582211673259735
Validation loss = 0.2454868108034134
Validation loss = 0.24810071289539337
Validation loss = 0.24521324038505554
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.2704295516014099
Validation loss = 0.2492562085390091
Validation loss = 0.2458738088607788
Validation loss = 0.24631750583648682
Validation loss = 0.24729128181934357
Validation loss = 0.24437136948108673
Validation loss = 0.24352292716503143
Validation loss = 0.24575427174568176
Validation loss = 0.24553939700126648
Validation loss = 0.24510106444358826
Validation loss = 0.24390244483947754
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 158      |
| Iteration     | 2        |
| MaximumReturn | 198      |
| MinimumReturn | 121      |
| TotalSamples  | 13332    |
----------------------------
itr #3 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.2692713141441345
Validation loss = 0.26333072781562805
Validation loss = 0.262201726436615
Validation loss = 0.2626727521419525
Validation loss = 0.2633354365825653
Validation loss = 0.2652711272239685
Validation loss = 0.26158520579338074
Validation loss = 0.26311367750167847
Validation loss = 0.26576757431030273
Validation loss = 0.26124393939971924
Validation loss = 0.26323747634887695
Validation loss = 0.26280319690704346
Validation loss = 0.2611750662326813
Validation loss = 0.26373469829559326
Validation loss = 0.26389598846435547
Validation loss = 0.26318708062171936
Validation loss = 0.261960506439209
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.27500423789024353
Validation loss = 0.26257750391960144
Validation loss = 0.2617465853691101
Validation loss = 0.26527872681617737
Validation loss = 0.2617674767971039
Validation loss = 0.2657611668109894
Validation loss = 0.2646196484565735
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.27390724420547485
Validation loss = 0.26299378275871277
Validation loss = 0.2629905045032501
Validation loss = 0.2714996039867401
Validation loss = 0.2611159086227417
Validation loss = 0.2673175632953644
Validation loss = 0.2606773376464844
Validation loss = 0.2611195743083954
Validation loss = 0.263298898935318
Validation loss = 0.2621423602104187
Validation loss = 0.26460549235343933
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.26460081338882446
Validation loss = 0.2627880275249481
Validation loss = 0.2652265727519989
Validation loss = 0.26084470748901367
Validation loss = 0.2636834383010864
Validation loss = 0.2643358111381531
Validation loss = 0.2620527744293213
Validation loss = 0.26293128728866577
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.2680870294570923
Validation loss = 0.26523491740226746
Validation loss = 0.26190829277038574
Validation loss = 0.26469090580940247
Validation loss = 0.2620502710342407
Validation loss = 0.2630614638328552
Validation loss = 0.265054851770401
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 3        |
| MaximumReturn | 199      |
| MinimumReturn | 120      |
| TotalSamples  | 16665    |
----------------------------
itr #4 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.2755141258239746
Validation loss = 0.27358710765838623
Validation loss = 0.2754586637020111
Validation loss = 0.27748745679855347
Validation loss = 0.2727377414703369
Validation loss = 0.27496469020843506
Validation loss = 0.2812632918357849
Validation loss = 0.27806878089904785
Validation loss = 0.2768312692642212
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.2837352752685547
Validation loss = 0.27889788150787354
Validation loss = 0.275404155254364
Validation loss = 0.2783673405647278
Validation loss = 0.2779499292373657
Validation loss = 0.2748410999774933
Validation loss = 0.2744792103767395
Validation loss = 0.277121901512146
Validation loss = 0.28056663274765015
Validation loss = 0.2758699655532837
Validation loss = 0.27465957403182983
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.2785416841506958
Validation loss = 0.27773919701576233
Validation loss = 0.2726457118988037
Validation loss = 0.27452999353408813
Validation loss = 0.2761833667755127
Validation loss = 0.27680712938308716
Validation loss = 0.2751910090446472
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.2810611128807068
Validation loss = 0.2762487828731537
Validation loss = 0.2748713493347168
Validation loss = 0.27649080753326416
Validation loss = 0.2765713334083557
Validation loss = 0.27737706899642944
Validation loss = 0.27412331104278564
Validation loss = 0.27957969903945923
Validation loss = 0.2767253816127777
Validation loss = 0.27586299180984497
Validation loss = 0.2777065634727478
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.2813688814640045
Validation loss = 0.27401718497276306
Validation loss = 0.2764369249343872
Validation loss = 0.2746705412864685
Validation loss = 0.2751185894012451
Validation loss = 0.27631545066833496
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 4        |
| MaximumReturn | 198      |
| MinimumReturn | 131      |
| TotalSamples  | 19998    |
----------------------------
itr #5 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.2881945073604584
Validation loss = 0.2865813076496124
Validation loss = 0.2869188189506531
Validation loss = 0.28548720479011536
Validation loss = 0.28480294346809387
Validation loss = 0.28617483377456665
Validation loss = 0.28696519136428833
Validation loss = 0.28646060824394226
Validation loss = 0.28673094511032104
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.288146048784256
Validation loss = 0.284895122051239
Validation loss = 0.2848677933216095
Validation loss = 0.28393030166625977
Validation loss = 0.2869381308555603
Validation loss = 0.28493279218673706
Validation loss = 0.2887227535247803
Validation loss = 0.28900858759880066
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.2900398373603821
Validation loss = 0.2855059504508972
Validation loss = 0.2850075662136078
Validation loss = 0.2851535677909851
Validation loss = 0.2853217124938965
Validation loss = 0.28473564982414246
Validation loss = 0.2862509787082672
Validation loss = 0.2850233316421509
Validation loss = 0.28580546379089355
Validation loss = 0.2888329327106476
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.288469135761261
Validation loss = 0.2885838449001312
Validation loss = 0.28612563014030457
Validation loss = 0.2848241925239563
Validation loss = 0.28794020414352417
Validation loss = 0.2833784222602844
Validation loss = 0.2879289388656616
Validation loss = 0.28780031204223633
Validation loss = 0.28462061285972595
Validation loss = 0.2871016561985016
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.28670746088027954
Validation loss = 0.28568631410598755
Validation loss = 0.2847505211830139
Validation loss = 0.2826693058013916
Validation loss = 0.2908453047275543
Validation loss = 0.2835429310798645
Validation loss = 0.2888145446777344
Validation loss = 0.28675323724746704
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 5        |
| MaximumReturn | 198      |
| MinimumReturn | 121      |
| TotalSamples  | 23331    |
----------------------------
itr #6 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.29610830545425415
Validation loss = 0.2972176671028137
Validation loss = 0.29655393958091736
Validation loss = 0.29757335782051086
Validation loss = 0.2977612614631653
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.2968091666698456
Validation loss = 0.29683035612106323
Validation loss = 0.29453009366989136
Validation loss = 0.29371020197868347
Validation loss = 0.29703423380851746
Validation loss = 0.29999998211860657
Validation loss = 0.293623149394989
Validation loss = 0.2972416579723358
Validation loss = 0.29646140336990356
Validation loss = 0.2947685122489929
Validation loss = 0.29962030053138733
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.29595819115638733
Validation loss = 0.2951589524745941
Validation loss = 0.294666588306427
Validation loss = 0.2964477837085724
Validation loss = 0.2937142550945282
Validation loss = 0.29583585262298584
Validation loss = 0.29655730724334717
Validation loss = 0.296467661857605
Validation loss = 0.2954410910606384
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.29679715633392334
Validation loss = 0.29587528109550476
Validation loss = 0.2977483570575714
Validation loss = 0.2958449125289917
Validation loss = 0.29875439405441284
Validation loss = 0.29670608043670654
Validation loss = 0.29864999651908875
Validation loss = 0.2965187728404999
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.29780563712120056
Validation loss = 0.29651662707328796
Validation loss = 0.29481926560401917
Validation loss = 0.2952946424484253
Validation loss = 0.29569321870803833
Validation loss = 0.29665449261665344
Validation loss = 0.29695725440979004
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 178      |
| Iteration     | 6        |
| MaximumReturn | 198      |
| MinimumReturn | 125      |
| TotalSamples  | 26664    |
----------------------------
itr #7 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3069801926612854
Validation loss = 0.30049964785575867
Validation loss = 0.3029932379722595
Validation loss = 0.3012295961380005
Validation loss = 0.30361858010292053
Validation loss = 0.30438366532325745
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3023756146430969
Validation loss = 0.30603334307670593
Validation loss = 0.3026983141899109
Validation loss = 0.30627527832984924
Validation loss = 0.30216526985168457
Validation loss = 0.30123817920684814
Validation loss = 0.3042862117290497
Validation loss = 0.3019125163555145
Validation loss = 0.30484700202941895
Validation loss = 0.3075503706932068
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.304177850484848
Validation loss = 0.30304378271102905
Validation loss = 0.30325114727020264
Validation loss = 0.30193793773651123
Validation loss = 0.3012700080871582
Validation loss = 0.3044695556163788
Validation loss = 0.30187124013900757
Validation loss = 0.30470019578933716
Validation loss = 0.30372464656829834
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3047559857368469
Validation loss = 0.30199405550956726
Validation loss = 0.3043765723705292
Validation loss = 0.3032549023628235
Validation loss = 0.30446040630340576
Validation loss = 0.3041427731513977
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3030494451522827
Validation loss = 0.3038889169692993
Validation loss = 0.301780641078949
Validation loss = 0.30466410517692566
Validation loss = 0.3026929795742035
Validation loss = 0.30501827597618103
Validation loss = 0.30164387822151184
Validation loss = 0.302123486995697
Validation loss = 0.30347469449043274
Validation loss = 0.30166348814964294
Validation loss = 0.303376704454422
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 7        |
| MaximumReturn | 198      |
| MinimumReturn | 123      |
| TotalSamples  | 29997    |
----------------------------
itr #8 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.30858343839645386
Validation loss = 0.3062269389629364
Validation loss = 0.3060447573661804
Validation loss = 0.31120750308036804
Validation loss = 0.3062492907047272
Validation loss = 0.30714499950408936
Validation loss = 0.3081151843070984
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.30764004588127136
Validation loss = 0.30850186944007874
Validation loss = 0.3077641427516937
Validation loss = 0.3089580535888672
Validation loss = 0.308946430683136
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.30807924270629883
Validation loss = 0.3071618378162384
Validation loss = 0.30515846610069275
Validation loss = 0.3080529272556305
Validation loss = 0.3073386251926422
Validation loss = 0.3054641783237457
Validation loss = 0.3053705096244812
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.30603402853012085
Validation loss = 0.30600449442863464
Validation loss = 0.3060757517814636
Validation loss = 0.30692166090011597
Validation loss = 0.3083997070789337
Validation loss = 0.3080713152885437
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.30660173296928406
Validation loss = 0.30755946040153503
Validation loss = 0.30609002709388733
Validation loss = 0.3075151741504669
Validation loss = 0.3092498183250427
Validation loss = 0.3060556948184967
Validation loss = 0.3061281144618988
Validation loss = 0.3074931800365448
Validation loss = 0.306538850069046
Validation loss = 0.3071628510951996
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 8        |
| MaximumReturn | 198      |
| MinimumReturn | 133      |
| TotalSamples  | 33330    |
----------------------------
itr #9 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.31330132484436035
Validation loss = 0.3121300935745239
Validation loss = 0.31065404415130615
Validation loss = 0.31206756830215454
Validation loss = 0.31175193190574646
Validation loss = 0.312106728553772
Validation loss = 0.3120634853839874
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.31313225626945496
Validation loss = 0.31221815943717957
Validation loss = 0.31571707129478455
Validation loss = 0.312613844871521
Validation loss = 0.3147069215774536
Validation loss = 0.31483379006385803
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3136587142944336
Validation loss = 0.3110649883747101
Validation loss = 0.31038621068000793
Validation loss = 0.3110544681549072
Validation loss = 0.3115799129009247
Validation loss = 0.311958372592926
Validation loss = 0.3122762441635132
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3169405460357666
Validation loss = 0.3124009966850281
Validation loss = 0.3124770224094391
Validation loss = 0.31204426288604736
Validation loss = 0.31237804889678955
Validation loss = 0.31303006410598755
Validation loss = 0.31149813532829285
Validation loss = 0.3117198944091797
Validation loss = 0.3126837909221649
Validation loss = 0.31344330310821533
Validation loss = 0.31242501735687256
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3139449954032898
Validation loss = 0.3155190348625183
Validation loss = 0.31322968006134033
Validation loss = 0.3133673071861267
Validation loss = 0.3117424547672272
Validation loss = 0.311964750289917
Validation loss = 0.31190216541290283
Validation loss = 0.31274452805519104
Validation loss = 0.3122183680534363
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 9        |
| MaximumReturn | 198      |
| MinimumReturn | 124      |
| TotalSamples  | 36663    |
----------------------------
itr #10 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3181547522544861
Validation loss = 0.31746429204940796
Validation loss = 0.3213472068309784
Validation loss = 0.31590536236763
Validation loss = 0.31629830598831177
Validation loss = 0.31729477643966675
Validation loss = 0.31809595227241516
Validation loss = 0.31687453389167786
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.31704679131507874
Validation loss = 0.3166918158531189
Validation loss = 0.31764471530914307
Validation loss = 0.31776121258735657
Validation loss = 0.3173934519290924
Validation loss = 0.3218444585800171
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.31599584221839905
Validation loss = 0.315998911857605
Validation loss = 0.3152347803115845
Validation loss = 0.3163765072822571
Validation loss = 0.31672728061676025
Validation loss = 0.3169575035572052
Validation loss = 0.31553542613983154
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3178688883781433
Validation loss = 0.3168896436691284
Validation loss = 0.3161909282207489
Validation loss = 0.3156316876411438
Validation loss = 0.3192201852798462
Validation loss = 0.3173324763774872
Validation loss = 0.3182370066642761
Validation loss = 0.31685417890548706
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3157200217247009
Validation loss = 0.3158300518989563
Validation loss = 0.31792593002319336
Validation loss = 0.31658419966697693
Validation loss = 0.31654036045074463
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 177      |
| Iteration     | 10       |
| MaximumReturn | 197      |
| MinimumReturn | 125      |
| TotalSamples  | 39996    |
----------------------------
itr #11 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.32283443212509155
Validation loss = 0.3236631751060486
Validation loss = 0.32419124245643616
Validation loss = 0.32346636056900024
Validation loss = 0.3269643187522888
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3259728252887726
Validation loss = 0.3236287236213684
Validation loss = 0.32496580481529236
Validation loss = 0.3249461054801941
Validation loss = 0.3244056701660156
Validation loss = 0.3266468942165375
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3229060769081116
Validation loss = 0.32514435052871704
Validation loss = 0.3241094946861267
Validation loss = 0.3234114944934845
Validation loss = 0.3227018713951111
Validation loss = 0.3235246241092682
Validation loss = 0.324075311422348
Validation loss = 0.323897123336792
Validation loss = 0.3242092728614807
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3238266408443451
Validation loss = 0.32519158720970154
Validation loss = 0.3236507475376129
Validation loss = 0.3241097927093506
Validation loss = 0.3241983950138092
Validation loss = 0.3239818811416626
Validation loss = 0.32434695959091187
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3214234709739685
Validation loss = 0.3233964443206787
Validation loss = 0.32340681552886963
Validation loss = 0.3217191994190216
Validation loss = 0.32434481382369995
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 157      |
| Iteration     | 11       |
| MaximumReturn | 196      |
| MinimumReturn | 117      |
| TotalSamples  | 43329    |
----------------------------
itr #12 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3300892412662506
Validation loss = 0.32743215560913086
Validation loss = 0.3302568793296814
Validation loss = 0.32771846652030945
Validation loss = 0.32643812894821167
Validation loss = 0.3282836675643921
Validation loss = 0.32861635088920593
Validation loss = 0.32783883810043335
Validation loss = 0.32878297567367554
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3291494846343994
Validation loss = 0.3285466134548187
Validation loss = 0.3295506536960602
Validation loss = 0.32907718420028687
Validation loss = 0.3305017352104187
Validation loss = 0.33099207282066345
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.32909002900123596
Validation loss = 0.3306909203529358
Validation loss = 0.3285984992980957
Validation loss = 0.32878556847572327
Validation loss = 0.32944750785827637
Validation loss = 0.32921740412712097
Validation loss = 0.33100929856300354
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3270081877708435
Validation loss = 0.3279925584793091
Validation loss = 0.3279969096183777
Validation loss = 0.3309623897075653
Validation loss = 0.33588290214538574
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.32659581303596497
Validation loss = 0.3277808129787445
Validation loss = 0.3274213671684265
Validation loss = 0.3289284408092499
Validation loss = 0.3296428918838501
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 12       |
| MaximumReturn | 198      |
| MinimumReturn | 124      |
| TotalSamples  | 46662    |
----------------------------
itr #13 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3327467441558838
Validation loss = 0.3317793011665344
Validation loss = 0.33240559697151184
Validation loss = 0.332466721534729
Validation loss = 0.3338818848133087
Validation loss = 0.33385559916496277
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.33419182896614075
Validation loss = 0.333446204662323
Validation loss = 0.3321290910243988
Validation loss = 0.33289340138435364
Validation loss = 0.33421555161476135
Validation loss = 0.33337458968162537
Validation loss = 0.3349038064479828
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3336520791053772
Validation loss = 0.33186423778533936
Validation loss = 0.33493295311927795
Validation loss = 0.33300626277923584
Validation loss = 0.3338140845298767
Validation loss = 0.3356075584888458
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.33395084738731384
Validation loss = 0.33361849188804626
Validation loss = 0.3331981599330902
Validation loss = 0.33412879705429077
Validation loss = 0.3360183835029602
Validation loss = 0.3340246081352234
Validation loss = 0.33461663126945496
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.332429975271225
Validation loss = 0.3311571180820465
Validation loss = 0.33469411730766296
Validation loss = 0.33241137862205505
Validation loss = 0.33326438069343567
Validation loss = 0.33286744356155396
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 13       |
| MaximumReturn | 197      |
| MinimumReturn | 118      |
| TotalSamples  | 49995    |
----------------------------
itr #14 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.33837878704071045
Validation loss = 0.33610886335372925
Validation loss = 0.3373361825942993
Validation loss = 0.33737170696258545
Validation loss = 0.33900320529937744
Validation loss = 0.33793365955352783
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.34146443009376526
Validation loss = 0.3373292088508606
Validation loss = 0.3393537402153015
Validation loss = 0.33824795484542847
Validation loss = 0.33706170320510864
Validation loss = 0.33786341547966003
Validation loss = 0.33723562955856323
Validation loss = 0.3383554220199585
Validation loss = 0.339465856552124
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.33621466159820557
Validation loss = 0.33594489097595215
Validation loss = 0.3369589149951935
Validation loss = 0.33721116185188293
Validation loss = 0.33658888936042786
Validation loss = 0.3399427831172943
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3376234173774719
Validation loss = 0.33846431970596313
Validation loss = 0.3367604911327362
Validation loss = 0.3364359140396118
Validation loss = 0.3397451341152191
Validation loss = 0.3384797275066376
Validation loss = 0.3377339541912079
Validation loss = 0.337985634803772
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.33437904715538025
Validation loss = 0.334057092666626
Validation loss = 0.33622726798057556
Validation loss = 0.33514484763145447
Validation loss = 0.3386138677597046
Validation loss = 0.33704251050949097
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 179      |
| Iteration     | 14       |
| MaximumReturn | 198      |
| MinimumReturn | 123      |
| TotalSamples  | 53328    |
----------------------------
itr #15 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3407290279865265
Validation loss = 0.34186333417892456
Validation loss = 0.3400683104991913
Validation loss = 0.34288856387138367
Validation loss = 0.34207403659820557
Validation loss = 0.3410607874393463
Validation loss = 0.34156352281570435
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.34240788221359253
Validation loss = 0.3412136435508728
Validation loss = 0.34294766187667847
Validation loss = 0.34244441986083984
Validation loss = 0.3434445559978485
Validation loss = 0.3460521996021271
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3402167856693268
Validation loss = 0.33971118927001953
Validation loss = 0.34042888879776
Validation loss = 0.33959436416625977
Validation loss = 0.3420222997665405
Validation loss = 0.34237563610076904
Validation loss = 0.3419785797595978
Validation loss = 0.3422219157218933
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3408784866333008
Validation loss = 0.341146320104599
Validation loss = 0.3412538766860962
Validation loss = 0.3428386449813843
Validation loss = 0.3429030776023865
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3376394510269165
Validation loss = 0.3396129310131073
Validation loss = 0.33843520283699036
Validation loss = 0.33897271752357483
Validation loss = 0.34003955125808716
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 163      |
| Iteration     | 15       |
| MaximumReturn | 197      |
| MinimumReturn | 118      |
| TotalSamples  | 56661    |
----------------------------
itr #16 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3453783690929413
Validation loss = 0.34620699286460876
Validation loss = 0.3455515205860138
Validation loss = 0.3448278605937958
Validation loss = 0.34705573320388794
Validation loss = 0.34732308983802795
Validation loss = 0.3463357388973236
Validation loss = 0.3463917374610901
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3470413386821747
Validation loss = 0.3458618223667145
Validation loss = 0.34699389338493347
Validation loss = 0.34831055998802185
Validation loss = 0.348646879196167
Validation loss = 0.34823963046073914
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.34484729170799255
Validation loss = 0.3437630236148834
Validation loss = 0.3441278338432312
Validation loss = 0.34655365347862244
Validation loss = 0.3459542691707611
Validation loss = 0.34723976254463196
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3449898362159729
Validation loss = 0.3457764685153961
Validation loss = 0.34430140256881714
Validation loss = 0.3451051414012909
Validation loss = 0.34504348039627075
Validation loss = 0.347549706697464
Validation loss = 0.3501887917518616
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.34221819043159485
Validation loss = 0.34322303533554077
Validation loss = 0.34356942772865295
Validation loss = 0.34255450963974
Validation loss = 0.34166595339775085
Validation loss = 0.3437276780605316
Validation loss = 0.3451336920261383
Validation loss = 0.34482342004776
Validation loss = 0.34540969133377075
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 16       |
| MaximumReturn | 197      |
| MinimumReturn | 128      |
| TotalSamples  | 59994    |
----------------------------
itr #17 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3480093777179718
Validation loss = 0.34889063239097595
Validation loss = 0.34883344173431396
Validation loss = 0.35278767347335815
Validation loss = 0.3505224883556366
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3495738208293915
Validation loss = 0.34897106885910034
Validation loss = 0.3487134277820587
Validation loss = 0.3505198061466217
Validation loss = 0.35148322582244873
Validation loss = 0.35162588953971863
Validation loss = 0.3547486364841461
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3481958210468292
Validation loss = 0.3496343195438385
Validation loss = 0.3500118851661682
Validation loss = 0.3495834171772003
Validation loss = 0.3496984541416168
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3483639657497406
Validation loss = 0.3475781977176666
Validation loss = 0.348762571811676
Validation loss = 0.3502499759197235
Validation loss = 0.35066497325897217
Validation loss = 0.35134369134902954
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3459845185279846
Validation loss = 0.3476172089576721
Validation loss = 0.3481246829032898
Validation loss = 0.3474343717098236
Validation loss = 0.348890483379364
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 17       |
| MaximumReturn | 197      |
| MinimumReturn | 123      |
| TotalSamples  | 63327    |
----------------------------
itr #18 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3544749319553375
Validation loss = 0.35270440578460693
Validation loss = 0.35233208537101746
Validation loss = 0.3538377285003662
Validation loss = 0.35460397601127625
Validation loss = 0.35402974486351013
Validation loss = 0.35424694418907166
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.35337120294570923
Validation loss = 0.3539894223213196
Validation loss = 0.35818126797676086
Validation loss = 0.35337597131729126
Validation loss = 0.35531681776046753
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.351862370967865
Validation loss = 0.3518228232860565
Validation loss = 0.35218048095703125
Validation loss = 0.3534802496433258
Validation loss = 0.35491496324539185
Validation loss = 0.35292622447013855
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3543214499950409
Validation loss = 0.35077181458473206
Validation loss = 0.35207465291023254
Validation loss = 0.352760910987854
Validation loss = 0.3525574207305908
Validation loss = 0.35345137119293213
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3501228392124176
Validation loss = 0.35144466161727905
Validation loss = 0.3499733507633209
Validation loss = 0.35124456882476807
Validation loss = 0.35098305344581604
Validation loss = 0.3531331717967987
Validation loss = 0.3517450988292694
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 18       |
| MaximumReturn | 197      |
| MinimumReturn | 130      |
| TotalSamples  | 66660    |
----------------------------
itr #19 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.35438740253448486
Validation loss = 0.3591592311859131
Validation loss = 0.35583117604255676
Validation loss = 0.3559562861919403
Validation loss = 0.3569346070289612
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3558835983276367
Validation loss = 0.35495397448539734
Validation loss = 0.3559013307094574
Validation loss = 0.3571122884750366
Validation loss = 0.35572946071624756
Validation loss = 0.3580860495567322
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.35544317960739136
Validation loss = 0.3572038412094116
Validation loss = 0.35486653447151184
Validation loss = 0.3551062345504761
Validation loss = 0.3549542725086212
Validation loss = 0.3559589982032776
Validation loss = 0.3550563454627991
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3548509478569031
Validation loss = 0.35365426540374756
Validation loss = 0.35562223196029663
Validation loss = 0.3554692268371582
Validation loss = 0.35648277401924133
Validation loss = 0.35868775844573975
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.35216620564460754
Validation loss = 0.3547772169113159
Validation loss = 0.35462263226509094
Validation loss = 0.35499057173728943
Validation loss = 0.3540002405643463
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 19       |
| MaximumReturn | 196      |
| MinimumReturn | 110      |
| TotalSamples  | 69993    |
----------------------------
itr #20 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.35876619815826416
Validation loss = 0.3570217192173004
Validation loss = 0.35719534754753113
Validation loss = 0.3588973581790924
Validation loss = 0.358982652425766
Validation loss = 0.3578030467033386
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3618975877761841
Validation loss = 0.3570052683353424
Validation loss = 0.36015018820762634
Validation loss = 0.3591707646846771
Validation loss = 0.3614519536495209
Validation loss = 0.3612494170665741
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3571073114871979
Validation loss = 0.36044764518737793
Validation loss = 0.35682231187820435
Validation loss = 0.36013466119766235
Validation loss = 0.36344248056411743
Validation loss = 0.35953935980796814
Validation loss = 0.36084097623825073
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.35858359932899475
Validation loss = 0.3576183617115021
Validation loss = 0.3580717444419861
Validation loss = 0.3581443428993225
Validation loss = 0.35995256900787354
Validation loss = 0.35939040780067444
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3554491400718689
Validation loss = 0.35658472776412964
Validation loss = 0.35720643401145935
Validation loss = 0.35641708970069885
Validation loss = 0.35653480887413025
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 20       |
| MaximumReturn | 197      |
| MinimumReturn | 115      |
| TotalSamples  | 73326    |
----------------------------
itr #21 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.36084625124931335
Validation loss = 0.3604883551597595
Validation loss = 0.36122989654541016
Validation loss = 0.3616771996021271
Validation loss = 0.36148184537887573
Validation loss = 0.36152535676956177
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.36116522550582886
Validation loss = 0.36230409145355225
Validation loss = 0.3620249629020691
Validation loss = 0.36328867077827454
Validation loss = 0.3620932400226593
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.362662136554718
Validation loss = 0.36212602257728577
Validation loss = 0.36062586307525635
Validation loss = 0.3637453615665436
Validation loss = 0.3615136742591858
Validation loss = 0.36404848098754883
Validation loss = 0.363255113363266
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3623294532299042
Validation loss = 0.3605477213859558
Validation loss = 0.3617445230484009
Validation loss = 0.3621910810470581
Validation loss = 0.3613916039466858
Validation loss = 0.3622814118862152
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.35966604948043823
Validation loss = 0.3586384057998657
Validation loss = 0.3594236969947815
Validation loss = 0.35882413387298584
Validation loss = 0.3607596158981323
Validation loss = 0.3596840798854828
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 21       |
| MaximumReturn | 197      |
| MinimumReturn | 132      |
| TotalSamples  | 76659    |
----------------------------
itr #22 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.36151739954948425
Validation loss = 0.36296990513801575
Validation loss = 0.3650945723056793
Validation loss = 0.36229729652404785
Validation loss = 0.36171430349349976
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.36117467284202576
Validation loss = 0.36310869455337524
Validation loss = 0.3637925386428833
Validation loss = 0.3625451326370239
Validation loss = 0.3649870455265045
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.36299067735671997
Validation loss = 0.3629664182662964
Validation loss = 0.36408698558807373
Validation loss = 0.3655126988887787
Validation loss = 0.36432069540023804
Validation loss = 0.36334970593452454
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.36282432079315186
Validation loss = 0.3614438474178314
Validation loss = 0.36423128843307495
Validation loss = 0.3624817728996277
Validation loss = 0.3663192689418793
Validation loss = 0.3652004301548004
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3594990372657776
Validation loss = 0.35904398560523987
Validation loss = 0.3596740663051605
Validation loss = 0.35987985134124756
Validation loss = 0.36027997732162476
Validation loss = 0.36120229959487915
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 163      |
| Iteration     | 22       |
| MaximumReturn | 197      |
| MinimumReturn | 119      |
| TotalSamples  | 79992    |
----------------------------
itr #23 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.36412662267684937
Validation loss = 0.3631788194179535
Validation loss = 0.3636415898799896
Validation loss = 0.36293095350265503
Validation loss = 0.366504967212677
Validation loss = 0.36331215500831604
Validation loss = 0.36511459946632385
Validation loss = 0.365943044424057
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.364865243434906
Validation loss = 0.36379995942115784
Validation loss = 0.36653566360473633
Validation loss = 0.3648097813129425
Validation loss = 0.36512255668640137
Validation loss = 0.3666304647922516
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3672640919685364
Validation loss = 0.36707884073257446
Validation loss = 0.36713555455207825
Validation loss = 0.3689730167388916
Validation loss = 0.36631637811660767
Validation loss = 0.36832040548324585
Validation loss = 0.3675635755062103
Validation loss = 0.36792317032814026
Validation loss = 0.3684067726135254
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3652965724468231
Validation loss = 0.3654201924800873
Validation loss = 0.3664592504501343
Validation loss = 0.36687350273132324
Validation loss = 0.3658648729324341
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.36216697096824646
Validation loss = 0.3638835847377777
Validation loss = 0.362731397151947
Validation loss = 0.3634796738624573
Validation loss = 0.36426693201065063
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 23       |
| MaximumReturn | 197      |
| MinimumReturn | 125      |
| TotalSamples  | 83325    |
----------------------------
itr #24 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3657984435558319
Validation loss = 0.36531972885131836
Validation loss = 0.36625927686691284
Validation loss = 0.3666650056838989
Validation loss = 0.36716359853744507
Validation loss = 0.3665609061717987
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3651108741760254
Validation loss = 0.3661075830459595
Validation loss = 0.3667565882205963
Validation loss = 0.36677324771881104
Validation loss = 0.369323194026947
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3702298700809479
Validation loss = 0.36912181973457336
Validation loss = 0.37017595767974854
Validation loss = 0.37104877829551697
Validation loss = 0.3697904646396637
Validation loss = 0.37062108516693115
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.366866797208786
Validation loss = 0.3669728636741638
Validation loss = 0.36514171957969666
Validation loss = 0.3662746250629425
Validation loss = 0.36653512716293335
Validation loss = 0.36854779720306396
Validation loss = 0.36857670545578003
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.36448758840560913
Validation loss = 0.36455148458480835
Validation loss = 0.3643052577972412
Validation loss = 0.36536145210266113
Validation loss = 0.3660803437232971
Validation loss = 0.3666757345199585
Validation loss = 0.3661327362060547
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 24       |
| MaximumReturn | 197      |
| MinimumReturn | 121      |
| TotalSamples  | 86658    |
----------------------------
itr #25 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.36991479992866516
Validation loss = 0.36917388439178467
Validation loss = 0.36947402358055115
Validation loss = 0.37008699774742126
Validation loss = 0.3728869557380676
Validation loss = 0.37051820755004883
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3681761920452118
Validation loss = 0.3691074252128601
Validation loss = 0.36899128556251526
Validation loss = 0.3695068061351776
Validation loss = 0.37117236852645874
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.37463998794555664
Validation loss = 0.3719569146633148
Validation loss = 0.3734467625617981
Validation loss = 0.37300392985343933
Validation loss = 0.3726707696914673
Validation loss = 0.37260448932647705
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3692227005958557
Validation loss = 0.3711695075035095
Validation loss = 0.3712731599807739
Validation loss = 0.3715707063674927
Validation loss = 0.3701688349246979
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.36774981021881104
Validation loss = 0.367757648229599
Validation loss = 0.36852023005485535
Validation loss = 0.3688172399997711
Validation loss = 0.3691660761833191
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 25       |
| MaximumReturn | 196      |
| MinimumReturn | 117      |
| TotalSamples  | 89991    |
----------------------------
itr #26 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3725278973579407
Validation loss = 0.37153592705726624
Validation loss = 0.37288954854011536
Validation loss = 0.37269890308380127
Validation loss = 0.3729268014431
Validation loss = 0.3735557496547699
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.37276989221572876
Validation loss = 0.37158825993537903
Validation loss = 0.371227890253067
Validation loss = 0.3738615810871124
Validation loss = 0.37279361486434937
Validation loss = 0.37145641446113586
Validation loss = 0.3742704689502716
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.37502261996269226
Validation loss = 0.37324151396751404
Validation loss = 0.3745855689048767
Validation loss = 0.37509840726852417
Validation loss = 0.3758234977722168
Validation loss = 0.37756258249282837
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.37342384457588196
Validation loss = 0.37233832478523254
Validation loss = 0.37291818857192993
Validation loss = 0.3726678490638733
Validation loss = 0.3739428222179413
Validation loss = 0.3731175661087036
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.36872589588165283
Validation loss = 0.3710423409938812
Validation loss = 0.3713022470474243
Validation loss = 0.36985188722610474
Validation loss = 0.3722453713417053
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 26       |
| MaximumReturn | 196      |
| MinimumReturn | 115      |
| TotalSamples  | 93324    |
----------------------------
itr #27 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3736565113067627
Validation loss = 0.37522634863853455
Validation loss = 0.37559786438941956
Validation loss = 0.3746356666088104
Validation loss = 0.3745841681957245
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.37434622645378113
Validation loss = 0.3763144016265869
Validation loss = 0.37418946623802185
Validation loss = 0.3749983310699463
Validation loss = 0.37591999769210815
Validation loss = 0.37691786885261536
Validation loss = 0.37625983357429504
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.37980568408966064
Validation loss = 0.37689512968063354
Validation loss = 0.3783797025680542
Validation loss = 0.37770378589630127
Validation loss = 0.37688884139060974
Validation loss = 0.37744373083114624
Validation loss = 0.3785242736339569
Validation loss = 0.3793000280857086
Validation loss = 0.3802279233932495
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3762504458427429
Validation loss = 0.37573567032814026
Validation loss = 0.3743431270122528
Validation loss = 0.3752346932888031
Validation loss = 0.37643665075302124
Validation loss = 0.3767383098602295
Validation loss = 0.3774758577346802
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.37142178416252136
Validation loss = 0.37063491344451904
Validation loss = 0.37144386768341064
Validation loss = 0.37362968921661377
Validation loss = 0.37203484773635864
Validation loss = 0.37332361936569214
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 27       |
| MaximumReturn | 197      |
| MinimumReturn | 132      |
| TotalSamples  | 96657    |
----------------------------
itr #28 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.37604036927223206
Validation loss = 0.3743870258331299
Validation loss = 0.3775043189525604
Validation loss = 0.377253919839859
Validation loss = 0.3766374886035919
Validation loss = 0.37792035937309265
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3762906491756439
Validation loss = 0.375972181558609
Validation loss = 0.37804940342903137
Validation loss = 0.3778190612792969
Validation loss = 0.379230260848999
Validation loss = 0.3762928545475006
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3794938623905182
Validation loss = 0.38036033511161804
Validation loss = 0.38019049167633057
Validation loss = 0.3802890479564667
Validation loss = 0.3804859220981598
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.37834951281547546
Validation loss = 0.3779169023036957
Validation loss = 0.37724438309669495
Validation loss = 0.37747618556022644
Validation loss = 0.3791753351688385
Validation loss = 0.378690242767334
Validation loss = 0.3796415328979492
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.37484288215637207
Validation loss = 0.3757871687412262
Validation loss = 0.37578150629997253
Validation loss = 0.37519189715385437
Validation loss = 0.37641826272010803
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 28       |
| MaximumReturn | 198      |
| MinimumReturn | 140      |
| TotalSamples  | 99990    |
----------------------------
itr #29 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.377146452665329
Validation loss = 0.3771325647830963
Validation loss = 0.3778667449951172
Validation loss = 0.3788191080093384
Validation loss = 0.38140448927879333
Validation loss = 0.37886419892311096
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3790668249130249
Validation loss = 0.3768950402736664
Validation loss = 0.37941867113113403
Validation loss = 0.3787498474121094
Validation loss = 0.3804958760738373
Validation loss = 0.3790903389453888
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.380630224943161
Validation loss = 0.38113757967948914
Validation loss = 0.3803047239780426
Validation loss = 0.3811025321483612
Validation loss = 0.3807872533798218
Validation loss = 0.3826148509979248
Validation loss = 0.3823279142379761
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.37961456179618835
Validation loss = 0.37889325618743896
Validation loss = 0.3795031011104584
Validation loss = 0.3808867335319519
Validation loss = 0.37945643067359924
Validation loss = 0.38172873854637146
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.37547817826271057
Validation loss = 0.3764806389808655
Validation loss = 0.3756673336029053
Validation loss = 0.37763580679893494
Validation loss = 0.3769737184047699
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 29       |
| MaximumReturn | 197      |
| MinimumReturn | 120      |
| TotalSamples  | 103323   |
----------------------------
itr #30 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.37868356704711914
Validation loss = 0.38049423694610596
Validation loss = 0.3781691789627075
Validation loss = 0.38199758529663086
Validation loss = 0.3809845745563507
Validation loss = 0.38046661019325256
Validation loss = 0.38142773509025574
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3793919086456299
Validation loss = 0.38053053617477417
Validation loss = 0.3799296021461487
Validation loss = 0.38181960582733154
Validation loss = 0.3804144859313965
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3845229744911194
Validation loss = 0.3836710751056671
Validation loss = 0.3835779130458832
Validation loss = 0.38275742530822754
Validation loss = 0.38305234909057617
Validation loss = 0.38282662630081177
Validation loss = 0.3858247697353363
Validation loss = 0.3839350640773773
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.38153278827667236
Validation loss = 0.3815968930721283
Validation loss = 0.3807845711708069
Validation loss = 0.37980639934539795
Validation loss = 0.3819471001625061
Validation loss = 0.38196447491645813
Validation loss = 0.3846506178379059
Validation loss = 0.3836563229560852
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.37523818016052246
Validation loss = 0.3778431713581085
Validation loss = 0.3771408200263977
Validation loss = 0.37700098752975464
Validation loss = 0.3770407736301422
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 30       |
| MaximumReturn | 197      |
| MinimumReturn | 118      |
| TotalSamples  | 106656   |
----------------------------
itr #31 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.38309457898139954
Validation loss = 0.3806873559951782
Validation loss = 0.3819539546966553
Validation loss = 0.3811514377593994
Validation loss = 0.38117510080337524
Validation loss = 0.38319072127342224
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.380537211894989
Validation loss = 0.38092294335365295
Validation loss = 0.3803638815879822
Validation loss = 0.38186749815940857
Validation loss = 0.38076937198638916
Validation loss = 0.3820018470287323
Validation loss = 0.38288113474845886
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3847518563270569
Validation loss = 0.38443636894226074
Validation loss = 0.3862166404724121
Validation loss = 0.3847048878669739
Validation loss = 0.3860522210597992
Validation loss = 0.3866252899169922
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.383725643157959
Validation loss = 0.3843432068824768
Validation loss = 0.38430270552635193
Validation loss = 0.3837968707084656
Validation loss = 0.38424062728881836
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.37708044052124023
Validation loss = 0.3774750828742981
Validation loss = 0.3810743987560272
Validation loss = 0.3774012327194214
Validation loss = 0.37899360060691833
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 31       |
| MaximumReturn | 197      |
| MinimumReturn | 116      |
| TotalSamples  | 109989   |
----------------------------
itr #32 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3832733929157257
Validation loss = 0.38172316551208496
Validation loss = 0.3821966350078583
Validation loss = 0.383370965719223
Validation loss = 0.3831609785556793
Validation loss = 0.38233697414398193
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.38215187191963196
Validation loss = 0.3833661377429962
Validation loss = 0.38391056656837463
Validation loss = 0.38371750712394714
Validation loss = 0.38278234004974365
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3872617483139038
Validation loss = 0.38511666655540466
Validation loss = 0.3865407705307007
Validation loss = 0.3871813118457794
Validation loss = 0.38809630274772644
Validation loss = 0.38635164499282837
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.38474810123443604
Validation loss = 0.3846975564956665
Validation loss = 0.3846348524093628
Validation loss = 0.38613128662109375
Validation loss = 0.3841296136379242
Validation loss = 0.3862172067165375
Validation loss = 0.385013610124588
Validation loss = 0.38832518458366394
Validation loss = 0.38687220215797424
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3800663650035858
Validation loss = 0.3786662817001343
Validation loss = 0.37837710976600647
Validation loss = 0.3797135651111603
Validation loss = 0.38062599301338196
Validation loss = 0.3801509141921997
Validation loss = 0.38176214694976807
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 32       |
| MaximumReturn | 197      |
| MinimumReturn | 122      |
| TotalSamples  | 113322   |
----------------------------
itr #33 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3842937648296356
Validation loss = 0.38364377617836
Validation loss = 0.38380879163742065
Validation loss = 0.38423341512680054
Validation loss = 0.38624313473701477
Validation loss = 0.38451024889945984
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.38431215286254883
Validation loss = 0.38489246368408203
Validation loss = 0.38320615887641907
Validation loss = 0.3833884596824646
Validation loss = 0.384777307510376
Validation loss = 0.3835817873477936
Validation loss = 0.38532310724258423
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.38555100560188293
Validation loss = 0.38614362478256226
Validation loss = 0.3872984051704407
Validation loss = 0.3880468010902405
Validation loss = 0.38741329312324524
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.38472604751586914
Validation loss = 0.3854053020477295
Validation loss = 0.38593536615371704
Validation loss = 0.3873389661312103
Validation loss = 0.38788264989852905
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3811197280883789
Validation loss = 0.38233423233032227
Validation loss = 0.3814656436443329
Validation loss = 0.38231590390205383
Validation loss = 0.3817870616912842
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 33       |
| MaximumReturn | 198      |
| MinimumReturn | 123      |
| TotalSamples  | 116655   |
----------------------------
itr #34 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.38492274284362793
Validation loss = 0.3860551118850708
Validation loss = 0.38766735792160034
Validation loss = 0.3865329623222351
Validation loss = 0.38679301738739014
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3844740688800812
Validation loss = 0.38330334424972534
Validation loss = 0.38594913482666016
Validation loss = 0.388817697763443
Validation loss = 0.3857896029949188
Validation loss = 0.38561511039733887
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.38695430755615234
Validation loss = 0.3879278302192688
Validation loss = 0.3889520764350891
Validation loss = 0.38770613074302673
Validation loss = 0.3875825107097626
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.38837549090385437
Validation loss = 0.3869057595729828
Validation loss = 0.3882550299167633
Validation loss = 0.3864419758319855
Validation loss = 0.3892167806625366
Validation loss = 0.38962340354919434
Validation loss = 0.3886584937572479
Validation loss = 0.3899923861026764
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3824976980686188
Validation loss = 0.3822799324989319
Validation loss = 0.3837466537952423
Validation loss = 0.3836168646812439
Validation loss = 0.3825415372848511
Validation loss = 0.385656476020813
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 34       |
| MaximumReturn | 198      |
| MinimumReturn | 96.9     |
| TotalSamples  | 119988   |
----------------------------
itr #35 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3856016993522644
Validation loss = 0.38577839732170105
Validation loss = 0.3869653344154358
Validation loss = 0.38561269640922546
Validation loss = 0.38639548420906067
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.38502243161201477
Validation loss = 0.38475707173347473
Validation loss = 0.3861997425556183
Validation loss = 0.38625988364219666
Validation loss = 0.3865603506565094
Validation loss = 0.38661038875579834
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3870032727718353
Validation loss = 0.38765135407447815
Validation loss = 0.3867465555667877
Validation loss = 0.3881538510322571
Validation loss = 0.3885246515274048
Validation loss = 0.3896142244338989
Validation loss = 0.38870441913604736
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3890208899974823
Validation loss = 0.38915979862213135
Validation loss = 0.38736799359321594
Validation loss = 0.3899482786655426
Validation loss = 0.3888508677482605
Validation loss = 0.3903394043445587
Validation loss = 0.3899082541465759
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.38403260707855225
Validation loss = 0.38325658440589905
Validation loss = 0.3826355040073395
Validation loss = 0.38254281878471375
Validation loss = 0.38215890526771545
Validation loss = 0.3850153684616089
Validation loss = 0.38568347692489624
Validation loss = 0.38689497113227844
Validation loss = 0.3849951922893524
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 35       |
| MaximumReturn | 198      |
| MinimumReturn | 137      |
| TotalSamples  | 123321   |
----------------------------
itr #36 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3862980008125305
Validation loss = 0.3847055733203888
Validation loss = 0.3867957592010498
Validation loss = 0.3880820572376251
Validation loss = 0.3868912160396576
Validation loss = 0.3876096308231354
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3878215551376343
Validation loss = 0.38737642765045166
Validation loss = 0.3877313435077667
Validation loss = 0.3899286389350891
Validation loss = 0.38624128699302673
Validation loss = 0.3889721632003784
Validation loss = 0.38999152183532715
Validation loss = 0.38828733563423157
Validation loss = 0.3893553614616394
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.38995233178138733
Validation loss = 0.38928115367889404
Validation loss = 0.3883551061153412
Validation loss = 0.38920795917510986
Validation loss = 0.38895827531814575
Validation loss = 0.38938477635383606
Validation loss = 0.39110857248306274
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.39062079787254333
Validation loss = 0.3894467055797577
Validation loss = 0.39025819301605225
Validation loss = 0.39088478684425354
Validation loss = 0.3908809721469879
Validation loss = 0.3895823061466217
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.38592657446861267
Validation loss = 0.38486966490745544
Validation loss = 0.3862849771976471
Validation loss = 0.3873588442802429
Validation loss = 0.385143905878067
Validation loss = 0.38744014501571655
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 36       |
| MaximumReturn | 198      |
| MinimumReturn | 99.4     |
| TotalSamples  | 126654   |
----------------------------
itr #37 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3858147859573364
Validation loss = 0.38607823848724365
Validation loss = 0.38520845770835876
Validation loss = 0.38709694147109985
Validation loss = 0.3854726254940033
Validation loss = 0.38693368434906006
Validation loss = 0.38814717531204224
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3880176246166229
Validation loss = 0.38859403133392334
Validation loss = 0.3877634108066559
Validation loss = 0.38926762342453003
Validation loss = 0.3880535960197449
Validation loss = 0.38951048254966736
Validation loss = 0.39129993319511414
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3893498480319977
Validation loss = 0.38797104358673096
Validation loss = 0.3890784680843353
Validation loss = 0.3890652358531952
Validation loss = 0.39014220237731934
Validation loss = 0.38936442136764526
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3921387195587158
Validation loss = 0.39064937829971313
Validation loss = 0.3897375464439392
Validation loss = 0.39195942878723145
Validation loss = 0.3898705840110779
Validation loss = 0.3938286304473877
Validation loss = 0.39144381880760193
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3869180977344513
Validation loss = 0.3860887587070465
Validation loss = 0.3859148919582367
Validation loss = 0.38651242852211
Validation loss = 0.38705945014953613
Validation loss = 0.3875155448913574
Validation loss = 0.387382447719574
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 37       |
| MaximumReturn | 198      |
| MinimumReturn | 118      |
| TotalSamples  | 129987   |
----------------------------
itr #38 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3870083689689636
Validation loss = 0.3861790597438812
Validation loss = 0.38820305466651917
Validation loss = 0.38632750511169434
Validation loss = 0.3879064917564392
Validation loss = 0.38864395022392273
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.39037835597991943
Validation loss = 0.39012810587882996
Validation loss = 0.3912501037120819
Validation loss = 0.39010968804359436
Validation loss = 0.3931080102920532
Validation loss = 0.390243262052536
Validation loss = 0.3907245397567749
Validation loss = 0.39144963026046753
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.39025476574897766
Validation loss = 0.3897886574268341
Validation loss = 0.3901296555995941
Validation loss = 0.3896314799785614
Validation loss = 0.39103078842163086
Validation loss = 0.39073601365089417
Validation loss = 0.39281028509140015
Validation loss = 0.3896120488643646
Validation loss = 0.39030328392982483
Validation loss = 0.3922763168811798
Validation loss = 0.39134323596954346
Validation loss = 0.3946724832057953
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3931913673877716
Validation loss = 0.39092981815338135
Validation loss = 0.39368584752082825
Validation loss = 0.3917064666748047
Validation loss = 0.3938246965408325
Validation loss = 0.3931431174278259
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3868370056152344
Validation loss = 0.3874502182006836
Validation loss = 0.3882679045200348
Validation loss = 0.387752503156662
Validation loss = 0.3886881172657013
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 38       |
| MaximumReturn | 198      |
| MinimumReturn | 128      |
| TotalSamples  | 133320   |
----------------------------
itr #39 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.38825082778930664
Validation loss = 0.3879384398460388
Validation loss = 0.38750773668289185
Validation loss = 0.38907212018966675
Validation loss = 0.38773611187934875
Validation loss = 0.392025351524353
Validation loss = 0.38823074102401733
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3918248116970062
Validation loss = 0.3914613127708435
Validation loss = 0.3919949531555176
Validation loss = 0.3903455138206482
Validation loss = 0.3916156589984894
Validation loss = 0.3926224112510681
Validation loss = 0.39399081468582153
Validation loss = 0.3931775391101837
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3920360803604126
Validation loss = 0.3925631642341614
Validation loss = 0.39224767684936523
Validation loss = 0.39170122146606445
Validation loss = 0.39409947395324707
Validation loss = 0.3924562931060791
Validation loss = 0.39444223046302795
Validation loss = 0.3953876197338104
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3930978775024414
Validation loss = 0.3924129009246826
Validation loss = 0.391943097114563
Validation loss = 0.39234018325805664
Validation loss = 0.3935675024986267
Validation loss = 0.39513295888900757
Validation loss = 0.39225316047668457
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.38731685280799866
Validation loss = 0.38804012537002563
Validation loss = 0.3879971206188202
Validation loss = 0.3891984820365906
Validation loss = 0.38796284794807434
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 39       |
| MaximumReturn | 198      |
| MinimumReturn | 136      |
| TotalSamples  | 136653   |
----------------------------
