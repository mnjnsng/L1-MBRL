Logging to experiments/pendulum/test-dir-exp/Mon-21-Nov-2022-08-11-44-PM-CST_pendulum_trpo_iteration_20_seed2231
Print configuration .....
{'env_name': 'pendulum', 'random_seeds': [3214, 2431, 2531, 2231], 'save_variables': False, 'model_save_dir': '/tmp/pendulum_models/', 'restore_variables': False, 'start_onpol_iter': 0, 'onpol_iters': 40, 'num_path_random': 25, 'num_path_onpol': 25, 'env_horizon': 200, 'max_train_data': 200000, 'max_val_data': 100000, 'discard_ratio': 0.0, 'dynamics': {'pre_training': {'mode': 'intrinsic_reward', 'itr': 0, 'policy_itr': 20}, 'model': 'nn', 'ensemble': True, 'ensemble_model_count': 5, 'enable_particle_ensemble': True, 'particles': 5, 'obs_var': 1.0, 'intrinsic_reward_coeff': 1.0, 'ita': 1.0, 'mode': 'random', 'val': True, 'n_layers': 4, 'hidden_size': 1000, 'activation': 'relu', 'batch_size': 1000, 'learning_rate': 0.001, 'reg_coeff': 0.0, 'epochs': 200, 'kfac_params': {'learning_rate': 0.1, 'damping': 0.001, 'momentum': 0.9, 'kl_clip': 0.0001, 'cov_ema_decay': 0.99}}, 'policy': {'network_shape': [64, 64], 'init_logstd': 0.0, 'activation': 'tanh', 'reinitialize_every_itr': False}, 'trpo': {'horizon': 200, 'gamma': 0.99, 'step_size': 0.01, 'iterations': 20, 'batch_size': 50000, 'gae': 0.95, 'visualization': False, 'visualize_iterations': [0]}, 'algo': 'trpo'}
Generating random rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating random rollouts.
Creating normalization for training data.
Done creating normalization for training data.
Particle ensemble enabled? True
An ensemble of 5 dynamics model <class 'model.dynamics.NNDynamicsModel'> initialized
Train dynamics model with intrinsic reward only? False
Pre-training enabled. Using only intrinsic reward.
Pre-training dynamics model for 0 iterations...
Done pre-training dynamics model.
Using external reward only.
itr #0 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.15550491213798523
Validation loss = 0.027035146951675415
Validation loss = 0.009341984055936337
Validation loss = 0.0032768426463007927
Validation loss = 0.0012419087579473853
Validation loss = 0.0006978940800763667
Validation loss = 0.0005208427319303155
Validation loss = 0.0004342953616287559
Validation loss = 0.000400320888729766
Validation loss = 0.0003862061130348593
Validation loss = 0.00038361421320587397
Validation loss = 0.00043868960347026587
Validation loss = 0.00037008358049206436
Validation loss = 0.00030689238337799907
Validation loss = 0.0004898975603282452
Validation loss = 0.0006907648057676852
Validation loss = 0.00035022120573557913
Validation loss = 0.0006158067262731493
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.15687662363052368
Validation loss = 0.027595054358243942
Validation loss = 0.007124603260308504
Validation loss = 0.003912969958037138
Validation loss = 0.0011870330199599266
Validation loss = 0.000711103668436408
Validation loss = 0.0005254494026303291
Validation loss = 0.0004876432940363884
Validation loss = 0.0004231492639519274
Validation loss = 0.0004102472448721528
Validation loss = 0.0003886054328177124
Validation loss = 0.00035882540396414697
Validation loss = 0.0005382520612329245
Validation loss = 0.00034585321554914117
Validation loss = 0.0005454049678519368
Validation loss = 0.000794929510448128
Validation loss = 0.0004359022423159331
Validation loss = 0.0006781029514968395
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.18080729246139526
Validation loss = 0.03060143254697323
Validation loss = 0.008741535246372223
Validation loss = 0.002717002760618925
Validation loss = 0.0010247111786156893
Validation loss = 0.0005974240484647453
Validation loss = 0.0005451216129586101
Validation loss = 0.000409382744692266
Validation loss = 0.00038421439239755273
Validation loss = 0.0003732351469807327
Validation loss = 0.0004190597392152995
Validation loss = 0.0003663077368400991
Validation loss = 0.007772666867822409
Validation loss = 0.0016473006689921021
Validation loss = 0.0006562872440554202
Validation loss = 0.0003743342822417617
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.1532978117465973
Validation loss = 0.034250807017087936
Validation loss = 0.006878257729113102
Validation loss = 0.003464669222012162
Validation loss = 0.001324279815889895
Validation loss = 0.0007692512590438128
Validation loss = 0.0005491604679264128
Validation loss = 0.0004543201648630202
Validation loss = 0.00044626399176195264
Validation loss = 0.0004016768652945757
Validation loss = 0.00042981954175047576
Validation loss = 0.0004774005792569369
Validation loss = 0.0004870063567068428
Validation loss = 0.0025998130440711975
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.1584247201681137
Validation loss = 0.024174995720386505
Validation loss = 0.00777799729257822
Validation loss = 0.0032217795960605145
Validation loss = 0.0014303887728601694
Validation loss = 0.000799506320618093
Validation loss = 0.0005805509281344712
Validation loss = 0.00047654760419391096
Validation loss = 0.0004549217119347304
Validation loss = 0.00044507349957711995
Validation loss = 0.00042700592894107103
Validation loss = 0.000375929637812078
Validation loss = 0.0003814584924839437
Validation loss = 0.00047925091348588467
Validation loss = 0.0006825606105849147
Validation loss = 0.0018019780982285738
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 0        |
| MaximumReturn | 199      |
| MinimumReturn | 124      |
| TotalSamples  | 6666     |
----------------------------
itr #1 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.01692480966448784
Validation loss = 0.003237911267206073
Validation loss = 0.0012394359800964594
Validation loss = 0.0011398937785997987
Validation loss = 0.0009432586957700551
Validation loss = 0.0012979416642338037
Validation loss = 0.000729339721146971
Validation loss = 0.003888914594426751
Validation loss = 0.001000526244752109
Validation loss = 0.0008837333880364895
Validation loss = 0.0010273748775944114
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.019524309784173965
Validation loss = 0.00376797909848392
Validation loss = 0.0013347255298867822
Validation loss = 0.000985330785624683
Validation loss = 0.0008530498016625643
Validation loss = 0.0010413742857053876
Validation loss = 0.0009635842288844287
Validation loss = 0.0016214452916756272
Validation loss = 0.0008416661876253784
Validation loss = 0.0020914163906127214
Validation loss = 0.0014196474803611636
Validation loss = 0.0009032649104483426
Validation loss = 0.0011503181885927916
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.01355049479752779
Validation loss = 0.0026129763573408127
Validation loss = 0.0012468484928831458
Validation loss = 0.0010397069854661822
Validation loss = 0.0010983749525621533
Validation loss = 0.0009669068385846913
Validation loss = 0.0012868006015196443
Validation loss = 0.000978741212747991
Validation loss = 0.0013684985460713506
Validation loss = 0.0015038567362353206
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.01310036052018404
Validation loss = 0.0023342950735241175
Validation loss = 0.001344808959402144
Validation loss = 0.0012774706119671464
Validation loss = 0.0009890160290524364
Validation loss = 0.0010802907636389136
Validation loss = 0.0007603227277286351
Validation loss = 0.000916222867090255
Validation loss = 0.0018570246174931526
Validation loss = 0.0012820499250665307
Validation loss = 0.0019372175447642803
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.015132617205381393
Validation loss = 0.003555257571861148
Validation loss = 0.0012523643672466278
Validation loss = 0.001077443128451705
Validation loss = 0.0009989532409235835
Validation loss = 0.0010374190751463175
Validation loss = 0.002265959745272994
Validation loss = 0.0012370662298053503
Validation loss = 0.0014614524552598596
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 1        |
| MaximumReturn | 199      |
| MinimumReturn | 135      |
| TotalSamples  | 9999     |
----------------------------
itr #2 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0053915767930448055
Validation loss = 0.0008501269039697945
Validation loss = 0.0006149425753392279
Validation loss = 0.0008428596192970872
Validation loss = 0.0014123645378276706
Validation loss = 0.00168099463917315
Validation loss = 0.0019349588546901941
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.005577053874731064
Validation loss = 0.0011594858951866627
Validation loss = 0.0017249655211344361
Validation loss = 0.0018216045573353767
Validation loss = 0.0007369563682004809
Validation loss = 0.0013911975547671318
Validation loss = 0.0017684802878648043
Validation loss = 0.0009984310017898679
Validation loss = 0.000586192705668509
Validation loss = 0.0021144128404557705
Validation loss = 0.0010382973123341799
Validation loss = 0.0005902177654206753
Validation loss = 0.0006286472780629992
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.007723311893641949
Validation loss = 0.0015984615311026573
Validation loss = 0.0009133201092481613
Validation loss = 0.0007855033618398011
Validation loss = 0.0009224596433341503
Validation loss = 0.0012657244224101305
Validation loss = 0.00417484762147069
Validation loss = 0.0007575372001156211
Validation loss = 0.0008172922534868121
Validation loss = 0.0019689039327204227
Validation loss = 0.0006930769886821508
Validation loss = 0.0014683647314086556
Validation loss = 0.0008884908747859299
Validation loss = 0.0017780838534235954
Validation loss = 0.0012476632837206125
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.015284024178981781
Validation loss = 0.0009854082018136978
Validation loss = 0.0008062703418545425
Validation loss = 0.000795616942923516
Validation loss = 0.0010601409012451768
Validation loss = 0.002091362839564681
Validation loss = 0.000933657051064074
Validation loss = 0.0014661314198747277
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0042623234912753105
Validation loss = 0.0014757983153685927
Validation loss = 0.0007899285410530865
Validation loss = 0.002138106618076563
Validation loss = 0.0027240561321377754
Validation loss = 0.0013000898761674762
Validation loss = 0.0009675545734353364
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 2        |
| MaximumReturn | 199      |
| MinimumReturn | 126      |
| TotalSamples  | 13332    |
----------------------------
itr #3 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.004556884523481131
Validation loss = 0.001228612381964922
Validation loss = 0.0015357093652710319
Validation loss = 0.0012848620535805821
Validation loss = 0.001318148453719914
Validation loss = 0.00072740443283692
Validation loss = 0.002219082787632942
Validation loss = 0.0008800726500339806
Validation loss = 0.0012326301075518131
Validation loss = 0.0012778068194165826
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.007388455793261528
Validation loss = 0.000659771088976413
Validation loss = 0.0007332735694944859
Validation loss = 0.001053269486874342
Validation loss = 0.0013042343780398369
Validation loss = 0.0006335019133985043
Validation loss = 0.0006977187586016953
Validation loss = 0.00240135146304965
Validation loss = 0.0006926904316060245
Validation loss = 0.0011365605751052499
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.002672335132956505
Validation loss = 0.0006489836960099638
Validation loss = 0.0007249770569615066
Validation loss = 0.001984985778108239
Validation loss = 0.0010106548434123397
Validation loss = 0.000758347159717232
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0033587906509637833
Validation loss = 0.0014237278373911977
Validation loss = 0.0013278970727697015
Validation loss = 0.0022703518625348806
Validation loss = 0.0009250296279788017
Validation loss = 0.001276045455597341
Validation loss = 0.0008415867923758924
Validation loss = 0.002287962706759572
Validation loss = 0.0007537009660154581
Validation loss = 0.001192183350212872
Validation loss = 0.0008565193857066333
Validation loss = 0.0007032577996142209
Validation loss = 0.0007040516356937587
Validation loss = 0.0012007440673187375
Validation loss = 0.0014220754383131862
Validation loss = 0.0012422299478203058
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.003686309792101383
Validation loss = 0.0007209572941064835
Validation loss = 0.0014502094127237797
Validation loss = 0.0007319506839849055
Validation loss = 0.0009930427186191082
Validation loss = 0.0006562243797816336
Validation loss = 0.0008367305272258818
Validation loss = 0.001407566829584539
Validation loss = 0.0009858348639681935
Validation loss = 0.0011546978494152427
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 3        |
| MaximumReturn | 199      |
| MinimumReturn | 119      |
| TotalSamples  | 16665    |
----------------------------
itr #4 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0021370593458414078
Validation loss = 0.0012176255695521832
Validation loss = 0.0005459699314087629
Validation loss = 0.00113190698903054
Validation loss = 0.0009380621486343443
Validation loss = 0.0008926305454224348
Validation loss = 0.0008569173514842987
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0016654448118060827
Validation loss = 0.0009269872680306435
Validation loss = 0.0013319719582796097
Validation loss = 0.0017599065322428942
Validation loss = 0.0006388332694768906
Validation loss = 0.0006701942766085267
Validation loss = 0.0009992207633331418
Validation loss = 0.0021410854533314705
Validation loss = 0.0012074493570253253
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.003808163572102785
Validation loss = 0.0006242821691557765
Validation loss = 0.0010057701729238033
Validation loss = 0.0029907983262091875
Validation loss = 0.0011622036108747125
Validation loss = 0.0005292606074362993
Validation loss = 0.0008497589733451605
Validation loss = 0.0007199522806331515
Validation loss = 0.0009083347977139056
Validation loss = 0.0013865127693861723
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0029540779069066048
Validation loss = 0.0014493651688098907
Validation loss = 0.0007815784774720669
Validation loss = 0.0009233737364411354
Validation loss = 0.0007225187728181481
Validation loss = 0.0008096190867945552
Validation loss = 0.0006521281902678311
Validation loss = 0.001518625533208251
Validation loss = 0.0023048496805131435
Validation loss = 0.0013771448284387589
Validation loss = 0.0007334883557632565
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.002959679812192917
Validation loss = 0.0007140991510823369
Validation loss = 0.0009088435908779502
Validation loss = 0.0017464144621044397
Validation loss = 0.002498022513464093
Validation loss = 0.0010158108780160546
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 4        |
| MaximumReturn | 199      |
| MinimumReturn | 130      |
| TotalSamples  | 19998    |
----------------------------
itr #5 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.003578917356207967
Validation loss = 0.000993580324575305
Validation loss = 0.0007953232852742076
Validation loss = 0.0005710552795790136
Validation loss = 0.0008033150807023048
Validation loss = 0.0013513545272871852
Validation loss = 0.0014440590748563409
Validation loss = 0.0012124121421948075
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.005996590480208397
Validation loss = 0.0005461420514620841
Validation loss = 0.0008962779538705945
Validation loss = 0.0010471829446032643
Validation loss = 0.0012058301363140345
Validation loss = 0.0007825517095625401
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0013594336342066526
Validation loss = 0.0008586340700276196
Validation loss = 0.0006616642931476235
Validation loss = 0.001371536636725068
Validation loss = 0.0008647890645079315
Validation loss = 0.001106374547816813
Validation loss = 0.001500442624092102
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0026329318061470985
Validation loss = 0.000748022401239723
Validation loss = 0.0009579224279150367
Validation loss = 0.0009696842171251774
Validation loss = 0.0014882147079333663
Validation loss = 0.0010968063725158572
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0012099520536139607
Validation loss = 0.0008107356843538582
Validation loss = 0.0012012647930532694
Validation loss = 0.0007520217332057655
Validation loss = 0.0012892575468868017
Validation loss = 0.0005733235739171505
Validation loss = 0.0007306010229513049
Validation loss = 0.0006958956946618855
Validation loss = 0.0008690531249158084
Validation loss = 0.0007089953869581223
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 163      |
| Iteration     | 5        |
| MaximumReturn | 199      |
| MinimumReturn | 120      |
| TotalSamples  | 23331    |
----------------------------
itr #6 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0008203849429264665
Validation loss = 0.002358698286116123
Validation loss = 0.0015651414869353175
Validation loss = 0.0007048634579405189
Validation loss = 0.001363042858429253
Validation loss = 0.0011029111919924617
Validation loss = 0.0006737794610671699
Validation loss = 0.0016399130690842867
Validation loss = 0.0007990037556737661
Validation loss = 0.0008668959490023553
Validation loss = 0.0008171164081431925
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.00192209891974926
Validation loss = 0.0010345539776608348
Validation loss = 0.001105159753933549
Validation loss = 0.0007435809820890427
Validation loss = 0.0019216322107240558
Validation loss = 0.001415264094248414
Validation loss = 0.0008461162797175348
Validation loss = 0.0006598205654881895
Validation loss = 0.0008531706407666206
Validation loss = 0.0006217170157469809
Validation loss = 0.001001004478894174
Validation loss = 0.0008098530233837664
Validation loss = 0.0008127438486553729
Validation loss = 0.0012089828960597515
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.006805570796132088
Validation loss = 0.0006095897406339645
Validation loss = 0.0005951781640760601
Validation loss = 0.0006234862958081067
Validation loss = 0.0013486371608451009
Validation loss = 0.0005827989079989493
Validation loss = 0.0016162778483703732
Validation loss = 0.0009745294810272753
Validation loss = 0.0008207370992749929
Validation loss = 0.0010873922146856785
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0010888132965192199
Validation loss = 0.0012460408033803105
Validation loss = 0.002417159965261817
Validation loss = 0.0012404272565618157
Validation loss = 0.0010856585577130318
Validation loss = 0.003195676952600479
Validation loss = 0.0006681109662167728
Validation loss = 0.0008176480769179761
Validation loss = 0.0009375247755087912
Validation loss = 0.0008690240210853517
Validation loss = 0.0011060260003432631
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0011946235317736864
Validation loss = 0.0007108179270289838
Validation loss = 0.0009262906387448311
Validation loss = 0.0010672607459127903
Validation loss = 0.0015444658929482102
Validation loss = 0.0011126510798931122
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 6        |
| MaximumReturn | 199      |
| MinimumReturn | 114      |
| TotalSamples  | 26664    |
----------------------------
itr #7 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0012989086098968983
Validation loss = 0.002404335653409362
Validation loss = 0.000736513698939234
Validation loss = 0.0007849187823012471
Validation loss = 0.0007571963942609727
Validation loss = 0.0011040718527510762
Validation loss = 0.0008094254299066961
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0009496273705735803
Validation loss = 0.0013936514733359218
Validation loss = 0.000607718073297292
Validation loss = 0.001571565167978406
Validation loss = 0.0006049951771274209
Validation loss = 0.0007524614920839667
Validation loss = 0.0008973703952506185
Validation loss = 0.0006642642547376454
Validation loss = 0.0006392021896317601
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0013984516263008118
Validation loss = 0.0006615041056647897
Validation loss = 0.0008483153069391847
Validation loss = 0.00083639204967767
Validation loss = 0.001027325284667313
Validation loss = 0.0007369900122284889
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0006954751443117857
Validation loss = 0.0006966422079131007
Validation loss = 0.0012878159759566188
Validation loss = 0.001141581917181611
Validation loss = 0.0008672469411976635
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.000898940721526742
Validation loss = 0.0024667754769325256
Validation loss = 0.0008268569363281131
Validation loss = 0.0010244639124721289
Validation loss = 0.0007577131618745625
Validation loss = 0.0010578316869214177
Validation loss = 0.0007505022804252803
Validation loss = 0.0009835942182689905
Validation loss = 0.0005868755979463458
Validation loss = 0.0008838312933221459
Validation loss = 0.001641780138015747
Validation loss = 0.0015031503280624747
Validation loss = 0.0007051717257127166
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 7        |
| MaximumReturn | 199      |
| MinimumReturn | 123      |
| TotalSamples  | 29997    |
----------------------------
itr #8 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0018119815504178405
Validation loss = 0.001032829051837325
Validation loss = 0.000583500019274652
Validation loss = 0.0008917513187043369
Validation loss = 0.0007717999396845698
Validation loss = 0.001623011427000165
Validation loss = 0.0008586004842072725
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0010203972924500704
Validation loss = 0.0009490365046076477
Validation loss = 0.0009173115831799805
Validation loss = 0.0009659734205342829
Validation loss = 0.0005666430806741118
Validation loss = 0.0006676770863123238
Validation loss = 0.0007325289770960808
Validation loss = 0.0006231162115000188
Validation loss = 0.0012902659364044666
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0020441818051040173
Validation loss = 0.0010891144629567862
Validation loss = 0.0009462455054745078
Validation loss = 0.0005334473098628223
Validation loss = 0.0008739383774809539
Validation loss = 0.0007754420512355864
Validation loss = 0.0007059348281472921
Validation loss = 0.0005307194078341126
Validation loss = 0.0010870936093851924
Validation loss = 0.0008014408522285521
Validation loss = 0.0011455030180513859
Validation loss = 0.0007211283082142472
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0006568757235072553
Validation loss = 0.0013488277327269316
Validation loss = 0.002318713581189513
Validation loss = 0.0006800064584240317
Validation loss = 0.0006221029907464981
Validation loss = 0.000608714995905757
Validation loss = 0.0010905272793024778
Validation loss = 0.0008171036606654525
Validation loss = 0.000840876076836139
Validation loss = 0.0009944743942469358
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0007853087736293674
Validation loss = 0.0007791880634613335
Validation loss = 0.000954357092268765
Validation loss = 0.0006544205243699253
Validation loss = 0.0010812238324433565
Validation loss = 0.0007539117359556258
Validation loss = 0.0006718748481944203
Validation loss = 0.0007191551267169416
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 8        |
| MaximumReturn | 199      |
| MinimumReturn | 130      |
| TotalSamples  | 33330    |
----------------------------
itr #9 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.001773406402207911
Validation loss = 0.0008306188974529505
Validation loss = 0.0019184688571840525
Validation loss = 0.0013722782023251057
Validation loss = 0.0005293950089253485
Validation loss = 0.0007923737866804004
Validation loss = 0.0006034572143107653
Validation loss = 0.0014142750296741724
Validation loss = 0.0006640949286520481
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0012971784453839064
Validation loss = 0.0006937423604540527
Validation loss = 0.004006786271929741
Validation loss = 0.0006029506330378354
Validation loss = 0.0006369834882207215
Validation loss = 0.0007081881631165743
Validation loss = 0.0007245013839565217
Validation loss = 0.0008032440091483295
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0017226961208507419
Validation loss = 0.0006214739987626672
Validation loss = 0.0009675383917056024
Validation loss = 0.0013699191622436047
Validation loss = 0.0009526051580905914
Validation loss = 0.0006591865094378591
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0011258609592914581
Validation loss = 0.0007114221807569265
Validation loss = 0.001304415287449956
Validation loss = 0.0005105588934384286
Validation loss = 0.0007071520667523146
Validation loss = 0.0007746712071821094
Validation loss = 0.0009270458831451833
Validation loss = 0.001436376478523016
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0008585661998949945
Validation loss = 0.0016988429706543684
Validation loss = 0.0007362423348240554
Validation loss = 0.0011722970521077514
Validation loss = 0.0005863448604941368
Validation loss = 0.001543669612146914
Validation loss = 0.0006802623393014073
Validation loss = 0.0007186621078290045
Validation loss = 0.0006210118881426752
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 181      |
| Iteration     | 9        |
| MaximumReturn | 199      |
| MinimumReturn | 143      |
| TotalSamples  | 36663    |
----------------------------
itr #10 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0015767946606501937
Validation loss = 0.0006621601642109454
Validation loss = 0.0014138722326606512
Validation loss = 0.0008727784734219313
Validation loss = 0.0015601430786773562
Validation loss = 0.0007363492040894926
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0014619478024542332
Validation loss = 0.000612578063737601
Validation loss = 0.0005492739728651941
Validation loss = 0.0008811208535917103
Validation loss = 0.0006230446160770953
Validation loss = 0.0007075433968566358
Validation loss = 0.0009275345946662128
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0008878849912434816
Validation loss = 0.0006209249841049314
Validation loss = 0.0008526271558366716
Validation loss = 0.0010158498771488667
Validation loss = 0.0009123963536694646
Validation loss = 0.0006427645566873252
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007677690591663122
Validation loss = 0.0009438040433451533
Validation loss = 0.0006502277101390064
Validation loss = 0.0007503487868234515
Validation loss = 0.0008774432935751975
Validation loss = 0.0007167311850935221
Validation loss = 0.0006446291226893663
Validation loss = 0.0008529194165021181
Validation loss = 0.0005093854269944131
Validation loss = 0.0009981464827433228
Validation loss = 0.0007631104672327638
Validation loss = 0.0005553679075092077
Validation loss = 0.0006499570445157588
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0013025110820308328
Validation loss = 0.0007816211436875165
Validation loss = 0.0005896476213820279
Validation loss = 0.0006037125131115317
Validation loss = 0.000819325156044215
Validation loss = 0.0009364510769955814
Validation loss = 0.0008689966052770615
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 10       |
| MaximumReturn | 199      |
| MinimumReturn | 102      |
| TotalSamples  | 39996    |
----------------------------
itr #11 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0007749732467345893
Validation loss = 0.0005858486401848495
Validation loss = 0.0006343141430988908
Validation loss = 0.000556575134396553
Validation loss = 0.0007326172199100256
Validation loss = 0.00104756117798388
Validation loss = 0.0008407073910348117
Validation loss = 0.0008647713693790138
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006043644389137626
Validation loss = 0.0007856813026592135
Validation loss = 0.0007474657031707466
Validation loss = 0.0005811470909975469
Validation loss = 0.0005081163835711777
Validation loss = 0.0007618762319907546
Validation loss = 0.00051266816444695
Validation loss = 0.0006797226378694177
Validation loss = 0.0005302801146171987
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0008888862212188542
Validation loss = 0.0009927543578669429
Validation loss = 0.0006413172814063728
Validation loss = 0.0011611903319135308
Validation loss = 0.0005557425902225077
Validation loss = 0.0007025209488347173
Validation loss = 0.0008265773067250848
Validation loss = 0.0006629735580645502
Validation loss = 0.0008923487621359527
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007072250591591001
Validation loss = 0.0006821087445132434
Validation loss = 0.0009174125152640045
Validation loss = 0.0006749016465619206
Validation loss = 0.0005894674104638398
Validation loss = 0.0006518512382172048
Validation loss = 0.00131078134290874
Validation loss = 0.001013071509078145
Validation loss = 0.0008145201718434691
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0010706812608987093
Validation loss = 0.0005205945344641805
Validation loss = 0.0007083724485710263
Validation loss = 0.0009235083125531673
Validation loss = 0.0005702361231669784
Validation loss = 0.0009478213032707572
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 11       |
| MaximumReturn | 199      |
| MinimumReturn | 117      |
| TotalSamples  | 43329    |
----------------------------
itr #12 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0007149429293349385
Validation loss = 0.0006621331558562815
Validation loss = 0.0007209930918179452
Validation loss = 0.0007652899366803467
Validation loss = 0.000848547148052603
Validation loss = 0.0007142689428292215
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0008120089769363403
Validation loss = 0.0008011468453332782
Validation loss = 0.0007249091868288815
Validation loss = 0.0006972603150643408
Validation loss = 0.0008255114080384374
Validation loss = 0.0006000125431455672
Validation loss = 0.0006404147716239095
Validation loss = 0.0005089191254228354
Validation loss = 0.0006741948309354484
Validation loss = 0.0012935405829921365
Validation loss = 0.0005294059519656003
Validation loss = 0.0007528890855610371
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0007530481670983136
Validation loss = 0.0006582775968126953
Validation loss = 0.0007808044319972396
Validation loss = 0.0006902883760631084
Validation loss = 0.001272678142413497
Validation loss = 0.0005844092811457813
Validation loss = 0.0006081202300265431
Validation loss = 0.0009030855726450682
Validation loss = 0.0005736444727517664
Validation loss = 0.0008211092208512127
Validation loss = 0.0007039385964162648
Validation loss = 0.000921606901101768
Validation loss = 0.0009255146724171937
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0006948768859729171
Validation loss = 0.0008088719332590699
Validation loss = 0.0005258959135971963
Validation loss = 0.0006379536353051662
Validation loss = 0.0008170154760591686
Validation loss = 0.0009742255206219852
Validation loss = 0.0006830404745414853
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0007312617963179946
Validation loss = 0.0006211464642547071
Validation loss = 0.0008189737563952804
Validation loss = 0.000578290200792253
Validation loss = 0.0014438560465350747
Validation loss = 0.0009322432451881468
Validation loss = 0.0007158611551858485
Validation loss = 0.0007169144228100777
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 12       |
| MaximumReturn | 199      |
| MinimumReturn | 125      |
| TotalSamples  | 46662    |
----------------------------
itr #13 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0010561509989202023
Validation loss = 0.0005419264198280871
Validation loss = 0.0010342439636588097
Validation loss = 0.0005952509236522019
Validation loss = 0.0006319411913864315
Validation loss = 0.0006086435751058161
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006631545256823301
Validation loss = 0.0019419349264353514
Validation loss = 0.0006644792738370597
Validation loss = 0.000560646818485111
Validation loss = 0.000780106580350548
Validation loss = 0.0005392327439039946
Validation loss = 0.0005310738342814147
Validation loss = 0.0006407044711522758
Validation loss = 0.0005488682654686272
Validation loss = 0.0006514327833428979
Validation loss = 0.000853869307320565
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0006678884965367615
Validation loss = 0.0012148726964369416
Validation loss = 0.0007254425436258316
Validation loss = 0.0005762256332673132
Validation loss = 0.0005656887078657746
Validation loss = 0.0005656302673742175
Validation loss = 0.0005997666739858687
Validation loss = 0.0006151153356768191
Validation loss = 0.0014562862925231457
Validation loss = 0.00047838810132816434
Validation loss = 0.0005876952200196683
Validation loss = 0.0006809124606661499
Validation loss = 0.0011945615988224745
Validation loss = 0.0006884537287987769
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007834375137463212
Validation loss = 0.0006110958638601005
Validation loss = 0.001079993206076324
Validation loss = 0.0006211224826984107
Validation loss = 0.0005729507538489997
Validation loss = 0.0008106193854473531
Validation loss = 0.0006710357847623527
Validation loss = 0.0005623438628390431
Validation loss = 0.0005564754246734083
Validation loss = 0.0006741199758835137
Validation loss = 0.0005512633360922337
Validation loss = 0.0006074053235352039
Validation loss = 0.0009228169801644981
Validation loss = 0.0005956038949079812
Validation loss = 0.0006853094091638923
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0013527165865525603
Validation loss = 0.0010950334835797548
Validation loss = 0.0008012051694095135
Validation loss = 0.0007172322366386652
Validation loss = 0.0006142163765616715
Validation loss = 0.000953183975070715
Validation loss = 0.001522708684206009
Validation loss = 0.0005755546735599637
Validation loss = 0.0007363836630247533
Validation loss = 0.000660634774249047
Validation loss = 0.0006061800522729754
Validation loss = 0.0011395476758480072
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 178      |
| Iteration     | 13       |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 49995    |
----------------------------
itr #14 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005875275819562376
Validation loss = 0.0006733492482453585
Validation loss = 0.0007415278814733028
Validation loss = 0.000863289984408766
Validation loss = 0.0006125012878328562
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0008519352413713932
Validation loss = 0.000499479821883142
Validation loss = 0.0005466469447128475
Validation loss = 0.0006837802357040346
Validation loss = 0.0009594067232683301
Validation loss = 0.0006218468188308179
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.000744879711419344
Validation loss = 0.0006328543531708419
Validation loss = 0.0006064464687369764
Validation loss = 0.0005079269758425653
Validation loss = 0.0008050028118304908
Validation loss = 0.000636241224128753
Validation loss = 0.0009458544664084911
Validation loss = 0.0012952492106705904
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.000573441619053483
Validation loss = 0.0005276363808661699
Validation loss = 0.0006385607412084937
Validation loss = 0.0012894519604742527
Validation loss = 0.0007545000407844782
Validation loss = 0.00072468415601179
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0007263286388479173
Validation loss = 0.0005876059294678271
Validation loss = 0.0007800653693266213
Validation loss = 0.0007116791675798595
Validation loss = 0.00066864222753793
Validation loss = 0.0005722370115108788
Validation loss = 0.0005043045966885984
Validation loss = 0.0006865984760224819
Validation loss = 0.0006082687759771943
Validation loss = 0.0011469037272036076
Validation loss = 0.0008158544660545886
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 14       |
| MaximumReturn | 199      |
| MinimumReturn | 122      |
| TotalSamples  | 53328    |
----------------------------
itr #15 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006858898559585214
Validation loss = 0.0007992978789843619
Validation loss = 0.0005875153583474457
Validation loss = 0.0006662943633273244
Validation loss = 0.0005520126433111727
Validation loss = 0.0008110359776765108
Validation loss = 0.0006931163952685893
Validation loss = 0.0006824504816904664
Validation loss = 0.00047249195631593466
Validation loss = 0.0005182934692129493
Validation loss = 0.0007380524184554815
Validation loss = 0.0006323609268292785
Validation loss = 0.0005806154222227633
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.001495353877544403
Validation loss = 0.0006640497595071793
Validation loss = 0.0006136178271844983
Validation loss = 0.0007726448238827288
Validation loss = 0.0006595785962417722
Validation loss = 0.0007990204612724483
Validation loss = 0.0006191026768647134
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0006516924477182329
Validation loss = 0.0005486378795467317
Validation loss = 0.0006622670916840434
Validation loss = 0.0005672603147104383
Validation loss = 0.0005813566385768354
Validation loss = 0.0005890637985430658
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005988043267279863
Validation loss = 0.000534124905243516
Validation loss = 0.0011209027143195271
Validation loss = 0.0007103913812898099
Validation loss = 0.0005528213805519044
Validation loss = 0.0006825468735769391
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0007027716492302716
Validation loss = 0.0006069134688004851
Validation loss = 0.0005445080460049212
Validation loss = 0.0008177204290404916
Validation loss = 0.0005614861147478223
Validation loss = 0.0006012231460772455
Validation loss = 0.000856589584145695
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 15       |
| MaximumReturn | 199      |
| MinimumReturn | 117      |
| TotalSamples  | 56661    |
----------------------------
itr #16 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006966475630179048
Validation loss = 0.0005257357843220234
Validation loss = 0.0005995577084831893
Validation loss = 0.000562592875212431
Validation loss = 0.0007552135502919555
Validation loss = 0.0006948759546503425
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.000565946102142334
Validation loss = 0.0006598230102099478
Validation loss = 0.0008087697788141668
Validation loss = 0.0006332180346362293
Validation loss = 0.0006789822364225984
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0007098240894265473
Validation loss = 0.0005939729162491858
Validation loss = 0.0007547464920207858
Validation loss = 0.0005710665136575699
Validation loss = 0.0006534437416121364
Validation loss = 0.0006243861862458289
Validation loss = 0.0005210224771872163
Validation loss = 0.0007241109269671142
Validation loss = 0.0006244308315217495
Validation loss = 0.0006520665483549237
Validation loss = 0.0006893431418575346
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005435176426544785
Validation loss = 0.001054612803272903
Validation loss = 0.0006409892230294645
Validation loss = 0.0005415137857198715
Validation loss = 0.0005185010959394276
Validation loss = 0.0007077394984662533
Validation loss = 0.0005972642684355378
Validation loss = 0.0005545935709960759
Validation loss = 0.0005770712159574032
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005992614896968007
Validation loss = 0.0006429149070754647
Validation loss = 0.000602679792791605
Validation loss = 0.0006863433518446982
Validation loss = 0.0006379581755027175
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 16       |
| MaximumReturn | 199      |
| MinimumReturn | 113      |
| TotalSamples  | 59994    |
----------------------------
itr #17 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0007525610853917897
Validation loss = 0.0006204157252795994
Validation loss = 0.0006318477098830044
Validation loss = 0.0007587622967548668
Validation loss = 0.0006990024703554809
Validation loss = 0.0007902898942120373
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0008094912045635283
Validation loss = 0.0006259593064896762
Validation loss = 0.0005340161151252687
Validation loss = 0.0005026688450016081
Validation loss = 0.0008461608667857945
Validation loss = 0.0005203300970606506
Validation loss = 0.000849906646180898
Validation loss = 0.0006008673226460814
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.000501269125379622
Validation loss = 0.0007037791074253619
Validation loss = 0.0006412231596186757
Validation loss = 0.0006147631793282926
Validation loss = 0.000562621105927974
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007951584993861616
Validation loss = 0.0007213618373498321
Validation loss = 0.0006163778598420322
Validation loss = 0.000554916390683502
Validation loss = 0.0005821342347189784
Validation loss = 0.0005545841413550079
Validation loss = 0.0006098630256019533
Validation loss = 0.000646051368676126
Validation loss = 0.0006366289453580976
Validation loss = 0.000706386927049607
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005877444054931402
Validation loss = 0.0006952155381441116
Validation loss = 0.0005617306451313198
Validation loss = 0.0005661540199071169
Validation loss = 0.0005589544307440519
Validation loss = 0.000631282280664891
Validation loss = 0.0005513410433195531
Validation loss = 0.0005809944705106318
Validation loss = 0.0006582756177522242
Validation loss = 0.0005495123332366347
Validation loss = 0.0006204330129548907
Validation loss = 0.0005368965794332325
Validation loss = 0.0010408618254587054
Validation loss = 0.0005463595152832568
Validation loss = 0.0008440538658760488
Validation loss = 0.0006604090449400246
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 17       |
| MaximumReturn | 199      |
| MinimumReturn | 125      |
| TotalSamples  | 63327    |
----------------------------
itr #18 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0017369327833876014
Validation loss = 0.0006101547041907907
Validation loss = 0.0005376879125833511
Validation loss = 0.0007106082048267126
Validation loss = 0.000585257716011256
Validation loss = 0.0006394379888661206
Validation loss = 0.0005211015231907368
Validation loss = 0.0006253570900298655
Validation loss = 0.0005457965307869017
Validation loss = 0.00076389528112486
Validation loss = 0.0005423941183835268
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.000504876661580056
Validation loss = 0.0006480919546447694
Validation loss = 0.0005651083774864674
Validation loss = 0.0007200500695034862
Validation loss = 0.000706553109921515
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.000555040140170604
Validation loss = 0.0008143672021105886
Validation loss = 0.0005522518767975271
Validation loss = 0.0005870581953786314
Validation loss = 0.0005240762257017195
Validation loss = 0.000534261402208358
Validation loss = 0.0005794502212665975
Validation loss = 0.0005734596634283662
Validation loss = 0.0006137270829640329
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005096426466479897
Validation loss = 0.0006381657440215349
Validation loss = 0.0005536979297176003
Validation loss = 0.0005317921750247478
Validation loss = 0.0005392038729041815
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0007972297025844455
Validation loss = 0.0005905698635615408
Validation loss = 0.00055457599228248
Validation loss = 0.0006760547985322773
Validation loss = 0.0007288869237527251
Validation loss = 0.0005765737150795758
Validation loss = 0.0004902618820779026
Validation loss = 0.0005732919671572745
Validation loss = 0.0006574628059752285
Validation loss = 0.0005738858017139137
Validation loss = 0.0005852414760738611
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 18       |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 66660    |
----------------------------
itr #19 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005933791981078684
Validation loss = 0.000609222159255296
Validation loss = 0.0008221544558182359
Validation loss = 0.0006144965882413089
Validation loss = 0.0008796522743068635
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005920041585341096
Validation loss = 0.000841894478071481
Validation loss = 0.0008483810815960169
Validation loss = 0.0005441164830699563
Validation loss = 0.0006723246187902987
Validation loss = 0.0005235759308561683
Validation loss = 0.0006934301345609128
Validation loss = 0.0007523495005443692
Validation loss = 0.0004974217736162245
Validation loss = 0.0006417909171432257
Validation loss = 0.0005066330777481198
Validation loss = 0.0006152702262625098
Validation loss = 0.0006097415462136269
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.000574177538510412
Validation loss = 0.0011823730310425162
Validation loss = 0.0005957240937277675
Validation loss = 0.000542952970135957
Validation loss = 0.0005799779901280999
Validation loss = 0.0006464793113991618
Validation loss = 0.0005488347378559411
Validation loss = 0.0006429844070225954
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005269511020742357
Validation loss = 0.0005388419376686215
Validation loss = 0.0008931494085118175
Validation loss = 0.0005271180998533964
Validation loss = 0.0005632460815832019
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005712862475775182
Validation loss = 0.0006037119892425835
Validation loss = 0.0007051266147755086
Validation loss = 0.0005794544122181833
Validation loss = 0.0005354332970455289
Validation loss = 0.000673537899274379
Validation loss = 0.0006538118468597531
Validation loss = 0.000561719061806798
Validation loss = 0.0007164516719058156
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 19       |
| MaximumReturn | 199      |
| MinimumReturn | 126      |
| TotalSamples  | 69993    |
----------------------------
itr #20 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006709958543069661
Validation loss = 0.0006182623328641057
Validation loss = 0.0006335665821097791
Validation loss = 0.0006699744844809175
Validation loss = 0.0006140028708614409
Validation loss = 0.0006601768545806408
Validation loss = 0.0005842646933160722
Validation loss = 0.0005418359651230276
Validation loss = 0.0005408949218690395
Validation loss = 0.0007206726004369557
Validation loss = 0.0005777007900178432
Validation loss = 0.0006305488059297204
Validation loss = 0.0006101044709794223
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006687034037895501
Validation loss = 0.0005593587993644178
Validation loss = 0.000576949561946094
Validation loss = 0.0005579376593232155
Validation loss = 0.0005795740289613605
Validation loss = 0.0007347355713136494
Validation loss = 0.0006526089855469763
Validation loss = 0.000674146693199873
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0005904308636672795
Validation loss = 0.0005563779850490391
Validation loss = 0.0005025658756494522
Validation loss = 0.000551010831259191
Validation loss = 0.0005777371698059142
Validation loss = 0.0006015970138832927
Validation loss = 0.0007170134340412915
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.000557687075342983
Validation loss = 0.0006609048577956855
Validation loss = 0.0006333561614155769
Validation loss = 0.0005785083631053567
Validation loss = 0.0004970026202499866
Validation loss = 0.0005785300745628774
Validation loss = 0.0005812948220409453
Validation loss = 0.0005953724612481892
Validation loss = 0.0007960402872413397
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005977781256660819
Validation loss = 0.0005647759535349905
Validation loss = 0.0008000282687135041
Validation loss = 0.0006319436361081898
Validation loss = 0.0005904461140744388
Validation loss = 0.0008674907730892301
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 20       |
| MaximumReturn | 199      |
| MinimumReturn | 108      |
| TotalSamples  | 73326    |
----------------------------
itr #21 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005237844889052212
Validation loss = 0.0006372814532369375
Validation loss = 0.0005390628939494491
Validation loss = 0.0005757813923992217
Validation loss = 0.0005509074544534087
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005146349431015551
Validation loss = 0.0005091820494271815
Validation loss = 0.0006583672948181629
Validation loss = 0.0005724375369027257
Validation loss = 0.0006104926578700542
Validation loss = 0.0005756373284384608
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0005735547747462988
Validation loss = 0.000571892480365932
Validation loss = 0.0005484326393343508
Validation loss = 0.0006171102868393064
Validation loss = 0.000591091753449291
Validation loss = 0.0006762014236301184
Validation loss = 0.0005031180917285383
Validation loss = 0.0005959991249255836
Validation loss = 0.0006057106656953692
Validation loss = 0.0005687595112249255
Validation loss = 0.0005250052781775594
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0010109002469107509
Validation loss = 0.0005523129948414862
Validation loss = 0.0005330261192284524
Validation loss = 0.0006491761887446046
Validation loss = 0.0005311283748596907
Validation loss = 0.0006798880640417337
Validation loss = 0.0006178034818731248
Validation loss = 0.0005338944029062986
Validation loss = 0.00058575882576406
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0006323198904283345
Validation loss = 0.000524379312992096
Validation loss = 0.0005795934703201056
Validation loss = 0.0005558941047638655
Validation loss = 0.0005497443489730358
Validation loss = 0.0006349004688672721
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 178      |
| Iteration     | 21       |
| MaximumReturn | 199      |
| MinimumReturn | 129      |
| TotalSamples  | 76659    |
----------------------------
itr #22 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006135953008197248
Validation loss = 0.000573247205466032
Validation loss = 0.0014931272016838193
Validation loss = 0.0006116835866123438
Validation loss = 0.0006217145710252225
Validation loss = 0.0006612582947127521
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0009342819103039801
Validation loss = 0.0005850762827321887
Validation loss = 0.0005717313033528626
Validation loss = 0.0009315883507952094
Validation loss = 0.0005112397484481335
Validation loss = 0.0007211928023025393
Validation loss = 0.0006006642361171544
Validation loss = 0.0006314261700026691
Validation loss = 0.0005401194794103503
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0008232644759118557
Validation loss = 0.0007509579882025719
Validation loss = 0.0005784075474366546
Validation loss = 0.0005695855943486094
Validation loss = 0.00048792612506076694
Validation loss = 0.0006459202850237489
Validation loss = 0.0005465896683745086
Validation loss = 0.0005571024375967681
Validation loss = 0.000568675110116601
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.000529384589754045
Validation loss = 0.0005983258597552776
Validation loss = 0.0005734515725634992
Validation loss = 0.0006230433355085552
Validation loss = 0.0006498950533568859
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005494787474162877
Validation loss = 0.0005964013398624957
Validation loss = 0.0006946157081983984
Validation loss = 0.0005422462709248066
Validation loss = 0.0005764313973486423
Validation loss = 0.0005604090983979404
Validation loss = 0.000693794630933553
Validation loss = 0.0005828535067848861
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 22       |
| MaximumReturn | 199      |
| MinimumReturn | 120      |
| TotalSamples  | 79992    |
----------------------------
itr #23 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0008891439065337181
Validation loss = 0.0005456575891003013
Validation loss = 0.0006240286747924984
Validation loss = 0.0005119991255924106
Validation loss = 0.0005927900783717632
Validation loss = 0.0007151787867769599
Validation loss = 0.0005685292999260128
Validation loss = 0.0005492655327543616
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006054976838640869
Validation loss = 0.0005069294711574912
Validation loss = 0.0006227140547707677
Validation loss = 0.0005108362529426813
Validation loss = 0.0006369827315211296
Validation loss = 0.0008126016473397613
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.000585158821195364
Validation loss = 0.0007227945025078952
Validation loss = 0.0007844509673304856
Validation loss = 0.0006247772253118455
Validation loss = 0.0005209239898249507
Validation loss = 0.0005961388233117759
Validation loss = 0.0005143241723999381
Validation loss = 0.0007392442203126848
Validation loss = 0.0005215505370870233
Validation loss = 0.0006541171460412443
Validation loss = 0.0005160094005987048
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0008439237135462463
Validation loss = 0.0005734174628742039
Validation loss = 0.0005560737918131053
Validation loss = 0.0005429033772088587
Validation loss = 0.0007732006488367915
Validation loss = 0.0005299335462041199
Validation loss = 0.0005379085196182132
Validation loss = 0.0005987836048007011
Validation loss = 0.0005716490559279919
Validation loss = 0.000614944554399699
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0007749575888738036
Validation loss = 0.0005740541964769363
Validation loss = 0.0006236738408915699
Validation loss = 0.0005241858889348805
Validation loss = 0.0005892786430194974
Validation loss = 0.0006684680702164769
Validation loss = 0.0005791918374598026
Validation loss = 0.0005951891653239727
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 177      |
| Iteration     | 23       |
| MaximumReturn | 199      |
| MinimumReturn | 149      |
| TotalSamples  | 83325    |
----------------------------
itr #24 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006998222088441253
Validation loss = 0.0005703842616640031
Validation loss = 0.0006486363126896322
Validation loss = 0.0006847534095868468
Validation loss = 0.0005846035783179104
Validation loss = 0.0007555602351203561
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006802502321079373
Validation loss = 0.0005720433546230197
Validation loss = 0.0005091771017760038
Validation loss = 0.0005394147010520101
Validation loss = 0.0005793945747427642
Validation loss = 0.0005732227582484484
Validation loss = 0.0006759993266314268
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.001105565344914794
Validation loss = 0.0006190456915646791
Validation loss = 0.0005857307696714997
Validation loss = 0.0006083181942813098
Validation loss = 0.0006058720173314214
Validation loss = 0.000571796961594373
Validation loss = 0.0007121414528228343
Validation loss = 0.0008452411275357008
Validation loss = 0.0005563304293900728
Validation loss = 0.0005373271415010095
Validation loss = 0.0006196798058226705
Validation loss = 0.0005596119444817305
Validation loss = 0.000549005635548383
Validation loss = 0.0007003757054917514
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005117731634527445
Validation loss = 0.0005916429217904806
Validation loss = 0.0005485335714183748
Validation loss = 0.0006088149966672063
Validation loss = 0.0005116222309879959
Validation loss = 0.0007460836786776781
Validation loss = 0.0005715854349546134
Validation loss = 0.0005298942560330033
Validation loss = 0.0005417835200205445
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0006528520025312901
Validation loss = 0.000533021695446223
Validation loss = 0.0005503041320480406
Validation loss = 0.0005385192343965173
Validation loss = 0.0006190603598952293
Validation loss = 0.000660304503981024
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 163      |
| Iteration     | 24       |
| MaximumReturn | 199      |
| MinimumReturn | 99       |
| TotalSamples  | 86658    |
----------------------------
itr #25 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006832394865341485
Validation loss = 0.0005096771055832505
Validation loss = 0.0005993308732286096
Validation loss = 0.0006481835152953863
Validation loss = 0.0007178676896728575
Validation loss = 0.0005477570812217891
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005769123672507703
Validation loss = 0.0005926804733462632
Validation loss = 0.0005373965250328183
Validation loss = 0.0005855553899891675
Validation loss = 0.0008424843545071781
Validation loss = 0.0005331581342034042
Validation loss = 0.0005726597155444324
Validation loss = 0.000663102837279439
Validation loss = 0.0006686390843242407
Validation loss = 0.0005516074597835541
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0005705976509489119
Validation loss = 0.0005434247432276607
Validation loss = 0.0006646499969065189
Validation loss = 0.0008274760330095887
Validation loss = 0.000504986965097487
Validation loss = 0.0005991096841171384
Validation loss = 0.0005573577363975346
Validation loss = 0.0006116490694694221
Validation loss = 0.0005401067319326103
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005352260777726769
Validation loss = 0.0005497939419001341
Validation loss = 0.00059460912598297
Validation loss = 0.0007407200755551457
Validation loss = 0.0006237151101231575
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005347223486751318
Validation loss = 0.000550507043953985
Validation loss = 0.0005187119240872562
Validation loss = 0.0005816066404804587
Validation loss = 0.0005489873001351953
Validation loss = 0.0005823404062539339
Validation loss = 0.0005340612260624766
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 25       |
| MaximumReturn | 199      |
| MinimumReturn | 115      |
| TotalSamples  | 89991    |
----------------------------
itr #26 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0007385704084299505
Validation loss = 0.0005352639709599316
Validation loss = 0.0006180107593536377
Validation loss = 0.0005223826155997813
Validation loss = 0.0005219202721491456
Validation loss = 0.0005546800675801933
Validation loss = 0.0006193026201799512
Validation loss = 0.0006117920274846256
Validation loss = 0.0006537677836604416
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0007321973680518568
Validation loss = 0.0005197392893023789
Validation loss = 0.0005824463441967964
Validation loss = 0.0006285274866968393
Validation loss = 0.0006415587267838418
Validation loss = 0.0005408796714618802
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0005292929708957672
Validation loss = 0.0006990772089920938
Validation loss = 0.0006544002098962665
Validation loss = 0.0006119570462033153
Validation loss = 0.0005493389908224344
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005729582044295967
Validation loss = 0.0006287725991569459
Validation loss = 0.0006181088392622769
Validation loss = 0.0005180740845389664
Validation loss = 0.0006293220212683082
Validation loss = 0.0005201243911869824
Validation loss = 0.0006524113123305142
Validation loss = 0.0005381411756388843
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005236063152551651
Validation loss = 0.0009525553905405104
Validation loss = 0.0006384481675922871
Validation loss = 0.0005616403068415821
Validation loss = 0.0005140269640833139
Validation loss = 0.0006439726566895843
Validation loss = 0.0005692531703971326
Validation loss = 0.0005740048945881426
Validation loss = 0.0005151762743480504
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 26       |
| MaximumReturn | 199      |
| MinimumReturn | 130      |
| TotalSamples  | 93324    |
----------------------------
itr #27 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005912747583352029
Validation loss = 0.0007505723042413592
Validation loss = 0.0005186513881199062
Validation loss = 0.000557652092538774
Validation loss = 0.0006873931852169335
Validation loss = 0.0006429904606193304
Validation loss = 0.0006556868320330977
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006093995179980993
Validation loss = 0.0008116207900457084
Validation loss = 0.0005911528132855892
Validation loss = 0.0005294016445986927
Validation loss = 0.0009499339503236115
Validation loss = 0.0005581417353823781
Validation loss = 0.0005717176827602088
Validation loss = 0.0006760223768651485
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0005259977770037949
Validation loss = 0.00048196292482316494
Validation loss = 0.0005467392620630562
Validation loss = 0.000657782016787678
Validation loss = 0.0005865978891961277
Validation loss = 0.0005450441385619342
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005800518556497991
Validation loss = 0.0005536496755667031
Validation loss = 0.0005500588449649513
Validation loss = 0.0005466491566039622
Validation loss = 0.0005392837920226157
Validation loss = 0.000517099688295275
Validation loss = 0.000569715688470751
Validation loss = 0.0006933989934623241
Validation loss = 0.0005034742061980069
Validation loss = 0.0005625608609989285
Validation loss = 0.0005050628678873181
Validation loss = 0.0006066573550924659
Validation loss = 0.000566699483897537
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0006123121129348874
Validation loss = 0.0005762479268014431
Validation loss = 0.0005542614962905645
Validation loss = 0.00048605704796500504
Validation loss = 0.000559831561986357
Validation loss = 0.0008407392888329923
Validation loss = 0.0005121894064359367
Validation loss = 0.0005544693558476865
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 27       |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 96657    |
----------------------------
itr #28 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005626506172120571
Validation loss = 0.000559168285690248
Validation loss = 0.0005512868519872427
Validation loss = 0.0005808728747069836
Validation loss = 0.0006392910145223141
Validation loss = 0.000581109372433275
Validation loss = 0.0005966212484054267
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005673004779964685
Validation loss = 0.0005160736618563533
Validation loss = 0.000499905610922724
Validation loss = 0.0006181856151670218
Validation loss = 0.0006307899602688849
Validation loss = 0.0005684851203113794
Validation loss = 0.0004978409269824624
Validation loss = 0.0006112274131737649
Validation loss = 0.0005251392140053213
Validation loss = 0.0004846123920287937
Validation loss = 0.0006130307447165251
Validation loss = 0.000560482672881335
Validation loss = 0.0005689283716492355
Validation loss = 0.0006068756920285523
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0005161284934729338
Validation loss = 0.000533721933607012
Validation loss = 0.0005839958321303129
Validation loss = 0.0006593880243599415
Validation loss = 0.0005468573071993887
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.000543456117156893
Validation loss = 0.0005325365345925093
Validation loss = 0.0006681994418613613
Validation loss = 0.0005177556886337698
Validation loss = 0.0006700808298774064
Validation loss = 0.0006629196577705443
Validation loss = 0.000616611388977617
Validation loss = 0.0006830610800534487
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005863835103809834
Validation loss = 0.0005429756711237133
Validation loss = 0.0006309757591225207
Validation loss = 0.0005282482597976923
Validation loss = 0.000505761825479567
Validation loss = 0.0005138667766004801
Validation loss = 0.0006507726502604783
Validation loss = 0.0005036373040638864
Validation loss = 0.0004862008208874613
Validation loss = 0.0006109087844379246
Validation loss = 0.0005213925032876432
Validation loss = 0.0005428061704151332
Validation loss = 0.00048716660239733756
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 28       |
| MaximumReturn | 199      |
| MinimumReturn | 123      |
| TotalSamples  | 99990    |
----------------------------
itr #29 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006357182282954454
Validation loss = 0.0005614144611172378
Validation loss = 0.0006674605538137257
Validation loss = 0.0005220920429565012
Validation loss = 0.0005497949314303696
Validation loss = 0.0005450319149531424
Validation loss = 0.000578448991291225
Validation loss = 0.0006501621101051569
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005821501254104078
Validation loss = 0.000510936661157757
Validation loss = 0.0005357071058824658
Validation loss = 0.0007362617761828005
Validation loss = 0.0005810830625705421
Validation loss = 0.0005739816115237772
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.000515222956892103
Validation loss = 0.0006215404137037694
Validation loss = 0.0007372855325229466
Validation loss = 0.0008399142534472048
Validation loss = 0.0005206064088270068
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.000518819666467607
Validation loss = 0.0006057441351003945
Validation loss = 0.0005216316203586757
Validation loss = 0.0005196362617425621
Validation loss = 0.0005475184298120439
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005383096868172288
Validation loss = 0.0005308257532306015
Validation loss = 0.0005497856182046235
Validation loss = 0.0005259987665340304
Validation loss = 0.0005963979638181627
Validation loss = 0.0005392264574766159
Validation loss = 0.0004976174095645547
Validation loss = 0.0005637351423501968
Validation loss = 0.0005311717395670712
Validation loss = 0.0005432944744825363
Validation loss = 0.0007078422349877656
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 29       |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 103323   |
----------------------------
itr #30 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0007743852911517024
Validation loss = 0.0005117917899042368
Validation loss = 0.000726918806321919
Validation loss = 0.000615681114140898
Validation loss = 0.0005677617155015469
Validation loss = 0.0005331268184818327
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005441457615233958
Validation loss = 0.0006560537149198353
Validation loss = 0.0005147792981006205
Validation loss = 0.0005241294857114553
Validation loss = 0.0007003778591752052
Validation loss = 0.0006431284127756953
Validation loss = 0.0006232530577108264
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0005063756252638996
Validation loss = 0.0005416204803623259
Validation loss = 0.0005212592659518123
Validation loss = 0.0005206653731875122
Validation loss = 0.0005232487455941737
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005470860050991178
Validation loss = 0.0007018066826276481
Validation loss = 0.000521427602507174
Validation loss = 0.0005164110916666687
Validation loss = 0.0007170873577706516
Validation loss = 0.0005900172400288284
Validation loss = 0.0005583776510320604
Validation loss = 0.0004872736462857574
Validation loss = 0.0005215970450080931
Validation loss = 0.0006309243617579341
Validation loss = 0.000543774978723377
Validation loss = 0.000634714204352349
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005023275152780116
Validation loss = 0.0005870906752534211
Validation loss = 0.0005840299418196082
Validation loss = 0.000536168459802866
Validation loss = 0.0005786624387837946
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 30       |
| MaximumReturn | 199      |
| MinimumReturn | 125      |
| TotalSamples  | 106656   |
----------------------------
itr #31 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005896155489608645
Validation loss = 0.0007392794359475374
Validation loss = 0.0005572919035330415
Validation loss = 0.0006547197699546814
Validation loss = 0.0005668430821970105
Validation loss = 0.0006192486034706235
Validation loss = 0.0005646078498102725
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005128761404193938
Validation loss = 0.0005282821366563439
Validation loss = 0.0005321018979884684
Validation loss = 0.0005361783551052213
Validation loss = 0.0005288926186040044
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0005790030700154603
Validation loss = 0.000558115541934967
Validation loss = 0.0005488504539243877
Validation loss = 0.0008294975268654525
Validation loss = 0.0005680730100721121
Validation loss = 0.0005815583863295615
Validation loss = 0.00055605114903301
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005546282045543194
Validation loss = 0.0005612726090475917
Validation loss = 0.0006543239578604698
Validation loss = 0.0005169486394152045
Validation loss = 0.0005529543268494308
Validation loss = 0.0005146167241036892
Validation loss = 0.0005240295431576669
Validation loss = 0.0006715442286804318
Validation loss = 0.0005242599872872233
Validation loss = 0.0005434004124253988
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0009346696897409856
Validation loss = 0.0006663371459580958
Validation loss = 0.0008344591478817165
Validation loss = 0.0005489260656759143
Validation loss = 0.0005242782644927502
Validation loss = 0.0006061476306058466
Validation loss = 0.0005678901798091829
Validation loss = 0.0005504548316821456
Validation loss = 0.0005466786096803844
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 31       |
| MaximumReturn | 199      |
| MinimumReturn | 130      |
| TotalSamples  | 109989   |
----------------------------
itr #32 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005136575200594962
Validation loss = 0.000557645398657769
Validation loss = 0.000608704227488488
Validation loss = 0.0005827009445056319
Validation loss = 0.0005411009769886732
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005802002851851285
Validation loss = 0.0006225905381143093
Validation loss = 0.0005076965317130089
Validation loss = 0.0005553161026909947
Validation loss = 0.0007337657152675092
Validation loss = 0.0007286662003025413
Validation loss = 0.0004996107891201973
Validation loss = 0.0005697638262063265
Validation loss = 0.0005134347011335194
Validation loss = 0.0005925147561356425
Validation loss = 0.000554374884814024
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.000542044173926115
Validation loss = 0.0005493730423040688
Validation loss = 0.0005807160050608218
Validation loss = 0.0005218679434619844
Validation loss = 0.0005971993086859584
Validation loss = 0.0005918464157730341
Validation loss = 0.0006016819970682263
Validation loss = 0.0005710934638045728
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005307567189447582
Validation loss = 0.0005765972891822457
Validation loss = 0.0005309003172442317
Validation loss = 0.0006018353742547333
Validation loss = 0.0007725950563326478
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005289639811962843
Validation loss = 0.0005774546298198402
Validation loss = 0.0005822278326377273
Validation loss = 0.000674959272146225
Validation loss = 0.0005844624247401953
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 32       |
| MaximumReturn | 199      |
| MinimumReturn | 95       |
| TotalSamples  | 113322   |
----------------------------
itr #33 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005459448439069092
Validation loss = 0.0005525631713680923
Validation loss = 0.0006251480081118643
Validation loss = 0.0005891686305403709
Validation loss = 0.0005591744557023048
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005741249769926071
Validation loss = 0.0005993820377625525
Validation loss = 0.0006213139859028161
Validation loss = 0.0005901145632378757
Validation loss = 0.0005828223074786365
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0006568433018401265
Validation loss = 0.0004989441949874163
Validation loss = 0.0004997628857381642
Validation loss = 0.0006547101656906307
Validation loss = 0.0005317967152222991
Validation loss = 0.0004992460599169135
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005733626312576234
Validation loss = 0.0005216120625846088
Validation loss = 0.0006699780351482332
Validation loss = 0.0008371903677470982
Validation loss = 0.000536463048774749
Validation loss = 0.0005324053927324712
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005269157118164003
Validation loss = 0.0005234120762906969
Validation loss = 0.0005487386370077729
Validation loss = 0.0005985182942822576
Validation loss = 0.0005265531945042312
Validation loss = 0.0005691206897608936
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 33       |
| MaximumReturn | 199      |
| MinimumReturn | 124      |
| TotalSamples  | 116655   |
----------------------------
itr #34 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005657372530549765
Validation loss = 0.0005550196510739625
Validation loss = 0.0005274081486277282
Validation loss = 0.0005060734110884368
Validation loss = 0.0006095621502026916
Validation loss = 0.0005143521702848375
Validation loss = 0.0005371295264922082
Validation loss = 0.0005241016624495387
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005258299643173814
Validation loss = 0.0006573473801836371
Validation loss = 0.0005473813507705927
Validation loss = 0.0005429441225714982
Validation loss = 0.0006594065926037729
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0005607503699138761
Validation loss = 0.0005367985577322543
Validation loss = 0.00048212221008725464
Validation loss = 0.0005423569236882031
Validation loss = 0.00054011499742046
Validation loss = 0.0007183744455687702
Validation loss = 0.0005869060405530035
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0006362649146467447
Validation loss = 0.0006441473960876465
Validation loss = 0.0005821868544444442
Validation loss = 0.0005124437739141285
Validation loss = 0.0009164354996755719
Validation loss = 0.0005275891744531691
Validation loss = 0.0005534394877031446
Validation loss = 0.0005452351178973913
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0004932094598188996
Validation loss = 0.0006365154404193163
Validation loss = 0.0005828982684761286
Validation loss = 0.0005208575748838484
Validation loss = 0.0005308688851073384
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 34       |
| MaximumReturn | 199      |
| MinimumReturn | 123      |
| TotalSamples  | 119988   |
----------------------------
itr #35 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005467206356115639
Validation loss = 0.0005166101036593318
Validation loss = 0.0005050090258009732
Validation loss = 0.0006109217065386474
Validation loss = 0.0006404715240933001
Validation loss = 0.0006135430885478854
Validation loss = 0.0005576461553573608
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0004999912925995886
Validation loss = 0.0006022733869031072
Validation loss = 0.0006391385686583817
Validation loss = 0.000584044901188463
Validation loss = 0.000513593025971204
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0005547801847569644
Validation loss = 0.0005370331928133965
Validation loss = 0.0006037550629116595
Validation loss = 0.0005814699688926339
Validation loss = 0.0005345933604985476
Validation loss = 0.000534751103259623
Validation loss = 0.0005791013827547431
Validation loss = 0.0005446412251330912
Validation loss = 0.0005478613893501461
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005767507245764136
Validation loss = 0.0006554464343935251
Validation loss = 0.0005618665600195527
Validation loss = 0.0005342356162145734
Validation loss = 0.0005975210224278271
Validation loss = 0.000529804325196892
Validation loss = 0.0005928154569119215
Validation loss = 0.0005989808705635369
Validation loss = 0.0005538808181881905
Validation loss = 0.0005042488337494433
Validation loss = 0.0005487229791469872
Validation loss = 0.0005073213251307607
Validation loss = 0.0005311484565027058
Validation loss = 0.0005249135429039598
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005699768662452698
Validation loss = 0.0005885984282940626
Validation loss = 0.0005622927565127611
Validation loss = 0.0005711437552236021
Validation loss = 0.0005885392893105745
Validation loss = 0.0005463913548737764
Validation loss = 0.0005529579939320683
Validation loss = 0.0005614855908788741
Validation loss = 0.0006385940359905362
Validation loss = 0.0005910024046897888
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 35       |
| MaximumReturn | 199      |
| MinimumReturn | 128      |
| TotalSamples  | 123321   |
----------------------------
itr #36 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006239803624339402
Validation loss = 0.0005831320304423571
Validation loss = 0.0005625421181321144
Validation loss = 0.0005850453162565827
Validation loss = 0.0004971396992914379
Validation loss = 0.0005031446926295757
Validation loss = 0.0005362073425203562
Validation loss = 0.0005010712193325162
Validation loss = 0.0005100429407320917
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0009947267826646566
Validation loss = 0.0006277914508245885
Validation loss = 0.0005222595646046102
Validation loss = 0.0005513208452612162
Validation loss = 0.0005054528010077775
Validation loss = 0.0005169337964616716
Validation loss = 0.0005860010860487819
Validation loss = 0.0005469847237691283
Validation loss = 0.0008584920433349907
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0005369170103222132
Validation loss = 0.0005687777884304523
Validation loss = 0.0006449814536608756
Validation loss = 0.0005106208263896406
Validation loss = 0.0005480656982399523
Validation loss = 0.0005897883675061166
Validation loss = 0.0006384277367033064
Validation loss = 0.0005176711129024625
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005155834369361401
Validation loss = 0.0005618738359771669
Validation loss = 0.000584583671297878
Validation loss = 0.0005739957559853792
Validation loss = 0.0006558471941389143
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005741631612181664
Validation loss = 0.0006237952620722353
Validation loss = 0.0005172904348000884
Validation loss = 0.0006843095179647207
Validation loss = 0.000497166533023119
Validation loss = 0.000641523627564311
Validation loss = 0.0005704392679035664
Validation loss = 0.000533449521753937
Validation loss = 0.000508189492393285
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 36       |
| MaximumReturn | 199      |
| MinimumReturn | 135      |
| TotalSamples  | 126654   |
----------------------------
itr #37 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.000601993640884757
Validation loss = 0.000516012660227716
Validation loss = 0.0005571428337134421
Validation loss = 0.0007135132909752429
Validation loss = 0.0006015719845890999
Validation loss = 0.0005667342920787632
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006274203769862652
Validation loss = 0.0005777075421065092
Validation loss = 0.0007966510020196438
Validation loss = 0.0006112196715548635
Validation loss = 0.000498443900141865
Validation loss = 0.0006755056674592197
Validation loss = 0.0004988681175746024
Validation loss = 0.0006217022892087698
Validation loss = 0.0005435830098576844
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0007109323632903397
Validation loss = 0.0005472154007293284
Validation loss = 0.000638287456240505
Validation loss = 0.0005500601837411523
Validation loss = 0.0005197519203647971
Validation loss = 0.0005082946154288948
Validation loss = 0.0006605378584936261
Validation loss = 0.0006145368097350001
Validation loss = 0.000507008284330368
Validation loss = 0.0005432199104689062
Validation loss = 0.0005249092937447131
Validation loss = 0.0004935337929055095
Validation loss = 0.0005187516799196601
Validation loss = 0.0005883495323359966
Validation loss = 0.0006172694847919047
Validation loss = 0.0005910753388889134
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.000560689892154187
Validation loss = 0.0005445526912808418
Validation loss = 0.000559156178496778
Validation loss = 0.0005555542302317917
Validation loss = 0.0005168019561097026
Validation loss = 0.000537804386112839
Validation loss = 0.0005529273767024279
Validation loss = 0.0005612631211988628
Validation loss = 0.000561246182769537
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0006240088841877878
Validation loss = 0.000608589849434793
Validation loss = 0.0004777852154802531
Validation loss = 0.0006134194554761052
Validation loss = 0.000525414536241442
Validation loss = 0.0005445326096378267
Validation loss = 0.0005625037010759115
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 37       |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 129987   |
----------------------------
itr #38 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005075138178654015
Validation loss = 0.0005456518847495317
Validation loss = 0.0006005247123539448
Validation loss = 0.0005074166692793369
Validation loss = 0.0005931541090831161
Validation loss = 0.0005748235853388906
Validation loss = 0.000507818884216249
Validation loss = 0.0005062461132183671
Validation loss = 0.0004950507427565753
Validation loss = 0.0005968358018435538
Validation loss = 0.0005227109068073332
Validation loss = 0.0005653221160173416
Validation loss = 0.0005605040933005512
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005403714603744447
Validation loss = 0.0006040948792360723
Validation loss = 0.0005512438365258276
Validation loss = 0.0005906300502829254
Validation loss = 0.0005963791045360267
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0005323034129105508
Validation loss = 0.0006421483703888953
Validation loss = 0.0005915272049605846
Validation loss = 0.0005378878558985889
Validation loss = 0.0005035867798142135
Validation loss = 0.0007567545399069786
Validation loss = 0.0005189932999201119
Validation loss = 0.0005461500259116292
Validation loss = 0.0005282066995278001
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0006503360345959663
Validation loss = 0.0005669725360348821
Validation loss = 0.000518851331435144
Validation loss = 0.0005189906805753708
Validation loss = 0.0006165323429740965
Validation loss = 0.0004976709024049342
Validation loss = 0.0005249280366115272
Validation loss = 0.0005047114100307226
Validation loss = 0.0005097357789054513
Validation loss = 0.000508636818267405
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005742349894717336
Validation loss = 0.0006461211596615613
Validation loss = 0.000576759863179177
Validation loss = 0.0005392929306253791
Validation loss = 0.0005509979091584682
Validation loss = 0.0005502323620021343
Validation loss = 0.0005895207286812365
Validation loss = 0.0006149186519905925
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 38       |
| MaximumReturn | 199      |
| MinimumReturn | 120      |
| TotalSamples  | 133320   |
----------------------------
itr #39 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.000510006386321038
Validation loss = 0.0006030774675309658
Validation loss = 0.000553563644643873
Validation loss = 0.0005753018194809556
Validation loss = 0.0005108381737954915
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005163361201994121
Validation loss = 0.0006028391071595252
Validation loss = 0.0005017961957491934
Validation loss = 0.0005096602253615856
Validation loss = 0.000568155839573592
Validation loss = 0.0005535588134080172
Validation loss = 0.0004883629153482616
Validation loss = 0.000528813514392823
Validation loss = 0.0005698019522242248
Validation loss = 0.0005357031477615237
Validation loss = 0.00058000348508358
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0005867732688784599
Validation loss = 0.0006443335441872478
Validation loss = 0.0005275214207358658
Validation loss = 0.0005182361928746104
Validation loss = 0.0005434043705463409
Validation loss = 0.0005378589848987758
Validation loss = 0.000508341530803591
Validation loss = 0.0006746268481947482
Validation loss = 0.0005367154954001307
Validation loss = 0.0005425521521829069
Validation loss = 0.0004974317853339016
Validation loss = 0.0005640837480314076
Validation loss = 0.0005909872124902904
Validation loss = 0.0005365771939978004
Validation loss = 0.0005014647031202912
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005572207737714052
Validation loss = 0.0005102994618937373
Validation loss = 0.0007606632425449789
Validation loss = 0.0006100254831835628
Validation loss = 0.0004912615404464304
Validation loss = 0.0005849708686582744
Validation loss = 0.0005810094298794866
Validation loss = 0.0005917781381867826
Validation loss = 0.0005264465580694377
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005177662242203951
Validation loss = 0.0005091738421469927
Validation loss = 0.000519949069712311
Validation loss = 0.0005307337851263583
Validation loss = 0.0005498118116520345
Validation loss = 0.0004995496128685772
Validation loss = 0.0005955992382951081
Validation loss = 0.0005986085161566734
Validation loss = 0.000475188106065616
Validation loss = 0.0005093883955851197
Validation loss = 0.0005404126713983715
Validation loss = 0.0006344913854263723
Validation loss = 0.0006300872773863375
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 178      |
| Iteration     | 39       |
| MaximumReturn | 199      |
| MinimumReturn | 125      |
| TotalSamples  | 136653   |
----------------------------
