Logging to experiments/pendulum/test-dir-exp/Mon-21-Nov-2022-08-11-44-PM-CST_pendulum_trpo_iteration_20_seed2431
Print configuration .....
{'env_name': 'pendulum', 'random_seeds': [3214, 2431, 2531, 2231], 'save_variables': False, 'model_save_dir': '/tmp/pendulum_models/', 'restore_variables': False, 'start_onpol_iter': 0, 'onpol_iters': 40, 'num_path_random': 25, 'num_path_onpol': 25, 'env_horizon': 200, 'max_train_data': 200000, 'max_val_data': 100000, 'discard_ratio': 0.0, 'dynamics': {'pre_training': {'mode': 'intrinsic_reward', 'itr': 0, 'policy_itr': 20}, 'model': 'nn', 'ensemble': True, 'ensemble_model_count': 5, 'enable_particle_ensemble': True, 'particles': 5, 'obs_var': 1.0, 'intrinsic_reward_coeff': 1.0, 'ita': 1.0, 'mode': 'random', 'val': True, 'n_layers': 4, 'hidden_size': 1000, 'activation': 'relu', 'batch_size': 1000, 'learning_rate': 0.001, 'reg_coeff': 0.0, 'epochs': 200, 'kfac_params': {'learning_rate': 0.1, 'damping': 0.001, 'momentum': 0.9, 'kl_clip': 0.0001, 'cov_ema_decay': 0.99}}, 'policy': {'network_shape': [64, 64], 'init_logstd': 0.0, 'activation': 'tanh', 'reinitialize_every_itr': False}, 'trpo': {'horizon': 200, 'gamma': 0.99, 'step_size': 0.01, 'iterations': 20, 'batch_size': 50000, 'gae': 0.95, 'visualization': False, 'visualize_iterations': [0]}, 'algo': 'trpo'}
Generating random rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating random rollouts.
Creating normalization for training data.
Done creating normalization for training data.
Particle ensemble enabled? True
An ensemble of 5 dynamics model <class 'model.dynamics.NNDynamicsModel'> initialized
Train dynamics model with intrinsic reward only? False
Pre-training enabled. Using only intrinsic reward.
Pre-training dynamics model for 0 iterations...
Done pre-training dynamics model.
Using external reward only.
itr #0 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.34107711911201477
Validation loss = 0.03741823136806488
Validation loss = 0.008189938962459564
Validation loss = 0.002445223508402705
Validation loss = 0.001355601823888719
Validation loss = 0.001011050189845264
Validation loss = 0.0008992546936497092
Validation loss = 0.0009288527071475983
Validation loss = 0.0012117358855903149
Validation loss = 0.0027321409434080124
Validation loss = 0.0013075759634375572
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.35803213715553284
Validation loss = 0.030723189935088158
Validation loss = 0.009596015326678753
Validation loss = 0.0036132056266069412
Validation loss = 0.0016741377767175436
Validation loss = 0.001212244387716055
Validation loss = 0.0009557233424857259
Validation loss = 0.0008997495751827955
Validation loss = 0.0008659314480610192
Validation loss = 0.0008459621458314359
Validation loss = 0.0011728123063221574
Validation loss = 0.0010472513968124986
Validation loss = 0.0011469011660665274
Validation loss = 0.0010532184969633818
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3177137076854706
Validation loss = 0.039134494960308075
Validation loss = 0.012683439068496227
Validation loss = 0.0040267580188810825
Validation loss = 0.001800870057195425
Validation loss = 0.0011439858935773373
Validation loss = 0.0008802986121736467
Validation loss = 0.0008981052087619901
Validation loss = 0.0008376803598366678
Validation loss = 0.0008251921972259879
Validation loss = 0.0007728675263933837
Validation loss = 0.0008000073721632361
Validation loss = 0.0007839628960937262
Validation loss = 0.0008266080403700471
Validation loss = 0.001087121432647109
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.31630632281303406
Validation loss = 0.02997945062816143
Validation loss = 0.011492851190268993
Validation loss = 0.002953123766928911
Validation loss = 0.001432883320376277
Validation loss = 0.0011074566282331944
Validation loss = 0.0009584896033629775
Validation loss = 0.0008827921701595187
Validation loss = 0.0010051373392343521
Validation loss = 0.0012395846424624324
Validation loss = 0.0007451770361512899
Validation loss = 0.0009227420669049025
Validation loss = 0.0012407536851242185
Validation loss = 0.0009160665795207024
Validation loss = 0.0009416893590241671
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.29342395067214966
Validation loss = 0.04012054577469826
Validation loss = 0.007979600690305233
Validation loss = 0.003877080511301756
Validation loss = 0.0018855498638004065
Validation loss = 0.001084457733668387
Validation loss = 0.0009580565383657813
Validation loss = 0.00087915820768103
Validation loss = 0.0007766071939840913
Validation loss = 0.0009174435981549323
Validation loss = 0.0008269030367955565
Validation loss = 0.0009405012242496014
Validation loss = 0.0007756787235848606
Validation loss = 0.001312293577939272
Validation loss = 0.0007439266773872077
Validation loss = 0.001165355322882533
Validation loss = 0.0009000210557132959
Validation loss = 0.0021281461231410503
Validation loss = 0.0011830395087599754
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 162      |
| Iteration     | 0        |
| MaximumReturn | 199      |
| MinimumReturn | 95.7     |
| TotalSamples  | 6666     |
----------------------------
itr #1 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.021244868636131287
Validation loss = 0.002275835955515504
Validation loss = 0.0016414265846833587
Validation loss = 0.0013392422115430236
Validation loss = 0.0018858802504837513
Validation loss = 0.001492829411290586
Validation loss = 0.0016004504868760705
Validation loss = 0.0019298753468319774
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.02861875295639038
Validation loss = 0.003142228350043297
Validation loss = 0.0015940186567604542
Validation loss = 0.001400907407514751
Validation loss = 0.0013729077763855457
Validation loss = 0.001071109320037067
Validation loss = 0.001995030790567398
Validation loss = 0.0018782926490530372
Validation loss = 0.003237705444917083
Validation loss = 0.0017185417236760259
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.018268147483468056
Validation loss = 0.002880912274122238
Validation loss = 0.0021780540701001883
Validation loss = 0.0015046376502141356
Validation loss = 0.0014395239995792508
Validation loss = 0.0015188344987109303
Validation loss = 0.0020168761257082224
Validation loss = 0.0013975990004837513
Validation loss = 0.0018615490989759564
Validation loss = 0.001976871630176902
Validation loss = 0.004532648716121912
Validation loss = 0.0017388748237863183
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.03886665776371956
Validation loss = 0.0035365447402000427
Validation loss = 0.0016192025505006313
Validation loss = 0.0013615129282698035
Validation loss = 0.001457041478715837
Validation loss = 0.0019368347711861134
Validation loss = 0.0013536703772842884
Validation loss = 0.0013322049053385854
Validation loss = 0.001680476125329733
Validation loss = 0.0024704141542315483
Validation loss = 0.0015831399941816926
Validation loss = 0.0021360209211707115
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.020835528150200844
Validation loss = 0.003704479895532131
Validation loss = 0.0018740660743787885
Validation loss = 0.002433989429846406
Validation loss = 0.0022221014369279146
Validation loss = 0.0012867203913629055
Validation loss = 0.002235881518572569
Validation loss = 0.0019400512101128697
Validation loss = 0.004214433953166008
Validation loss = 0.002043425105512142
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 1        |
| MaximumReturn | 199      |
| MinimumReturn | 120      |
| TotalSamples  | 9999     |
----------------------------
itr #2 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.005838322918862104
Validation loss = 0.002519842702895403
Validation loss = 0.0012596556916832924
Validation loss = 0.004206090234220028
Validation loss = 0.002192223444581032
Validation loss = 0.0011514205252751708
Validation loss = 0.0016417447477579117
Validation loss = 0.001420966349542141
Validation loss = 0.0018508397042751312
Validation loss = 0.0012401400599628687
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.006097009871155024
Validation loss = 0.0017501444090157747
Validation loss = 0.0010911559220403433
Validation loss = 0.0021183521021157503
Validation loss = 0.0011874799383804202
Validation loss = 0.0018550228560343385
Validation loss = 0.003782542422413826
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.007202950306236744
Validation loss = 0.0013675799127668142
Validation loss = 0.003708362579345703
Validation loss = 0.002780424663797021
Validation loss = 0.0013156727654859424
Validation loss = 0.001384897856041789
Validation loss = 0.0025259037502110004
Validation loss = 0.0019232105696573853
Validation loss = 0.001242766622453928
Validation loss = 0.0014573454391211271
Validation loss = 0.0014424684923142195
Validation loss = 0.003996150568127632
Validation loss = 0.0016254449728876352
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.005534901283681393
Validation loss = 0.0016632707556709647
Validation loss = 0.002298798179253936
Validation loss = 0.0027793024200946093
Validation loss = 0.001278334530070424
Validation loss = 0.001903362339362502
Validation loss = 0.005152000114321709
Validation loss = 0.002126172184944153
Validation loss = 0.0020495629869401455
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.007596385665237904
Validation loss = 0.0020634722895920277
Validation loss = 0.0012809212785214186
Validation loss = 0.0012684606481343508
Validation loss = 0.001998378662392497
Validation loss = 0.002991021377965808
Validation loss = 0.0020135159138590097
Validation loss = 0.0038850842975080013
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 2        |
| MaximumReturn | 199      |
| MinimumReturn | 122      |
| TotalSamples  | 13332    |
----------------------------
itr #3 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.01325937733054161
Validation loss = 0.0015019457787275314
Validation loss = 0.0015901788137853146
Validation loss = 0.0013378072762861848
Validation loss = 0.001630029291845858
Validation loss = 0.0026644812896847725
Validation loss = 0.0016428222879767418
Validation loss = 0.001395262312144041
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0072775972075760365
Validation loss = 0.001770688802935183
Validation loss = 0.001781171071343124
Validation loss = 0.0037801796570420265
Validation loss = 0.0018086518393829465
Validation loss = 0.0027819795068353415
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.004734165035188198
Validation loss = 0.001050150254741311
Validation loss = 0.0021903621964156628
Validation loss = 0.007287172134965658
Validation loss = 0.0014111739583313465
Validation loss = 0.0015181266935542226
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.005042436998337507
Validation loss = 0.002469460479915142
Validation loss = 0.0012276888592168689
Validation loss = 0.0020934862550348043
Validation loss = 0.0029652866069227457
Validation loss = 0.0026383677031844854
Validation loss = 0.0030785405542701483
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.005419803783297539
Validation loss = 0.001525163184851408
Validation loss = 0.0009682184900157154
Validation loss = 0.003306136466562748
Validation loss = 0.002005941467359662
Validation loss = 0.0017117136158049107
Validation loss = 0.002455400535836816
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 3        |
| MaximumReturn | 199      |
| MinimumReturn | 116      |
| TotalSamples  | 16665    |
----------------------------
itr #4 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0024698355700820684
Validation loss = 0.0012261837255209684
Validation loss = 0.0012544712517410517
Validation loss = 0.001270391047000885
Validation loss = 0.0014373429585248232
Validation loss = 0.0016626894939690828
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0028139252681285143
Validation loss = 0.0021545025520026684
Validation loss = 0.0010413432028144598
Validation loss = 0.001739387633278966
Validation loss = 0.001175830140709877
Validation loss = 0.0026179119013249874
Validation loss = 0.0017016470665112138
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.002966976957395673
Validation loss = 0.0010575602063909173
Validation loss = 0.003367682918906212
Validation loss = 0.0016896903980523348
Validation loss = 0.001691729063168168
Validation loss = 0.0026947222650051117
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00409154687076807
Validation loss = 0.0014821570366621017
Validation loss = 0.0017036327626556158
Validation loss = 0.002732995431870222
Validation loss = 0.0016823806799948215
Validation loss = 0.0015571583062410355
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.006861981935799122
Validation loss = 0.0013104507233947515
Validation loss = 0.0014940965920686722
Validation loss = 0.0013595323543995619
Validation loss = 0.00163774355314672
Validation loss = 0.0023991779889911413
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 4        |
| MaximumReturn | 199      |
| MinimumReturn | 123      |
| TotalSamples  | 19998    |
----------------------------
itr #5 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.001933250343427062
Validation loss = 0.0013009767280891538
Validation loss = 0.0035843984223902225
Validation loss = 0.001378005719743669
Validation loss = 0.0013431345578283072
Validation loss = 0.001233699033036828
Validation loss = 0.0015259445644915104
Validation loss = 0.002144280821084976
Validation loss = 0.002164930570870638
Validation loss = 0.001132623990997672
Validation loss = 0.001405012677423656
Validation loss = 0.0014188557397574186
Validation loss = 0.002441821387037635
Validation loss = 0.0018529355293139815
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0026168262120336294
Validation loss = 0.001413947669789195
Validation loss = 0.0016520144417881966
Validation loss = 0.0013924086233600974
Validation loss = 0.0016640124376863241
Validation loss = 0.0036619429010897875
Validation loss = 0.0010260798735544086
Validation loss = 0.001272216672077775
Validation loss = 0.0018416072707623243
Validation loss = 0.0019295495003461838
Validation loss = 0.001883210614323616
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00209707859903574
Validation loss = 0.0011540388222783804
Validation loss = 0.001923039206303656
Validation loss = 0.0011838807258754969
Validation loss = 0.0016635708743706346
Validation loss = 0.0013007719535380602
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00485644256696105
Validation loss = 0.002324413973838091
Validation loss = 0.0021802796982228756
Validation loss = 0.0009204621310345829
Validation loss = 0.0023492895998060703
Validation loss = 0.002753955777734518
Validation loss = 0.0015948141226544976
Validation loss = 0.0016479594632983208
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.004999409895390272
Validation loss = 0.0014773162547498941
Validation loss = 0.0012761906255036592
Validation loss = 0.0026159847620874643
Validation loss = 0.0016015328001230955
Validation loss = 0.0017118558753281832
Validation loss = 0.001056891167536378
Validation loss = 0.0012483249884098768
Validation loss = 0.002272604499012232
Validation loss = 0.002254161750897765
Validation loss = 0.0012489162618294358
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 5        |
| MaximumReturn | 199      |
| MinimumReturn | 124      |
| TotalSamples  | 23331    |
----------------------------
itr #6 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0022110731806606054
Validation loss = 0.0010862525086849928
Validation loss = 0.001289435545913875
Validation loss = 0.001471312833018601
Validation loss = 0.0013018720783293247
Validation loss = 0.0009039234137162566
Validation loss = 0.0017423775279894471
Validation loss = 0.0009441329166293144
Validation loss = 0.0011181544978171587
Validation loss = 0.0012510091764852405
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.007320028264075518
Validation loss = 0.0014394045574590564
Validation loss = 0.0014051190810278058
Validation loss = 0.0013759284047409892
Validation loss = 0.002204613061621785
Validation loss = 0.001503862557001412
Validation loss = 0.003153872676193714
Validation loss = 0.0010569465812295675
Validation loss = 0.0013945361133664846
Validation loss = 0.0014277989976108074
Validation loss = 0.0013213808415457606
Validation loss = 0.0022248325403779745
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.007362143602222204
Validation loss = 0.001710812677629292
Validation loss = 0.0012190134730190039
Validation loss = 0.0012231669388711452
Validation loss = 0.0018578532617539167
Validation loss = 0.0010646908776834607
Validation loss = 0.0018681336659938097
Validation loss = 0.0011562873842194676
Validation loss = 0.0013419230235740542
Validation loss = 0.0011805918766185641
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0015606475062668324
Validation loss = 0.001666997093707323
Validation loss = 0.001565636950545013
Validation loss = 0.0017988082254305482
Validation loss = 0.0017353636212646961
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0021239412017166615
Validation loss = 0.0010799403535202146
Validation loss = 0.0015340539393946528
Validation loss = 0.002571982564404607
Validation loss = 0.0013460447080433369
Validation loss = 0.0012215852038934827
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 6        |
| MaximumReturn | 198      |
| MinimumReturn | 129      |
| TotalSamples  | 26664    |
----------------------------
itr #7 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0017421977827325463
Validation loss = 0.0015083766775205731
Validation loss = 0.0021893978118896484
Validation loss = 0.0011455949861556292
Validation loss = 0.0015199712943285704
Validation loss = 0.0016270766500383615
Validation loss = 0.0013849798124283552
Validation loss = 0.001121862675063312
Validation loss = 0.0011012161849066615
Validation loss = 0.0015327155124396086
Validation loss = 0.0008746355306357145
Validation loss = 0.0009161931229755282
Validation loss = 0.0014244831399992108
Validation loss = 0.0009938734583556652
Validation loss = 0.0008390113362111151
Validation loss = 0.0014473357005044818
Validation loss = 0.0013218815438449383
Validation loss = 0.001180885243229568
Validation loss = 0.0009773178026080132
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.002758797723799944
Validation loss = 0.0013369370717555285
Validation loss = 0.0012132555712014437
Validation loss = 0.001182912033982575
Validation loss = 0.0010206270962953568
Validation loss = 0.002060168655589223
Validation loss = 0.0019790572114288807
Validation loss = 0.001055232365615666
Validation loss = 0.0012295001652091742
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0019697831012308598
Validation loss = 0.001059316797181964
Validation loss = 0.0008068351307883859
Validation loss = 0.001508242217823863
Validation loss = 0.0008477448718622327
Validation loss = 0.0016589314909651875
Validation loss = 0.0011736400192603469
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.002193152206018567
Validation loss = 0.0016283791046589613
Validation loss = 0.002395734190940857
Validation loss = 0.0017595806857571006
Validation loss = 0.0010841122129932046
Validation loss = 0.000898463127668947
Validation loss = 0.0022411015816032887
Validation loss = 0.0021424873266369104
Validation loss = 0.0010809405939653516
Validation loss = 0.0016372972168028355
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0019454392604529858
Validation loss = 0.0015704217366874218
Validation loss = 0.0009297834476456046
Validation loss = 0.0008692772244103253
Validation loss = 0.0021081874147057533
Validation loss = 0.0012591580161824822
Validation loss = 0.0011336839525029063
Validation loss = 0.0010552193270996213
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 7        |
| MaximumReturn | 199      |
| MinimumReturn | 132      |
| TotalSamples  | 29997    |
----------------------------
itr #8 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0014554292429238558
Validation loss = 0.0009945629863068461
Validation loss = 0.0010004299692809582
Validation loss = 0.0012615412706509233
Validation loss = 0.00078477623173967
Validation loss = 0.0020609735511243343
Validation loss = 0.0007726842304691672
Validation loss = 0.0007883750949986279
Validation loss = 0.0008219508454203606
Validation loss = 0.0007950764847919345
Validation loss = 0.0009355473448522389
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0013930638087913394
Validation loss = 0.0011749453842639923
Validation loss = 0.0009312839829362929
Validation loss = 0.0011750345584005117
Validation loss = 0.0018922804156318307
Validation loss = 0.0009677730267867446
Validation loss = 0.001035728259012103
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.001212278031744063
Validation loss = 0.0008326203678734601
Validation loss = 0.0018913570092990994
Validation loss = 0.001054546912200749
Validation loss = 0.0009494292316958308
Validation loss = 0.00173567037563771
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0015716316411271691
Validation loss = 0.0015813310164958239
Validation loss = 0.0013455543667078018
Validation loss = 0.0010644588619470596
Validation loss = 0.0013452075654640794
Validation loss = 0.0024609703104943037
Validation loss = 0.0012604993535205722
Validation loss = 0.0011576761025935411
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0014256983995437622
Validation loss = 0.001496488694101572
Validation loss = 0.0008745990344323218
Validation loss = 0.0007677351823076606
Validation loss = 0.001908961683511734
Validation loss = 0.0013348241336643696
Validation loss = 0.0013277853140607476
Validation loss = 0.001008233055472374
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 8        |
| MaximumReturn | 199      |
| MinimumReturn | 114      |
| TotalSamples  | 33330    |
----------------------------
itr #9 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0013562728418037295
Validation loss = 0.001032116822898388
Validation loss = 0.0017329290276393294
Validation loss = 0.0010425206273794174
Validation loss = 0.0007961206138134003
Validation loss = 0.0011497379746288061
Validation loss = 0.0010655014775693417
Validation loss = 0.0011322640348225832
Validation loss = 0.001033994136378169
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.001212886767461896
Validation loss = 0.0013141442323103547
Validation loss = 0.0008086309535428882
Validation loss = 0.0010986838024109602
Validation loss = 0.0009532804833725095
Validation loss = 0.0011698718881234527
Validation loss = 0.0010829797247424722
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.002815869404003024
Validation loss = 0.0025134244933724403
Validation loss = 0.0012176125310361385
Validation loss = 0.0007265288149937987
Validation loss = 0.0010939741041511297
Validation loss = 0.001244169194251299
Validation loss = 0.0008546521421521902
Validation loss = 0.001165549154393375
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0011223445180803537
Validation loss = 0.0010060416534543037
Validation loss = 0.0016146611887961626
Validation loss = 0.0010593109764158726
Validation loss = 0.0010503113735467196
Validation loss = 0.0007969734142534435
Validation loss = 0.000916084973141551
Validation loss = 0.0008386712870560586
Validation loss = 0.0015555506106466055
Validation loss = 0.0011571921641007066
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0018599688773974776
Validation loss = 0.0007856021984480321
Validation loss = 0.001050999155268073
Validation loss = 0.0010017469758167863
Validation loss = 0.0010310697834938765
Validation loss = 0.0008195411064662039
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 9        |
| MaximumReturn | 199      |
| MinimumReturn | 114      |
| TotalSamples  | 36663    |
----------------------------
itr #10 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0008905840222723782
Validation loss = 0.0009939988376572728
Validation loss = 0.0012330948375165462
Validation loss = 0.0008509138133376837
Validation loss = 0.0009617109899409115
Validation loss = 0.0008841741364449263
Validation loss = 0.001087577547878027
Validation loss = 0.0008069306495599449
Validation loss = 0.0007557173375971615
Validation loss = 0.0005670808022841811
Validation loss = 0.0007767081260681152
Validation loss = 0.000989730004221201
Validation loss = 0.0008234406122937799
Validation loss = 0.0010578344808891416
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0009097899892367423
Validation loss = 0.0016243859427049756
Validation loss = 0.0013472969876602292
Validation loss = 0.001130095450207591
Validation loss = 0.001698362990282476
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.000763521296903491
Validation loss = 0.0014816138427704573
Validation loss = 0.0011221630265936255
Validation loss = 0.0007440160261467099
Validation loss = 0.0011196041014045477
Validation loss = 0.0008336702012456954
Validation loss = 0.001033162698149681
Validation loss = 0.0014687805669382215
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0010327979689463973
Validation loss = 0.0011398743372410536
Validation loss = 0.0013281676219776273
Validation loss = 0.0007664609584026039
Validation loss = 0.0010210290784016252
Validation loss = 0.0010185523424297571
Validation loss = 0.000687845516949892
Validation loss = 0.0013306665932759643
Validation loss = 0.0008222766919061542
Validation loss = 0.0011086664162576199
Validation loss = 0.0009999906178563833
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0015242265071719885
Validation loss = 0.0010850451653823256
Validation loss = 0.0007051530410535634
Validation loss = 0.0008166040643118322
Validation loss = 0.0010835197754204273
Validation loss = 0.0008771560387685895
Validation loss = 0.0007172072073444724
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 10       |
| MaximumReturn | 199      |
| MinimumReturn | 123      |
| TotalSamples  | 39996    |
----------------------------
itr #11 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0013814257690683007
Validation loss = 0.0008309410768561065
Validation loss = 0.000722649390809238
Validation loss = 0.0008356004254892468
Validation loss = 0.0007708884077146649
Validation loss = 0.0011590864742174745
Validation loss = 0.0006081099854782224
Validation loss = 0.0009152181446552277
Validation loss = 0.0008839790825732052
Validation loss = 0.0009568657842464745
Validation loss = 0.0008525910088792443
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0014076391234993935
Validation loss = 0.002311452990397811
Validation loss = 0.0008649863302707672
Validation loss = 0.0009927480714395642
Validation loss = 0.0008360535721294582
Validation loss = 0.000606366025749594
Validation loss = 0.001162329688668251
Validation loss = 0.001299166353419423
Validation loss = 0.0011054102797061205
Validation loss = 0.0012128263479098678
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0019517118344083428
Validation loss = 0.0010492473375052214
Validation loss = 0.0006222025258466601
Validation loss = 0.000875266152434051
Validation loss = 0.0006366966408677399
Validation loss = 0.0021857707761228085
Validation loss = 0.0012490765657275915
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0010483432561159134
Validation loss = 0.0010652727214619517
Validation loss = 0.001067228615283966
Validation loss = 0.0009376808302477002
Validation loss = 0.0011706853983923793
Validation loss = 0.000994364614598453
Validation loss = 0.0007957700872793794
Validation loss = 0.0006634712335653603
Validation loss = 0.001137390499934554
Validation loss = 0.0009710858575999737
Validation loss = 0.0011908828746527433
Validation loss = 0.0012878733687102795
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0012675300240516663
Validation loss = 0.001586059806868434
Validation loss = 0.0014631798258051276
Validation loss = 0.0009268316207453609
Validation loss = 0.0008531323401257396
Validation loss = 0.0007532162708230317
Validation loss = 0.0012944566551595926
Validation loss = 0.001667379168793559
Validation loss = 0.0007801034371368587
Validation loss = 0.0009850601200014353
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 181      |
| Iteration     | 11       |
| MaximumReturn | 199      |
| MinimumReturn | 130      |
| TotalSamples  | 43329    |
----------------------------
itr #12 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0008696441655047238
Validation loss = 0.0008403058163821697
Validation loss = 0.0010424277279525995
Validation loss = 0.00084255775436759
Validation loss = 0.0006440534489229321
Validation loss = 0.0008936189115047455
Validation loss = 0.0007608341984450817
Validation loss = 0.0006693719769828022
Validation loss = 0.000609248410910368
Validation loss = 0.0011734156869351864
Validation loss = 0.000653665978461504
Validation loss = 0.0007485132664442062
Validation loss = 0.0009289964800700545
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0013412772677838802
Validation loss = 0.001008940627798438
Validation loss = 0.0009126202203333378
Validation loss = 0.0016682696295902133
Validation loss = 0.0007703126757405698
Validation loss = 0.0016020610928535461
Validation loss = 0.001126033952459693
Validation loss = 0.0008478888194076717
Validation loss = 0.0009988837409764528
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00191969214938581
Validation loss = 0.0013222380075603724
Validation loss = 0.0011093744542449713
Validation loss = 0.0014197762357071042
Validation loss = 0.0007206649170257151
Validation loss = 0.0009355208603665233
Validation loss = 0.0009956401772797108
Validation loss = 0.0009943591430783272
Validation loss = 0.0006960176979191601
Validation loss = 0.0007093905587680638
Validation loss = 0.0010705654276534915
Validation loss = 0.0007069469429552555
Validation loss = 0.0006896121776662767
Validation loss = 0.0009564103675074875
Validation loss = 0.0008219020091928542
Validation loss = 0.0012616250896826386
Validation loss = 0.0007379542221315205
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007536827470175922
Validation loss = 0.0009592006681486964
Validation loss = 0.0009551161783747375
Validation loss = 0.002009639283642173
Validation loss = 0.0007794945267960429
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0011428643483668566
Validation loss = 0.0007091865409165621
Validation loss = 0.0010564798722043633
Validation loss = 0.0008819042122922838
Validation loss = 0.0006931532989256084
Validation loss = 0.0010093589080497622
Validation loss = 0.0007599788368679583
Validation loss = 0.0008533743093721569
Validation loss = 0.0010670904302969575
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 12       |
| MaximumReturn | 198      |
| MinimumReturn | 126      |
| TotalSamples  | 46662    |
----------------------------
itr #13 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006148659740574658
Validation loss = 0.0006743071717210114
Validation loss = 0.0006752954795956612
Validation loss = 0.0007396664586849511
Validation loss = 0.0011976471869274974
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0007931043510325253
Validation loss = 0.0014385208487510681
Validation loss = 0.0006876021507196128
Validation loss = 0.0011587059125304222
Validation loss = 0.000850206648465246
Validation loss = 0.000786814431194216
Validation loss = 0.0008176307892426848
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.001260923920199275
Validation loss = 0.000959160621277988
Validation loss = 0.000732649932615459
Validation loss = 0.0006599109037779272
Validation loss = 0.001094001461751759
Validation loss = 0.0007018374162726104
Validation loss = 0.0006348598981276155
Validation loss = 0.0011024476261809468
Validation loss = 0.0008698223973624408
Validation loss = 0.0008906881557777524
Validation loss = 0.0008027328294701874
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.001104809925891459
Validation loss = 0.0007549806032329798
Validation loss = 0.0006958338781259954
Validation loss = 0.0008636026177555323
Validation loss = 0.0009963063057512045
Validation loss = 0.0006916458951309323
Validation loss = 0.0008841253584250808
Validation loss = 0.000855268444865942
Validation loss = 0.0006539271562360227
Validation loss = 0.0007678261608816683
Validation loss = 0.0009541952167637646
Validation loss = 0.0006893805111758411
Validation loss = 0.0007520486251451075
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0010714156087487936
Validation loss = 0.0007589912856929004
Validation loss = 0.0008059930405579507
Validation loss = 0.0009379549883306026
Validation loss = 0.0010062853107228875
Validation loss = 0.0007353663677349687
Validation loss = 0.0007699034758843482
Validation loss = 0.0009580563637427986
Validation loss = 0.0007005133084021509
Validation loss = 0.0008006430580280721
Validation loss = 0.0007479360210709274
Validation loss = 0.0007410991820506752
Validation loss = 0.001442336244508624
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 13       |
| MaximumReturn | 199      |
| MinimumReturn | 121      |
| TotalSamples  | 49995    |
----------------------------
itr #14 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.000636956247035414
Validation loss = 0.0006197808543220162
Validation loss = 0.0011841696687042713
Validation loss = 0.0006153089343570173
Validation loss = 0.0005231230752542615
Validation loss = 0.0007812442490831017
Validation loss = 0.0009389757178723812
Validation loss = 0.0006344480207189918
Validation loss = 0.0005948979523964226
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0009857222903519869
Validation loss = 0.0007450404809787869
Validation loss = 0.0009648107225075364
Validation loss = 0.0009616675088182092
Validation loss = 0.0007862123311497271
Validation loss = 0.0006920239538885653
Validation loss = 0.0007753625395707786
Validation loss = 0.0007887824904173613
Validation loss = 0.0006526628858409822
Validation loss = 0.0006662550731562078
Validation loss = 0.0007625932921655476
Validation loss = 0.0007983304094523191
Validation loss = 0.0007980606169439852
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0007298163254745305
Validation loss = 0.0016527775442227721
Validation loss = 0.0006233120220713317
Validation loss = 0.0008720066980458796
Validation loss = 0.0007099003414623439
Validation loss = 0.0006922630709595978
Validation loss = 0.0007833793060854077
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0006755280774086714
Validation loss = 0.0012424117885529995
Validation loss = 0.0009046640479937196
Validation loss = 0.0005857674987055361
Validation loss = 0.0008021053508855402
Validation loss = 0.0008412262541241944
Validation loss = 0.0008968844776973128
Validation loss = 0.001070998958311975
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0010088215349242091
Validation loss = 0.0008451978792436421
Validation loss = 0.0006246389821171761
Validation loss = 0.0005906614824198186
Validation loss = 0.0006434957613237202
Validation loss = 0.0009351731860078871
Validation loss = 0.0007745948969386518
Validation loss = 0.0007851037662476301
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 14       |
| MaximumReturn | 199      |
| MinimumReturn | 121      |
| TotalSamples  | 53328    |
----------------------------
itr #15 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0009041522280313075
Validation loss = 0.0007124644471332431
Validation loss = 0.0006796351517550647
Validation loss = 0.0006427696789614856
Validation loss = 0.0006898983847349882
Validation loss = 0.0011159441201016307
Validation loss = 0.0008618333959020674
Validation loss = 0.0007334048277698457
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0007382754702121019
Validation loss = 0.0009267076384276152
Validation loss = 0.0006621122010983527
Validation loss = 0.0005756954196840525
Validation loss = 0.0008514613728038967
Validation loss = 0.0006991871050558984
Validation loss = 0.0006982122431509197
Validation loss = 0.0006539514288306236
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0007134120096452534
Validation loss = 0.0007588416920043528
Validation loss = 0.0005654616397805512
Validation loss = 0.0009465481271035969
Validation loss = 0.0007827531080693007
Validation loss = 0.0006293253973126411
Validation loss = 0.00081395119195804
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007823575870133936
Validation loss = 0.0008356447797268629
Validation loss = 0.0008417359204031527
Validation loss = 0.000788268051110208
Validation loss = 0.001025254838168621
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0006640509818680584
Validation loss = 0.0006328086601570249
Validation loss = 0.0007169396849349141
Validation loss = 0.0009395468514412642
Validation loss = 0.0008956985548138618
Validation loss = 0.0007371821557171643
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 15       |
| MaximumReturn | 199      |
| MinimumReturn | 134      |
| TotalSamples  | 56661    |
----------------------------
itr #16 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006918256403878331
Validation loss = 0.0007049388368614018
Validation loss = 0.000672429334372282
Validation loss = 0.0006766141741536558
Validation loss = 0.0006439033895730972
Validation loss = 0.0007161977700889111
Validation loss = 0.0007764341426081955
Validation loss = 0.000668082560878247
Validation loss = 0.0007770024240016937
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0007675469969399273
Validation loss = 0.0006794590735808015
Validation loss = 0.0009411113569512963
Validation loss = 0.0007039050688035786
Validation loss = 0.000664012914057821
Validation loss = 0.0005291571724228561
Validation loss = 0.0008513987995684147
Validation loss = 0.000863348541315645
Validation loss = 0.0007740326109342277
Validation loss = 0.0007037999457679689
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0007739022257737815
Validation loss = 0.0008257042500190437
Validation loss = 0.0009388708858750761
Validation loss = 0.001037953537888825
Validation loss = 0.0005950563354417682
Validation loss = 0.0006154438597150147
Validation loss = 0.0008167197229340672
Validation loss = 0.000613883079495281
Validation loss = 0.0006592560675926507
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0009827020112425089
Validation loss = 0.0006002858863212168
Validation loss = 0.0007542428211309016
Validation loss = 0.000988456537015736
Validation loss = 0.0008001994574442506
Validation loss = 0.0009189273114316165
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005514038493856788
Validation loss = 0.0007308544591069221
Validation loss = 0.0008096348610706627
Validation loss = 0.0006396104581654072
Validation loss = 0.0010155753698199987
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 16       |
| MaximumReturn | 199      |
| MinimumReturn | 117      |
| TotalSamples  | 59994    |
----------------------------
itr #17 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006140914047136903
Validation loss = 0.000613790878560394
Validation loss = 0.0006174753070808947
Validation loss = 0.0005244211643002927
Validation loss = 0.000554026395548135
Validation loss = 0.0006047766655683517
Validation loss = 0.000521775393281132
Validation loss = 0.0007357258582487702
Validation loss = 0.0005747920367866755
Validation loss = 0.0005413478356786072
Validation loss = 0.0005111576756462455
Validation loss = 0.0006227495614439249
Validation loss = 0.0006732682813890278
Validation loss = 0.0006781688425689936
Validation loss = 0.0007651616469956934
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.000619138649199158
Validation loss = 0.0006300946697592735
Validation loss = 0.0008367733680643141
Validation loss = 0.0008929678006097674
Validation loss = 0.0007709040073677897
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0006257332279346883
Validation loss = 0.0005533352959901094
Validation loss = 0.0008515244699083269
Validation loss = 0.0013921801000833511
Validation loss = 0.0006202594959177077
Validation loss = 0.0006595237646251917
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0006457692361436784
Validation loss = 0.0007115713669918478
Validation loss = 0.0008210573578253388
Validation loss = 0.0008676272118464112
Validation loss = 0.0006749607273377478
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0007274485542438924
Validation loss = 0.0005943002179265022
Validation loss = 0.0006217666668817401
Validation loss = 0.0006597069441340864
Validation loss = 0.0006748589803464711
Validation loss = 0.0006098997546359897
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 17       |
| MaximumReturn | 199      |
| MinimumReturn | 131      |
| TotalSamples  | 63327    |
----------------------------
itr #18 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0007861174526624382
Validation loss = 0.0007245442247949541
Validation loss = 0.0006633757147938013
Validation loss = 0.0006327999872155488
Validation loss = 0.0008452085894532502
Validation loss = 0.0006504737539216876
Validation loss = 0.0007223104475997388
Validation loss = 0.0004898653132840991
Validation loss = 0.0006523702759295702
Validation loss = 0.000600916042458266
Validation loss = 0.0005035815993323922
Validation loss = 0.0006190708954818547
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006811899365857244
Validation loss = 0.0007064013043418527
Validation loss = 0.0008147858316078782
Validation loss = 0.0005740646156482399
Validation loss = 0.0007788715302012861
Validation loss = 0.0005544519517570734
Validation loss = 0.0006358626415021718
Validation loss = 0.0005431376048363745
Validation loss = 0.0006051011150702834
Validation loss = 0.0007157324580475688
Validation loss = 0.0006694670300930738
Validation loss = 0.0005635961424559355
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0008074428769759834
Validation loss = 0.0007049216073937714
Validation loss = 0.0006087266956456006
Validation loss = 0.0006253950414247811
Validation loss = 0.000619878526777029
Validation loss = 0.000935975054744631
Validation loss = 0.000990876811556518
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005608875071629882
Validation loss = 0.0009680353105068207
Validation loss = 0.0006193986046127975
Validation loss = 0.0006317169754765928
Validation loss = 0.000605687964707613
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005533962976187468
Validation loss = 0.0007144059636630118
Validation loss = 0.0009390992927365005
Validation loss = 0.0006538823363371193
Validation loss = 0.0006132625858299434
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 18       |
| MaximumReturn | 199      |
| MinimumReturn | 118      |
| TotalSamples  | 66660    |
----------------------------
itr #19 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006738367956131697
Validation loss = 0.0004634995711967349
Validation loss = 0.0005145204486325383
Validation loss = 0.0005955504020676017
Validation loss = 0.0005168388597667217
Validation loss = 0.0007950086146593094
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0007293038070201874
Validation loss = 0.0006956972647458315
Validation loss = 0.0005413924809545279
Validation loss = 0.0006452174857258797
Validation loss = 0.0006434121751226485
Validation loss = 0.0005266397492960095
Validation loss = 0.0005990898353047669
Validation loss = 0.0006499275914393365
Validation loss = 0.0007560792146250606
Validation loss = 0.0005556669202633202
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0006106836954131722
Validation loss = 0.0008380063809454441
Validation loss = 0.0007353705004788935
Validation loss = 0.0008444330887869
Validation loss = 0.0006753738271072507
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005386205157265067
Validation loss = 0.0007056199247017503
Validation loss = 0.0006995322182774544
Validation loss = 0.0006957771838642657
Validation loss = 0.0006725251441821456
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0006387130124494433
Validation loss = 0.000690293381921947
Validation loss = 0.0008044952410273254
Validation loss = 0.0006456251139752567
Validation loss = 0.0005792281008325517
Validation loss = 0.0006931802490726113
Validation loss = 0.0006366267334669828
Validation loss = 0.0005413489998318255
Validation loss = 0.0008379130740649998
Validation loss = 0.0006139300530776381
Validation loss = 0.0006369617767632008
Validation loss = 0.0007589522865600884
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 19       |
| MaximumReturn | 199      |
| MinimumReturn | 88.2     |
| TotalSamples  | 69993    |
----------------------------
itr #20 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0007728651398792863
Validation loss = 0.0006269129808060825
Validation loss = 0.0006696204654872417
Validation loss = 0.0006209374405443668
Validation loss = 0.0006214678287506104
Validation loss = 0.0008936781086958945
Validation loss = 0.0006823245203122497
Validation loss = 0.0008326436509378254
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006867495831102133
Validation loss = 0.0007337419083341956
Validation loss = 0.0005942051066085696
Validation loss = 0.0008714562281966209
Validation loss = 0.0006980065372772515
Validation loss = 0.0005955731612630188
Validation loss = 0.0006789455655962229
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0007570508169010282
Validation loss = 0.0007080786162987351
Validation loss = 0.0008238108712248504
Validation loss = 0.0007280315621756017
Validation loss = 0.0008111690985970199
Validation loss = 0.0006901049637235701
Validation loss = 0.0007170239696279168
Validation loss = 0.0005343271768651903
Validation loss = 0.0007411121041513979
Validation loss = 0.0005658561130985618
Validation loss = 0.0007011647685430944
Validation loss = 0.0006158543401397765
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0008776507456786931
Validation loss = 0.0006135755102150142
Validation loss = 0.0005954744410701096
Validation loss = 0.0009408020414412022
Validation loss = 0.0005547776818275452
Validation loss = 0.0005578339332714677
Validation loss = 0.0007836567820049822
Validation loss = 0.0006744484417140484
Validation loss = 0.0005943301948718727
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0008309039403684437
Validation loss = 0.0005965654272586107
Validation loss = 0.00083709298633039
Validation loss = 0.00059799465816468
Validation loss = 0.000619773636572063
Validation loss = 0.0006873466772958636
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 20       |
| MaximumReturn | 199      |
| MinimumReturn | 95.1     |
| TotalSamples  | 73326    |
----------------------------
itr #21 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005878507508896291
Validation loss = 0.0005715329316444695
Validation loss = 0.0005528772599063814
Validation loss = 0.000521537906024605
Validation loss = 0.0006863839807920158
Validation loss = 0.0005093280924484134
Validation loss = 0.0007677260437048972
Validation loss = 0.0010172674665227532
Validation loss = 0.000498690758831799
Validation loss = 0.0007044437807053328
Validation loss = 0.0005030944012105465
Validation loss = 0.0007958863861858845
Validation loss = 0.00047435652231797576
Validation loss = 0.0007069265702739358
Validation loss = 0.000558705476578325
Validation loss = 0.0006559437606483698
Validation loss = 0.000531268771737814
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006173120345920324
Validation loss = 0.0005994415259920061
Validation loss = 0.0005620426964014769
Validation loss = 0.0006692285533063114
Validation loss = 0.0005029425956308842
Validation loss = 0.0008075431105680764
Validation loss = 0.0004980311496183276
Validation loss = 0.0015946184284985065
Validation loss = 0.0005951204802840948
Validation loss = 0.0007093657623045146
Validation loss = 0.0006034940015524626
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0007662354037165642
Validation loss = 0.0007099438807927072
Validation loss = 0.0006207397091202438
Validation loss = 0.0006034199614077806
Validation loss = 0.0005945510347373784
Validation loss = 0.0007374273263849318
Validation loss = 0.0007572534959763288
Validation loss = 0.0005981257418170571
Validation loss = 0.000848686380777508
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0006412367220036685
Validation loss = 0.0006221673102118075
Validation loss = 0.0008506955346092582
Validation loss = 0.0007267852197401226
Validation loss = 0.0007592062465846539
Validation loss = 0.0007106339908204973
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0006069636438041925
Validation loss = 0.0006111602415330708
Validation loss = 0.0009063788456842303
Validation loss = 0.0006215309258550406
Validation loss = 0.0005139741697348654
Validation loss = 0.000546079536434263
Validation loss = 0.0004896720056422055
Validation loss = 0.0005594862159341574
Validation loss = 0.0007730633369646966
Validation loss = 0.0005676646833308041
Validation loss = 0.0004803576157428324
Validation loss = 0.0006122633349150419
Validation loss = 0.0005636442801915109
Validation loss = 0.0005393450846895576
Validation loss = 0.0006401445716619492
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 21       |
| MaximumReturn | 199      |
| MinimumReturn | 120      |
| TotalSamples  | 76659    |
----------------------------
itr #22 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006164315273053944
Validation loss = 0.0005874861963093281
Validation loss = 0.0006580906920135021
Validation loss = 0.0005920594558119774
Validation loss = 0.00047291986993514
Validation loss = 0.0006133915740065277
Validation loss = 0.0005470472387969494
Validation loss = 0.0007072247099131346
Validation loss = 0.0006372585776261985
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005869747255928814
Validation loss = 0.0006052362732589245
Validation loss = 0.0008274830761365592
Validation loss = 0.0007643053540959954
Validation loss = 0.0006703472463414073
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0008090330520644784
Validation loss = 0.0008190273074433208
Validation loss = 0.0006648062262684107
Validation loss = 0.0005690523539669812
Validation loss = 0.0005725468508899212
Validation loss = 0.0010911814169958234
Validation loss = 0.0005209340015426278
Validation loss = 0.0006774620851501822
Validation loss = 0.0006830878555774689
Validation loss = 0.0006768610328435898
Validation loss = 0.0006250861915759742
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0006105095962993801
Validation loss = 0.0006780742551200092
Validation loss = 0.0005034200730733573
Validation loss = 0.0005013647023588419
Validation loss = 0.000540914770681411
Validation loss = 0.0005752954166382551
Validation loss = 0.0005722898640669882
Validation loss = 0.0005863083642907441
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0006194590823724866
Validation loss = 0.0006434249808080494
Validation loss = 0.0010050451382994652
Validation loss = 0.0005866045830771327
Validation loss = 0.001023588702082634
Validation loss = 0.0006111888797022402
Validation loss = 0.0006609718548133969
Validation loss = 0.0006067955982871354
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 22       |
| MaximumReturn | 198      |
| MinimumReturn | 98.3     |
| TotalSamples  | 79992    |
----------------------------
itr #23 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006024131434969604
Validation loss = 0.0006522844778373837
Validation loss = 0.0004908525152131915
Validation loss = 0.000696762406732887
Validation loss = 0.0006698378128930926
Validation loss = 0.0005888571031391621
Validation loss = 0.000660087272990495
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006740362150594592
Validation loss = 0.0006182987126521766
Validation loss = 0.0006064257468096912
Validation loss = 0.000504792551510036
Validation loss = 0.0005180216976441443
Validation loss = 0.0006061227759346366
Validation loss = 0.0005919482791796327
Validation loss = 0.0005860271630808711
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00048521682037971914
Validation loss = 0.0006663378444500268
Validation loss = 0.0005644412594847381
Validation loss = 0.0006918298313394189
Validation loss = 0.0005317406030371785
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0006905352929607034
Validation loss = 0.0005600849399343133
Validation loss = 0.0006075009587220848
Validation loss = 0.0007219123654067516
Validation loss = 0.0005962296854704618
Validation loss = 0.0008004260016605258
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0007478799670934677
Validation loss = 0.0005916097434237599
Validation loss = 0.0005657164147123694
Validation loss = 0.0005581002915278077
Validation loss = 0.0006904785404913127
Validation loss = 0.0006662750383839011
Validation loss = 0.0006666945409961045
Validation loss = 0.0008182027377188206
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 23       |
| MaximumReturn | 199      |
| MinimumReturn | 129      |
| TotalSamples  | 83325    |
----------------------------
itr #24 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006503404001705348
Validation loss = 0.0006494298577308655
Validation loss = 0.0005175396800041199
Validation loss = 0.0004980383673682809
Validation loss = 0.0005474323988892138
Validation loss = 0.0004925633547827601
Validation loss = 0.0006673437892459333
Validation loss = 0.0005531397182494402
Validation loss = 0.0006372622447088361
Validation loss = 0.0007375997956842184
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005938423564657569
Validation loss = 0.000520354718901217
Validation loss = 0.0006332389893941581
Validation loss = 0.0008069967152550817
Validation loss = 0.0005698667955584824
Validation loss = 0.000602397252805531
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.000692644389346242
Validation loss = 0.0007382616167888045
Validation loss = 0.0005120394635014236
Validation loss = 0.0005110999918542802
Validation loss = 0.0006472839741036296
Validation loss = 0.0007726124022156
Validation loss = 0.0005610929219983518
Validation loss = 0.0005561332800425589
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0004990956513211131
Validation loss = 0.0006136578740552068
Validation loss = 0.0006454282556660473
Validation loss = 0.0005223043845035136
Validation loss = 0.0007407865487039089
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0006212844746187329
Validation loss = 0.0005735556478612125
Validation loss = 0.0005745428497903049
Validation loss = 0.000576020625885576
Validation loss = 0.0010259567061439157
Validation loss = 0.0005971619393676519
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 24       |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 86658    |
----------------------------
itr #25 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006000608555041254
Validation loss = 0.0004952948656864464
Validation loss = 0.0007151563768275082
Validation loss = 0.0006854403181932867
Validation loss = 0.0006185024976730347
Validation loss = 0.0006839059060439467
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005598087445832789
Validation loss = 0.0005734011647291481
Validation loss = 0.0010964509565383196
Validation loss = 0.0005776126054115593
Validation loss = 0.0005023707053624094
Validation loss = 0.0005466078291647136
Validation loss = 0.0008328711264766753
Validation loss = 0.0005165004986338317
Validation loss = 0.0005454535130411386
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0007040914497338235
Validation loss = 0.0005278005846776068
Validation loss = 0.0005656296270899475
Validation loss = 0.0005501281702890992
Validation loss = 0.0005629553925246
Validation loss = 0.00047948246356099844
Validation loss = 0.0005301255150698125
Validation loss = 0.0006523188785649836
Validation loss = 0.000614833552390337
Validation loss = 0.0005335345631465316
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007102236268110573
Validation loss = 0.0006241154624149203
Validation loss = 0.0006003185408189893
Validation loss = 0.0007192401099018753
Validation loss = 0.0006742611294612288
Validation loss = 0.0006067798240110278
Validation loss = 0.0006871573277749121
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005574761889874935
Validation loss = 0.0006179833435453475
Validation loss = 0.0007153760525397956
Validation loss = 0.0005987850017845631
Validation loss = 0.0005166313494555652
Validation loss = 0.0005702202906832099
Validation loss = 0.000623223080765456
Validation loss = 0.0013608784647658467
Validation loss = 0.0005135548999533057
Validation loss = 0.0006208293489180505
Validation loss = 0.0006695524207316339
Validation loss = 0.000556614832021296
Validation loss = 0.0005454711499623954
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 25       |
| MaximumReturn | 199      |
| MinimumReturn | 115      |
| TotalSamples  | 89991    |
----------------------------
itr #26 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005552771617658436
Validation loss = 0.0004832165432162583
Validation loss = 0.001199675491079688
Validation loss = 0.0007155871135182679
Validation loss = 0.00048415816854685545
Validation loss = 0.0004967722343280911
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0007259899284690619
Validation loss = 0.00048746823449619114
Validation loss = 0.0005960449925623834
Validation loss = 0.0005412014434114099
Validation loss = 0.0008762337383814156
Validation loss = 0.0009922472527250648
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.000531794095877558
Validation loss = 0.0005688791279681027
Validation loss = 0.000680131430272013
Validation loss = 0.0005190934170968831
Validation loss = 0.0006613500881940126
Validation loss = 0.0006047391798347235
Validation loss = 0.0005828203284181654
Validation loss = 0.0005438678781501949
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.000644221028778702
Validation loss = 0.0005822064122185111
Validation loss = 0.0006170585402287543
Validation loss = 0.0005092161009088159
Validation loss = 0.0004765232733916491
Validation loss = 0.0005798761267215014
Validation loss = 0.000665241910610348
Validation loss = 0.0005571229266934097
Validation loss = 0.0006368592148646712
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0007162658730521798
Validation loss = 0.0005019893869757652
Validation loss = 0.0005234804120846093
Validation loss = 0.0005273295100778341
Validation loss = 0.0006213297601789236
Validation loss = 0.0005563682061620057
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 26       |
| MaximumReturn | 199      |
| MinimumReturn | 100      |
| TotalSamples  | 93324    |
----------------------------
itr #27 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005497853271663189
Validation loss = 0.0005297401803545654
Validation loss = 0.0009268815629184246
Validation loss = 0.0006142309866845608
Validation loss = 0.000544679700396955
Validation loss = 0.0005127618205733597
Validation loss = 0.0005327908438630402
Validation loss = 0.0005966756725683808
Validation loss = 0.0005353637970983982
Validation loss = 0.0006189807318150997
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0007039008778519928
Validation loss = 0.000519242777954787
Validation loss = 0.0006900305743329227
Validation loss = 0.0004841960617341101
Validation loss = 0.0005328265251591802
Validation loss = 0.0009027565829455853
Validation loss = 0.0005437489598989487
Validation loss = 0.0004988655564375222
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00046782809658907354
Validation loss = 0.0005586534971371293
Validation loss = 0.0006136605516076088
Validation loss = 0.0005530835478566587
Validation loss = 0.0005504453438334167
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.000657840515486896
Validation loss = 0.0005015739006921649
Validation loss = 0.00047244070447050035
Validation loss = 0.0006497841095551848
Validation loss = 0.0006650291616097093
Validation loss = 0.0005507936584763229
Validation loss = 0.0005279983161017299
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005589795764535666
Validation loss = 0.0007200526306405663
Validation loss = 0.0005312178400345147
Validation loss = 0.0005699727917090058
Validation loss = 0.00046279054367914796
Validation loss = 0.000542114838026464
Validation loss = 0.0005438282387331128
Validation loss = 0.0004877714964095503
Validation loss = 0.0006457939743995667
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 27       |
| MaximumReturn | 199      |
| MinimumReturn | 121      |
| TotalSamples  | 96657    |
----------------------------
itr #28 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006202603690326214
Validation loss = 0.0005486292648129165
Validation loss = 0.0005375405307859182
Validation loss = 0.0005795071483589709
Validation loss = 0.0004994570626877248
Validation loss = 0.0007259876583702862
Validation loss = 0.0006092169205658138
Validation loss = 0.0005321445059962571
Validation loss = 0.000506001350004226
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0007242244319058955
Validation loss = 0.0005635897978208959
Validation loss = 0.0005252236151136458
Validation loss = 0.0005289625260047615
Validation loss = 0.0006272777682170272
Validation loss = 0.000803320377599448
Validation loss = 0.0006046187481842935
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0007167404401116073
Validation loss = 0.0005159795400686562
Validation loss = 0.00046584042138420045
Validation loss = 0.000541806744877249
Validation loss = 0.0006619289633817971
Validation loss = 0.0004741487791761756
Validation loss = 0.0004789331869687885
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0006457560230046511
Validation loss = 0.000584242632612586
Validation loss = 0.000524644332472235
Validation loss = 0.0005139711429364979
Validation loss = 0.0006615042220801115
Validation loss = 0.0005877994117327034
Validation loss = 0.0006753777270205319
Validation loss = 0.0005159722059033811
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005310929846018553
Validation loss = 0.0004785293713212013
Validation loss = 0.0006775616784580052
Validation loss = 0.000606715795584023
Validation loss = 0.0006366379093378782
Validation loss = 0.0004825560317840427
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 28       |
| MaximumReturn | 199      |
| MinimumReturn | 113      |
| TotalSamples  | 99990    |
----------------------------
itr #29 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0007052514120005071
Validation loss = 0.0005561074358411133
Validation loss = 0.00044731891830451787
Validation loss = 0.0006571842823177576
Validation loss = 0.00048025441356003284
Validation loss = 0.0005384203977882862
Validation loss = 0.0005287514650262892
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006531109684146941
Validation loss = 0.00046769040636718273
Validation loss = 0.0004885168164037168
Validation loss = 0.000522390881087631
Validation loss = 0.0004982220707461238
Validation loss = 0.0005290059489198029
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0006650530267506838
Validation loss = 0.0005527830217033625
Validation loss = 0.0004964913241565228
Validation loss = 0.0005203713662922382
Validation loss = 0.0005618510185740888
Validation loss = 0.0006103022606112063
Validation loss = 0.000495868269354105
Validation loss = 0.0005093039362691343
Validation loss = 0.0005040101241320372
Validation loss = 0.0005742955254390836
Validation loss = 0.0004953187890350819
Validation loss = 0.0008481209515593946
Validation loss = 0.0006264097173698246
Validation loss = 0.0006854834500700235
Validation loss = 0.0005292127025313675
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.000610975781455636
Validation loss = 0.000561389431823045
Validation loss = 0.0005784609820693731
Validation loss = 0.000772384344600141
Validation loss = 0.000478658068459481
Validation loss = 0.0005379175418056548
Validation loss = 0.0005989058408886194
Validation loss = 0.0005523517611436546
Validation loss = 0.0005113501101732254
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005211257375776768
Validation loss = 0.0005159334396012127
Validation loss = 0.0005030052270740271
Validation loss = 0.0004990440211258829
Validation loss = 0.0005737959290854633
Validation loss = 0.0005001019453629851
Validation loss = 0.0006508511141873896
Validation loss = 0.0005915143992751837
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 182      |
| Iteration     | 29       |
| MaximumReturn | 199      |
| MinimumReturn | 135      |
| TotalSamples  | 103323   |
----------------------------
itr #30 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0004592431942000985
Validation loss = 0.0006410278147086501
Validation loss = 0.0004987171851098537
Validation loss = 0.0006137897726148367
Validation loss = 0.0005427427822723985
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005262636695988476
Validation loss = 0.0006873371894471347
Validation loss = 0.0005292510613799095
Validation loss = 0.0007158723892644048
Validation loss = 0.0005729327094741166
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0005686868098564446
Validation loss = 0.0005154089885763824
Validation loss = 0.0005836508353240788
Validation loss = 0.0005855905474163592
Validation loss = 0.0005352880107238889
Validation loss = 0.0006302438559941947
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005280360346660018
Validation loss = 0.0005491597694344819
Validation loss = 0.0006198100745677948
Validation loss = 0.000576300430111587
Validation loss = 0.0005404811236076057
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005710563855245709
Validation loss = 0.0005943119176663458
Validation loss = 0.0006468262290582061
Validation loss = 0.0005173297831788659
Validation loss = 0.0005761627107858658
Validation loss = 0.0005651558749377728
Validation loss = 0.0006641148938797414
Validation loss = 0.0005298664909787476
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 30       |
| MaximumReturn | 198      |
| MinimumReturn | 117      |
| TotalSamples  | 106656   |
----------------------------
itr #31 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006580736371688545
Validation loss = 0.0005308768595568836
Validation loss = 0.0005439174128696322
Validation loss = 0.00046907688374631107
Validation loss = 0.0005048734019510448
Validation loss = 0.0005270759575068951
Validation loss = 0.0005245871725492179
Validation loss = 0.0005019453237764537
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005343550001271069
Validation loss = 0.000570851843804121
Validation loss = 0.0005733942380174994
Validation loss = 0.000550825847312808
Validation loss = 0.0005818253848701715
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0004912634030915797
Validation loss = 0.0005455201026052237
Validation loss = 0.0006245880504138768
Validation loss = 0.0007146799471229315
Validation loss = 0.00047349726082757115
Validation loss = 0.0004947800189256668
Validation loss = 0.0005828229477629066
Validation loss = 0.0005200317245908082
Validation loss = 0.00046778435353189707
Validation loss = 0.0004803189658559859
Validation loss = 0.0005953431245870888
Validation loss = 0.0005142338923178613
Validation loss = 0.0005335410824045539
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005260772304609418
Validation loss = 0.0005141854053363204
Validation loss = 0.0005374367465265095
Validation loss = 0.0005625126068480313
Validation loss = 0.0004917544429190457
Validation loss = 0.0004747662751469761
Validation loss = 0.0005970698548480868
Validation loss = 0.0005417588981799781
Validation loss = 0.000548951793462038
Validation loss = 0.0007577885990031064
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.000631213653832674
Validation loss = 0.0005940941045992076
Validation loss = 0.0005135741084814072
Validation loss = 0.0005731024430133402
Validation loss = 0.000595776888076216
Validation loss = 0.0005238970625214279
Validation loss = 0.0005280075711198151
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 162      |
| Iteration     | 31       |
| MaximumReturn | 199      |
| MinimumReturn | 110      |
| TotalSamples  | 109989   |
----------------------------
itr #32 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006134107825346291
Validation loss = 0.0005034871865063906
Validation loss = 0.0005519937840290368
Validation loss = 0.00044027905096299946
Validation loss = 0.0006536614964716136
Validation loss = 0.0005018704105168581
Validation loss = 0.0005397000350058079
Validation loss = 0.00047222734428942204
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005184111651033163
Validation loss = 0.0005491871852427721
Validation loss = 0.0005126497708261013
Validation loss = 0.0005489375907927752
Validation loss = 0.0005105186719447374
Validation loss = 0.0005587276536971331
Validation loss = 0.0004914465243928134
Validation loss = 0.0006241296650841832
Validation loss = 0.0004900490748696029
Validation loss = 0.0005442337715066969
Validation loss = 0.0004706471518147737
Validation loss = 0.0004791742831002921
Validation loss = 0.0010132803581655025
Validation loss = 0.0005513716023415327
Validation loss = 0.0005183435860089958
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0007178116939030588
Validation loss = 0.000571545388083905
Validation loss = 0.0005172835080884397
Validation loss = 0.0005620588781312108
Validation loss = 0.0005830845911987126
Validation loss = 0.000691843859385699
Validation loss = 0.0004939542268402874
Validation loss = 0.000652732967864722
Validation loss = 0.0005449892487376928
Validation loss = 0.0005497036036103964
Validation loss = 0.0005943076102994382
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005283940117806196
Validation loss = 0.0005098137189634144
Validation loss = 0.0004967624554410577
Validation loss = 0.0005010206368751824
Validation loss = 0.0005111912614665926
Validation loss = 0.000654427451081574
Validation loss = 0.0004851737176068127
Validation loss = 0.0004944046377204359
Validation loss = 0.0005899149691686034
Validation loss = 0.0005303844809532166
Validation loss = 0.0006889412179589272
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005249336827546358
Validation loss = 0.00047696480760350823
Validation loss = 0.0007033721194602549
Validation loss = 0.0005566361360251904
Validation loss = 0.0004369422676973045
Validation loss = 0.0005231975228525698
Validation loss = 0.00047376519069075584
Validation loss = 0.0005295199225656688
Validation loss = 0.0005628311191685498
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 32       |
| MaximumReturn | 199      |
| MinimumReturn | 117      |
| TotalSamples  | 113322   |
----------------------------
itr #33 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005570161156356335
Validation loss = 0.00056478101760149
Validation loss = 0.0004935857141390443
Validation loss = 0.0007947624544613063
Validation loss = 0.0005278317839838564
Validation loss = 0.0006421143189072609
Validation loss = 0.0005100229755043983
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.00048200777382589877
Validation loss = 0.0005087008466944098
Validation loss = 0.0005179345025680959
Validation loss = 0.0007491805590689182
Validation loss = 0.00047607827582396567
Validation loss = 0.0005312979337759316
Validation loss = 0.000559688254725188
Validation loss = 0.0005743159563280642
Validation loss = 0.0005071738851256669
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0006657308549620211
Validation loss = 0.0005374642787501216
Validation loss = 0.0006199098424986005
Validation loss = 0.0004878213512711227
Validation loss = 0.00045101094292476773
Validation loss = 0.0005230264505371451
Validation loss = 0.0005235165590420365
Validation loss = 0.0004919348866678774
Validation loss = 0.0005193090764805675
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005412483587861061
Validation loss = 0.0005910497275181115
Validation loss = 0.0005247773369774222
Validation loss = 0.0006759212119504809
Validation loss = 0.00047658322728239
Validation loss = 0.0005596341216005385
Validation loss = 0.0005320655764080584
Validation loss = 0.0004748873761855066
Validation loss = 0.000652169284876436
Validation loss = 0.0005133842350915074
Validation loss = 0.0004941433435305953
Validation loss = 0.0006842814618721604
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0006503514014184475
Validation loss = 0.0005325003294274211
Validation loss = 0.0005515263765119016
Validation loss = 0.0004953416064381599
Validation loss = 0.0005846393178217113
Validation loss = 0.00047916927724145353
Validation loss = 0.0006469115614891052
Validation loss = 0.0004961685044690967
Validation loss = 0.0006060622981749475
Validation loss = 0.0004925052635371685
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 33       |
| MaximumReturn | 199      |
| MinimumReturn | 103      |
| TotalSamples  | 116655   |
----------------------------
itr #34 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005341782234609127
Validation loss = 0.00045415578642860055
Validation loss = 0.0004589271266013384
Validation loss = 0.0005776053876616061
Validation loss = 0.000501976115629077
Validation loss = 0.0005912956548854709
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0004760936717502773
Validation loss = 0.0005841069505549967
Validation loss = 0.0005220478633418679
Validation loss = 0.0004991064197383821
Validation loss = 0.0005997398402541876
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0005974833038635552
Validation loss = 0.00046030053636059165
Validation loss = 0.0006447715568356216
Validation loss = 0.0005554421804845333
Validation loss = 0.0006788991158828139
Validation loss = 0.0005004145787097514
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005568192573264241
Validation loss = 0.0004939999198541045
Validation loss = 0.000570300908293575
Validation loss = 0.0005434222402982414
Validation loss = 0.0005378739442676306
Validation loss = 0.0005350378341972828
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005124852759763598
Validation loss = 0.0005377285415306687
Validation loss = 0.0005159365246072412
Validation loss = 0.00048083034926094115
Validation loss = 0.0005337008624337614
Validation loss = 0.0005339846829883754
Validation loss = 0.0005019275704398751
Validation loss = 0.000448582781245932
Validation loss = 0.0005900506512261927
Validation loss = 0.00046365809976123273
Validation loss = 0.0005247429362498224
Validation loss = 0.0006183087825775146
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 34       |
| MaximumReturn | 199      |
| MinimumReturn | 103      |
| TotalSamples  | 119988   |
----------------------------
itr #35 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00048712690477259457
Validation loss = 0.0004997430951334536
Validation loss = 0.00051972980145365
Validation loss = 0.000453924760222435
Validation loss = 0.0006748795276507735
Validation loss = 0.0004560858360491693
Validation loss = 0.0004855410079471767
Validation loss = 0.0004667491593863815
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.000514184997882694
Validation loss = 0.0005907004233449697
Validation loss = 0.000524387345649302
Validation loss = 0.0007470320560969412
Validation loss = 0.000475213339086622
Validation loss = 0.0007121217204257846
Validation loss = 0.0004842303751502186
Validation loss = 0.0005008961306884885
Validation loss = 0.0004827743978239596
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.000528889533597976
Validation loss = 0.0004915027529932559
Validation loss = 0.0004933738382533193
Validation loss = 0.0005075098597444594
Validation loss = 0.0005291555426083505
Validation loss = 0.0004432860005181283
Validation loss = 0.0004814940330106765
Validation loss = 0.00046186838881112635
Validation loss = 0.0004931877483613789
Validation loss = 0.0004996335483156145
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005709739634767175
Validation loss = 0.0005910738254897296
Validation loss = 0.00048134036478586495
Validation loss = 0.0004869871772825718
Validation loss = 0.0005723268259316683
Validation loss = 0.0005035351496189833
Validation loss = 0.0005602116580121219
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0004801316244993359
Validation loss = 0.0005154874525032938
Validation loss = 0.0004694651288446039
Validation loss = 0.0005462478729896247
Validation loss = 0.0004912939039058983
Validation loss = 0.0004857257008552551
Validation loss = 0.0004985047853551805
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 35       |
| MaximumReturn | 199      |
| MinimumReturn | 93       |
| TotalSamples  | 123321   |
----------------------------
itr #36 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.000541387707926333
Validation loss = 0.0004987760912626982
Validation loss = 0.0005179057479836047
Validation loss = 0.00048275222070515156
Validation loss = 0.0005449524614959955
Validation loss = 0.0006424481980502605
Validation loss = 0.0007545943954028189
Validation loss = 0.0004661201674025506
Validation loss = 0.0004884150112047791
Validation loss = 0.0004841830814257264
Validation loss = 0.0006037935381755233
Validation loss = 0.0005092218052595854
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005367094418033957
Validation loss = 0.0005864744889549911
Validation loss = 0.00045449446770362556
Validation loss = 0.0005424846895039082
Validation loss = 0.0004364685737527907
Validation loss = 0.0006221172516234219
Validation loss = 0.0004963653045706451
Validation loss = 0.0005460077663883567
Validation loss = 0.0005015801871195436
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0005297490861266851
Validation loss = 0.0005135221290402114
Validation loss = 0.0004869997501373291
Validation loss = 0.0006007521878927946
Validation loss = 0.0005192183307372034
Validation loss = 0.00048174147377721965
Validation loss = 0.00044579824316315353
Validation loss = 0.0005598648567683995
Validation loss = 0.0005555172101594508
Validation loss = 0.0006006452022120357
Validation loss = 0.0005170262884348631
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005012978799641132
Validation loss = 0.0004938212223351002
Validation loss = 0.00044886337127536535
Validation loss = 0.0005297564202919602
Validation loss = 0.0004595413920469582
Validation loss = 0.0009614768205210567
Validation loss = 0.000645043735858053
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0004679723351728171
Validation loss = 0.0004928951966576278
Validation loss = 0.0004916000179946423
Validation loss = 0.000484675430925563
Validation loss = 0.0005596702103503048
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 159      |
| Iteration     | 36       |
| MaximumReturn | 199      |
| MinimumReturn | 94.5     |
| TotalSamples  | 126654   |
----------------------------
itr #37 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005065848235972226
Validation loss = 0.0005225803470239043
Validation loss = 0.0005161369917914271
Validation loss = 0.0004671317001339048
Validation loss = 0.0005689541576430202
Validation loss = 0.0005590325454249978
Validation loss = 0.0005720918998122215
Validation loss = 0.0006600163178518414
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.000528309028595686
Validation loss = 0.0004654687363654375
Validation loss = 0.0005232497351244092
Validation loss = 0.0005405276897363365
Validation loss = 0.0006993244169279933
Validation loss = 0.0006596033344976604
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0004776494752150029
Validation loss = 0.00048728796537034214
Validation loss = 0.00056482944637537
Validation loss = 0.0004775445850100368
Validation loss = 0.0005573819507844746
Validation loss = 0.0005632648826576769
Validation loss = 0.0005664498312398791
Validation loss = 0.0005507671157829463
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0006153041031211615
Validation loss = 0.0004606596485245973
Validation loss = 0.00047351320972666144
Validation loss = 0.0005082805291749537
Validation loss = 0.0004765029007103294
Validation loss = 0.0005615924019366503
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005232493858784437
Validation loss = 0.0005094429361633956
Validation loss = 0.0004601281543727964
Validation loss = 0.0005181100568734109
Validation loss = 0.0005884279380552471
Validation loss = 0.0005726548261009157
Validation loss = 0.0005816908087581396
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 160      |
| Iteration     | 37       |
| MaximumReturn | 195      |
| MinimumReturn | 120      |
| TotalSamples  | 129987   |
----------------------------
itr #38 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005408126162365079
Validation loss = 0.0004571344761643559
Validation loss = 0.0004396954027470201
Validation loss = 0.0007913405424915254
Validation loss = 0.0005600652657449245
Validation loss = 0.00046170438872650266
Validation loss = 0.0004954192554578185
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0004379847669042647
Validation loss = 0.0005327844992280006
Validation loss = 0.00048552866792306304
Validation loss = 0.00046265008859336376
Validation loss = 0.0005156560800969601
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.000561318127438426
Validation loss = 0.0004500358481891453
Validation loss = 0.0004788724472746253
Validation loss = 0.0004882612556684762
Validation loss = 0.0004947931156493723
Validation loss = 0.00046889850636944175
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005038169329054654
Validation loss = 0.0004731894005089998
Validation loss = 0.0005152576486580074
Validation loss = 0.0004673324874602258
Validation loss = 0.0005929401959292591
Validation loss = 0.00044098368380218744
Validation loss = 0.00048690009862184525
Validation loss = 0.0007058936753310263
Validation loss = 0.0004984214901924133
Validation loss = 0.0005582300364039838
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0006519160233438015
Validation loss = 0.0005529982154257596
Validation loss = 0.0004949721624143422
Validation loss = 0.0006803455180488527
Validation loss = 0.000425953243393451
Validation loss = 0.0004655241791624576
Validation loss = 0.0004623620770871639
Validation loss = 0.00043633044697344303
Validation loss = 0.000528075557667762
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 38       |
| MaximumReturn | 199      |
| MinimumReturn | 123      |
| TotalSamples  | 133320   |
----------------------------
itr #39 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005017892108298838
Validation loss = 0.0005029769381508231
Validation loss = 0.00048789303400553763
Validation loss = 0.0004983487888239324
Validation loss = 0.0005734529113397002
Validation loss = 0.0004715858958661556
Validation loss = 0.0004895617021247745
Validation loss = 0.00044007832184433937
Validation loss = 0.000527088122908026
Validation loss = 0.0006028767093084753
Validation loss = 0.00048377556959167123
Validation loss = 0.000628276786301285
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0004948308924213052
Validation loss = 0.0005016129580326378
Validation loss = 0.000558061758056283
Validation loss = 0.0005956291570328176
Validation loss = 0.0005847997963428497
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.000536559906322509
Validation loss = 0.00045973030501045287
Validation loss = 0.0005045629804953933
Validation loss = 0.0006312034674920142
Validation loss = 0.0004928132984787226
Validation loss = 0.0004627027956303209
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00046851483057253063
Validation loss = 0.000488725199829787
Validation loss = 0.0005452716141007841
Validation loss = 0.00048570052604191005
Validation loss = 0.0006273961043916643
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00045526045141741633
Validation loss = 0.0004770181840285659
Validation loss = 0.00043622960220091045
Validation loss = 0.0005248415982350707
Validation loss = 0.0006323521956801414
Validation loss = 0.00048490765038877726
Validation loss = 0.00046958817983977497
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 39       |
| MaximumReturn | 199      |
| MinimumReturn | 114      |
| TotalSamples  | 136653   |
----------------------------
