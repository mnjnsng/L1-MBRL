Logging to experiments/pendulum/test-dir-exp/Tue-22-Nov-2022-03-22-28-PM-CST_pendulum_trpo_iteration_20_seed3214
Print configuration .....
{'env_name': 'pendulum', 'random_seeds': [3214, 2431, 2531, 2231], 'save_variables': False, 'model_save_dir': '/tmp/pendulum_models/', 'restore_variables': False, 'start_onpol_iter': 0, 'onpol_iters': 40, 'num_path_random': 25, 'num_path_onpol': 25, 'env_horizon': 200, 'max_train_data': 200000, 'max_val_data': 100000, 'discard_ratio': 0.0, 'dynamics': {'pre_training': {'mode': 'intrinsic_reward', 'itr': 0, 'policy_itr': 20}, 'model': 'nn', 'ensemble': True, 'ensemble_model_count': 5, 'enable_particle_ensemble': True, 'particles': 5, 'obs_var': 1.0, 'intrinsic_reward_coeff': 1.0, 'ita': 1.0, 'mode': 'random', 'val': True, 'n_layers': 4, 'hidden_size': 1000, 'activation': 'relu', 'batch_size': 1000, 'learning_rate': 0.001, 'reg_coeff': 0.0, 'epochs': 200, 'kfac_params': {'learning_rate': 0.1, 'damping': 0.001, 'momentum': 0.9, 'kl_clip': 0.0001, 'cov_ema_decay': 0.99}}, 'policy': {'network_shape': [64, 64], 'init_logstd': 0.0, 'activation': 'tanh', 'reinitialize_every_itr': False}, 'trpo': {'horizon': 200, 'gamma': 0.99, 'step_size': 0.01, 'iterations': 20, 'batch_size': 50000, 'gae': 0.95, 'visualization': False, 'visualize_iterations': [0]}, 'algo': 'trpo'}
Generating random rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating random rollouts.
Creating normalization for training data.
Done creating normalization for training data.
Particle ensemble enabled? True
An ensemble of 5 dynamics model <class 'model.dynamics.NNDynamicsModel'> initialized
Train dynamics model with intrinsic reward only? False
Pre-training enabled. Using only intrinsic reward.
Pre-training dynamics model for 0 iterations...
Done pre-training dynamics model.
Using external reward only.
itr #0 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.16960906982421875
Validation loss = 0.02347155287861824
Validation loss = 0.006703571416437626
Validation loss = 0.0016512122238054872
Validation loss = 0.0008017669315449893
Validation loss = 0.0005538002587854862
Validation loss = 0.0004369189264252782
Validation loss = 0.00045146627235226333
Validation loss = 0.0009319951641373336
Validation loss = 0.000363086728611961
Validation loss = 0.007614889647811651
Validation loss = 0.0006172843859530985
Validation loss = 0.0005507288151420653
Validation loss = 0.0003487163921818137
Validation loss = 0.00027501536533236504
Validation loss = 0.0003017527924384922
Validation loss = 0.00033095068647526205
Validation loss = 0.0002792105369735509
Validation loss = 0.0006405143067240715
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.18051330745220184
Validation loss = 0.01770789921283722
Validation loss = 0.005535581614822149
Validation loss = 0.001823229482397437
Validation loss = 0.0008157950942404568
Validation loss = 0.0004966502310708165
Validation loss = 0.0004136589413974434
Validation loss = 0.0003696438216138631
Validation loss = 0.00036544946487993
Validation loss = 0.00031851441599428654
Validation loss = 0.0003418106643948704
Validation loss = 0.0004270906501915306
Validation loss = 0.0014153618831187487
Validation loss = 0.00047986285062506795
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.18124212324619293
Validation loss = 0.020893465727567673
Validation loss = 0.0068370141088962555
Validation loss = 0.002411683090031147
Validation loss = 0.0008077116799540818
Validation loss = 0.000554107129573822
Validation loss = 0.00043484882917255163
Validation loss = 0.0004280493303667754
Validation loss = 0.00035603917785920203
Validation loss = 0.0004327978822402656
Validation loss = 0.00044072119635529816
Validation loss = 0.002264612354338169
Validation loss = 0.0006715374765917659
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.17366769909858704
Validation loss = 0.026872601360082626
Validation loss = 0.0065380590967834
Validation loss = 0.002476330380886793
Validation loss = 0.0010687100002542138
Validation loss = 0.0006836341344751418
Validation loss = 0.0005364936077967286
Validation loss = 0.0004173569323029369
Validation loss = 0.0003785950248129666
Validation loss = 0.0003485684865154326
Validation loss = 0.0003756471851374954
Validation loss = 0.00033974667894653976
Validation loss = 0.0003399405104573816
Validation loss = 0.0003472162352409214
Validation loss = 0.0009336206712760031
Validation loss = 0.0006320633692666888
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.17296986281871796
Validation loss = 0.0313275009393692
Validation loss = 0.007274162489920855
Validation loss = 0.0024101203307509422
Validation loss = 0.0010710529750213027
Validation loss = 0.000588448834605515
Validation loss = 0.00046908928197808564
Validation loss = 0.0004029381962027401
Validation loss = 0.00037186642293818295
Validation loss = 0.00035352143459022045
Validation loss = 0.00036728382110595703
Validation loss = 0.0003095526190008968
Validation loss = 0.00036282825749367476
Validation loss = 0.0003027936618309468
Validation loss = 0.0003674910112749785
Validation loss = 0.0005034910282120109
Validation loss = 0.0003009965585079044
Validation loss = 0.00035139374085702
Validation loss = 0.0013757548294961452
Validation loss = 0.0004658421385101974
Validation loss = 0.00036583279143087566
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 0        |
| MaximumReturn | 199      |
| MinimumReturn | 128      |
| TotalSamples  | 6666     |
----------------------------
itr #1 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.024544402956962585
Validation loss = 0.002993452362716198
Validation loss = 0.0012430715141817927
Validation loss = 0.0009355940856039524
Validation loss = 0.001337482244707644
Validation loss = 0.0009983257623389363
Validation loss = 0.0013171043246984482
Validation loss = 0.0009819555561989546
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.01286986842751503
Validation loss = 0.003369389334693551
Validation loss = 0.0011579931015148759
Validation loss = 0.001432994962669909
Validation loss = 0.001222077407874167
Validation loss = 0.0012107943184673786
Validation loss = 0.001032030675560236
Validation loss = 0.0011228661751374602
Validation loss = 0.0017506947042420506
Validation loss = 0.0011664447374641895
Validation loss = 0.0015702671371400356
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.018575308844447136
Validation loss = 0.002762468531727791
Validation loss = 0.001368946861475706
Validation loss = 0.003528689732775092
Validation loss = 0.0011345027014613152
Validation loss = 0.0012027285993099213
Validation loss = 0.0011263518827036023
Validation loss = 0.0008400360238738358
Validation loss = 0.0009266231209039688
Validation loss = 0.0016038849717006087
Validation loss = 0.0009599272161722183
Validation loss = 0.0018976195715367794
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.015330832451581955
Validation loss = 0.0024319333024322987
Validation loss = 0.001201825449243188
Validation loss = 0.0009543116320855916
Validation loss = 0.0010658232495188713
Validation loss = 0.0015483273891732097
Validation loss = 0.0009681220981292427
Validation loss = 0.0008519843104295433
Validation loss = 0.0010562919778749347
Validation loss = 0.0013367648934945464
Validation loss = 0.001401239074766636
Validation loss = 0.0012811976484954357
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.014478686265647411
Validation loss = 0.0027144469786435366
Validation loss = 0.0012120777973905206
Validation loss = 0.000935000367462635
Validation loss = 0.0009043621830642223
Validation loss = 0.0010054028825834394
Validation loss = 0.000973033020272851
Validation loss = 0.0017706201178953052
Validation loss = 0.000921202648896724
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 1        |
| MaximumReturn | 199      |
| MinimumReturn | 103      |
| TotalSamples  | 9999     |
----------------------------
itr #2 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.004484283737838268
Validation loss = 0.000973541580606252
Validation loss = 0.001286942046135664
Validation loss = 0.0008675809949636459
Validation loss = 0.0010366970673203468
Validation loss = 0.0010354339610785246
Validation loss = 0.0009399846312589943
Validation loss = 0.0009494997793808579
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.004063753876835108
Validation loss = 0.0021561228204518557
Validation loss = 0.0010326019255444407
Validation loss = 0.0008168034255504608
Validation loss = 0.0012443212326616049
Validation loss = 0.0009393728105351329
Validation loss = 0.0011437858920544386
Validation loss = 0.001670989440754056
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.015512463636696339
Validation loss = 0.0010251839412376285
Validation loss = 0.0007861859048716724
Validation loss = 0.00152315862942487
Validation loss = 0.000776236061938107
Validation loss = 0.0009021146106533706
Validation loss = 0.002176825422793627
Validation loss = 0.0008461064426228404
Validation loss = 0.0026429530698806047
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.007439629174768925
Validation loss = 0.001319164875894785
Validation loss = 0.0007561510428786278
Validation loss = 0.0009350312175229192
Validation loss = 0.00074835802661255
Validation loss = 0.0007845564978197217
Validation loss = 0.0017387883272022009
Validation loss = 0.0010816854191944003
Validation loss = 0.0012205581879243255
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.01050555519759655
Validation loss = 0.0012539292220026255
Validation loss = 0.0006933633703738451
Validation loss = 0.0010540190851315856
Validation loss = 0.0010760014411062002
Validation loss = 0.000821522087790072
Validation loss = 0.001095459796488285
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 179      |
| Iteration     | 2        |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 13332    |
----------------------------
itr #3 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.003369770245626569
Validation loss = 0.0007298311102204025
Validation loss = 0.0008196339476853609
Validation loss = 0.0013085833052173257
Validation loss = 0.0008753639995120466
Validation loss = 0.0009320050594396889
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.004205845762044191
Validation loss = 0.0013318202691152692
Validation loss = 0.0007172767072916031
Validation loss = 0.0008711071568541229
Validation loss = 0.0013448651880025864
Validation loss = 0.0011266822693869472
Validation loss = 0.0007850858964957297
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0031723009888082743
Validation loss = 0.000694050919264555
Validation loss = 0.0007957216002978384
Validation loss = 0.0012027357006445527
Validation loss = 0.0011437054490670562
Validation loss = 0.0008458005613647401
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0019442002521827817
Validation loss = 0.000716187059879303
Validation loss = 0.001538143027573824
Validation loss = 0.0010097598424181342
Validation loss = 0.0008463255944661796
Validation loss = 0.0009642229997552931
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.004686175379902124
Validation loss = 0.0016398924635723233
Validation loss = 0.0009504597983323038
Validation loss = 0.0012182820355519652
Validation loss = 0.0006189057021401823
Validation loss = 0.0009859161218628287
Validation loss = 0.0005962893483228981
Validation loss = 0.0009472708334214985
Validation loss = 0.0008785261888988316
Validation loss = 0.0017489362508058548
Validation loss = 0.0008999125566333532
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 177      |
| Iteration     | 3        |
| MaximumReturn | 199      |
| MinimumReturn | 121      |
| TotalSamples  | 16665    |
----------------------------
itr #4 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00176306685898453
Validation loss = 0.0018329822923988104
Validation loss = 0.0005965736927464604
Validation loss = 0.0009715491905808449
Validation loss = 0.0006740826065652072
Validation loss = 0.0008876477368175983
Validation loss = 0.0016570938751101494
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0021342933177948
Validation loss = 0.0006127611850388348
Validation loss = 0.0008820624207146466
Validation loss = 0.0006879990687593818
Validation loss = 0.0007849126122891903
Validation loss = 0.0020732656121253967
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.002107943408191204
Validation loss = 0.000915651093237102
Validation loss = 0.0007244169246405363
Validation loss = 0.003503404324874282
Validation loss = 0.0008891780744306743
Validation loss = 0.0004986354615539312
Validation loss = 0.0024313582107424736
Validation loss = 0.0017491979524493217
Validation loss = 0.001984561327844858
Validation loss = 0.002722684759646654
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0029095113277435303
Validation loss = 0.0009635946480557323
Validation loss = 0.00227865157648921
Validation loss = 0.0008751281420700252
Validation loss = 0.0006266947602853179
Validation loss = 0.0006513592088595033
Validation loss = 0.0008332532597705722
Validation loss = 0.002323310589417815
Validation loss = 0.0009537663427181542
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.001169444527477026
Validation loss = 0.002358843106776476
Validation loss = 0.0014773012371733785
Validation loss = 0.0006677968194708228
Validation loss = 0.0007447793032042682
Validation loss = 0.0008559364941902459
Validation loss = 0.0007303952588699758
Validation loss = 0.0006177874747663736
Validation loss = 0.001079420791938901
Validation loss = 0.0016172952018678188
Validation loss = 0.0006529978127218783
Validation loss = 0.0011815038742497563
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 4        |
| MaximumReturn | 199      |
| MinimumReturn | 115      |
| TotalSamples  | 19998    |
----------------------------
itr #5 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0012597612803801894
Validation loss = 0.000537541345693171
Validation loss = 0.0011229587253183126
Validation loss = 0.0006778415990993381
Validation loss = 0.0011234271805733442
Validation loss = 0.0008105039596557617
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0016155994962900877
Validation loss = 0.00045446943840943277
Validation loss = 0.001505914842709899
Validation loss = 0.0005325886886566877
Validation loss = 0.0005631924723275006
Validation loss = 0.0013923916267231107
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0041657704859972
Validation loss = 0.0005150045035406947
Validation loss = 0.0016392506659030914
Validation loss = 0.0007497845799662173
Validation loss = 0.0005937315290793777
Validation loss = 0.0008920059772208333
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0010157076176255941
Validation loss = 0.0006665875553153455
Validation loss = 0.0006963292835280299
Validation loss = 0.001378301763907075
Validation loss = 0.0006509810336865485
Validation loss = 0.0009766656439751387
Validation loss = 0.0010966227855533361
Validation loss = 0.0010266124736517668
Validation loss = 0.0009764540009200573
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0011080059921368957
Validation loss = 0.0011242287000641227
Validation loss = 0.0010987452697008848
Validation loss = 0.001169080613180995
Validation loss = 0.0010512197623029351
Validation loss = 0.00289939739741385
Validation loss = 0.0009329208987765014
Validation loss = 0.0011482581030577421
Validation loss = 0.0008127139881253242
Validation loss = 0.0005382969393394887
Validation loss = 0.0011727161472663283
Validation loss = 0.0013101889053359628
Validation loss = 0.0005057341186329722
Validation loss = 0.0005426977877505124
Validation loss = 0.00048564825556240976
Validation loss = 0.004091853741556406
Validation loss = 0.0007445784867741168
Validation loss = 0.0005845893756486475
Validation loss = 0.003202077466994524
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 181      |
| Iteration     | 5        |
| MaximumReturn | 199      |
| MinimumReturn | 133      |
| TotalSamples  | 23331    |
----------------------------
itr #6 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0011399287031963468
Validation loss = 0.0011806781403720379
Validation loss = 0.0012297035427764058
Validation loss = 0.0006883355090394616
Validation loss = 0.0007295824470929801
Validation loss = 0.0007463926449418068
Validation loss = 0.0006137052550911903
Validation loss = 0.0006596717867068946
Validation loss = 0.002050638198852539
Validation loss = 0.0008181070443242788
Validation loss = 0.0007029923144727945
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0016180041711777449
Validation loss = 0.0008108215988613665
Validation loss = 0.001950782141648233
Validation loss = 0.0010089256102219224
Validation loss = 0.0009583162027411163
Validation loss = 0.0006171310087665915
Validation loss = 0.0007729707867838442
Validation loss = 0.0006737637449987233
Validation loss = 0.0008398736244998872
Validation loss = 0.0007904756348580122
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0010797627037391067
Validation loss = 0.0006439182325266302
Validation loss = 0.0007541864761151373
Validation loss = 0.0006103675696067512
Validation loss = 0.002949624555185437
Validation loss = 0.0012563976924866438
Validation loss = 0.0006060102605260909
Validation loss = 0.0005147745250724256
Validation loss = 0.0015598091995343566
Validation loss = 0.0007354984409175813
Validation loss = 0.0013439381727948785
Validation loss = 0.0010994633194059134
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0011652070097625256
Validation loss = 0.0008818982169032097
Validation loss = 0.0007653041393496096
Validation loss = 0.000868416391313076
Validation loss = 0.0012079618172720075
Validation loss = 0.0006727706058882177
Validation loss = 0.0005883364356122911
Validation loss = 0.0006139502511359751
Validation loss = 0.0004110596782993525
Validation loss = 0.0010830594692379236
Validation loss = 0.001649038982577622
Validation loss = 0.0005092585342936218
Validation loss = 0.0007972175371833146
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0011238260194659233
Validation loss = 0.0004585992428474128
Validation loss = 0.0007642165292054415
Validation loss = 0.0012576833833009005
Validation loss = 0.0005426469724625349
Validation loss = 0.0008855481282807887
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 6        |
| MaximumReturn | 199      |
| MinimumReturn | 121      |
| TotalSamples  | 26664    |
----------------------------
itr #7 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006003364105708897
Validation loss = 0.0008997158729471266
Validation loss = 0.0005799599457532167
Validation loss = 0.0019128757994621992
Validation loss = 0.0006624524830840528
Validation loss = 0.0009263277752324939
Validation loss = 0.0004820923204533756
Validation loss = 0.0006821558345109224
Validation loss = 0.0006738408701494336
Validation loss = 0.0006178769981488585
Validation loss = 0.00039191750693134964
Validation loss = 0.0009292599279433489
Validation loss = 0.0003094293351750821
Validation loss = 0.0005982220754958689
Validation loss = 0.0010403452906757593
Validation loss = 0.0005065330187790096
Validation loss = 0.0007061405922286212
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0007213811040855944
Validation loss = 0.0007976710330694914
Validation loss = 0.000746645440813154
Validation loss = 0.000658184930216521
Validation loss = 0.0005380097427405417
Validation loss = 0.0012723985128104687
Validation loss = 0.00073350960155949
Validation loss = 0.0007511970470659435
Validation loss = 0.0013103069504722953
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0009153158171102405
Validation loss = 0.0007243282743729651
Validation loss = 0.000713006011210382
Validation loss = 0.0011598143028095365
Validation loss = 0.0005618435679934919
Validation loss = 0.0010318170534446836
Validation loss = 0.0005771024152636528
Validation loss = 0.0009606623207218945
Validation loss = 0.000471828825538978
Validation loss = 0.0004930815193802118
Validation loss = 0.0005103321163915098
Validation loss = 0.0009832048090174794
Validation loss = 0.0006086596404202282
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0009461374720558524
Validation loss = 0.0008263581548817456
Validation loss = 0.0004404779174365103
Validation loss = 0.0010962177766487002
Validation loss = 0.000616040953900665
Validation loss = 0.0006119571044109762
Validation loss = 0.0007014281000010669
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0007747120107524097
Validation loss = 0.0008413295727223158
Validation loss = 0.0006773868226446211
Validation loss = 0.0012523424811661243
Validation loss = 0.000996192218735814
Validation loss = 0.0005839205696247518
Validation loss = 0.0005793498712591827
Validation loss = 0.0004953586030751467
Validation loss = 0.0006057526334188879
Validation loss = 0.0012525176862254739
Validation loss = 0.0005374210886657238
Validation loss = 0.0007094346801750362
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 164      |
| Iteration     | 7        |
| MaximumReturn | 199      |
| MinimumReturn | 112      |
| TotalSamples  | 29997    |
----------------------------
itr #8 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0004864339716732502
Validation loss = 0.001544320140965283
Validation loss = 0.0005150592187419534
Validation loss = 0.0006178891635499895
Validation loss = 0.0005128856864757836
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0008910658652894199
Validation loss = 0.0007835148717276752
Validation loss = 0.0005895238136872649
Validation loss = 0.000521636800840497
Validation loss = 0.0004608837771229446
Validation loss = 0.0008155795512720942
Validation loss = 0.000512737431563437
Validation loss = 0.0010626281145960093
Validation loss = 0.0008576614200137556
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.000709796731825918
Validation loss = 0.0006685274420306087
Validation loss = 0.0006469673826359212
Validation loss = 0.0009644952951930463
Validation loss = 0.003667139681056142
Validation loss = 0.0003418647975195199
Validation loss = 0.0005193842807784677
Validation loss = 0.000449160928837955
Validation loss = 0.0007035116432234645
Validation loss = 0.0003911615058314055
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0008589259232394397
Validation loss = 0.0004943283856846392
Validation loss = 0.0009375326917506754
Validation loss = 0.0003966052026953548
Validation loss = 0.0005318003240972757
Validation loss = 0.0010082704247906804
Validation loss = 0.000929335888940841
Validation loss = 0.00043901908793486655
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.001113996608182788
Validation loss = 0.0003226728003937751
Validation loss = 0.0004567131691146642
Validation loss = 0.0005448812153190374
Validation loss = 0.0005238197045400739
Validation loss = 0.0003591814893297851
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 8        |
| MaximumReturn | 199      |
| MinimumReturn | 123      |
| TotalSamples  | 33330    |
----------------------------
itr #9 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006901307497173548
Validation loss = 0.0006876456318423152
Validation loss = 0.0007304471801035106
Validation loss = 0.0006138347671367228
Validation loss = 0.0006648720009252429
Validation loss = 0.0003649044956546277
Validation loss = 0.0008426379645243287
Validation loss = 0.0003665320109575987
Validation loss = 0.0011591800721362233
Validation loss = 0.00034399068681523204
Validation loss = 0.0005194726400077343
Validation loss = 0.0011506269220262766
Validation loss = 0.00029452648595906794
Validation loss = 0.0006443112506531179
Validation loss = 0.0003267632855568081
Validation loss = 0.0005505636800080538
Validation loss = 0.0016632298938930035
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006660743383690715
Validation loss = 0.0007258442346937954
Validation loss = 0.0006360436091199517
Validation loss = 0.0005221339524723589
Validation loss = 0.0007867686217650771
Validation loss = 0.0007711953367106616
Validation loss = 0.0006152448477223516
Validation loss = 0.0006256785709410906
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00041622272692620754
Validation loss = 0.0025492869317531586
Validation loss = 0.0005996942636556923
Validation loss = 0.0006464943871833384
Validation loss = 0.0002807340642903
Validation loss = 0.0004637491365429014
Validation loss = 0.0003648479178082198
Validation loss = 0.000573849945794791
Validation loss = 0.00046775565715506673
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0004295088292565197
Validation loss = 0.00040114272269420326
Validation loss = 0.0015170410042628646
Validation loss = 0.00033463904401287436
Validation loss = 0.0003365045413374901
Validation loss = 0.0004869512631557882
Validation loss = 0.00038805388612672687
Validation loss = 0.0007191268960013986
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0011429672595113516
Validation loss = 0.0003185693931300193
Validation loss = 0.0006850718054920435
Validation loss = 0.00066422822419554
Validation loss = 0.0006854417733848095
Validation loss = 0.00044481217628344893
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 9        |
| MaximumReturn | 199      |
| MinimumReturn | 123      |
| TotalSamples  | 36663    |
----------------------------
itr #10 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.000503474147990346
Validation loss = 0.000339283054927364
Validation loss = 0.00039784604450687766
Validation loss = 0.0004275513638276607
Validation loss = 0.000625480548478663
Validation loss = 0.00037016376154497266
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0016275171656161547
Validation loss = 0.00034068350214511156
Validation loss = 0.000771913502831012
Validation loss = 0.0005964793381281197
Validation loss = 0.0008382931700907648
Validation loss = 0.0004965520929545164
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00034811079967767
Validation loss = 0.00039450498297810555
Validation loss = 0.0014158511767163873
Validation loss = 0.0003515948774293065
Validation loss = 0.00042760540964081883
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0009630804415792227
Validation loss = 0.0004719328717328608
Validation loss = 0.0006078525329940021
Validation loss = 0.0007409064564853907
Validation loss = 0.000847491028252989
Validation loss = 0.0004717656993307173
Validation loss = 0.0004021081840619445
Validation loss = 0.0004930620780214667
Validation loss = 0.0004214655782561749
Validation loss = 0.000598004786297679
Validation loss = 0.0002809508587233722
Validation loss = 0.000446444028057158
Validation loss = 0.0002943064318969846
Validation loss = 0.0004318876890465617
Validation loss = 0.0003548377426341176
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005914378562010825
Validation loss = 0.000509204575791955
Validation loss = 0.0004616821534000337
Validation loss = 0.0003409771015867591
Validation loss = 0.00037096525193192065
Validation loss = 0.00042890215991064906
Validation loss = 0.0004468766273930669
Validation loss = 0.00045548021444119513
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 10       |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 39996    |
----------------------------
itr #11 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0003830124041996896
Validation loss = 0.0005280608893372118
Validation loss = 0.00033279164927080274
Validation loss = 0.0005132741061970592
Validation loss = 0.0004678433761000633
Validation loss = 0.0005848073051311076
Validation loss = 0.0003737088409252465
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005021087126806378
Validation loss = 0.00042960973223671317
Validation loss = 0.000495814485475421
Validation loss = 0.00025012734113261104
Validation loss = 0.0008191107772290707
Validation loss = 0.00040703019476495683
Validation loss = 0.00034341547871008515
Validation loss = 0.000276759295957163
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0009666403639130294
Validation loss = 0.0002866968570742756
Validation loss = 0.00042110984213650227
Validation loss = 0.0005306422826834023
Validation loss = 0.00045359035721048713
Validation loss = 0.0003241213853470981
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0004830744583159685
Validation loss = 0.0004121694655623287
Validation loss = 0.0006156370509415865
Validation loss = 0.0002836993080563843
Validation loss = 0.0003862337034661323
Validation loss = 0.0003966455697081983
Validation loss = 0.0002579576103016734
Validation loss = 0.0008873421465978026
Validation loss = 0.0004031207936350256
Validation loss = 0.0003479660954326391
Validation loss = 0.00036663637729361653
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.001527656801044941
Validation loss = 0.00025151792215183377
Validation loss = 0.0005368138081394136
Validation loss = 0.00034594847238622606
Validation loss = 0.00039635738357901573
Validation loss = 0.00034385922481305897
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 177      |
| Iteration     | 11       |
| MaximumReturn | 199      |
| MinimumReturn | 115      |
| TotalSamples  | 43329    |
----------------------------
itr #12 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00033181620528921485
Validation loss = 0.00042047840543091297
Validation loss = 0.0003822576836682856
Validation loss = 0.0004608675080817193
Validation loss = 0.0006906987982802093
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006640991778112948
Validation loss = 0.0003599186020437628
Validation loss = 0.0004483544616959989
Validation loss = 0.000508306547999382
Validation loss = 0.0013878480531275272
Validation loss = 0.0006100767059251666
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0004136817005928606
Validation loss = 0.0004938978236168623
Validation loss = 0.0005146798794157803
Validation loss = 0.0006698064971715212
Validation loss = 0.00027709422283805907
Validation loss = 0.0006676609045825899
Validation loss = 0.000400870805606246
Validation loss = 0.0002088934852508828
Validation loss = 0.0004903748049400747
Validation loss = 0.00050260842544958
Validation loss = 0.00040779219125397503
Validation loss = 0.0003554151044227183
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0004143858968745917
Validation loss = 0.00036006810842081904
Validation loss = 0.0004544983967207372
Validation loss = 0.0009363375138491392
Validation loss = 0.0003968909732066095
Validation loss = 0.0002465183788444847
Validation loss = 0.0003484704939182848
Validation loss = 0.00035211225622333586
Validation loss = 0.00028842533356510103
Validation loss = 0.0002418040530756116
Validation loss = 0.0004985029809176922
Validation loss = 0.0005912967608310282
Validation loss = 0.0007040164200589061
Validation loss = 0.0002247207739856094
Validation loss = 0.00041139445966109633
Validation loss = 0.00037070090183988214
Validation loss = 0.0002988270134665072
Validation loss = 0.0005479067913256586
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005970217753201723
Validation loss = 0.0003102865011896938
Validation loss = 0.00043375289533287287
Validation loss = 0.0003874624671880156
Validation loss = 0.00033610445098020136
Validation loss = 0.00031692840275354683
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 179      |
| Iteration     | 12       |
| MaximumReturn | 199      |
| MinimumReturn | 125      |
| TotalSamples  | 46662    |
----------------------------
itr #13 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00030118602444417775
Validation loss = 0.0002709734544623643
Validation loss = 0.0006114608841016889
Validation loss = 0.0003887720522470772
Validation loss = 0.00027498372946865857
Validation loss = 0.00028933974681422114
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.00034289786708541214
Validation loss = 0.00021303133689798415
Validation loss = 0.0003744644927792251
Validation loss = 0.00040149493725039065
Validation loss = 0.0005045391153544188
Validation loss = 0.0004696894611697644
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0007571998867206275
Validation loss = 0.0003005697508342564
Validation loss = 0.0004809791862498969
Validation loss = 0.0004917595651932061
Validation loss = 0.0003656237677205354
Validation loss = 0.0004758093855343759
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0003402646689210087
Validation loss = 0.0005225393106229603
Validation loss = 0.0002919847029261291
Validation loss = 0.0005001678946428001
Validation loss = 0.00034247792791575193
Validation loss = 0.00032872389419935644
Validation loss = 0.0003768033056985587
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0004262615111656487
Validation loss = 0.0009470244986005127
Validation loss = 0.00039824016857892275
Validation loss = 0.0005485955625772476
Validation loss = 0.00026059526135213673
Validation loss = 0.0005902240518480539
Validation loss = 0.00029553435160778463
Validation loss = 0.000495095388032496
Validation loss = 0.0002579647989477962
Validation loss = 0.0003555396688170731
Validation loss = 0.00028486057999543846
Validation loss = 0.0004062371444888413
Validation loss = 0.00048777536721900105
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 13       |
| MaximumReturn | 199      |
| MinimumReturn | 125      |
| TotalSamples  | 49995    |
----------------------------
itr #14 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00038025007233954966
Validation loss = 0.00023351363779511303
Validation loss = 0.0004660772392526269
Validation loss = 0.0003099397290498018
Validation loss = 0.0003841527795884758
Validation loss = 0.00027043899171985686
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.00055647426052019
Validation loss = 0.0006135941366665065
Validation loss = 0.0005026236176490784
Validation loss = 0.0002936349483206868
Validation loss = 0.0003873565001413226
Validation loss = 0.0002742393990047276
Validation loss = 0.00044899695785716176
Validation loss = 0.0007886296370998025
Validation loss = 0.0003371484635863453
Validation loss = 0.0002404836704954505
Validation loss = 0.00030775487539358437
Validation loss = 0.0003623559605330229
Validation loss = 0.0005442771944217384
Validation loss = 0.000610068382229656
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0006580296321772039
Validation loss = 0.00029173085931688547
Validation loss = 0.00036126963095739484
Validation loss = 0.0003322804986964911
Validation loss = 0.0004621892876457423
Validation loss = 0.00038519673398695886
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0002578787971287966
Validation loss = 0.0003953523118980229
Validation loss = 0.00024448425392620265
Validation loss = 0.0005159528227522969
Validation loss = 0.000235732106375508
Validation loss = 0.0006424977909773588
Validation loss = 0.00023768554092384875
Validation loss = 0.0001904909877339378
Validation loss = 0.00022782006999477744
Validation loss = 0.0003779037215281278
Validation loss = 0.00031631867750547826
Validation loss = 0.0002750132407527417
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00048171397065743804
Validation loss = 0.0006127650267444551
Validation loss = 0.0004966064589098096
Validation loss = 0.0002127980114892125
Validation loss = 0.00029979177634231746
Validation loss = 0.00025964630185626447
Validation loss = 0.0002526904863771051
Validation loss = 0.00035608679172582924
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 14       |
| MaximumReturn | 199      |
| MinimumReturn | 126      |
| TotalSamples  | 53328    |
----------------------------
itr #15 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00027181036421097815
Validation loss = 0.0004249218327458948
Validation loss = 0.00041946725104935467
Validation loss = 0.00045314585440792143
Validation loss = 0.0003140334156341851
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.00026339577743783593
Validation loss = 0.0003577661409508437
Validation loss = 0.0003341113624628633
Validation loss = 0.0005753893638029695
Validation loss = 0.00044626573799178004
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0003565851366147399
Validation loss = 0.00039472660864703357
Validation loss = 0.00021965426276437938
Validation loss = 0.00033301141229458153
Validation loss = 0.0002933442883659154
Validation loss = 0.000297244027024135
Validation loss = 0.0002384849067311734
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00035548704909160733
Validation loss = 0.00039532085065729916
Validation loss = 0.00022563028323929757
Validation loss = 0.0003996831947006285
Validation loss = 0.00016875372966751456
Validation loss = 0.0002378026838414371
Validation loss = 0.0004133826878387481
Validation loss = 0.0002065259322989732
Validation loss = 0.0003767313319258392
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00024412501079495996
Validation loss = 0.0005088649922981858
Validation loss = 0.0003034784458577633
Validation loss = 0.0003627909754868597
Validation loss = 0.00035360196488909423
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 15       |
| MaximumReturn | 199      |
| MinimumReturn | 125      |
| TotalSamples  | 56661    |
----------------------------
itr #16 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0003132556448690593
Validation loss = 0.0002930510963778943
Validation loss = 0.00040491457912139595
Validation loss = 0.00019248532771598548
Validation loss = 0.0003515800053719431
Validation loss = 0.00022213241027202457
Validation loss = 0.00039144870243035257
Validation loss = 0.00033880191040225327
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0002937007520813495
Validation loss = 0.0003170936251990497
Validation loss = 0.00037889525992795825
Validation loss = 0.00024247141845989972
Validation loss = 0.000567885406780988
Validation loss = 0.0003233383467886597
Validation loss = 0.0008731838897801936
Validation loss = 0.0002780308132059872
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0002864205162040889
Validation loss = 0.0002257485466543585
Validation loss = 0.00034754033549688756
Validation loss = 0.0003248928696848452
Validation loss = 0.00034891400719061494
Validation loss = 0.0001983574329642579
Validation loss = 0.0003946393553633243
Validation loss = 0.00037180245271883905
Validation loss = 0.0004645884328056127
Validation loss = 0.00032269282382912934
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005181764718145132
Validation loss = 0.000339018675731495
Validation loss = 0.00019313332450110465
Validation loss = 0.00022722258290741593
Validation loss = 0.0004536409105639905
Validation loss = 0.0003928949881810695
Validation loss = 0.00018965828348882496
Validation loss = 0.00020444793335627764
Validation loss = 0.00039493266376666725
Validation loss = 0.00032568248570896685
Validation loss = 0.00022721551067661494
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0008532257052138448
Validation loss = 0.0002003592235269025
Validation loss = 0.000353498529875651
Validation loss = 0.00046254743938334286
Validation loss = 0.0002381623344263062
Validation loss = 0.0002220266469521448
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 16       |
| MaximumReturn | 199      |
| MinimumReturn | 117      |
| TotalSamples  | 59994    |
----------------------------
itr #17 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0003858053532894701
Validation loss = 0.00021705580002162606
Validation loss = 0.0003520609752740711
Validation loss = 0.0002181921008741483
Validation loss = 0.0009284078259952366
Validation loss = 0.00024872386711649597
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0004380127356853336
Validation loss = 0.00022829038789495826
Validation loss = 0.00020661222515627742
Validation loss = 0.0003530644753482193
Validation loss = 0.00035728703369386494
Validation loss = 0.00041242866427637637
Validation loss = 0.0003783759893849492
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00027575192507356405
Validation loss = 0.00037892244290560484
Validation loss = 0.0003467784554231912
Validation loss = 0.00018912344239652157
Validation loss = 0.0002847743744496256
Validation loss = 0.00020855964976362884
Validation loss = 0.00018399287364445627
Validation loss = 0.0003416504187043756
Validation loss = 0.00020117628446314484
Validation loss = 0.0001758108555804938
Validation loss = 0.0003147278039250523
Validation loss = 0.0005998156266286969
Validation loss = 0.0003403340233489871
Validation loss = 0.00031301830313168466
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00017573604418430477
Validation loss = 0.00020218308782204986
Validation loss = 0.0002036412915913388
Validation loss = 0.00041190997580997646
Validation loss = 0.0002923927968367934
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0003009313077200204
Validation loss = 0.0003279687953181565
Validation loss = 0.0002954653464257717
Validation loss = 0.00027190340915694833
Validation loss = 0.00025385423214174807
Validation loss = 0.0003426461771596223
Validation loss = 0.00016502459766343236
Validation loss = 0.0007795410347171128
Validation loss = 0.00025501044001430273
Validation loss = 0.0001911431027110666
Validation loss = 0.0005296402960084379
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 178      |
| Iteration     | 17       |
| MaximumReturn | 199      |
| MinimumReturn | 129      |
| TotalSamples  | 63327    |
----------------------------
itr #18 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.000450325635029003
Validation loss = 0.0005225242348387837
Validation loss = 0.00020784966181963682
Validation loss = 0.0005247315275482833
Validation loss = 0.0007767301867716014
Validation loss = 0.0003868800704367459
Validation loss = 0.00021248085249681026
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005541796563193202
Validation loss = 0.0003156399470753968
Validation loss = 0.0004458661423996091
Validation loss = 0.0002121508150594309
Validation loss = 0.00031382867018692195
Validation loss = 0.00031021403265185654
Validation loss = 0.00046412539086304605
Validation loss = 0.00026598721160553396
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00027381363906897604
Validation loss = 0.0006721623940393329
Validation loss = 0.0005288250395096838
Validation loss = 0.00030147036886774004
Validation loss = 0.0002723098441492766
Validation loss = 0.00019516629981808364
Validation loss = 0.0002153785462724045
Validation loss = 0.00017908320296555758
Validation loss = 0.0003032465174328536
Validation loss = 0.00022796470148023218
Validation loss = 0.0002524743031244725
Validation loss = 0.00018833752255886793
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00024855215451680124
Validation loss = 0.0003203846572432667
Validation loss = 0.00024388993915636092
Validation loss = 0.000393842114135623
Validation loss = 0.00016856905131135136
Validation loss = 0.00030457149841822684
Validation loss = 0.00021039384591858834
Validation loss = 0.0002980177232529968
Validation loss = 0.0002427048166282475
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00035688377101905644
Validation loss = 0.0003658487694337964
Validation loss = 0.0004407591768540442
Validation loss = 0.0003836440737359226
Validation loss = 0.00027915931423194706
Validation loss = 0.0002487899037078023
Validation loss = 0.00041784896166063845
Validation loss = 0.0002613613032735884
Validation loss = 0.00027402248815633357
Validation loss = 0.00027345199487172067
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 178      |
| Iteration     | 18       |
| MaximumReturn | 199      |
| MinimumReturn | 133      |
| TotalSamples  | 66660    |
----------------------------
itr #19 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0004277362604625523
Validation loss = 0.00032795159495435655
Validation loss = 0.00036495248787105083
Validation loss = 0.00019888397946488112
Validation loss = 0.00031976180616766214
Validation loss = 0.0004995802300982177
Validation loss = 0.00021540180023293942
Validation loss = 0.00036076424294151366
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.00045392554602585733
Validation loss = 0.00021567691874224693
Validation loss = 0.00016799654986243695
Validation loss = 0.00032847796683199704
Validation loss = 0.0006054769037291408
Validation loss = 0.00031056217267178
Validation loss = 0.00025105822714976966
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00039478036342188716
Validation loss = 0.00019283477740827948
Validation loss = 0.00031212990870699286
Validation loss = 0.0001696806139079854
Validation loss = 0.00020893958571832627
Validation loss = 0.0002571518998593092
Validation loss = 0.00014586080214940012
Validation loss = 0.0002477112866472453
Validation loss = 0.00030810991302132607
Validation loss = 0.0002569638891145587
Validation loss = 0.00017001434753183275
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0004543497634585947
Validation loss = 0.00018055782129522413
Validation loss = 0.00017962956917472184
Validation loss = 0.0002213301049778238
Validation loss = 0.0002808585995808244
Validation loss = 0.0002371749869780615
Validation loss = 0.0002660782483872026
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00021252880105748773
Validation loss = 0.00020402632071636617
Validation loss = 0.00018596170411910862
Validation loss = 0.0002801459049805999
Validation loss = 0.0001837754825828597
Validation loss = 0.00019286581664346159
Validation loss = 0.00019544023962225765
Validation loss = 0.00028648984152823687
Validation loss = 0.0004780071321874857
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 19       |
| MaximumReturn | 199      |
| MinimumReturn | 117      |
| TotalSamples  | 69993    |
----------------------------
itr #20 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00034722290001809597
Validation loss = 0.00020220661826897413
Validation loss = 0.00043808616464957595
Validation loss = 0.0006353204953484237
Validation loss = 0.00021907579503022134
Validation loss = 0.00020080199465155602
Validation loss = 0.00023347479873336852
Validation loss = 0.0001762955798767507
Validation loss = 0.00043884056503884494
Validation loss = 0.00024703811504878104
Validation loss = 0.0002565333852544427
Validation loss = 0.0001802274928195402
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.00041111986502073705
Validation loss = 0.00024156445579137653
Validation loss = 0.0012546826619654894
Validation loss = 0.00032847299007698894
Validation loss = 0.0002155633846996352
Validation loss = 0.000506314798258245
Validation loss = 0.00019840005552396178
Validation loss = 0.00021120422752574086
Validation loss = 0.000218156652408652
Validation loss = 0.00019057509780395776
Validation loss = 0.00027212355053052306
Validation loss = 0.00041621082345955074
Validation loss = 0.00020536017837002873
Validation loss = 0.00022740851272828877
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00022075121523812413
Validation loss = 0.00024346914142370224
Validation loss = 0.00021468126215040684
Validation loss = 0.0003851616056635976
Validation loss = 0.0002481195260770619
Validation loss = 0.0003503775515127927
Validation loss = 0.0003163987712468952
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00020205581677146256
Validation loss = 0.00018957392603624612
Validation loss = 0.0003421582223381847
Validation loss = 0.0002192402316723019
Validation loss = 0.00024745523114688694
Validation loss = 0.00022121942311059684
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0002431464527035132
Validation loss = 0.00014633398677688092
Validation loss = 0.00020557908283080906
Validation loss = 0.00019704473379533738
Validation loss = 0.00041437966865487397
Validation loss = 0.00023500669340137392
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 20       |
| MaximumReturn | 199      |
| MinimumReturn | 117      |
| TotalSamples  | 73326    |
----------------------------
itr #21 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0001565124694025144
Validation loss = 0.000162767362780869
Validation loss = 0.00019304787565488368
Validation loss = 0.00020543894788715988
Validation loss = 0.0001798735756892711
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0003054282860830426
Validation loss = 0.00031966983806341887
Validation loss = 0.00044197915121912956
Validation loss = 0.00023070091265253723
Validation loss = 0.00029184046434238553
Validation loss = 0.0003063102485612035
Validation loss = 0.00035810191184282303
Validation loss = 0.0003467487113084644
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0002277708990732208
Validation loss = 0.00019792276725638658
Validation loss = 0.00032326937071047723
Validation loss = 0.0003153159923385829
Validation loss = 0.00016892762505449355
Validation loss = 0.00022816940327174962
Validation loss = 0.0002521096612326801
Validation loss = 0.00020758590835612267
Validation loss = 0.00019661098485812545
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0002708388492465019
Validation loss = 0.00020574082736857235
Validation loss = 0.00015809678006917238
Validation loss = 0.0002073175710393116
Validation loss = 0.0003844515886157751
Validation loss = 0.0005917702219448984
Validation loss = 0.00017619946447666734
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00019849000091198832
Validation loss = 0.00027949208742938936
Validation loss = 0.000326059409417212
Validation loss = 0.0002444803831167519
Validation loss = 0.00033741293009370565
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 21       |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 76659    |
----------------------------
itr #22 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00016903375217225403
Validation loss = 0.00022481686028186232
Validation loss = 0.00030973213142715394
Validation loss = 0.00023361291096080095
Validation loss = 0.00013836751168128103
Validation loss = 0.00037954977597109973
Validation loss = 0.00025054148864001036
Validation loss = 0.00024577771546319127
Validation loss = 0.000225634707021527
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.00025344849564135075
Validation loss = 0.00030171594698913395
Validation loss = 0.00018193588766735047
Validation loss = 0.0004550639714580029
Validation loss = 0.0002892982156481594
Validation loss = 0.0001885711244540289
Validation loss = 0.0003652625600807369
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0001930151047417894
Validation loss = 0.0004830459365621209
Validation loss = 0.00029011996230110526
Validation loss = 0.00014554364315699786
Validation loss = 0.0002047964808298275
Validation loss = 0.0001891387946670875
Validation loss = 0.00021751612075604498
Validation loss = 0.0001646055025048554
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00026869657449424267
Validation loss = 0.00015255335893016309
Validation loss = 0.00017899861268233508
Validation loss = 0.0001709070784272626
Validation loss = 0.0001642358402023092
Validation loss = 0.00041263678576797247
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00025360676227137446
Validation loss = 0.00018495468248147517
Validation loss = 0.00025049163377843797
Validation loss = 0.0001640932314330712
Validation loss = 0.00017977858078666031
Validation loss = 0.00051986234029755
Validation loss = 0.0001564906124258414
Validation loss = 0.00023580933338962495
Validation loss = 0.00017822340305428952
Validation loss = 0.00022920202172826976
Validation loss = 0.0002699506876524538
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 177      |
| Iteration     | 22       |
| MaximumReturn | 199      |
| MinimumReturn | 130      |
| TotalSamples  | 79992    |
----------------------------
itr #23 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00015876771067269146
Validation loss = 0.00020721275359392166
Validation loss = 0.0002027402661042288
Validation loss = 0.0002515522646717727
Validation loss = 0.00019146068370901048
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0004828424716833979
Validation loss = 0.0003867381310556084
Validation loss = 0.00020554833463393152
Validation loss = 0.00016584715922363102
Validation loss = 0.0005882054101675749
Validation loss = 0.00022760378487873822
Validation loss = 0.0001980048546101898
Validation loss = 0.00024471167125739157
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00019929611880797893
Validation loss = 0.00024707562988623977
Validation loss = 0.00016857506125234067
Validation loss = 0.00034361620782874525
Validation loss = 0.00021958346769679338
Validation loss = 0.00032426806865260005
Validation loss = 0.00015141518088057637
Validation loss = 0.0003841679426841438
Validation loss = 0.00011216121492907405
Validation loss = 0.00031431164825335145
Validation loss = 0.0001726011687424034
Validation loss = 0.00016458661411888897
Validation loss = 0.00025176681810989976
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00032128096790984273
Validation loss = 0.00018803949933499098
Validation loss = 0.0002288521791342646
Validation loss = 0.0002587791532278061
Validation loss = 0.00020954215142410249
Validation loss = 0.0001352254330413416
Validation loss = 0.00020357253379188478
Validation loss = 0.00028428161749616265
Validation loss = 0.00014585604367312044
Validation loss = 0.00011400695802876726
Validation loss = 0.00019014983263332397
Validation loss = 0.00019405553757678717
Validation loss = 0.00022128931595943868
Validation loss = 0.00012076046550646424
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005035075591877103
Validation loss = 0.0002196573477704078
Validation loss = 0.00033968238858506083
Validation loss = 0.00023336247249972075
Validation loss = 0.00029199509299360216
Validation loss = 0.00045440392568707466
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 23       |
| MaximumReturn | 199      |
| MinimumReturn | 121      |
| TotalSamples  | 83325    |
----------------------------
itr #24 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0004344595654401928
Validation loss = 0.0004172382759861648
Validation loss = 0.0002568124036770314
Validation loss = 0.00037347484612837434
Validation loss = 0.00019926080130971968
Validation loss = 0.00022824821644462645
Validation loss = 0.0002959596167784184
Validation loss = 0.0002656662545632571
Validation loss = 0.00011410687147872522
Validation loss = 0.0004587142029777169
Validation loss = 0.0003139546897727996
Validation loss = 0.00015555611753370613
Validation loss = 0.00013755523832514882
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.00035249534994363785
Validation loss = 0.0002429152518743649
Validation loss = 0.00028601192752830684
Validation loss = 0.00025528730475343764
Validation loss = 0.0002481644041836262
Validation loss = 0.00018713268218562007
Validation loss = 0.00015891078510321677
Validation loss = 0.0001431374257663265
Validation loss = 0.000365036801667884
Validation loss = 0.0002686747466214001
Validation loss = 0.00011145082680741325
Validation loss = 0.00036201474722474813
Validation loss = 0.0003722900873981416
Validation loss = 0.00019587510905694216
Validation loss = 0.00013938294432591647
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00014109312905929983
Validation loss = 0.00020007796410936862
Validation loss = 0.0002458660746924579
Validation loss = 0.00035063549876213074
Validation loss = 0.0001461344800191
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00018030426872428507
Validation loss = 0.00016814196715131402
Validation loss = 0.00018248404376208782
Validation loss = 0.0001694628590485081
Validation loss = 0.00018249009735882282
Validation loss = 0.0003231351438444108
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00035070054582320154
Validation loss = 0.00029154683579690754
Validation loss = 0.00014696159632876515
Validation loss = 0.000332474650349468
Validation loss = 0.0001378392771584913
Validation loss = 0.00016901385970413685
Validation loss = 0.00019156798953190446
Validation loss = 0.00017694248526822776
Validation loss = 0.00025086640380322933
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 24       |
| MaximumReturn | 199      |
| MinimumReturn | 117      |
| TotalSamples  | 86658    |
----------------------------
itr #25 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00022332645312417299
Validation loss = 0.0005691731348633766
Validation loss = 0.0001962710084626451
Validation loss = 0.00026843551313504577
Validation loss = 0.00014091693446971476
Validation loss = 0.0005578174605034292
Validation loss = 0.00012649322161450982
Validation loss = 0.0001672233920544386
Validation loss = 0.00013800401939079165
Validation loss = 0.00014931477198842913
Validation loss = 0.00016847325605340302
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.00033506634645164013
Validation loss = 0.0002511251368559897
Validation loss = 0.00018456742691341788
Validation loss = 0.00015564086788799614
Validation loss = 0.0001235694217029959
Validation loss = 0.0006317198276519775
Validation loss = 0.00014005172124598175
Validation loss = 0.00013110641157254577
Validation loss = 0.00016863879864104092
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0002327050460735336
Validation loss = 0.0001666532043600455
Validation loss = 0.00013385101919993758
Validation loss = 0.00015784264542162418
Validation loss = 0.0001666572643443942
Validation loss = 0.0002904766006395221
Validation loss = 0.00016406449140049517
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007510147406719625
Validation loss = 0.00011921117402380332
Validation loss = 0.00014946320152375847
Validation loss = 0.0003032563254237175
Validation loss = 0.0003877939307130873
Validation loss = 0.00019813128164969385
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0003497809811960906
Validation loss = 0.0003997105814050883
Validation loss = 0.00017086898151319474
Validation loss = 0.00018191155686508864
Validation loss = 0.00016065406089182943
Validation loss = 0.00016643271374050528
Validation loss = 0.00018867547623813152
Validation loss = 0.00023736743605695665
Validation loss = 0.0001941401424119249
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 25       |
| MaximumReturn | 199      |
| MinimumReturn | 126      |
| TotalSamples  | 89991    |
----------------------------
itr #26 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.000243349393713288
Validation loss = 0.00022393646941054612
Validation loss = 0.00015989210805855691
Validation loss = 0.00020143379515502602
Validation loss = 0.00036390716559253633
Validation loss = 0.0002330345887457952
Validation loss = 0.0002281248162034899
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.00012992467964068055
Validation loss = 0.0002579901192802936
Validation loss = 0.000121239987493027
Validation loss = 0.00020622987358365208
Validation loss = 0.00022630863531958312
Validation loss = 0.00020381981448736042
Validation loss = 0.00012629214324988425
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.000260417175013572
Validation loss = 0.0001294230023631826
Validation loss = 0.00014455069322139025
Validation loss = 0.0002714489819481969
Validation loss = 0.00022382641327567399
Validation loss = 0.0001771376555552706
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00022954797896090895
Validation loss = 0.0001671934442128986
Validation loss = 0.0002521296264603734
Validation loss = 0.0002277328894706443
Validation loss = 0.00025401648599654436
Validation loss = 9.633235458750278e-05
Validation loss = 0.0001561663520988077
Validation loss = 0.0002737594477366656
Validation loss = 0.0001413822319591418
Validation loss = 0.00018404089496470988
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00018357831868343055
Validation loss = 0.00019700413395185024
Validation loss = 0.00011680897296173498
Validation loss = 0.0004226572054903954
Validation loss = 0.0001487124100094661
Validation loss = 0.00016660202527418733
Validation loss = 0.00012367538874968886
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 26       |
| MaximumReturn | 199      |
| MinimumReturn | 135      |
| TotalSamples  | 93324    |
----------------------------
itr #27 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0001237372198374942
Validation loss = 0.00017585643217898905
Validation loss = 0.0002543232694733888
Validation loss = 0.00016046709788497537
Validation loss = 0.0001407731615472585
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0002386684500379488
Validation loss = 0.00016988279821816832
Validation loss = 0.00036574245314113796
Validation loss = 0.00034967748797498643
Validation loss = 0.0002098284603562206
Validation loss = 0.0001212244969792664
Validation loss = 0.0001547229621792212
Validation loss = 0.00027730053989216685
Validation loss = 0.00019144358520861715
Validation loss = 0.0004941179067827761
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00021564286726061255
Validation loss = 0.00029033186729066074
Validation loss = 0.0001125695780501701
Validation loss = 8.423859981121495e-05
Validation loss = 0.00010708460467867553
Validation loss = 0.00045016894000582397
Validation loss = 0.0001504764222772792
Validation loss = 0.0002274128928547725
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00020081033289898187
Validation loss = 0.00014829121937509626
Validation loss = 0.0001637627137824893
Validation loss = 0.00017440450028516352
Validation loss = 0.0002549431228544563
Validation loss = 0.0001617198286112398
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00018474497483111918
Validation loss = 0.0003161055501550436
Validation loss = 0.00019901531049981713
Validation loss = 0.00013087564730085433
Validation loss = 0.0003335780056659132
Validation loss = 0.00018375275249127299
Validation loss = 0.0001388534001307562
Validation loss = 0.0002090911875711754
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 27       |
| MaximumReturn | 199      |
| MinimumReturn | 117      |
| TotalSamples  | 96657    |
----------------------------
itr #28 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00017181631119456142
Validation loss = 0.0001385752548230812
Validation loss = 0.00020709936507046223
Validation loss = 0.0002850211167242378
Validation loss = 0.00019599823281168938
Validation loss = 0.0001127750365412794
Validation loss = 0.0002044427819782868
Validation loss = 0.00019059462647419423
Validation loss = 0.0001766547648003325
Validation loss = 0.00019560482178349048
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.00018609862308949232
Validation loss = 0.0003311683831270784
Validation loss = 0.00015822907153051347
Validation loss = 0.0001409490214427933
Validation loss = 0.00020649372891057283
Validation loss = 0.0003036694251932204
Validation loss = 0.00012569762475322932
Validation loss = 0.0003387510369066149
Validation loss = 0.00015943498874548823
Validation loss = 0.00023862697707954794
Validation loss = 0.00042339516221545637
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0003841769939754158
Validation loss = 0.00036499174893833697
Validation loss = 0.0001837331656133756
Validation loss = 0.00036803624243475497
Validation loss = 0.00012883303861599416
Validation loss = 0.00024228706024587154
Validation loss = 0.00018678803462535143
Validation loss = 0.00018735263438429683
Validation loss = 0.00022965692915022373
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00038822705391794443
Validation loss = 0.0002346612891415134
Validation loss = 0.0001719803549349308
Validation loss = 0.0001710431679384783
Validation loss = 0.00016393496480304748
Validation loss = 0.00011946637096116319
Validation loss = 0.00026027363492175937
Validation loss = 0.00010273753287037835
Validation loss = 0.00014325020310934633
Validation loss = 0.00011490401084301993
Validation loss = 0.00018887990154325962
Validation loss = 0.00020946335280314088
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0001571794564370066
Validation loss = 0.0001954147155629471
Validation loss = 0.00022550161520484835
Validation loss = 0.00012494920520111918
Validation loss = 0.00017046563152689487
Validation loss = 0.00013526622205972672
Validation loss = 0.00022765361063648015
Validation loss = 0.00012873999367002398
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 28       |
| MaximumReturn | 199      |
| MinimumReturn | 121      |
| TotalSamples  | 99990    |
----------------------------
itr #29 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00013675399532075971
Validation loss = 0.00019585709378588945
Validation loss = 0.00017348217079415917
Validation loss = 0.00033798321965150535
Validation loss = 0.00013590749586001039
Validation loss = 0.00015141593758016825
Validation loss = 0.0001590632600709796
Validation loss = 0.00017329050751868635
Validation loss = 0.00019746294128708541
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0001483399246353656
Validation loss = 0.000200164649868384
Validation loss = 0.00014311236736830324
Validation loss = 0.00013963630772195756
Validation loss = 0.0002161929733119905
Validation loss = 0.00016092168516479433
Validation loss = 0.00014870079758111387
Validation loss = 0.00023722609330434352
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00024730354198254645
Validation loss = 0.0003491438110359013
Validation loss = 0.0002039663668256253
Validation loss = 0.00017926756117958575
Validation loss = 0.0001790319220162928
Validation loss = 0.00016325498290825635
Validation loss = 0.0005406660493463278
Validation loss = 0.00012511761451605707
Validation loss = 0.00016717257676646113
Validation loss = 0.00024985006893984973
Validation loss = 0.00013224175199866295
Validation loss = 0.00023386273824144155
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00014520854165311903
Validation loss = 9.902620513457805e-05
Validation loss = 0.0001334713597316295
Validation loss = 0.00012524696649052203
Validation loss = 0.00010832824773387983
Validation loss = 0.00018831623310688883
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00012958480510860682
Validation loss = 0.00016692961798980832
Validation loss = 0.00015510845696553588
Validation loss = 0.00031628378201276064
Validation loss = 0.00022711926430929452
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 29       |
| MaximumReturn | 199      |
| MinimumReturn | 131      |
| TotalSamples  | 103323   |
----------------------------
itr #30 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00029748762608505785
Validation loss = 0.00011037171498173848
Validation loss = 0.0001942269882420078
Validation loss = 0.00015150351100601256
Validation loss = 0.00016097479965537786
Validation loss = 0.0001142098699347116
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.00022957523469813168
Validation loss = 0.00014157043187879026
Validation loss = 0.00014364090748131275
Validation loss = 0.0001381117181153968
Validation loss = 0.00024609672254882753
Validation loss = 8.3684288256336e-05
Validation loss = 0.00012137569137848914
Validation loss = 0.00016686934395693243
Validation loss = 0.00011734422150766477
Validation loss = 0.0001819706812966615
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00010532613669056445
Validation loss = 0.00017518435197416693
Validation loss = 0.0006312436307780445
Validation loss = 0.00014721779734827578
Validation loss = 0.00018354896747041494
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.000157594055053778
Validation loss = 0.00032926490530371666
Validation loss = 0.00011995845852652565
Validation loss = 0.000336447759764269
Validation loss = 0.00015132481348700821
Validation loss = 9.393110667588189e-05
Validation loss = 0.0001770688541000709
Validation loss = 0.00019753161177504808
Validation loss = 0.00011463651753729209
Validation loss = 0.00011924937280127779
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00020704057533293962
Validation loss = 0.00036478060064837337
Validation loss = 0.00023570987104903907
Validation loss = 9.68616659520194e-05
Validation loss = 0.00012778221571352333
Validation loss = 0.00013030263653490692
Validation loss = 0.00010260617273161188
Validation loss = 0.00011056350194849074
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 30       |
| MaximumReturn | 199      |
| MinimumReturn | 91.9     |
| TotalSamples  | 106656   |
----------------------------
itr #31 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00012799543037544936
Validation loss = 0.00011016982898581773
Validation loss = 0.00027944002067670226
Validation loss = 0.00034469328238628805
Validation loss = 0.000126988350530155
Validation loss = 0.000387601088732481
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0002650381065905094
Validation loss = 0.0001565524289617315
Validation loss = 0.00010429549729451537
Validation loss = 0.0001749408256728202
Validation loss = 0.00019716615497600287
Validation loss = 0.00011494054342620075
Validation loss = 0.00014444912085309625
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0003236762713640928
Validation loss = 0.00013621759717352688
Validation loss = 0.00022072854335419834
Validation loss = 0.00015943043399602175
Validation loss = 0.00010462963837198913
Validation loss = 0.00018063405877910554
Validation loss = 0.00018863490549847484
Validation loss = 0.00012747864820994437
Validation loss = 0.0002979945857077837
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00010612425830913708
Validation loss = 0.000228271062951535
Validation loss = 8.71871889103204e-05
Validation loss = 0.00019373674876987934
Validation loss = 0.0006466642953455448
Validation loss = 0.000271838391199708
Validation loss = 0.00010249767365166917
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00019672579946927726
Validation loss = 0.000217940571019426
Validation loss = 0.00022387086937669665
Validation loss = 0.00011548445763764903
Validation loss = 0.00044364057248458266
Validation loss = 0.00012856473040301353
Validation loss = 0.00023346369562204927
Validation loss = 0.000486298551550135
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 31       |
| MaximumReturn | 199      |
| MinimumReturn | 108      |
| TotalSamples  | 109989   |
----------------------------
itr #32 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00021123004262335598
Validation loss = 9.077687718672678e-05
Validation loss = 0.00010242495773127303
Validation loss = 0.00011433366307755932
Validation loss = 0.00018089069635607302
Validation loss = 0.00014021273818798363
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0001530779991298914
Validation loss = 0.00022734231606591493
Validation loss = 0.00022062251809984446
Validation loss = 0.000239072585827671
Validation loss = 0.00027258385671302676
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0001262621080968529
Validation loss = 0.0001868310064310208
Validation loss = 9.991166007239372e-05
Validation loss = 0.00034642740502022207
Validation loss = 0.00011824752436950803
Validation loss = 0.00014545601152349263
Validation loss = 0.00018297764472663403
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00011173401435371488
Validation loss = 0.00017846026457846165
Validation loss = 0.0002991708170156926
Validation loss = 0.00022496689052786678
Validation loss = 0.0002710994449444115
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 9.205981768900529e-05
Validation loss = 0.00013751574442721903
Validation loss = 0.00021587398077826947
Validation loss = 9.772439807420596e-05
Validation loss = 0.00016222336853388697
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 164      |
| Iteration     | 32       |
| MaximumReturn | 199      |
| MinimumReturn | 104      |
| TotalSamples  | 113322   |
----------------------------
itr #33 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00025348839699290693
Validation loss = 0.00018620530318003148
Validation loss = 0.00019259138207416981
Validation loss = 0.00012150210386607796
Validation loss = 8.038502710405737e-05
Validation loss = 0.0002296218735864386
Validation loss = 0.00023542674898635596
Validation loss = 0.00018645315140020102
Validation loss = 9.960740862879902e-05
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0001526110718259588
Validation loss = 0.0001526552514405921
Validation loss = 0.00010521502554183826
Validation loss = 0.00017057628429029137
Validation loss = 0.00012412173964548856
Validation loss = 0.00010195341747021303
Validation loss = 0.0001981892710318789
Validation loss = 0.0003508067165967077
Validation loss = 0.00025888968957588077
Validation loss = 0.00012031148798996583
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00013162769027985632
Validation loss = 0.00014364563685376197
Validation loss = 9.661378135206178e-05
Validation loss = 0.0002338719496037811
Validation loss = 0.00011721278133336455
Validation loss = 0.0001592640473973006
Validation loss = 0.0001500634680269286
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0002293297293363139
Validation loss = 0.00018268993881065398
Validation loss = 0.0001468988339183852
Validation loss = 0.00010126845154445618
Validation loss = 0.00016579667862970382
Validation loss = 0.00012079896259820089
Validation loss = 0.0001506384287495166
Validation loss = 0.0002258914610138163
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00019206060096621513
Validation loss = 0.00029309751698747277
Validation loss = 8.043966226978227e-05
Validation loss = 0.00016475857410114259
Validation loss = 0.0001286028273170814
Validation loss = 0.00016136061458382756
Validation loss = 0.00016395215061493218
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 33       |
| MaximumReturn | 199      |
| MinimumReturn | 122      |
| TotalSamples  | 116655   |
----------------------------
itr #34 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00023150468769017607
Validation loss = 0.00030401311232708395
Validation loss = 0.00010388764349045232
Validation loss = 0.0001990870659938082
Validation loss = 0.00010499972995603457
Validation loss = 0.0003331214829813689
Validation loss = 0.00010698119149310514
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.00013032843708060682
Validation loss = 0.0005452733021229506
Validation loss = 0.00011677481234073639
Validation loss = 0.0002692311245482415
Validation loss = 0.00012238493945915252
Validation loss = 0.00010106920672114938
Validation loss = 0.0003307744627818465
Validation loss = 0.00012368056923151016
Validation loss = 0.00014124493463896215
Validation loss = 0.00011653131514322013
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00030955130932852626
Validation loss = 0.00014446220302488655
Validation loss = 0.0001470782735850662
Validation loss = 0.0001266117178602144
Validation loss = 0.00021072753588669002
Validation loss = 0.0001044816744979471
Validation loss = 9.566945664118975e-05
Validation loss = 0.0001062497467501089
Validation loss = 0.00017937128723133355
Validation loss = 0.00017733169079292566
Validation loss = 0.00018930966325569898
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00012852766667492688
Validation loss = 0.00013248973118606955
Validation loss = 0.00010388171358499676
Validation loss = 0.00010644317080732435
Validation loss = 0.000139831259730272
Validation loss = 0.00028156067128293216
Validation loss = 0.00019439282186795026
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00018192247080150992
Validation loss = 0.00011927403102163225
Validation loss = 0.00012658568448387086
Validation loss = 0.0001899333146866411
Validation loss = 0.00036140531301498413
Validation loss = 0.00012959928426425904
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 34       |
| MaximumReturn | 199      |
| MinimumReturn | 121      |
| TotalSamples  | 119988   |
----------------------------
itr #35 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 8.121968130581081e-05
Validation loss = 0.00017666684288997203
Validation loss = 8.817311754683033e-05
Validation loss = 8.051415352383628e-05
Validation loss = 0.00012198220792924985
Validation loss = 0.00015473604435101151
Validation loss = 0.00017299146566074342
Validation loss = 0.0002422622055746615
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 9.364482684759423e-05
Validation loss = 0.00011012317554559559
Validation loss = 9.744176350068301e-05
Validation loss = 0.0002286473027197644
Validation loss = 0.00020933554333169013
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.000125645034131594
Validation loss = 0.00018901165458373725
Validation loss = 0.00010496002505533397
Validation loss = 0.0001407207892043516
Validation loss = 0.00011256185825914145
Validation loss = 0.00012974497803952545
Validation loss = 0.00010182771075051278
Validation loss = 0.0001231801143148914
Validation loss = 0.00020094626233913004
Validation loss = 0.0001758672297000885
Validation loss = 0.0002197268040617928
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00021132973779458553
Validation loss = 0.00010360602755099535
Validation loss = 7.583806291222572e-05
Validation loss = 9.72005000221543e-05
Validation loss = 0.0001017800168483518
Validation loss = 0.0003183474764227867
Validation loss = 0.00014822314551565796
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0001468158734496683
Validation loss = 0.00012395120575092733
Validation loss = 9.023459278978407e-05
Validation loss = 0.00015281337255146354
Validation loss = 0.000223311988520436
Validation loss = 0.0001660857378738001
Validation loss = 9.958200826076791e-05
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 35       |
| MaximumReturn | 199      |
| MinimumReturn | 126      |
| TotalSamples  | 123321   |
----------------------------
itr #36 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 8.290108962683007e-05
Validation loss = 0.0002843606926035136
Validation loss = 0.00013093891902826726
Validation loss = 0.00021820698748342693
Validation loss = 0.00011837223428301513
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0001488476264057681
Validation loss = 0.00010366221977164969
Validation loss = 0.00014973116049077362
Validation loss = 0.0002691762347240001
Validation loss = 0.00018717575585469604
Validation loss = 0.00015886726032476872
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00011209373769816011
Validation loss = 8.231057290686294e-05
Validation loss = 0.00015489933139178902
Validation loss = 0.00010287338227499276
Validation loss = 0.00019272652571089566
Validation loss = 9.746986324898899e-05
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00017122988356277347
Validation loss = 7.877260213717818e-05
Validation loss = 9.865101310424507e-05
Validation loss = 0.00015097641153261065
Validation loss = 0.00011168905621161684
Validation loss = 9.109332313528284e-05
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0002670107642188668
Validation loss = 0.00020961801055818796
Validation loss = 0.00013015244621783495
Validation loss = 0.0001968236465472728
Validation loss = 8.700629405211657e-05
Validation loss = 0.00010308486525900662
Validation loss = 0.00011546068708412349
Validation loss = 0.00010950008436338976
Validation loss = 0.00011196611740160733
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 36       |
| MaximumReturn | 199      |
| MinimumReturn | 130      |
| TotalSamples  | 126654   |
----------------------------
itr #37 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00021484363242052495
Validation loss = 0.00011999162961728871
Validation loss = 0.00014662339526694268
Validation loss = 0.00024206309171859175
Validation loss = 0.00013718928676098585
Validation loss = 0.00013506453251466155
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.00019609688024502248
Validation loss = 0.00010297279368387535
Validation loss = 0.00013287682668305933
Validation loss = 9.38293305807747e-05
Validation loss = 0.00019051447452511638
Validation loss = 0.00024658680194988847
Validation loss = 6.569517427124083e-05
Validation loss = 9.183968359138817e-05
Validation loss = 0.00017088299500755966
Validation loss = 0.0001541126548545435
Validation loss = 0.0001118954096455127
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 8.561112190363929e-05
Validation loss = 9.910287917591631e-05
Validation loss = 0.0002011244068853557
Validation loss = 0.00024737988132983446
Validation loss = 0.00014066197036299855
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 9.919791773427278e-05
Validation loss = 0.00010864482464967296
Validation loss = 0.00014656849089078605
Validation loss = 0.00013370125088840723
Validation loss = 0.0002840215456672013
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00012509201769717038
Validation loss = 0.00018299346265848726
Validation loss = 0.00013041443889960647
Validation loss = 0.00025553780142217875
Validation loss = 0.00014621073205489665
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 37       |
| MaximumReturn | 199      |
| MinimumReturn | 130      |
| TotalSamples  | 129987   |
----------------------------
itr #38 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00028612883761525154
Validation loss = 0.00015703779354225844
Validation loss = 0.00012133134441683069
Validation loss = 0.0001548494037706405
Validation loss = 0.00012127616355428472
Validation loss = 0.00012661713117267936
Validation loss = 9.050942026078701e-05
Validation loss = 0.00018282032397110015
Validation loss = 0.00020059759845025837
Validation loss = 0.00011730252299457788
Validation loss = 0.0001147194707300514
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.00016462418716400862
Validation loss = 0.00012259767390787601
Validation loss = 0.00010920983913820237
Validation loss = 0.000146188132930547
Validation loss = 0.0002002997644012794
Validation loss = 0.00019207471632398665
Validation loss = 0.00010440998448757455
Validation loss = 0.00011031411122530699
Validation loss = 0.00010227122402284294
Validation loss = 0.0002689656976144761
Validation loss = 0.00014800428471062332
Validation loss = 0.00011758451728383079
Validation loss = 0.00010589294106466696
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 9.76926094153896e-05
Validation loss = 0.0001199487887788564
Validation loss = 0.00022249513131100684
Validation loss = 0.00011170910875080153
Validation loss = 0.00024376643705181777
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 8.150942448992282e-05
Validation loss = 0.0001441667991457507
Validation loss = 0.00012876378605142236
Validation loss = 0.0001103428949136287
Validation loss = 0.0001194912038045004
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00011355558672221377
Validation loss = 0.00011085350706707686
Validation loss = 0.00010950696014333516
Validation loss = 0.00011995022214250639
Validation loss = 0.00013455402222461998
Validation loss = 9.685089025879279e-05
Validation loss = 0.00026433245511725545
Validation loss = 9.160769695881754e-05
Validation loss = 0.00023760458861943334
Validation loss = 0.00016410839452873915
Validation loss = 0.00017337256576865911
Validation loss = 0.00011537827231222764
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 38       |
| MaximumReturn | 199      |
| MinimumReturn | 121      |
| TotalSamples  | 133320   |
----------------------------
itr #39 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0002031511685345322
Validation loss = 0.00012538822193164378
Validation loss = 0.00017172664229292423
Validation loss = 0.0001891568535938859
Validation loss = 0.0002318879123777151
Validation loss = 9.370025509269908e-05
Validation loss = 0.0001696990802884102
Validation loss = 0.0001240698475157842
Validation loss = 0.0001024216107907705
Validation loss = 0.00018700170039664954
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 8.442287798970938e-05
Validation loss = 0.000433238223195076
Validation loss = 9.168135875370353e-05
Validation loss = 0.00019751154468394816
Validation loss = 0.00012256046466063708
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 8.727370004635304e-05
Validation loss = 0.00012488861102610826
Validation loss = 0.00021297302737366408
Validation loss = 0.00019091986177954823
Validation loss = 0.0003090972895734012
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00022956285101827234
Validation loss = 9.994551510317251e-05
Validation loss = 0.00018733370234258473
Validation loss = 0.00011363685189280659
Validation loss = 0.0001856957096606493
Validation loss = 8.704442734597251e-05
Validation loss = 8.916577644413337e-05
Validation loss = 0.00010543716780375689
Validation loss = 8.521306881448254e-05
Validation loss = 0.00020953988132532686
Validation loss = 0.00011323051148792729
Validation loss = 9.736901120049879e-05
Validation loss = 0.00011652021203190088
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00011416414781706408
Validation loss = 0.00014034649939276278
Validation loss = 0.00010882852802751586
Validation loss = 0.00010824822675203905
Validation loss = 0.0002107807667925954
Validation loss = 0.00015378602256532758
Validation loss = 0.00019499023619573563
Validation loss = 0.00013474684965331107
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
Path 1 | total_timesteps 200.
Path 2 | total_timesteps 400.
Path 3 | total_timesteps 600.
Path 4 | total_timesteps 800.
Path 5 | total_timesteps 1000.
Path 6 | total_timesteps 1200.
Path 7 | total_timesteps 1400.
Path 8 | total_timesteps 1600.
Path 9 | total_timesteps 1800.
Path 10 | total_timesteps 2000.
Path 11 | total_timesteps 2200.
Path 12 | total_timesteps 2400.
Path 13 | total_timesteps 2600.
Path 14 | total_timesteps 2800.
Path 15 | total_timesteps 3000.
Path 16 | total_timesteps 3200.
Path 17 | total_timesteps 3400.
Path 18 | total_timesteps 3600.
Path 19 | total_timesteps 3800.
Path 20 | total_timesteps 4000.
Path 21 | total_timesteps 4200.
Path 22 | total_timesteps 4400.
Path 23 | total_timesteps 4600.
Path 24 | total_timesteps 4800.
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 39       |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 136653   |
----------------------------
