Logging to experiments/pendulum/Pendulum_Action_Noise/w100e0.5_seed2531
Print configuration .....
{'env_name': 'pendulum', 'random_seeds': [3214, 2431, 2531, 2231], 'save_variables': False, 'model_save_dir': '/tmp/pendulum_models/', 'restore_variables': False, 'start_onpol_iter': 0, 'onpol_iters': 40, 'num_path_random': 25, 'num_path_onpol': 25, 'env_horizon': 200, 'max_train_data': 200000, 'max_val_data': 100000, 'discard_ratio': 0.0, 'dynamics': {'pre_training': {'mode': 'intrinsic_reward', 'itr': 0, 'policy_itr': 20}, 'model': 'nn', 'ensemble': True, 'ensemble_model_count': 5, 'enable_particle_ensemble': True, 'particles': 5, 'obs_var': 1.0, 'intrinsic_reward_coeff': 1.0, 'ita': 1.0, 'mode': 'random', 'val': True, 'n_layers': 4, 'hidden_size': 1000, 'activation': 'relu', 'batch_size': 1000, 'learning_rate': 0.001, 'reg_coeff': 0.0, 'epochs': 200, 'kfac_params': {'learning_rate': 0.1, 'damping': 0.001, 'momentum': 0.9, 'kl_clip': 0.0001, 'cov_ema_decay': 0.99}}, 'policy': {'network_shape': [64, 64], 'init_logstd': 0.0, 'activation': 'tanh', 'reinitialize_every_itr': False}, 'trpo': {'horizon': 200, 'gamma': 0.99, 'step_size': 0.01, 'iterations': 20, 'batch_size': 50000, 'gae': 0.95, 'visualization': False, 'visualize_iterations': [0]}, 'algo': 'trpo'}
Generating random rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating random rollouts.
Creating normalization for training data.
Done creating normalization for training data.
Particle ensemble enabled? True
An ensemble of 5 dynamics model <class 'model.dynamics.NNDynamicsModel'> initialized
Train dynamics model with intrinsic reward only? False
Pre-training enabled. Using only intrinsic reward.
Pre-training dynamics model for 0 iterations...
Done pre-training dynamics model.
Using external reward only.
itr #0 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.7185370922088623
Validation loss = 0.06260697543621063
Validation loss = 0.0195644348859787
Validation loss = 0.006937035825103521
Validation loss = 0.004531435668468475
Validation loss = 0.0034299534745514393
Validation loss = 0.0031222477555274963
Validation loss = 0.002859588712453842
Validation loss = 0.0035088376607745886
Validation loss = 0.00321015901863575
Validation loss = 0.004616763908416033
Validation loss = 0.004211800172924995
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.7601373791694641
Validation loss = 0.09941928833723068
Validation loss = 0.0330982506275177
Validation loss = 0.012091003358364105
Validation loss = 0.005685389041900635
Validation loss = 0.004378550685942173
Validation loss = 0.0035438963677734137
Validation loss = 0.003249591449275613
Validation loss = 0.0032806689850986004
Validation loss = 0.003068062709644437
Validation loss = 0.0034699239768087864
Validation loss = 0.004044217523187399
Validation loss = 0.003286193823441863
Validation loss = 0.0029879806097596884
Validation loss = 0.003991912584751844
Validation loss = 0.004478579852730036
Validation loss = 0.002884663175791502
Validation loss = 0.018888790160417557
Validation loss = 0.004887825343757868
Validation loss = 0.0034556055907160044
Validation loss = 0.003053826279938221
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.7609515190124512
Validation loss = 0.07044960558414459
Validation loss = 0.017924847081303596
Validation loss = 0.006294038146734238
Validation loss = 0.004744734149426222
Validation loss = 0.0036147276405245066
Validation loss = 0.0030517762061208487
Validation loss = 0.003188185393810272
Validation loss = 0.0029630442149937153
Validation loss = 0.012724564410746098
Validation loss = 0.006353634409606457
Validation loss = 0.003308675717562437
Validation loss = 0.003205374348908663
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.7553473114967346
Validation loss = 0.09319519996643066
Validation loss = 0.03178929537534714
Validation loss = 0.011389290913939476
Validation loss = 0.005219611339271069
Validation loss = 0.004290506709367037
Validation loss = 0.0034271443728357553
Validation loss = 0.0031719703692942858
Validation loss = 0.003153631929308176
Validation loss = 0.0031572477892041206
Validation loss = 0.003760125720873475
Validation loss = 0.003272764151915908
Validation loss = 0.0040740217082202435
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.7276965975761414
Validation loss = 0.07213757932186127
Validation loss = 0.01635524444282055
Validation loss = 0.007005853112787008
Validation loss = 0.004432594869285822
Validation loss = 0.0038020412903279066
Validation loss = 0.0032508564181625843
Validation loss = 0.0030574609991163015
Validation loss = 0.0032618495170027018
Validation loss = 0.0033638915047049522
Validation loss = 0.00405150605365634
Validation loss = 0.002956368727609515
Validation loss = 0.004551272839307785
Validation loss = 0.004200011026114225
Validation loss = 0.004381129052489996
Validation loss = 0.004409101326018572
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 177      |
| Iteration     | 0        |
| MaximumReturn | 199      |
| MinimumReturn | 98.6     |
| TotalSamples  | 6666     |
----------------------------
itr #1 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.02381414733827114
Validation loss = 0.003364335745573044
Validation loss = 0.0020556277595460415
Validation loss = 0.0040389359928667545
Validation loss = 0.0022628195583820343
Validation loss = 0.004778021480888128
Validation loss = 0.0018324609845876694
Validation loss = 0.00425303028896451
Validation loss = 0.0020588815677911043
Validation loss = 0.0026594034861773252
Validation loss = 0.0017887130379676819
Validation loss = 0.004217796493321657
Validation loss = 0.003904556157067418
Validation loss = 0.002369448309764266
Validation loss = 0.0019732273649424314
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0189838670194149
Validation loss = 0.0036419208627194166
Validation loss = 0.002223423682153225
Validation loss = 0.0019994904287159443
Validation loss = 0.002464663004502654
Validation loss = 0.0022903901990503073
Validation loss = 0.0025964153464883566
Validation loss = 0.0029106230940669775
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.015999507158994675
Validation loss = 0.004435369279235601
Validation loss = 0.0029005177784711123
Validation loss = 0.002120261313393712
Validation loss = 0.0026411551516503096
Validation loss = 0.002479715272784233
Validation loss = 0.0021242587827146053
Validation loss = 0.0027304477989673615
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.017029087990522385
Validation loss = 0.005088037345558405
Validation loss = 0.002420829376205802
Validation loss = 0.0019044284708797932
Validation loss = 0.0018419664120301604
Validation loss = 0.004428555723279715
Validation loss = 0.0021345671266317368
Validation loss = 0.006267193704843521
Validation loss = 0.0024796815123409033
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.02307419292628765
Validation loss = 0.003983963280916214
Validation loss = 0.002067023189738393
Validation loss = 0.0019594677723944187
Validation loss = 0.005215099547058344
Validation loss = 0.0024928010534495115
Validation loss = 0.003404771676287055
Validation loss = 0.002610423369333148
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 177      |
| Iteration     | 1        |
| MaximumReturn | 199      |
| MinimumReturn | 116      |
| TotalSamples  | 9999     |
----------------------------
itr #2 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.006486391182988882
Validation loss = 0.0027924443129450083
Validation loss = 0.0013877173187211156
Validation loss = 0.002352894051000476
Validation loss = 0.0027734029572457075
Validation loss = 0.0017932932823896408
Validation loss = 0.001882334821857512
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.00974251888692379
Validation loss = 0.002113894559442997
Validation loss = 0.007020375691354275
Validation loss = 0.005649769213050604
Validation loss = 0.0014319352339953184
Validation loss = 0.0021722125820815563
Validation loss = 0.00188935617916286
Validation loss = 0.0022111935541033745
Validation loss = 0.0020820777863264084
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.009974798187613487
Validation loss = 0.001902772462926805
Validation loss = 0.0014092389028519392
Validation loss = 0.0059133367612957954
Validation loss = 0.002019118517637253
Validation loss = 0.0021819609683007
Validation loss = 0.002355809323489666
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.007133531384170055
Validation loss = 0.0028751385398209095
Validation loss = 0.0027651465497910976
Validation loss = 0.0016555783804506063
Validation loss = 0.004924145061522722
Validation loss = 0.002309495583176613
Validation loss = 0.0016283675795421004
Validation loss = 0.002673405222594738
Validation loss = 0.0023163314908742905
Validation loss = 0.0017320786137133837
Validation loss = 0.003915177658200264
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.007486340589821339
Validation loss = 0.0020403405651450157
Validation loss = 0.0012469401117414236
Validation loss = 0.0022281426936388016
Validation loss = 0.0014147456968203187
Validation loss = 0.003499915823340416
Validation loss = 0.0019372152164578438
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 2        |
| MaximumReturn | 199      |
| MinimumReturn | 143      |
| TotalSamples  | 13332    |
----------------------------
itr #3 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.007099780719727278
Validation loss = 0.002840150147676468
Validation loss = 0.0017012053867802024
Validation loss = 0.002504264237359166
Validation loss = 0.0013099327916279435
Validation loss = 0.0023101435508579016
Validation loss = 0.004255239386111498
Validation loss = 0.0016064037336036563
Validation loss = 0.0020874624606221914
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.020461196079850197
Validation loss = 0.0018128418596461415
Validation loss = 0.0020274589769542217
Validation loss = 0.0022380505688488483
Validation loss = 0.0014735896838828921
Validation loss = 0.00314411916770041
Validation loss = 0.0018319665687158704
Validation loss = 0.0020436663180589676
Validation loss = 0.003445796901360154
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.008347845636308193
Validation loss = 0.0026032363530248404
Validation loss = 0.0029770273249596357
Validation loss = 0.0019371678354218602
Validation loss = 0.0018566733924672008
Validation loss = 0.0023947334848344326
Validation loss = 0.0020059808157384396
Validation loss = 0.00344193447381258
Validation loss = 0.0018728052964434028
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.004532352089881897
Validation loss = 0.0011994054075330496
Validation loss = 0.0016774850664660335
Validation loss = 0.0025705171283334494
Validation loss = 0.0036647843662649393
Validation loss = 0.003926489036530256
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.008188323117792606
Validation loss = 0.001600246294401586
Validation loss = 0.0031749149784445763
Validation loss = 0.006581141147762537
Validation loss = 0.004243120085448027
Validation loss = 0.004653134848922491
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 3        |
| MaximumReturn | 199      |
| MinimumReturn | 128      |
| TotalSamples  | 16665    |
----------------------------
itr #4 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0029541910625994205
Validation loss = 0.0021824210416525602
Validation loss = 0.0017265367787331343
Validation loss = 0.0015691642183810472
Validation loss = 0.002887423848733306
Validation loss = 0.0013889812398701906
Validation loss = 0.0012037199921905994
Validation loss = 0.002031728159636259
Validation loss = 0.0010273007210344076
Validation loss = 0.0021792305633425713
Validation loss = 0.0016478485194966197
Validation loss = 0.0016912998398765922
Validation loss = 0.0013240479165688157
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0022580502554774284
Validation loss = 0.0021159856114536524
Validation loss = 0.0011367934057489038
Validation loss = 0.003057875670492649
Validation loss = 0.0010218624956905842
Validation loss = 0.0019306326285004616
Validation loss = 0.0016391072422266006
Validation loss = 0.0027353500481694937
Validation loss = 0.001032927306368947
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00249159662052989
Validation loss = 0.0015375773655250669
Validation loss = 0.007631243672221899
Validation loss = 0.0013278687838464975
Validation loss = 0.0013084097299724817
Validation loss = 0.001125234761275351
Validation loss = 0.0015456121182069182
Validation loss = 0.002721251454204321
Validation loss = 0.005010552238672972
Validation loss = 0.0016954285092651844
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.002633135300129652
Validation loss = 0.001243645092472434
Validation loss = 0.006991088856011629
Validation loss = 0.0017257981235161424
Validation loss = 0.0012715309858322144
Validation loss = 0.00201586470939219
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.001996300183236599
Validation loss = 0.0013527258997783065
Validation loss = 0.002179121132940054
Validation loss = 0.0016766402404755354
Validation loss = 0.0029168392065912485
Validation loss = 0.0016932289581745863
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 177      |
| Iteration     | 4        |
| MaximumReturn | 199      |
| MinimumReturn | 134      |
| TotalSamples  | 19998    |
----------------------------
itr #5 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0076770419254899025
Validation loss = 0.0009394189110025764
Validation loss = 0.0016454111319035292
Validation loss = 0.00170272181276232
Validation loss = 0.001021996489726007
Validation loss = 0.006203801371157169
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.001368614030070603
Validation loss = 0.0009483558242209256
Validation loss = 0.003921166993677616
Validation loss = 0.0008701143087819219
Validation loss = 0.0013849372044205666
Validation loss = 0.0021023869048804045
Validation loss = 0.004515937529504299
Validation loss = 0.0009094161214306951
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.004228942096233368
Validation loss = 0.004446596838533878
Validation loss = 0.0009895433904603124
Validation loss = 0.0018948044162243605
Validation loss = 0.0017643207684159279
Validation loss = 0.0016560330986976624
Validation loss = 0.003185110632330179
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0018057789420709014
Validation loss = 0.0011122763389721513
Validation loss = 0.002183877397328615
Validation loss = 0.0020069328602403402
Validation loss = 0.0017602710286155343
Validation loss = 0.0012367794988676906
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0014563084114342928
Validation loss = 0.001884279539808631
Validation loss = 0.0015716325724497437
Validation loss = 0.0012676186161115766
Validation loss = 0.001549253356643021
Validation loss = 0.0011788900010287762
Validation loss = 0.0030505259055644274
Validation loss = 0.0021936027333140373
Validation loss = 0.0015553842531517148
Validation loss = 0.001757942489348352
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 5        |
| MaximumReturn | 199      |
| MinimumReturn | 126      |
| TotalSamples  | 23331    |
----------------------------
itr #6 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0026689739897847176
Validation loss = 0.0008691286202520132
Validation loss = 0.0010882028145715594
Validation loss = 0.0034667409490793943
Validation loss = 0.0008447708678431809
Validation loss = 0.0020089636091142893
Validation loss = 0.0023702760227024555
Validation loss = 0.0016538016498088837
Validation loss = 0.0012738668592646718
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.001429754076525569
Validation loss = 0.0013559471117332578
Validation loss = 0.0009026897605508566
Validation loss = 0.001074344851076603
Validation loss = 0.0012974552810192108
Validation loss = 0.0009828355396166444
Validation loss = 0.0014395200414583087
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.002835378982126713
Validation loss = 0.001535560586489737
Validation loss = 0.001950045581907034
Validation loss = 0.0015768891898915172
Validation loss = 0.0016223075799643993
Validation loss = 0.0015394763322547078
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.002072626492008567
Validation loss = 0.0013517976040020585
Validation loss = 0.0037289613392204046
Validation loss = 0.0010381381725892425
Validation loss = 0.0013936319155618548
Validation loss = 0.0011467066360637546
Validation loss = 0.0012451045913621783
Validation loss = 0.0017323321662843227
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.002416022354736924
Validation loss = 0.0018823974533006549
Validation loss = 0.0010961671359837055
Validation loss = 0.0033670365810394287
Validation loss = 0.00098953687120229
Validation loss = 0.0014769895933568478
Validation loss = 0.0009837206453084946
Validation loss = 0.0030608875676989555
Validation loss = 0.008347623981535435
Validation loss = 0.0012688052374869585
Validation loss = 0.0008979706326499581
Validation loss = 0.0009771576151251793
Validation loss = 0.0013324236497282982
Validation loss = 0.0008987486362457275
Validation loss = 0.0011359242489561439
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 6        |
| MaximumReturn | 199      |
| MinimumReturn | 131      |
| TotalSamples  | 26664    |
----------------------------
itr #7 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0016533801099285483
Validation loss = 0.0014439485967159271
Validation loss = 0.0011705511715263128
Validation loss = 0.0010326370829716325
Validation loss = 0.001769463298842311
Validation loss = 0.0009835832752287388
Validation loss = 0.0010860022157430649
Validation loss = 0.0014127598842605948
Validation loss = 0.0009397231042385101
Validation loss = 0.0012809738982468843
Validation loss = 0.0015790502075105906
Validation loss = 0.0010121953673660755
Validation loss = 0.0008808370330370963
Validation loss = 0.0009789828909561038
Validation loss = 0.0013295605313032866
Validation loss = 0.0018745546694844961
Validation loss = 0.0010702498257160187
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.002242055255919695
Validation loss = 0.0008296021842397749
Validation loss = 0.0012058226857334375
Validation loss = 0.0014042967231944203
Validation loss = 0.0010062565561383963
Validation loss = 0.0008904329733923078
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0019201977411285043
Validation loss = 0.001240597222931683
Validation loss = 0.0018173919524997473
Validation loss = 0.001895169261842966
Validation loss = 0.0009033433743752539
Validation loss = 0.0012977712322026491
Validation loss = 0.0017916603246703744
Validation loss = 0.0017039428930729628
Validation loss = 0.0010031018173322082
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.003537057898938656
Validation loss = 0.0018742616521194577
Validation loss = 0.0009816783713176847
Validation loss = 0.0009373441571369767
Validation loss = 0.0010775093687698245
Validation loss = 0.0011275715660303831
Validation loss = 0.0008666242356412113
Validation loss = 0.001780583173967898
Validation loss = 0.0012569826794788241
Validation loss = 0.0011337677715346217
Validation loss = 0.0013567532878369093
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00445884745568037
Validation loss = 0.0013184744166210294
Validation loss = 0.0016592814354225993
Validation loss = 0.001330587430857122
Validation loss = 0.0020271388348191977
Validation loss = 0.0015005114255473018
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 7        |
| MaximumReturn | 199      |
| MinimumReturn | 120      |
| TotalSamples  | 29997    |
----------------------------
itr #8 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0014468362787738442
Validation loss = 0.000874412537086755
Validation loss = 0.0009871687507256866
Validation loss = 0.0008071170304901898
Validation loss = 0.0011357298353686929
Validation loss = 0.0027393377386033535
Validation loss = 0.001297817099839449
Validation loss = 0.0007986363489180803
Validation loss = 0.0009962570620700717
Validation loss = 0.0008740710909478366
Validation loss = 0.000838659587316215
Validation loss = 0.001229874906130135
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0033127260394394398
Validation loss = 0.0009852497605606914
Validation loss = 0.0009731203899718821
Validation loss = 0.001246443367563188
Validation loss = 0.0011763457441702485
Validation loss = 0.0008424642728641629
Validation loss = 0.0009037702111527324
Validation loss = 0.0016315088141709566
Validation loss = 0.0008845728007145226
Validation loss = 0.00168950657825917
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0014981989515945315
Validation loss = 0.0012860584538429976
Validation loss = 0.0012377726379781961
Validation loss = 0.0009698208305053413
Validation loss = 0.0011427383869886398
Validation loss = 0.003072145162150264
Validation loss = 0.001708221505396068
Validation loss = 0.0009672064334154129
Validation loss = 0.0014792446745559573
Validation loss = 0.0012610995909199119
Validation loss = 0.0010578799992799759
Validation loss = 0.0008328507537953556
Validation loss = 0.001867347047664225
Validation loss = 0.0011094965739175677
Validation loss = 0.001395536120980978
Validation loss = 0.0015780910616740584
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0019264639122411609
Validation loss = 0.0011675205314531922
Validation loss = 0.0010589209850877523
Validation loss = 0.0010116563644260168
Validation loss = 0.00327362772077322
Validation loss = 0.0007742931484244764
Validation loss = 0.000674218637868762
Validation loss = 0.0011846197303384542
Validation loss = 0.0019475892186164856
Validation loss = 0.002635139273479581
Validation loss = 0.0006895405822433531
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0011935506481677294
Validation loss = 0.0011078498791903257
Validation loss = 0.0013577383942902088
Validation loss = 0.000758813344873488
Validation loss = 0.0007152390317060053
Validation loss = 0.0009575355215929449
Validation loss = 0.0011023763800039887
Validation loss = 0.0019058686448261142
Validation loss = 0.0008132437360472977
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 8        |
| MaximumReturn | 199      |
| MinimumReturn | 98.7     |
| TotalSamples  | 33330    |
----------------------------
itr #9 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0011915296781808138
Validation loss = 0.0008550396887585521
Validation loss = 0.0012413315707817674
Validation loss = 0.0008057100931182504
Validation loss = 0.0012219889322295785
Validation loss = 0.0009155754232779145
Validation loss = 0.0009927637875080109
Validation loss = 0.0007107083802111447
Validation loss = 0.0010476178722456098
Validation loss = 0.0007814017590135336
Validation loss = 0.0008442446123808622
Validation loss = 0.0012312975013628602
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.001992541365325451
Validation loss = 0.0009002708829939365
Validation loss = 0.0006967898225411773
Validation loss = 0.0019187361467629671
Validation loss = 0.0007285986794158816
Validation loss = 0.0011274879798293114
Validation loss = 0.0007130485610105097
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0013518515042960644
Validation loss = 0.0008182718884199858
Validation loss = 0.0007998330402188003
Validation loss = 0.0014891214668750763
Validation loss = 0.0010709482012316585
Validation loss = 0.0014765146188437939
Validation loss = 0.0013565495610237122
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007641835836693645
Validation loss = 0.0007461795466952026
Validation loss = 0.0015091634122654796
Validation loss = 0.0010783224133774638
Validation loss = 0.0008154304814524949
Validation loss = 0.0012181679485365748
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.000841573579236865
Validation loss = 0.0009007356129586697
Validation loss = 0.0012826903257519007
Validation loss = 0.0013695508241653442
Validation loss = 0.0013140623923391104
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 9        |
| MaximumReturn | 199      |
| MinimumReturn | 126      |
| TotalSamples  | 36663    |
----------------------------
itr #10 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006551109254360199
Validation loss = 0.0009147975943051279
Validation loss = 0.0006553987041115761
Validation loss = 0.0009494182886555791
Validation loss = 0.0014054561033844948
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0007631742628291249
Validation loss = 0.0011651203967630863
Validation loss = 0.0008333366713486612
Validation loss = 0.0013891918351873755
Validation loss = 0.0009790761396288872
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.000990130822174251
Validation loss = 0.0010127270361408591
Validation loss = 0.0007586042629554868
Validation loss = 0.0010088668204843998
Validation loss = 0.0008515401277691126
Validation loss = 0.0007501192158088088
Validation loss = 0.0009355820366181433
Validation loss = 0.0012874605599790812
Validation loss = 0.0007435887819156051
Validation loss = 0.000832799298223108
Validation loss = 0.0007377307047136128
Validation loss = 0.0009572861017659307
Validation loss = 0.0008107249741442502
Validation loss = 0.0006981704500503838
Validation loss = 0.0022489160764962435
Validation loss = 0.0005901961703784764
Validation loss = 0.0008216279675252736
Validation loss = 0.0007228704635053873
Validation loss = 0.0010951515287160873
Validation loss = 0.0007515953620895743
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.000989782391116023
Validation loss = 0.0006504431366920471
Validation loss = 0.001056013279594481
Validation loss = 0.0010801439639180899
Validation loss = 0.0011187075870111585
Validation loss = 0.0008123192237690091
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0016797432908788323
Validation loss = 0.0010517012560740113
Validation loss = 0.0010207733139395714
Validation loss = 0.0012503539910539985
Validation loss = 0.0008982683066278696
Validation loss = 0.0011615551775321364
Validation loss = 0.0010023905197158456
Validation loss = 0.0007925301906652749
Validation loss = 0.0011069371830672026
Validation loss = 0.0008869252633303404
Validation loss = 0.0009447538177482784
Validation loss = 0.0012224656529724598
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 10       |
| MaximumReturn | 199      |
| MinimumReturn | 129      |
| TotalSamples  | 39996    |
----------------------------
itr #11 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0008486420847475529
Validation loss = 0.0009162299102172256
Validation loss = 0.0007452065474353731
Validation loss = 0.0010582904797047377
Validation loss = 0.000662280130200088
Validation loss = 0.0007492454024031758
Validation loss = 0.0016736503457650542
Validation loss = 0.0007717779953964055
Validation loss = 0.0007213315111584961
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0007960842922329903
Validation loss = 0.001904807984828949
Validation loss = 0.0007579674129374325
Validation loss = 0.0020228519570082426
Validation loss = 0.0009037365089170635
Validation loss = 0.0008227277430705726
Validation loss = 0.0009217417682521045
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0007543818210251629
Validation loss = 0.0008377686026506126
Validation loss = 0.0010995895136147738
Validation loss = 0.0019375417614355683
Validation loss = 0.0008508536848239601
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00105841807089746
Validation loss = 0.0008706935914233327
Validation loss = 0.0014175638789311051
Validation loss = 0.0008256459841504693
Validation loss = 0.0006452067755162716
Validation loss = 0.0009544159402139485
Validation loss = 0.0010646095033735037
Validation loss = 0.0007867892272770405
Validation loss = 0.001012657885439694
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0008743487996980548
Validation loss = 0.001254706527106464
Validation loss = 0.0007029048283584416
Validation loss = 0.0009585165535099804
Validation loss = 0.0008159769931808114
Validation loss = 0.0006880193832330406
Validation loss = 0.0009411025675944984
Validation loss = 0.0009136893786489964
Validation loss = 0.0007526917615905404
Validation loss = 0.0010078279301524162
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 11       |
| MaximumReturn | 198      |
| MinimumReturn | 128      |
| TotalSamples  | 43329    |
----------------------------
itr #12 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00087001733481884
Validation loss = 0.0009568071691319346
Validation loss = 0.0007986418204382062
Validation loss = 0.0007174581405706704
Validation loss = 0.0012489225482568145
Validation loss = 0.0007486687391065061
Validation loss = 0.0009075791458599269
Validation loss = 0.0006583420909009874
Validation loss = 0.0016090992139652371
Validation loss = 0.001574923750013113
Validation loss = 0.000978107564151287
Validation loss = 0.0006283331895247102
Validation loss = 0.0008740636985749006
Validation loss = 0.0009849593043327332
Validation loss = 0.0007477016188204288
Validation loss = 0.0009096996509470046
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0008608883945271373
Validation loss = 0.0011843988904729486
Validation loss = 0.0009157296735793352
Validation loss = 0.0009328101295977831
Validation loss = 0.0007307468913495541
Validation loss = 0.0007879483164288104
Validation loss = 0.0014835646143183112
Validation loss = 0.000788323872257024
Validation loss = 0.0008012845064513385
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0010464716469869018
Validation loss = 0.0007269091438502073
Validation loss = 0.0007343319593928754
Validation loss = 0.0008803551900200546
Validation loss = 0.0008958327234722674
Validation loss = 0.0007321132579818368
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0016211515758186579
Validation loss = 0.0007539098151028156
Validation loss = 0.0008792630396783352
Validation loss = 0.0006936764693818986
Validation loss = 0.0008423661929555237
Validation loss = 0.001354175852611661
Validation loss = 0.0006985837244428694
Validation loss = 0.0012669445713981986
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0008975528180599213
Validation loss = 0.0009247088455595076
Validation loss = 0.001868712017312646
Validation loss = 0.0007511393050663173
Validation loss = 0.0007201916305348277
Validation loss = 0.0006824321462772787
Validation loss = 0.001402009860612452
Validation loss = 0.001102265901863575
Validation loss = 0.0007846176158636808
Validation loss = 0.0007192870252765715
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 156      |
| Iteration     | 12       |
| MaximumReturn | 199      |
| MinimumReturn | 112      |
| TotalSamples  | 46662    |
----------------------------
itr #13 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0009624945814721286
Validation loss = 0.0011218441650271416
Validation loss = 0.0007567451102659106
Validation loss = 0.000818709668237716
Validation loss = 0.0007547891582362354
Validation loss = 0.0008155152318067849
Validation loss = 0.0011654396075755358
Validation loss = 0.0008117146207951009
Validation loss = 0.0010933217126876116
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0010981351369991899
Validation loss = 0.0007789049413986504
Validation loss = 0.0010971560841426253
Validation loss = 0.0009477140847593546
Validation loss = 0.0007178289233706892
Validation loss = 0.0009076073183678091
Validation loss = 0.0008258436573669314
Validation loss = 0.0012863905867561698
Validation loss = 0.0009010550566017628
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0008023140835575759
Validation loss = 0.0008892996702343225
Validation loss = 0.0007333312532864511
Validation loss = 0.0009754258207976818
Validation loss = 0.0007685437449254096
Validation loss = 0.000732152839191258
Validation loss = 0.0008022803231142461
Validation loss = 0.0010695679811760783
Validation loss = 0.0008024172275327146
Validation loss = 0.0010726876789703965
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0016334867104887962
Validation loss = 0.0010210243053734303
Validation loss = 0.0010045039234682918
Validation loss = 0.0008716376614756882
Validation loss = 0.000886419031303376
Validation loss = 0.0009678356582298875
Validation loss = 0.0007346319616772234
Validation loss = 0.0012413947843015194
Validation loss = 0.0012641689972952008
Validation loss = 0.0007407072698697448
Validation loss = 0.0008871924364939332
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0008601008448749781
Validation loss = 0.001083742710761726
Validation loss = 0.0007557635544799268
Validation loss = 0.0008544743177480996
Validation loss = 0.000915257609449327
Validation loss = 0.0008112622308544815
Validation loss = 0.0010779689764603972
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 13       |
| MaximumReturn | 199      |
| MinimumReturn | 118      |
| TotalSamples  | 49995    |
----------------------------
itr #14 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0009760431130416691
Validation loss = 0.0009031175868585706
Validation loss = 0.0007797300931997597
Validation loss = 0.0009397921967320144
Validation loss = 0.0008237450383603573
Validation loss = 0.0007674801163375378
Validation loss = 0.0011972029460594058
Validation loss = 0.0009117624722421169
Validation loss = 0.0008271845290437341
Validation loss = 0.0009693686733953655
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0008004658157005906
Validation loss = 0.001383898314088583
Validation loss = 0.000896720215678215
Validation loss = 0.0008208518265746534
Validation loss = 0.00130451750010252
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.001017443835735321
Validation loss = 0.0007710576173849404
Validation loss = 0.0007463900255970657
Validation loss = 0.001633070525713265
Validation loss = 0.001044858479872346
Validation loss = 0.0008543265867047012
Validation loss = 0.001264719758182764
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007922780350781977
Validation loss = 0.0010284790769219398
Validation loss = 0.000802773458417505
Validation loss = 0.0008678685408085585
Validation loss = 0.0012473645620048046
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0008928699535317719
Validation loss = 0.0008038763771764934
Validation loss = 0.0007881178171373904
Validation loss = 0.0007557361386716366
Validation loss = 0.0008716679876670241
Validation loss = 0.0009034033864736557
Validation loss = 0.0008266913937404752
Validation loss = 0.0010663735447451472
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 14       |
| MaximumReturn | 199      |
| MinimumReturn | 114      |
| TotalSamples  | 53328    |
----------------------------
itr #15 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0010249667102470994
Validation loss = 0.0009124729549512267
Validation loss = 0.0008704580832272768
Validation loss = 0.0011515425285324454
Validation loss = 0.0009865988977253437
Validation loss = 0.0010232796194031835
Validation loss = 0.0011184642789885402
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0021912851370871067
Validation loss = 0.0009127380908466876
Validation loss = 0.0011237103026360273
Validation loss = 0.0011068192543461919
Validation loss = 0.0012328161392360926
Validation loss = 0.0009745632414706051
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0008960810955613852
Validation loss = 0.0009075292618945241
Validation loss = 0.0012233078014105558
Validation loss = 0.0007936941110529006
Validation loss = 0.0017404998652637005
Validation loss = 0.0008832097519189119
Validation loss = 0.0008931964985094965
Validation loss = 0.000964082486461848
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0009718947694636881
Validation loss = 0.0008577480912208557
Validation loss = 0.0010047864634543657
Validation loss = 0.0008603054448030889
Validation loss = 0.0015029312344267964
Validation loss = 0.0010262003634124994
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.001374822692014277
Validation loss = 0.0008839961956255138
Validation loss = 0.0009605854284018278
Validation loss = 0.000958129414357245
Validation loss = 0.0013801766326650977
Validation loss = 0.0009169281693175435
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 15       |
| MaximumReturn | 199      |
| MinimumReturn | 124      |
| TotalSamples  | 56661    |
----------------------------
itr #16 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0010055445600301027
Validation loss = 0.0008475330541841686
Validation loss = 0.0008896670187823474
Validation loss = 0.0010175812058150768
Validation loss = 0.0007622229750268161
Validation loss = 0.0009222197695635259
Validation loss = 0.0007700510905124247
Validation loss = 0.0008340132189914584
Validation loss = 0.0009052454261109233
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0008122111321426928
Validation loss = 0.0008752366411499679
Validation loss = 0.000925286381971091
Validation loss = 0.0008284015930257738
Validation loss = 0.0009765188442543149
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.000828070449642837
Validation loss = 0.0010600265813991427
Validation loss = 0.0007968201534822583
Validation loss = 0.0009740345412865281
Validation loss = 0.0010635457001626492
Validation loss = 0.0011027294676750898
Validation loss = 0.0008609825745224953
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00107744918204844
Validation loss = 0.0010120951337739825
Validation loss = 0.0011314668226987123
Validation loss = 0.0013346308842301369
Validation loss = 0.0009442712180316448
Validation loss = 0.0008840633672662079
Validation loss = 0.0013532449956983328
Validation loss = 0.0015276417834684253
Validation loss = 0.0009079230367206037
Validation loss = 0.0008697950397618115
Validation loss = 0.0009674026514403522
Validation loss = 0.0008338048937730491
Validation loss = 0.0013688448816537857
Validation loss = 0.0008390048751607537
Validation loss = 0.0010845664655789733
Validation loss = 0.0009568911627866328
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0008474363130517304
Validation loss = 0.0010936384787783027
Validation loss = 0.0008922989363782108
Validation loss = 0.0010085934773087502
Validation loss = 0.0016401706961914897
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 16       |
| MaximumReturn | 199      |
| MinimumReturn | 123      |
| TotalSamples  | 59994    |
----------------------------
itr #17 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0009609831613488495
Validation loss = 0.0010095835896208882
Validation loss = 0.0011097000679001212
Validation loss = 0.0011196223786100745
Validation loss = 0.0009151722770184278
Validation loss = 0.001191904884763062
Validation loss = 0.000982698635198176
Validation loss = 0.001000086311250925
Validation loss = 0.001096558291465044
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.001066661672666669
Validation loss = 0.0014494695933535695
Validation loss = 0.0011167924385517836
Validation loss = 0.000986890634521842
Validation loss = 0.0014293004060164094
Validation loss = 0.0010488847037777305
Validation loss = 0.0010126273846253753
Validation loss = 0.0010570037411525846
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.001136207371018827
Validation loss = 0.0009993045823648572
Validation loss = 0.00093744887271896
Validation loss = 0.0010020050685852766
Validation loss = 0.0009056107373908162
Validation loss = 0.0010410068789497018
Validation loss = 0.0012634646845981479
Validation loss = 0.0009578433237038553
Validation loss = 0.000917675206437707
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0009445399045944214
Validation loss = 0.0013796028215438128
Validation loss = 0.0009353283094242215
Validation loss = 0.001202313811518252
Validation loss = 0.0014106380986049771
Validation loss = 0.0010016087908297777
Validation loss = 0.0015113259432837367
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00113668420817703
Validation loss = 0.0009970349492505193
Validation loss = 0.0011113988002762198
Validation loss = 0.0008880961686372757
Validation loss = 0.001115358085371554
Validation loss = 0.0011403094977140427
Validation loss = 0.0009979342576116323
Validation loss = 0.001000529620796442
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 17       |
| MaximumReturn | 199      |
| MinimumReturn | 132      |
| TotalSamples  | 63327    |
----------------------------
itr #18 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0012836162932217121
Validation loss = 0.001369180390611291
Validation loss = 0.0010333271929994226
Validation loss = 0.0011433500330895185
Validation loss = 0.0010752014350146055
Validation loss = 0.0011473089689388871
Validation loss = 0.0010588353034108877
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.001854244968853891
Validation loss = 0.0010745963081717491
Validation loss = 0.0020436132326722145
Validation loss = 0.0010367028880864382
Validation loss = 0.0011553087970241904
Validation loss = 0.0012256710324436426
Validation loss = 0.0012530784588307142
Validation loss = 0.0013471005950123072
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0015839478000998497
Validation loss = 0.0010521671501919627
Validation loss = 0.0011819193605333567
Validation loss = 0.001301386160776019
Validation loss = 0.0010655246442183852
Validation loss = 0.0011807417031377554
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.001325934543274343
Validation loss = 0.001098524546250701
Validation loss = 0.0011186553165316582
Validation loss = 0.0011047673178836703
Validation loss = 0.001127952360548079
Validation loss = 0.0010399490129202604
Validation loss = 0.0010239180410280824
Validation loss = 0.0010637317318469286
Validation loss = 0.0011172242229804397
Validation loss = 0.0013681771233677864
Validation loss = 0.0010627580340951681
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.001131874043494463
Validation loss = 0.0011117896065115929
Validation loss = 0.0012521565658971667
Validation loss = 0.001366304000839591
Validation loss = 0.0012682345695793629
Validation loss = 0.0011319167679175735
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 163      |
| Iteration     | 18       |
| MaximumReturn | 198      |
| MinimumReturn | 112      |
| TotalSamples  | 66660    |
----------------------------
itr #19 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.001455456716939807
Validation loss = 0.0012109031667932868
Validation loss = 0.0012925248593091965
Validation loss = 0.0012422931613400578
Validation loss = 0.001511100446805358
Validation loss = 0.0015819895779713988
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0012845282908529043
Validation loss = 0.0015410168562084436
Validation loss = 0.0017053146148100495
Validation loss = 0.0012912476668134332
Validation loss = 0.0013418005546554923
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0018264232203364372
Validation loss = 0.0013089944841340184
Validation loss = 0.0011920497054234147
Validation loss = 0.0012324202107265592
Validation loss = 0.001413974561728537
Validation loss = 0.0013085172977298498
Validation loss = 0.0013501029461622238
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0013539820211008191
Validation loss = 0.0014131700154393911
Validation loss = 0.001367565942928195
Validation loss = 0.0012991991825401783
Validation loss = 0.001232363865710795
Validation loss = 0.0013462601928040385
Validation loss = 0.001390388235449791
Validation loss = 0.001297400682233274
Validation loss = 0.0013511907309293747
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0015755761414766312
Validation loss = 0.0014423421816900373
Validation loss = 0.001216102042235434
Validation loss = 0.0013973446330055594
Validation loss = 0.001335656619630754
Validation loss = 0.001519949990324676
Validation loss = 0.0012338523520156741
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 19       |
| MaximumReturn | 199      |
| MinimumReturn | 139      |
| TotalSamples  | 69993    |
----------------------------
itr #20 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0013700566487386823
Validation loss = 0.0014647501520812511
Validation loss = 0.0013498448533937335
Validation loss = 0.001371350372210145
Validation loss = 0.0013627068838104606
Validation loss = 0.0016918908804655075
Validation loss = 0.00136211805511266
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0016522262012585998
Validation loss = 0.0014018673682585359
Validation loss = 0.0014661517925560474
Validation loss = 0.0019078301265835762
Validation loss = 0.0013469153782352805
Validation loss = 0.00145806185901165
Validation loss = 0.001449468545615673
Validation loss = 0.001382565125823021
Validation loss = 0.0013184811687096953
Validation loss = 0.0013800588203594089
Validation loss = 0.0014147015754133463
Validation loss = 0.0014711599797010422
Validation loss = 0.0014113154029473662
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.001278725336305797
Validation loss = 0.001301460200920701
Validation loss = 0.0013505766401067376
Validation loss = 0.001284136320464313
Validation loss = 0.0013064794475212693
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0013394278939813375
Validation loss = 0.0013705035671591759
Validation loss = 0.0013805704656988382
Validation loss = 0.0012735407799482346
Validation loss = 0.0013705329038202763
Validation loss = 0.0013489563716575503
Validation loss = 0.00131699547637254
Validation loss = 0.0013010347029194236
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0014462266117334366
Validation loss = 0.0018602863419800997
Validation loss = 0.0013294691452756524
Validation loss = 0.0014154983218759298
Validation loss = 0.00171824696008116
Validation loss = 0.00287464982829988
Validation loss = 0.0013309299247339368
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 182      |
| Iteration     | 20       |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 73326    |
----------------------------
itr #21 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0013556329067796469
Validation loss = 0.0012741991085931659
Validation loss = 0.0013095104368403554
Validation loss = 0.00133144436404109
Validation loss = 0.0013360289158299565
Validation loss = 0.0014059252571314573
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0013434737920761108
Validation loss = 0.0013407678343355656
Validation loss = 0.0014469606103375554
Validation loss = 0.0012930118246003985
Validation loss = 0.0013393250992521644
Validation loss = 0.0014748143730685115
Validation loss = 0.0014000347582623363
Validation loss = 0.0016981296939775348
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0018469028873369098
Validation loss = 0.001446263981051743
Validation loss = 0.0013538767816498876
Validation loss = 0.001371291116811335
Validation loss = 0.001335600158199668
Validation loss = 0.0013332292437553406
Validation loss = 0.0015699879731982946
Validation loss = 0.0018811362097039819
Validation loss = 0.0012893262319266796
Validation loss = 0.0013335931580513716
Validation loss = 0.0012439469574019313
Validation loss = 0.0014677096623927355
Validation loss = 0.0013005002401769161
Validation loss = 0.001268042717128992
Validation loss = 0.0012625509407371283
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0012702483218163252
Validation loss = 0.0015506008639931679
Validation loss = 0.0012670361902564764
Validation loss = 0.0014398631174117327
Validation loss = 0.0014487896114587784
Validation loss = 0.0013055694289505482
Validation loss = 0.001313346903771162
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0017392332665622234
Validation loss = 0.001322269905358553
Validation loss = 0.0012994020944461226
Validation loss = 0.00129836262203753
Validation loss = 0.0015019093407317996
Validation loss = 0.0015191363636404276
Validation loss = 0.0013862336054444313
Validation loss = 0.0014679557643830776
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 21       |
| MaximumReturn | 199      |
| MinimumReturn | 126      |
| TotalSamples  | 76659    |
----------------------------
itr #22 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.001532536232843995
Validation loss = 0.0014270287938416004
Validation loss = 0.0014059621607884765
Validation loss = 0.0015088560758158565
Validation loss = 0.0016578651266172528
Validation loss = 0.0014107031747698784
Validation loss = 0.001843995414674282
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.001481618033722043
Validation loss = 0.0014865584671497345
Validation loss = 0.001418750500306487
Validation loss = 0.0016402479959651828
Validation loss = 0.0015916133997961879
Validation loss = 0.0017438087379559875
Validation loss = 0.0014177042758092284
Validation loss = 0.0014049573801457882
Validation loss = 0.0015847220784053206
Validation loss = 0.0014982172288000584
Validation loss = 0.0014952883357182145
Validation loss = 0.0014711288968101144
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0015821614069864154
Validation loss = 0.001432226039469242
Validation loss = 0.0014585022581741214
Validation loss = 0.0014609917998313904
Validation loss = 0.0015714223263785243
Validation loss = 0.0014339699409902096
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0015273612225428224
Validation loss = 0.0014599566347897053
Validation loss = 0.0013569938018918037
Validation loss = 0.001419682172127068
Validation loss = 0.0017336615128442645
Validation loss = 0.0014367337571457028
Validation loss = 0.0014976141974329948
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.001472735428251326
Validation loss = 0.0014383038505911827
Validation loss = 0.0014701245818287134
Validation loss = 0.0015809092437848449
Validation loss = 0.001464426750317216
Validation loss = 0.0014619670109823346
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 22       |
| MaximumReturn | 199      |
| MinimumReturn | 129      |
| TotalSamples  | 79992    |
----------------------------
itr #23 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0017709105741232634
Validation loss = 0.0019049446564167738
Validation loss = 0.0018407481256872416
Validation loss = 0.0017654268303886056
Validation loss = 0.002020600950345397
Validation loss = 0.0018478467827662826
Validation loss = 0.001913547865115106
Validation loss = 0.0018232535803690553
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0020475368946790695
Validation loss = 0.002099151024594903
Validation loss = 0.0020862319506704807
Validation loss = 0.0019480397459119558
Validation loss = 0.0018345378339290619
Validation loss = 0.0017841097433120012
Validation loss = 0.0018524050246924162
Validation loss = 0.0018048131605610251
Validation loss = 0.001758865313604474
Validation loss = 0.001792900264263153
Validation loss = 0.0018397262319922447
Validation loss = 0.0018809750908985734
Validation loss = 0.0018448602641001344
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.001878900802694261
Validation loss = 0.0017482038820162416
Validation loss = 0.0019057076424360275
Validation loss = 0.0019233906641602516
Validation loss = 0.001915853819809854
Validation loss = 0.0018708959687501192
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0021936132106930017
Validation loss = 0.0019821147434413433
Validation loss = 0.002385628642514348
Validation loss = 0.0019389226799830794
Validation loss = 0.002015573438256979
Validation loss = 0.001983718015253544
Validation loss = 0.001792374299839139
Validation loss = 0.0018600430339574814
Validation loss = 0.0018362498376518488
Validation loss = 0.0017609447240829468
Validation loss = 0.0017341983038932085
Validation loss = 0.0018533136462792754
Validation loss = 0.0018992137629538774
Validation loss = 0.0020281097386032343
Validation loss = 0.0018528547370806336
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0018487293273210526
Validation loss = 0.0019122965168207884
Validation loss = 0.001954722683876753
Validation loss = 0.0021753492765128613
Validation loss = 0.0018569633830338717
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 180      |
| Iteration     | 23       |
| MaximumReturn | 199      |
| MinimumReturn | 134      |
| TotalSamples  | 83325    |
----------------------------
itr #24 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0019464577781036496
Validation loss = 0.0019647441804409027
Validation loss = 0.0020008583087474108
Validation loss = 0.002024802379310131
Validation loss = 0.001996588194742799
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0020195383112877607
Validation loss = 0.0019095102325081825
Validation loss = 0.002053445903584361
Validation loss = 0.002053639618679881
Validation loss = 0.0020270945969969034
Validation loss = 0.002294199774041772
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.001964772352948785
Validation loss = 0.002246010582894087
Validation loss = 0.0018961898749694228
Validation loss = 0.0019809226505458355
Validation loss = 0.001945564872585237
Validation loss = 0.0019322847947478294
Validation loss = 0.002139952965080738
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.002021468011662364
Validation loss = 0.0019264997681602836
Validation loss = 0.002104399027302861
Validation loss = 0.002041623927652836
Validation loss = 0.0019455455476418138
Validation loss = 0.00202003656886518
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0019528509583324194
Validation loss = 0.002053613308817148
Validation loss = 0.0020743883214890957
Validation loss = 0.0020431119482964277
Validation loss = 0.002005001762881875
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 164      |
| Iteration     | 24       |
| MaximumReturn | 199      |
| MinimumReturn | 122      |
| TotalSamples  | 86658    |
----------------------------
itr #25 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0026396645698696375
Validation loss = 0.002476123394444585
Validation loss = 0.0023295977152884007
Validation loss = 0.0023705908097326756
Validation loss = 0.0025736468378454447
Validation loss = 0.0024538151919841766
Validation loss = 0.0023969735484570265
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0023478162474930286
Validation loss = 0.0023849240969866514
Validation loss = 0.0024547968059778214
Validation loss = 0.0023864922113716602
Validation loss = 0.002389325061812997
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0023262116592377424
Validation loss = 0.002449972787871957
Validation loss = 0.0024111890234053135
Validation loss = 0.0027082443702965975
Validation loss = 0.00231492449529469
Validation loss = 0.002441452583298087
Validation loss = 0.0023069442249834538
Validation loss = 0.002381883328780532
Validation loss = 0.0025559309870004654
Validation loss = 0.0024479611311107874
Validation loss = 0.0024200170300900936
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.002313474426046014
Validation loss = 0.0024850410409271717
Validation loss = 0.0024167653173208237
Validation loss = 0.0023847806733101606
Validation loss = 0.002353729447349906
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.002411248628050089
Validation loss = 0.0023793154396116734
Validation loss = 0.0023527040611952543
Validation loss = 0.002314526354894042
Validation loss = 0.002322669606655836
Validation loss = 0.002336731879040599
Validation loss = 0.0024247905239462852
Validation loss = 0.002337854588404298
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 25       |
| MaximumReturn | 199      |
| MinimumReturn | 132      |
| TotalSamples  | 89991    |
----------------------------
itr #26 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.002530873054638505
Validation loss = 0.0025424514897167683
Validation loss = 0.0025728910695761442
Validation loss = 0.002541824011132121
Validation loss = 0.0024767830036580563
Validation loss = 0.002430441090837121
Validation loss = 0.0024627759121358395
Validation loss = 0.002538464032113552
Validation loss = 0.0024460586719214916
Validation loss = 0.002482392592355609
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.002451925305649638
Validation loss = 0.002439837669953704
Validation loss = 0.0024865029845386744
Validation loss = 0.002639140235260129
Validation loss = 0.0024847874883562326
Validation loss = 0.002556339604780078
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.002483137184754014
Validation loss = 0.0024312203750014305
Validation loss = 0.002688067266717553
Validation loss = 0.0027244528755545616
Validation loss = 0.002494371961802244
Validation loss = 0.0024234247393906116
Validation loss = 0.0024816018994897604
Validation loss = 0.002665766282007098
Validation loss = 0.002755401423200965
Validation loss = 0.002456763293594122
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00255121779628098
Validation loss = 0.002480756025761366
Validation loss = 0.002555043203756213
Validation loss = 0.002482837997376919
Validation loss = 0.002492076251655817
Validation loss = 0.002483608201146126
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.002535358304157853
Validation loss = 0.002493795473128557
Validation loss = 0.0025533679872751236
Validation loss = 0.0027764856349676847
Validation loss = 0.002665858715772629
Validation loss = 0.0024741780944168568
Validation loss = 0.0024859474506229162
Validation loss = 0.0024453941732645035
Validation loss = 0.002466089092195034
Validation loss = 0.002451227279379964
Validation loss = 0.002593374578282237
Validation loss = 0.0025656726211309433
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 26       |
| MaximumReturn | 199      |
| MinimumReturn | 118      |
| TotalSamples  | 93324    |
----------------------------
itr #27 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.002701021498069167
Validation loss = 0.002732897875830531
Validation loss = 0.0026719593442976475
Validation loss = 0.00273953378200531
Validation loss = 0.002658385317772627
Validation loss = 0.0026836623437702656
Validation loss = 0.0028644606936722994
Validation loss = 0.002856460865586996
Validation loss = 0.002722971374168992
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0026647632475942373
Validation loss = 0.0030730506405234337
Validation loss = 0.0028022644110023975
Validation loss = 0.0027644631918519735
Validation loss = 0.002753855660557747
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0026840430218726397
Validation loss = 0.0028121154755353928
Validation loss = 0.0026853010058403015
Validation loss = 0.00263177789747715
Validation loss = 0.0026952261105179787
Validation loss = 0.002733655506744981
Validation loss = 0.0027474653907120228
Validation loss = 0.002726908540353179
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.002622049069032073
Validation loss = 0.002660552505403757
Validation loss = 0.002702447585761547
Validation loss = 0.002736895577982068
Validation loss = 0.0026855485048145056
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.002782476367428899
Validation loss = 0.0026817608159035444
Validation loss = 0.0027682199142873287
Validation loss = 0.0027029395569115877
Validation loss = 0.0027952457312494516
Validation loss = 0.002637850120663643
Validation loss = 0.0027548593934625387
Validation loss = 0.002677131211385131
Validation loss = 0.002746554557234049
Validation loss = 0.002703926060348749
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 27       |
| MaximumReturn | 199      |
| MinimumReturn | 121      |
| TotalSamples  | 96657    |
----------------------------
itr #28 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.003201837884262204
Validation loss = 0.0032870883587747812
Validation loss = 0.0031844365876168013
Validation loss = 0.0031865809578448534
Validation loss = 0.0032729171216487885
Validation loss = 0.003502172650769353
Validation loss = 0.0033634956926107407
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0032605391461402178
Validation loss = 0.00348258507438004
Validation loss = 0.00328008527867496
Validation loss = 0.0033639369066804647
Validation loss = 0.003282599151134491
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0034855541307479143
Validation loss = 0.003187604248523712
Validation loss = 0.003257314907386899
Validation loss = 0.003308042883872986
Validation loss = 0.003484574845060706
Validation loss = 0.003283228725194931
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.003452032571658492
Validation loss = 0.003315357491374016
Validation loss = 0.0033605366479605436
Validation loss = 0.003943128045648336
Validation loss = 0.0032163590658456087
Validation loss = 0.0032168023753911257
Validation loss = 0.0033254201989620924
Validation loss = 0.00320424884557724
Validation loss = 0.003193578915670514
Validation loss = 0.0031504130456596613
Validation loss = 0.003284111386165023
Validation loss = 0.0032084647100418806
Validation loss = 0.0032721038442105055
Validation loss = 0.003192926524206996
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0033882856369018555
Validation loss = 0.003214549971744418
Validation loss = 0.003202628344297409
Validation loss = 0.0033636335283517838
Validation loss = 0.0031744821462780237
Validation loss = 0.00325917755253613
Validation loss = 0.0032281933818012476
Validation loss = 0.0034785002935677767
Validation loss = 0.003174652112647891
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 28       |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 99990    |
----------------------------
itr #29 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.004245577845722437
Validation loss = 0.00407000444829464
Validation loss = 0.003990728408098221
Validation loss = 0.004344602581113577
Validation loss = 0.004122483544051647
Validation loss = 0.0040341936983168125
Validation loss = 0.004192288964986801
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.004211949650198221
Validation loss = 0.004206799436360598
Validation loss = 0.004185978788882494
Validation loss = 0.004108909983187914
Validation loss = 0.004180430434644222
Validation loss = 0.004408583510667086
Validation loss = 0.004084596410393715
Validation loss = 0.004378034267574549
Validation loss = 0.004131348337978125
Validation loss = 0.004135584458708763
Validation loss = 0.004223617725074291
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.004125971347093582
Validation loss = 0.0042821913957595825
Validation loss = 0.004027215763926506
Validation loss = 0.004222347401082516
Validation loss = 0.0040331254713237286
Validation loss = 0.004099319223314524
Validation loss = 0.004044370260089636
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00454523554071784
Validation loss = 0.004250523634254932
Validation loss = 0.004358902107924223
Validation loss = 0.004058200865983963
Validation loss = 0.004088225774466991
Validation loss = 0.004259270150214434
Validation loss = 0.004085402004420757
Validation loss = 0.004123918246477842
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.004144662991166115
Validation loss = 0.004028339870274067
Validation loss = 0.00409331452101469
Validation loss = 0.004033856093883514
Validation loss = 0.0040782601572573185
Validation loss = 0.004159705247730017
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 29       |
| MaximumReturn | 199      |
| MinimumReturn | 130      |
| TotalSamples  | 103323   |
----------------------------
itr #30 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.004166299942880869
Validation loss = 0.004088015761226416
Validation loss = 0.004223862197250128
Validation loss = 0.0042898827232420444
Validation loss = 0.0041891708970069885
Validation loss = 0.004205614328384399
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.004115039482712746
Validation loss = 0.004144572652876377
Validation loss = 0.004193984903395176
Validation loss = 0.004299914464354515
Validation loss = 0.004090811591595411
Validation loss = 0.004064168781042099
Validation loss = 0.004568492062389851
Validation loss = 0.004199810791760683
Validation loss = 0.004138483200222254
Validation loss = 0.004259254317730665
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.004374334122985601
Validation loss = 0.0041569857858121395
Validation loss = 0.004167226608842611
Validation loss = 0.004153793212026358
Validation loss = 0.004077842924743891
Validation loss = 0.004445425700396299
Validation loss = 0.004068621434271336
Validation loss = 0.004381300415843725
Validation loss = 0.00413515092805028
Validation loss = 0.004012437537312508
Validation loss = 0.004229568876326084
Validation loss = 0.0041308351792395115
Validation loss = 0.004090053960680962
Validation loss = 0.00417141942307353
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.004161647986620665
Validation loss = 0.004186971578747034
Validation loss = 0.004143522586673498
Validation loss = 0.004090583883225918
Validation loss = 0.004059605300426483
Validation loss = 0.004143986385315657
Validation loss = 0.0040726731531322
Validation loss = 0.0040939562022686005
Validation loss = 0.0041256858967244625
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.004300277680158615
Validation loss = 0.00411214679479599
Validation loss = 0.004178227391093969
Validation loss = 0.004216284956783056
Validation loss = 0.00408658292144537
Validation loss = 0.004109763540327549
Validation loss = 0.004163992125540972
Validation loss = 0.004064198117703199
Validation loss = 0.004073259420692921
Validation loss = 0.004116673022508621
Validation loss = 0.004198320209980011
Validation loss = 0.004125922918319702
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 30       |
| MaximumReturn | 199      |
| MinimumReturn | 130      |
| TotalSamples  | 106656   |
----------------------------
itr #31 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.004594498313963413
Validation loss = 0.0045621651224792
Validation loss = 0.004706792067736387
Validation loss = 0.004547835793346167
Validation loss = 0.0048520504496991634
Validation loss = 0.004657447803765535
Validation loss = 0.004759271163493395
Validation loss = 0.00456756167113781
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.004546666517853737
Validation loss = 0.004799167159944773
Validation loss = 0.0050499276258051395
Validation loss = 0.004558757413178682
Validation loss = 0.00462739635258913
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.004658044315874577
Validation loss = 0.004707671236246824
Validation loss = 0.0045618112199008465
Validation loss = 0.004672304727137089
Validation loss = 0.004553605802357197
Validation loss = 0.004642324522137642
Validation loss = 0.004560838453471661
Validation loss = 0.004578399006277323
Validation loss = 0.004953439347445965
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.004655416589230299
Validation loss = 0.004657802637666464
Validation loss = 0.004709947854280472
Validation loss = 0.0046982597559690475
Validation loss = 0.004679552745074034
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0046315426006913185
Validation loss = 0.0046669552102684975
Validation loss = 0.004639653023332357
Validation loss = 0.004692132119089365
Validation loss = 0.004562338814139366
Validation loss = 0.0045313965529203415
Validation loss = 0.004884906578809023
Validation loss = 0.004660591948777437
Validation loss = 0.0046311430633068085
Validation loss = 0.004777065012603998
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 181      |
| Iteration     | 31       |
| MaximumReturn | 199      |
| MinimumReturn | 119      |
| TotalSamples  | 109989   |
----------------------------
itr #32 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.005552817136049271
Validation loss = 0.005374963860958815
Validation loss = 0.005169633775949478
Validation loss = 0.00530995661392808
Validation loss = 0.005236164666712284
Validation loss = 0.005203205160796642
Validation loss = 0.0051652006804943085
Validation loss = 0.005176882725208998
Validation loss = 0.005108325742185116
Validation loss = 0.005284964106976986
Validation loss = 0.005330618470907211
Validation loss = 0.005302843637764454
Validation loss = 0.005208540707826614
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.005187871400266886
Validation loss = 0.005227021872997284
Validation loss = 0.0051892525516450405
Validation loss = 0.00521719828248024
Validation loss = 0.005184573587030172
Validation loss = 0.005135579500347376
Validation loss = 0.005087462719529867
Validation loss = 0.005195814184844494
Validation loss = 0.005899839103221893
Validation loss = 0.005266985390335321
Validation loss = 0.005179245490580797
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0053206542506814
Validation loss = 0.005406584125012159
Validation loss = 0.005257015116512775
Validation loss = 0.005314521957188845
Validation loss = 0.005324359517544508
Validation loss = 0.005432124715298414
Validation loss = 0.005150672513991594
Validation loss = 0.0051517179235816
Validation loss = 0.0053232708014547825
Validation loss = 0.005380122922360897
Validation loss = 0.005185743328183889
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.005149536300450563
Validation loss = 0.005183268804103136
Validation loss = 0.005141783505678177
Validation loss = 0.005588318686932325
Validation loss = 0.005231300834566355
Validation loss = 0.005247374065220356
Validation loss = 0.005398966837674379
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.005359728820621967
Validation loss = 0.00515567185357213
Validation loss = 0.005095211323350668
Validation loss = 0.005192085634917021
Validation loss = 0.005280943121761084
Validation loss = 0.005205260589718819
Validation loss = 0.0053365337662398815
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 32       |
| MaximumReturn | 199      |
| MinimumReturn | 120      |
| TotalSamples  | 113322   |
----------------------------
itr #33 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.005620377603918314
Validation loss = 0.005387171637266874
Validation loss = 0.005371761042624712
Validation loss = 0.005460652988404036
Validation loss = 0.005483647342771292
Validation loss = 0.0054221386089921
Validation loss = 0.005434686783701181
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.005300728138536215
Validation loss = 0.005382416304200888
Validation loss = 0.005447495263069868
Validation loss = 0.005438904743641615
Validation loss = 0.005609327461570501
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.005481712985783815
Validation loss = 0.005498589016497135
Validation loss = 0.0054848454892635345
Validation loss = 0.005373225547373295
Validation loss = 0.005449042655527592
Validation loss = 0.005319531541317701
Validation loss = 0.005512283183634281
Validation loss = 0.005473468918353319
Validation loss = 0.00545101473107934
Validation loss = 0.005337890237569809
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0053741284646093845
Validation loss = 0.005493832286447287
Validation loss = 0.005383546929806471
Validation loss = 0.0053766039200127125
Validation loss = 0.005337671376764774
Validation loss = 0.00565314618870616
Validation loss = 0.0054282560013234615
Validation loss = 0.005346581339836121
Validation loss = 0.005472933407872915
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.005477503873407841
Validation loss = 0.005418818909674883
Validation loss = 0.005443396512418985
Validation loss = 0.005322404205799103
Validation loss = 0.0053880224004387856
Validation loss = 0.005400575697422028
Validation loss = 0.005385322030633688
Validation loss = 0.005455471575260162
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 178      |
| Iteration     | 33       |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 116655   |
----------------------------
itr #34 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.006133789196610451
Validation loss = 0.005964257288724184
Validation loss = 0.005916123744100332
Validation loss = 0.00588570348918438
Validation loss = 0.005935139488428831
Validation loss = 0.005884310230612755
Validation loss = 0.005855242256075144
Validation loss = 0.005945761222392321
Validation loss = 0.005871687084436417
Validation loss = 0.0059355986304581165
Validation loss = 0.005924370139837265
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.005910235457122326
Validation loss = 0.005870967637747526
Validation loss = 0.006000866647809744
Validation loss = 0.0058837472461164
Validation loss = 0.005806536413729191
Validation loss = 0.006064767949283123
Validation loss = 0.005892971996217966
Validation loss = 0.005904499441385269
Validation loss = 0.005839049816131592
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.006003427319228649
Validation loss = 0.005944441072642803
Validation loss = 0.00598455686122179
Validation loss = 0.0059574986808001995
Validation loss = 0.006075727753341198
Validation loss = 0.0058244322426617146
Validation loss = 0.005911418702453375
Validation loss = 0.0060021416284143925
Validation loss = 0.006042116787284613
Validation loss = 0.005809139460325241
Validation loss = 0.006093750242143869
Validation loss = 0.005990069359540939
Validation loss = 0.005947243422269821
Validation loss = 0.005791040137410164
Validation loss = 0.006064395420253277
Validation loss = 0.0057834479957818985
Validation loss = 0.00583482114598155
Validation loss = 0.005854764021933079
Validation loss = 0.005939318798482418
Validation loss = 0.006015121471136808
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.006043736822903156
Validation loss = 0.006120526697486639
Validation loss = 0.0058516343124210835
Validation loss = 0.005864572711288929
Validation loss = 0.005913061089813709
Validation loss = 0.006006903480738401
Validation loss = 0.005824354011565447
Validation loss = 0.005903847049921751
Validation loss = 0.0058679175563156605
Validation loss = 0.0058838059194386005
Validation loss = 0.005848826840519905
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.005968546029180288
Validation loss = 0.005991301499307156
Validation loss = 0.005898802541196346
Validation loss = 0.006541594862937927
Validation loss = 0.00581691088154912
Validation loss = 0.006014989223331213
Validation loss = 0.005936948582530022
Validation loss = 0.005939567927271128
Validation loss = 0.00603336188942194
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 34       |
| MaximumReturn | 198      |
| MinimumReturn | 127      |
| TotalSamples  | 119988   |
----------------------------
itr #35 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0059616356156766415
Validation loss = 0.006010981742292643
Validation loss = 0.006022968329489231
Validation loss = 0.005936151836067438
Validation loss = 0.005973139312118292
Validation loss = 0.005885503254830837
Validation loss = 0.00619058171287179
Validation loss = 0.005872259847819805
Validation loss = 0.006053017918020487
Validation loss = 0.006157968658953905
Validation loss = 0.005821514409035444
Validation loss = 0.006029369309544563
Validation loss = 0.005969199351966381
Validation loss = 0.0058887130580842495
Validation loss = 0.005856935866177082
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.005928846541792154
Validation loss = 0.006075576413422823
Validation loss = 0.005867943167686462
Validation loss = 0.005920758470892906
Validation loss = 0.00588745204731822
Validation loss = 0.0059568071737885475
Validation loss = 0.0059463949874043465
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0059514958411455154
Validation loss = 0.005952266976237297
Validation loss = 0.006070225033909082
Validation loss = 0.0060192495584487915
Validation loss = 0.005950516555458307
Validation loss = 0.005843206774443388
Validation loss = 0.006047459784895182
Validation loss = 0.005967054981738329
Validation loss = 0.0059553030878305435
Validation loss = 0.006044629029929638
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.005892314948141575
Validation loss = 0.005881155375391245
Validation loss = 0.005935716442763805
Validation loss = 0.0059603676199913025
Validation loss = 0.005837701261043549
Validation loss = 0.005964741110801697
Validation loss = 0.005928425118327141
Validation loss = 0.006105964072048664
Validation loss = 0.006321232300251722
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0060153985396027565
Validation loss = 0.005961650982499123
Validation loss = 0.005940950009971857
Validation loss = 0.005995850544422865
Validation loss = 0.006030706688761711
Validation loss = 0.005965602118521929
Validation loss = 0.005978291388601065
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 35       |
| MaximumReturn | 199      |
| MinimumReturn | 133      |
| TotalSamples  | 123321   |
----------------------------
itr #36 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.006243349984288216
Validation loss = 0.006144456099718809
Validation loss = 0.006136670242995024
Validation loss = 0.006035450380295515
Validation loss = 0.006153610534965992
Validation loss = 0.006023205816745758
Validation loss = 0.006009191740304232
Validation loss = 0.006045657675713301
Validation loss = 0.006229107268154621
Validation loss = 0.0061651491560041904
Validation loss = 0.006109924521297216
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.006161071360111237
Validation loss = 0.006013767793774605
Validation loss = 0.006208944600075483
Validation loss = 0.005978862755000591
Validation loss = 0.0061216470785439014
Validation loss = 0.006053102668374777
Validation loss = 0.0060324901714921
Validation loss = 0.006102364975959063
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.006218052003532648
Validation loss = 0.006058271508663893
Validation loss = 0.0059887804090976715
Validation loss = 0.005961569491773844
Validation loss = 0.005968221928924322
Validation loss = 0.00607848959043622
Validation loss = 0.006193029694259167
Validation loss = 0.006140084937214851
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00632466422393918
Validation loss = 0.006064885761588812
Validation loss = 0.006101739592850208
Validation loss = 0.006451588124036789
Validation loss = 0.006162229925394058
Validation loss = 0.006036388222128153
Validation loss = 0.006017470732331276
Validation loss = 0.006183897145092487
Validation loss = 0.00633386941626668
Validation loss = 0.006008095573633909
Validation loss = 0.006477858871221542
Validation loss = 0.006058673840016127
Validation loss = 0.0062033324502408504
Validation loss = 0.006050494499504566
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.006273479200899601
Validation loss = 0.006030456628650427
Validation loss = 0.006166808772832155
Validation loss = 0.0059820315800607204
Validation loss = 0.006061881314963102
Validation loss = 0.0060750567354261875
Validation loss = 0.006042137276381254
Validation loss = 0.006105649750679731
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 161      |
| Iteration     | 36       |
| MaximumReturn | 199      |
| MinimumReturn | 114      |
| TotalSamples  | 126654   |
----------------------------
itr #37 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.006141109392046928
Validation loss = 0.006231370382010937
Validation loss = 0.006111149676144123
Validation loss = 0.006515714805573225
Validation loss = 0.006194333545863628
Validation loss = 0.006232619751244783
Validation loss = 0.006171784829348326
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.006206894293427467
Validation loss = 0.0061407838948071
Validation loss = 0.006204552017152309
Validation loss = 0.0064200107008218765
Validation loss = 0.0061501325108110905
Validation loss = 0.00622465368360281
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0062052239663898945
Validation loss = 0.006266924552619457
Validation loss = 0.0062371790409088135
Validation loss = 0.006253369152545929
Validation loss = 0.006187499966472387
Validation loss = 0.006197268143296242
Validation loss = 0.006529431790113449
Validation loss = 0.006245950702577829
Validation loss = 0.006268034689128399
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.006168830208480358
Validation loss = 0.00633967574685812
Validation loss = 0.00641170609742403
Validation loss = 0.006185629405081272
Validation loss = 0.006191846448928118
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.006255699321627617
Validation loss = 0.006271539721637964
Validation loss = 0.006187829189002514
Validation loss = 0.006176171824336052
Validation loss = 0.006171572022140026
Validation loss = 0.006237511057406664
Validation loss = 0.0062056733295321465
Validation loss = 0.006230240222066641
Validation loss = 0.0061361282132565975
Validation loss = 0.00638244254514575
Validation loss = 0.006183442194014788
Validation loss = 0.00628606928512454
Validation loss = 0.006211974658071995
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 37       |
| MaximumReturn | 199      |
| MinimumReturn | 123      |
| TotalSamples  | 129987   |
----------------------------
itr #38 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.006379061844199896
Validation loss = 0.006322185043245554
Validation loss = 0.006406533997505903
Validation loss = 0.006431101355701685
Validation loss = 0.006360451225191355
Validation loss = 0.006311729550361633
Validation loss = 0.006303237285465002
Validation loss = 0.006323877722024918
Validation loss = 0.006316125858575106
Validation loss = 0.006369275506585836
Validation loss = 0.0062691508792340755
Validation loss = 0.006402429658919573
Validation loss = 0.006414341740310192
Validation loss = 0.006412651855498552
Validation loss = 0.006344964727759361
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.006428674329072237
Validation loss = 0.006439156364649534
Validation loss = 0.006368097383528948
Validation loss = 0.00628276402130723
Validation loss = 0.006270373240113258
Validation loss = 0.006329446565359831
Validation loss = 0.006540597416460514
Validation loss = 0.006339960265904665
Validation loss = 0.006314503028988838
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.006444637663662434
Validation loss = 0.006311759352684021
Validation loss = 0.006307417992502451
Validation loss = 0.006265423260629177
Validation loss = 0.006438659969717264
Validation loss = 0.006336904596537352
Validation loss = 0.006326280999928713
Validation loss = 0.0063374582678079605
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.006422495003789663
Validation loss = 0.006521215196698904
Validation loss = 0.006454590242356062
Validation loss = 0.006429816130548716
Validation loss = 0.00633900985121727
Validation loss = 0.006309915333986282
Validation loss = 0.006439378950744867
Validation loss = 0.0063887848518788815
Validation loss = 0.006292262114584446
Validation loss = 0.006390028167515993
Validation loss = 0.006325826048851013
Validation loss = 0.006395361386239529
Validation loss = 0.006347347982227802
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.006432841997593641
Validation loss = 0.0064776064828038216
Validation loss = 0.0063248579390347
Validation loss = 0.006527699995785952
Validation loss = 0.006477504502981901
Validation loss = 0.006401903461664915
Validation loss = 0.00630847318097949
Validation loss = 0.00634206784889102
Validation loss = 0.006371879018843174
Validation loss = 0.0067661236971616745
Validation loss = 0.006297862157225609
Validation loss = 0.006345931440591812
Validation loss = 0.006450972054153681
Validation loss = 0.006302230525761843
Validation loss = 0.006306366063654423
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 38       |
| MaximumReturn | 199      |
| MinimumReturn | 113      |
| TotalSamples  | 133320   |
----------------------------
itr #39 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.006527728866785765
Validation loss = 0.006455324124544859
Validation loss = 0.006569070275872946
Validation loss = 0.006472024135291576
Validation loss = 0.006570980418473482
Validation loss = 0.006440982688218355
Validation loss = 0.006570419296622276
Validation loss = 0.006493296008557081
Validation loss = 0.00643931096419692
Validation loss = 0.00657891109585762
Validation loss = 0.006425637751817703
Validation loss = 0.006605645641684532
Validation loss = 0.0064705307595431805
Validation loss = 0.0066757104359567165
Validation loss = 0.006488760467618704
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.006661606952548027
Validation loss = 0.006444687955081463
Validation loss = 0.00654258020222187
Validation loss = 0.006645584478974342
Validation loss = 0.00642563309520483
Validation loss = 0.006436941213905811
Validation loss = 0.006550254300236702
Validation loss = 0.006425905507057905
Validation loss = 0.006499134935438633
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.006524357944726944
Validation loss = 0.006480193696916103
Validation loss = 0.0065145837143063545
Validation loss = 0.0067796711809933186
Validation loss = 0.006525785196572542
Validation loss = 0.006610027048736811
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.006490518804639578
Validation loss = 0.006566372700035572
Validation loss = 0.0064577218145132065
Validation loss = 0.006735008675605059
Validation loss = 0.006883830763399601
Validation loss = 0.006543389521539211
Validation loss = 0.006615635007619858
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.006487765349447727
Validation loss = 0.006522912532091141
Validation loss = 0.006457836367189884
Validation loss = 0.006646814290434122
Validation loss = 0.006524659227579832
Validation loss = 0.006547991186380386
Validation loss = 0.006685011088848114
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 164      |
| Iteration     | 39       |
| MaximumReturn | 199      |
| MinimumReturn | 122      |
| TotalSamples  | 136653   |
----------------------------
