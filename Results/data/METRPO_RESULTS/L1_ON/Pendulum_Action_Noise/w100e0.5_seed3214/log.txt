Logging to experiments/pendulum/Pendulum_Action_Noise/w100e0.5_seed3214
Print configuration .....
{'env_name': 'pendulum', 'random_seeds': [3214, 2431, 2531, 2231], 'save_variables': False, 'model_save_dir': '/tmp/pendulum_models/', 'restore_variables': False, 'start_onpol_iter': 0, 'onpol_iters': 40, 'num_path_random': 25, 'num_path_onpol': 25, 'env_horizon': 200, 'max_train_data': 200000, 'max_val_data': 100000, 'discard_ratio': 0.0, 'dynamics': {'pre_training': {'mode': 'intrinsic_reward', 'itr': 0, 'policy_itr': 20}, 'model': 'nn', 'ensemble': True, 'ensemble_model_count': 5, 'enable_particle_ensemble': True, 'particles': 5, 'obs_var': 1.0, 'intrinsic_reward_coeff': 1.0, 'ita': 1.0, 'mode': 'random', 'val': True, 'n_layers': 4, 'hidden_size': 1000, 'activation': 'relu', 'batch_size': 1000, 'learning_rate': 0.001, 'reg_coeff': 0.0, 'epochs': 200, 'kfac_params': {'learning_rate': 0.1, 'damping': 0.001, 'momentum': 0.9, 'kl_clip': 0.0001, 'cov_ema_decay': 0.99}}, 'policy': {'network_shape': [64, 64], 'init_logstd': 0.0, 'activation': 'tanh', 'reinitialize_every_itr': False}, 'trpo': {'horizon': 200, 'gamma': 0.99, 'step_size': 0.01, 'iterations': 20, 'batch_size': 50000, 'gae': 0.95, 'visualization': False, 'visualize_iterations': [0]}, 'algo': 'trpo'}
Generating random rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating random rollouts.
Creating normalization for training data.
Done creating normalization for training data.
Particle ensemble enabled? True
An ensemble of 5 dynamics model <class 'model.dynamics.NNDynamicsModel'> initialized
Train dynamics model with intrinsic reward only? False
Pre-training enabled. Using only intrinsic reward.
Pre-training dynamics model for 0 iterations...
Done pre-training dynamics model.
Using external reward only.
itr #0 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.052826911211013794
Validation loss = 0.026157725602388382
Validation loss = 0.006394538097083569
Validation loss = 0.0026380394119769335
Validation loss = 0.0006869037752039731
Validation loss = 0.0003316782822366804
Validation loss = 0.000235081315622665
Validation loss = 0.00016872028936631978
Validation loss = 0.00015453394735231996
Validation loss = 0.00012629377306438982
Validation loss = 0.00018271745648235083
Validation loss = 9.796773520065472e-05
Validation loss = 0.00016485214291606098
Validation loss = 0.00012560657341964543
Validation loss = 0.00019145853002555668
Validation loss = 0.004307090770453215
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.07497783750295639
Validation loss = 0.014111020602285862
Validation loss = 0.00806103553622961
Validation loss = 0.002101138001307845
Validation loss = 0.0005609110812656581
Validation loss = 0.00023049922310747206
Validation loss = 0.00016978468920569867
Validation loss = 0.000141955868457444
Validation loss = 0.00013325737381819636
Validation loss = 0.0001356277643935755
Validation loss = 0.00033392422483302653
Validation loss = 0.00017319482867605984
Validation loss = 0.003808662062510848
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.10490795224905014
Validation loss = 0.01796114817261696
Validation loss = 0.007363275624811649
Validation loss = 0.002065428998321295
Validation loss = 0.0006061322055757046
Validation loss = 0.00025670178001746535
Validation loss = 0.00016653016791678965
Validation loss = 0.00013261493586469442
Validation loss = 0.00012086988135706633
Validation loss = 0.00010490866407053545
Validation loss = 0.00012093083205400035
Validation loss = 0.00010415015276521444
Validation loss = 0.00045390549348667264
Validation loss = 0.00018513355462346226
Validation loss = 0.001176550635136664
Validation loss = 0.0005437108338810503
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.09941835701465607
Validation loss = 0.023502090945839882
Validation loss = 0.00677131162956357
Validation loss = 0.001730079296976328
Validation loss = 0.000550801691133529
Validation loss = 0.0002587811031844467
Validation loss = 0.00022990364232100546
Validation loss = 0.00014666577044408768
Validation loss = 0.0001415488077327609
Validation loss = 0.00013475987361744046
Validation loss = 0.00013486060197465122
Validation loss = 0.00020072190091013908
Validation loss = 0.00016482379578519613
Validation loss = 0.00021987447689753026
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.09375128895044327
Validation loss = 0.015201940201222897
Validation loss = 0.0070878551341593266
Validation loss = 0.002170681720599532
Validation loss = 0.0005452054901979864
Validation loss = 0.00030872022034600377
Validation loss = 0.00018218568584416062
Validation loss = 0.00016544872778467834
Validation loss = 0.00013392399705480784
Validation loss = 0.00014056620420888066
Validation loss = 0.0001562230900162831
Validation loss = 0.0001130690798163414
Validation loss = 0.001306740683503449
Validation loss = 0.00016866294026840478
Validation loss = 0.00016742793377488852
Validation loss = 0.0003876217233482748
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 0        |
| MaximumReturn | 199      |
| MinimumReturn | 116      |
| TotalSamples  | 6666     |
----------------------------
itr #1 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.008723851293325424
Validation loss = 0.002515481784939766
Validation loss = 0.0013329380890354514
Validation loss = 0.0012340099783614278
Validation loss = 0.0018465309403836727
Validation loss = 0.0012274576583877206
Validation loss = 0.001718258368782699
Validation loss = 0.002863107016310096
Validation loss = 0.01609313115477562
Validation loss = 0.001810935907997191
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.014231550507247448
Validation loss = 0.0025334851816296577
Validation loss = 0.0013648919994011521
Validation loss = 0.001244747661985457
Validation loss = 0.0012884108582511544
Validation loss = 0.0011690146056935191
Validation loss = 0.0018858319381251931
Validation loss = 0.001250276225619018
Validation loss = 0.001262765727005899
Validation loss = 0.001392404199577868
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.010524981655180454
Validation loss = 0.0020139298867434263
Validation loss = 0.0014986664755269885
Validation loss = 0.0018072075909003615
Validation loss = 0.0011970661580562592
Validation loss = 0.001337977941147983
Validation loss = 0.001743631437420845
Validation loss = 0.002299102721735835
Validation loss = 0.0015007779002189636
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.008107639849185944
Validation loss = 0.0022378701251000166
Validation loss = 0.0013046114472672343
Validation loss = 0.001272988272830844
Validation loss = 0.0013929695123806596
Validation loss = 0.0034347434993833303
Validation loss = 0.0015094670234248042
Validation loss = 0.0012019624700769782
Validation loss = 0.0030099779833108187
Validation loss = 0.001288139377720654
Validation loss = 0.0011977179674431682
Validation loss = 0.0023662152234464884
Validation loss = 0.001141627668403089
Validation loss = 0.0030422320123761892
Validation loss = 0.0012480494333431125
Validation loss = 0.0013308754423633218
Validation loss = 0.0021189299877732992
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.016530904918909073
Validation loss = 0.0033867729362100363
Validation loss = 0.001506837666966021
Validation loss = 0.001267485087737441
Validation loss = 0.0030264882370829582
Validation loss = 0.0013488950207829475
Validation loss = 0.0017179957358166575
Validation loss = 0.001454705372452736
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 181      |
| Iteration     | 1        |
| MaximumReturn | 199      |
| MinimumReturn | 118      |
| TotalSamples  | 9999     |
----------------------------
itr #2 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0034356217365711927
Validation loss = 0.0012381135020405054
Validation loss = 0.0009851133218035102
Validation loss = 0.0009895224357023835
Validation loss = 0.0010419862810522318
Validation loss = 0.0014185351319611073
Validation loss = 0.001095510320737958
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.005730294622480869
Validation loss = 0.001193447271361947
Validation loss = 0.0010652309283614159
Validation loss = 0.0011678407900035381
Validation loss = 0.0009610590641386807
Validation loss = 0.002161166863515973
Validation loss = 0.001012897351756692
Validation loss = 0.0012066568015143275
Validation loss = 0.001169219147413969
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.004558199550956488
Validation loss = 0.0012394380755722523
Validation loss = 0.0012275369372218847
Validation loss = 0.0011765779927372932
Validation loss = 0.0012932887766510248
Validation loss = 0.0010439945617690682
Validation loss = 0.0023752336855977774
Validation loss = 0.0010721060680225492
Validation loss = 0.0015528604853898287
Validation loss = 0.001041731215082109
Validation loss = 0.0010206701699644327
Validation loss = 0.0018003650475293398
Validation loss = 0.0010255137458443642
Validation loss = 0.001117301406338811
Validation loss = 0.0017095308285206556
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.003980887122452259
Validation loss = 0.0011763462098315358
Validation loss = 0.001099316170439124
Validation loss = 0.0009333919733762741
Validation loss = 0.00097231927793473
Validation loss = 0.0010016473243013024
Validation loss = 0.0010524468962103128
Validation loss = 0.0019009234383702278
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0034021344035863876
Validation loss = 0.001407715491950512
Validation loss = 0.0010278580011799932
Validation loss = 0.001160878804512322
Validation loss = 0.0012385165318846703
Validation loss = 0.0009773625060915947
Validation loss = 0.002205724362283945
Validation loss = 0.0009923884645104408
Validation loss = 0.001002938486635685
Validation loss = 0.0011804618407040834
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 2        |
| MaximumReturn | 199      |
| MinimumReturn | 122      |
| TotalSamples  | 13332    |
----------------------------
itr #3 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0033043595030903816
Validation loss = 0.0010586971184238791
Validation loss = 0.001923864590935409
Validation loss = 0.0013482806971296668
Validation loss = 0.0013360553421080112
Validation loss = 0.0012194114970043302
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0032962970435619354
Validation loss = 0.001129885553382337
Validation loss = 0.0011193245882168412
Validation loss = 0.0024677664041519165
Validation loss = 0.0011710190447047353
Validation loss = 0.0010337803978472948
Validation loss = 0.0011012080358341336
Validation loss = 0.0010813337285071611
Validation loss = 0.0023593876976519823
Validation loss = 0.0019022637279704213
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.003404231509193778
Validation loss = 0.0011850346345454454
Validation loss = 0.0009781478438526392
Validation loss = 0.0009935505222529173
Validation loss = 0.002290933858603239
Validation loss = 0.0028694700449705124
Validation loss = 0.001166942878626287
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.004656800534576178
Validation loss = 0.0010867343517020345
Validation loss = 0.0010245759040117264
Validation loss = 0.0009864185703918338
Validation loss = 0.001170678180642426
Validation loss = 0.0009403162985108793
Validation loss = 0.0016619404777884483
Validation loss = 0.001275670132599771
Validation loss = 0.0011064250720664859
Validation loss = 0.0014060214161872864
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0024290005676448345
Validation loss = 0.0010058922925963998
Validation loss = 0.0013853494310751557
Validation loss = 0.0013377112336456776
Validation loss = 0.0017490723403170705
Validation loss = 0.0012091001262888312
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 3        |
| MaximumReturn | 199      |
| MinimumReturn | 123      |
| TotalSamples  | 16665    |
----------------------------
itr #4 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.002740010153502226
Validation loss = 0.0009507953654974699
Validation loss = 0.0009534876444377005
Validation loss = 0.0018794098868966103
Validation loss = 0.0009275468764826655
Validation loss = 0.0013978186761960387
Validation loss = 0.0011318952310830355
Validation loss = 0.0012348945019766688
Validation loss = 0.0010716556571424007
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0008949738694354892
Validation loss = 0.0013911848654970527
Validation loss = 0.0007731658988632262
Validation loss = 0.0011301049962639809
Validation loss = 0.0010476713068783283
Validation loss = 0.002203946700319648
Validation loss = 0.0008548152982257307
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.002078976249322295
Validation loss = 0.0018966399366036057
Validation loss = 0.000878971884958446
Validation loss = 0.0011874772608280182
Validation loss = 0.001040368340909481
Validation loss = 0.0014690349344164133
Validation loss = 0.0007371142273768783
Validation loss = 0.0016389923403039575
Validation loss = 0.0008318284526467323
Validation loss = 0.001466421177610755
Validation loss = 0.001030494226142764
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0018425020389258862
Validation loss = 0.0007428561220876873
Validation loss = 0.0007699569105170667
Validation loss = 0.001168352086097002
Validation loss = 0.0011695318389683962
Validation loss = 0.0007370846578851342
Validation loss = 0.0011291076662018895
Validation loss = 0.000849090632982552
Validation loss = 0.0012218562187626958
Validation loss = 0.000905643857549876
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.003811117261648178
Validation loss = 0.000994303380139172
Validation loss = 0.001575241331011057
Validation loss = 0.0012311654863879085
Validation loss = 0.0020570745691657066
Validation loss = 0.0009055598056875169
Validation loss = 0.002586610382422805
Validation loss = 0.0008986483444459736
Validation loss = 0.001181538449600339
Validation loss = 0.0009497938444837928
Validation loss = 0.0008889154996722937
Validation loss = 0.0008788537816144526
Validation loss = 0.0008674128330312669
Validation loss = 0.0009297702345065773
Validation loss = 0.0009336845832876861
Validation loss = 0.0009603508515283465
Validation loss = 0.0008250816026702523
Validation loss = 0.0007642199052497745
Validation loss = 0.0015829111216589808
Validation loss = 0.0007621307158842683
Validation loss = 0.0012581150513142347
Validation loss = 0.001021252479404211
Validation loss = 0.000993637484498322
Validation loss = 0.001085572992451489
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 4        |
| MaximumReturn | 199      |
| MinimumReturn | 122      |
| TotalSamples  | 19998    |
----------------------------
itr #5 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0018089072545990348
Validation loss = 0.0012836960377171636
Validation loss = 0.00277832243591547
Validation loss = 0.0008184835314750671
Validation loss = 0.0014365706592798233
Validation loss = 0.0008295603911392391
Validation loss = 0.0009471484227105975
Validation loss = 0.0012308924924582243
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.002017817460000515
Validation loss = 0.0009780005784705281
Validation loss = 0.002605048706755042
Validation loss = 0.001283913617953658
Validation loss = 0.0007987628923729062
Validation loss = 0.0020475531928241253
Validation loss = 0.001244169077835977
Validation loss = 0.0015228062402456999
Validation loss = 0.0007192600169219077
Validation loss = 0.0010057066101580858
Validation loss = 0.0010151418391615152
Validation loss = 0.0008982846629805863
Validation loss = 0.0015064559411257505
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0018592268461361527
Validation loss = 0.0009834737284108996
Validation loss = 0.0009827035246416926
Validation loss = 0.0011622472666203976
Validation loss = 0.0009733345359563828
Validation loss = 0.0016912504797801375
Validation loss = 0.001140646985732019
Validation loss = 0.0011912161717191339
Validation loss = 0.000698496529366821
Validation loss = 0.0007525971159338951
Validation loss = 0.001523400191217661
Validation loss = 0.0015128306113183498
Validation loss = 0.00074896210571751
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00198933994397521
Validation loss = 0.003827484790235758
Validation loss = 0.0007844124920666218
Validation loss = 0.0014053527265787125
Validation loss = 0.0009801069973036647
Validation loss = 0.0007823848864063621
Validation loss = 0.0007172499317675829
Validation loss = 0.0017679022857919335
Validation loss = 0.0006639439379796386
Validation loss = 0.0010663651628419757
Validation loss = 0.002003410132601857
Validation loss = 0.0006353232311084867
Validation loss = 0.002866912167519331
Validation loss = 0.0007080743089318275
Validation loss = 0.0009141323971562088
Validation loss = 0.0008507570018991828
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0013104146346449852
Validation loss = 0.0007780290907248855
Validation loss = 0.0014131662901490927
Validation loss = 0.0009793674107640982
Validation loss = 0.0009977527661249042
Validation loss = 0.0006090773968026042
Validation loss = 0.0009655543835833669
Validation loss = 0.0007565453415736556
Validation loss = 0.0012938880827277899
Validation loss = 0.0008682593470439315
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 5        |
| MaximumReturn | 199      |
| MinimumReturn | 118      |
| TotalSamples  | 23331    |
----------------------------
itr #6 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0017285161884501576
Validation loss = 0.0013521640794351697
Validation loss = 0.000716971408110112
Validation loss = 0.0010730358771979809
Validation loss = 0.0013062420766800642
Validation loss = 0.0007528074202127755
Validation loss = 0.0006564338691532612
Validation loss = 0.002497516805306077
Validation loss = 0.0008861334645189345
Validation loss = 0.0009016140829771757
Validation loss = 0.0005992348305881023
Validation loss = 0.0009294108022004366
Validation loss = 0.0008216275018639863
Validation loss = 0.0008267933735623956
Validation loss = 0.000737546943128109
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0013835803838446736
Validation loss = 0.0007445563096553087
Validation loss = 0.0007894594455137849
Validation loss = 0.000985638122074306
Validation loss = 0.0013620766112580895
Validation loss = 0.0013126191915944219
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0015506952768191695
Validation loss = 0.0007453674334101379
Validation loss = 0.0008216897258535028
Validation loss = 0.0009525079512968659
Validation loss = 0.0006510003004223108
Validation loss = 0.0008143685408867896
Validation loss = 0.001287751947529614
Validation loss = 0.0009521479369141161
Validation loss = 0.001954901497811079
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0008598806452937424
Validation loss = 0.001116491504944861
Validation loss = 0.0009972034022212029
Validation loss = 0.0015261855442076921
Validation loss = 0.0006801423733122647
Validation loss = 0.0008724449435248971
Validation loss = 0.0009018945856951177
Validation loss = 0.0005897353403270245
Validation loss = 0.0007395146531052887
Validation loss = 0.000783959636464715
Validation loss = 0.0009635809692554176
Validation loss = 0.0005895540816709399
Validation loss = 0.001400142558850348
Validation loss = 0.0006089233211241663
Validation loss = 0.0005901958793401718
Validation loss = 0.0008331538992933929
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0010065756505355239
Validation loss = 0.0007406769436784089
Validation loss = 0.0018146614311262965
Validation loss = 0.0006897800485603511
Validation loss = 0.000678636715747416
Validation loss = 0.0009675262845121324
Validation loss = 0.0007949345163069665
Validation loss = 0.000819723994936794
Validation loss = 0.0007209418108686805
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 178      |
| Iteration     | 6        |
| MaximumReturn | 199      |
| MinimumReturn | 121      |
| TotalSamples  | 26664    |
----------------------------
itr #7 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005556616233661771
Validation loss = 0.0007327549392357469
Validation loss = 0.0006961929029785097
Validation loss = 0.0010699194390326738
Validation loss = 0.0008855002233758569
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.001384414848871529
Validation loss = 0.0005386053817346692
Validation loss = 0.00065630953758955
Validation loss = 0.0015943162143230438
Validation loss = 0.0006598656182177365
Validation loss = 0.000750531442463398
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0012232919689267874
Validation loss = 0.0006317449733614922
Validation loss = 0.0006410091882571578
Validation loss = 0.0025537661276757717
Validation loss = 0.0006171842687763274
Validation loss = 0.0016018958995118737
Validation loss = 0.0006935801939107478
Validation loss = 0.0007836575387045741
Validation loss = 0.0009066032944247127
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007186372531577945
Validation loss = 0.0006349241593852639
Validation loss = 0.0006449614302255213
Validation loss = 0.0007398923044092953
Validation loss = 0.00046633946476504207
Validation loss = 0.0012064494658261538
Validation loss = 0.0006539735477417707
Validation loss = 0.0006868120981380343
Validation loss = 0.0012093654368072748
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.001520074438303709
Validation loss = 0.0005547452019527555
Validation loss = 0.0005825482658110559
Validation loss = 0.0007696800748817623
Validation loss = 0.0008838411886245012
Validation loss = 0.0006854496896266937
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 177      |
| Iteration     | 7        |
| MaximumReturn | 199      |
| MinimumReturn | 129      |
| TotalSamples  | 29997    |
----------------------------
itr #8 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0007193377241492271
Validation loss = 0.0018081963062286377
Validation loss = 0.0006218564813025296
Validation loss = 0.0005821312079206109
Validation loss = 0.0006731406319886446
Validation loss = 0.0006721109966747463
Validation loss = 0.0005484791472554207
Validation loss = 0.0005818415665999055
Validation loss = 0.001392557518556714
Validation loss = 0.0007220831466838717
Validation loss = 0.0008666807552799582
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0008262422634288669
Validation loss = 0.0007865063962526619
Validation loss = 0.000674540875479579
Validation loss = 0.0008751580608077347
Validation loss = 0.0011251614196226
Validation loss = 0.0006102352635934949
Validation loss = 0.0009645074605941772
Validation loss = 0.0005652930121868849
Validation loss = 0.001573266927152872
Validation loss = 0.0005559313576668501
Validation loss = 0.0007175003411248326
Validation loss = 0.0006159627228043973
Validation loss = 0.0016176130156964064
Validation loss = 0.0005391446175053716
Validation loss = 0.000871317635755986
Validation loss = 0.0005195570411160588
Validation loss = 0.000618536549154669
Validation loss = 0.0005200731102377176
Validation loss = 0.0007829054957255721
Validation loss = 0.0004818062297999859
Validation loss = 0.0007157146465033293
Validation loss = 0.000581165193580091
Validation loss = 0.0006619106861762702
Validation loss = 0.0007953489548526704
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0010827365331351757
Validation loss = 0.000857086677569896
Validation loss = 0.0007525458349846303
Validation loss = 0.0012794618960469961
Validation loss = 0.0005528974579647183
Validation loss = 0.0008741023484617472
Validation loss = 0.0005706907832063735
Validation loss = 0.0005906245787627995
Validation loss = 0.0005668288213200867
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007847967790439725
Validation loss = 0.000688382366206497
Validation loss = 0.0004935184260830283
Validation loss = 0.0006132174166850746
Validation loss = 0.0005248074303381145
Validation loss = 0.0006346963346004486
Validation loss = 0.0007666233577765524
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0012401092099025846
Validation loss = 0.0006738443626090884
Validation loss = 0.0005593841779045761
Validation loss = 0.0006732323672622442
Validation loss = 0.0006262628012336791
Validation loss = 0.0008840120281092823
Validation loss = 0.000749979866668582
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 159      |
| Iteration     | 8        |
| MaximumReturn | 199      |
| MinimumReturn | 119      |
| TotalSamples  | 33330    |
----------------------------
itr #9 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0007120715454220772
Validation loss = 0.0004973385948687792
Validation loss = 0.0006482243188656867
Validation loss = 0.0005828517605550587
Validation loss = 0.0009819082915782928
Validation loss = 0.0007606374565511942
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006034542457200587
Validation loss = 0.0006023505120538175
Validation loss = 0.0006030888180248439
Validation loss = 0.0006880477303639054
Validation loss = 0.0005001270328648388
Validation loss = 0.0004902300424873829
Validation loss = 0.0004969589062966406
Validation loss = 0.0005075049703009427
Validation loss = 0.0005641923635266721
Validation loss = 0.0008352375589311123
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00145883415825665
Validation loss = 0.000616017438005656
Validation loss = 0.0006209752173162997
Validation loss = 0.0011194527614861727
Validation loss = 0.0005058564129285514
Validation loss = 0.0006228574784472585
Validation loss = 0.0005159491556696594
Validation loss = 0.0006283725379034877
Validation loss = 0.0005286734085530043
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007846790831536055
Validation loss = 0.0005874735070392489
Validation loss = 0.0005247280350886285
Validation loss = 0.0010397263104096055
Validation loss = 0.0005251120892353356
Validation loss = 0.0006244463147595525
Validation loss = 0.0005160542204976082
Validation loss = 0.0005737179308198392
Validation loss = 0.0005641347379423678
Validation loss = 0.0008270393591374159
Validation loss = 0.0005868212901987135
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0006697524222545326
Validation loss = 0.0006794919027015567
Validation loss = 0.000842234876472503
Validation loss = 0.0005765326786786318
Validation loss = 0.001046356512233615
Validation loss = 0.000720519048627466
Validation loss = 0.000626689288765192
Validation loss = 0.0009359492687508464
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 9        |
| MaximumReturn | 199      |
| MinimumReturn | 114      |
| TotalSamples  | 36663    |
----------------------------
itr #10 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0010786168277263641
Validation loss = 0.000668630818836391
Validation loss = 0.0005822888342663646
Validation loss = 0.0005228319787420332
Validation loss = 0.0006529384991154075
Validation loss = 0.0005461219698190689
Validation loss = 0.0005615593399852514
Validation loss = 0.000531537167262286
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.002062021754682064
Validation loss = 0.000521387904882431
Validation loss = 0.0005782708176411688
Validation loss = 0.0004783589392900467
Validation loss = 0.0006379826809279621
Validation loss = 0.0005304761580191553
Validation loss = 0.0004439072508830577
Validation loss = 0.0005701008485630155
Validation loss = 0.0004921795334666967
Validation loss = 0.00044586556032299995
Validation loss = 0.0005536258104257286
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0005324706435203552
Validation loss = 0.0006159543409012258
Validation loss = 0.0006066845962777734
Validation loss = 0.00079068768536672
Validation loss = 0.000565093127079308
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0011409188155084848
Validation loss = 0.0004591030301526189
Validation loss = 0.0006354569341056049
Validation loss = 0.0004899347550235689
Validation loss = 0.0005413219914771616
Validation loss = 0.0005125191528350115
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0008989145280793309
Validation loss = 0.0005240228492766619
Validation loss = 0.0005671402323059738
Validation loss = 0.0006391183706000447
Validation loss = 0.0007839630707167089
Validation loss = 0.0005387777928262949
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 10       |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 39996    |
----------------------------
itr #11 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0014978433027863503
Validation loss = 0.0005069092730991542
Validation loss = 0.0007902863435447216
Validation loss = 0.0007827780209481716
Validation loss = 0.0004785930213984102
Validation loss = 0.0005471833283081651
Validation loss = 0.0005590166547335684
Validation loss = 0.0007261224091053009
Validation loss = 0.0005625077756121755
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006342431297525764
Validation loss = 0.0006591631681658328
Validation loss = 0.000620995881035924
Validation loss = 0.0005307329120114446
Validation loss = 0.0005704091163352132
Validation loss = 0.0007890037377364933
Validation loss = 0.0009858915582299232
Validation loss = 0.0005531428032554686
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0007071514264680445
Validation loss = 0.0004880096821580082
Validation loss = 0.0008357265032827854
Validation loss = 0.0004959123907610774
Validation loss = 0.0005644800839945674
Validation loss = 0.000557567982468754
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00047884354717098176
Validation loss = 0.0007651771302334964
Validation loss = 0.0006085144123062491
Validation loss = 0.0005206470959819853
Validation loss = 0.0004818778543267399
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005470582982525229
Validation loss = 0.0006975579890422523
Validation loss = 0.000527110300026834
Validation loss = 0.000577678089030087
Validation loss = 0.0012024575844407082
Validation loss = 0.0005209067021496594
Validation loss = 0.0006540495669469237
Validation loss = 0.0006882388843223453
Validation loss = 0.0005971319042146206
Validation loss = 0.0014631375670433044
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 11       |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 43329    |
----------------------------
itr #12 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.000573886907659471
Validation loss = 0.0005037610535509884
Validation loss = 0.0006549389217980206
Validation loss = 0.0006189172272570431
Validation loss = 0.0006629046984016895
Validation loss = 0.0007087746053002775
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006187299732118845
Validation loss = 0.000624387408606708
Validation loss = 0.0007986988057382405
Validation loss = 0.000544783950317651
Validation loss = 0.0006205244571901858
Validation loss = 0.0014446830609813333
Validation loss = 0.0004998180083930492
Validation loss = 0.0004655198135878891
Validation loss = 0.00048306124517694116
Validation loss = 0.0005098672700114548
Validation loss = 0.0006517620058730245
Validation loss = 0.0005470721516758204
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0008367562550120056
Validation loss = 0.000813355203717947
Validation loss = 0.000469906663056463
Validation loss = 0.0005616231937892735
Validation loss = 0.0008275366853922606
Validation loss = 0.0006593546713702381
Validation loss = 0.0004966891137883067
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007609457825310528
Validation loss = 0.0005453970516100526
Validation loss = 0.0008421659003943205
Validation loss = 0.0007300919387489557
Validation loss = 0.0005202880711294711
Validation loss = 0.0005509992479346693
Validation loss = 0.000573285564314574
Validation loss = 0.000490441219881177
Validation loss = 0.0006585543160326779
Validation loss = 0.0005670393002219498
Validation loss = 0.0005117752589285374
Validation loss = 0.0006821897113695741
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0012971694814041257
Validation loss = 0.0005936637171544135
Validation loss = 0.0005272063426673412
Validation loss = 0.0008888588054105639
Validation loss = 0.0005006014253012836
Validation loss = 0.0005621319287456572
Validation loss = 0.0005560709978453815
Validation loss = 0.0005024985293857753
Validation loss = 0.0005328516708686948
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 12       |
| MaximumReturn | 199      |
| MinimumReturn | 121      |
| TotalSamples  | 46662    |
----------------------------
itr #13 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0015401497948914766
Validation loss = 0.0004910157294943929
Validation loss = 0.0005356934270821512
Validation loss = 0.001274616806767881
Validation loss = 0.000602079788222909
Validation loss = 0.0006331216427497566
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0008623070316389203
Validation loss = 0.0005943032447248697
Validation loss = 0.0006631087744608521
Validation loss = 0.0004924502572976053
Validation loss = 0.0007055904134176672
Validation loss = 0.0004969162400811911
Validation loss = 0.0006051455275155604
Validation loss = 0.0006535541615448892
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0006498807924799621
Validation loss = 0.0009573348215781152
Validation loss = 0.0005283597274683416
Validation loss = 0.0005346675170585513
Validation loss = 0.000578747596591711
Validation loss = 0.0008431594469584525
Validation loss = 0.0007025940576568246
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0010701592545956373
Validation loss = 0.0005208241054788232
Validation loss = 0.0005952853243798018
Validation loss = 0.0005604841862805188
Validation loss = 0.0005567883490584791
Validation loss = 0.000816953310277313
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0006634894525632262
Validation loss = 0.0005490552866831422
Validation loss = 0.0008826497360132635
Validation loss = 0.0006169970729388297
Validation loss = 0.0005320977652445436
Validation loss = 0.0006991457776166499
Validation loss = 0.0006286147399805486
Validation loss = 0.0009812876814976335
Validation loss = 0.000551556411664933
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 13       |
| MaximumReturn | 199      |
| MinimumReturn | 129      |
| TotalSamples  | 49995    |
----------------------------
itr #14 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0007932503358460963
Validation loss = 0.0005637804861180484
Validation loss = 0.0005381176015362144
Validation loss = 0.0006761259865015745
Validation loss = 0.0008316776948049664
Validation loss = 0.0006815972737967968
Validation loss = 0.0004977916833013296
Validation loss = 0.0008253771811723709
Validation loss = 0.0006204601377248764
Validation loss = 0.0006072416435927153
Validation loss = 0.00123295106459409
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.000793159706518054
Validation loss = 0.0005307649844326079
Validation loss = 0.0006151001434773207
Validation loss = 0.0005509116454049945
Validation loss = 0.0005224884953349829
Validation loss = 0.0007544137770310044
Validation loss = 0.0005790610448457301
Validation loss = 0.0005950727500021458
Validation loss = 0.0009523727931082249
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00071665458381176
Validation loss = 0.0008072019554674625
Validation loss = 0.0005462814006023109
Validation loss = 0.0006405761232599616
Validation loss = 0.0007312513189390302
Validation loss = 0.0005361941293813288
Validation loss = 0.0005413788021542132
Validation loss = 0.0006565317744389176
Validation loss = 0.0007346559432335198
Validation loss = 0.0005516201490536332
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0008067443850450218
Validation loss = 0.0007695673266425729
Validation loss = 0.0006055627600289881
Validation loss = 0.0008261501789093018
Validation loss = 0.0006530017126351595
Validation loss = 0.0005591001245193183
Validation loss = 0.000549518212210387
Validation loss = 0.0005415123305283487
Validation loss = 0.0005264350911602378
Validation loss = 0.0007245188462547958
Validation loss = 0.0005580892320722342
Validation loss = 0.0005770806455984712
Validation loss = 0.0006563000497408211
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005602769670076668
Validation loss = 0.0006615984602831304
Validation loss = 0.0008687792578712106
Validation loss = 0.0006215755711309612
Validation loss = 0.0006992632406763732
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 14       |
| MaximumReturn | 199      |
| MinimumReturn | 132      |
| TotalSamples  | 53328    |
----------------------------
itr #15 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006467915372923017
Validation loss = 0.0006180243799462914
Validation loss = 0.0005843527032993734
Validation loss = 0.0005888435407541692
Validation loss = 0.0005476996884681284
Validation loss = 0.0006247892742976546
Validation loss = 0.0006287539144977927
Validation loss = 0.0006151258130557835
Validation loss = 0.0008568297489546239
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006908399518579245
Validation loss = 0.0005762104410678148
Validation loss = 0.000627949892077595
Validation loss = 0.0007618995732627809
Validation loss = 0.0005944216391071677
Validation loss = 0.000545138434972614
Validation loss = 0.000697110197506845
Validation loss = 0.000625617743935436
Validation loss = 0.0005842578248120844
Validation loss = 0.000583239016123116
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0015640933997929096
Validation loss = 0.0006177902687340975
Validation loss = 0.0005833679460920393
Validation loss = 0.0006149510154500604
Validation loss = 0.0006393485236912966
Validation loss = 0.0006601099739782512
Validation loss = 0.000744546705391258
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.000589073752053082
Validation loss = 0.0005871999310329556
Validation loss = 0.0005889054154977202
Validation loss = 0.0009603716898709536
Validation loss = 0.0005499619292095304
Validation loss = 0.0006554601131938398
Validation loss = 0.000890694442205131
Validation loss = 0.000701736775226891
Validation loss = 0.0006712278700433671
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005989148048684001
Validation loss = 0.000579818501137197
Validation loss = 0.0006464656908065081
Validation loss = 0.0006293950136750937
Validation loss = 0.0006156919989734888
Validation loss = 0.0005626113852486014
Validation loss = 0.0006048688082955778
Validation loss = 0.0005617262795567513
Validation loss = 0.0005756326718255877
Validation loss = 0.0007473920122720301
Validation loss = 0.0005626164493151009
Validation loss = 0.0005409289733506739
Validation loss = 0.0006904670153744519
Validation loss = 0.0007252737414091825
Validation loss = 0.0006345603615045547
Validation loss = 0.0007308638305403292
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 15       |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 56661    |
----------------------------
itr #16 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006522535113617778
Validation loss = 0.000606298737693578
Validation loss = 0.000601026404183358
Validation loss = 0.0006774821667931974
Validation loss = 0.0005551275098696351
Validation loss = 0.0005759507184848189
Validation loss = 0.0006686995620839298
Validation loss = 0.0005567037733271718
Validation loss = 0.0006073419936001301
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006078212754800916
Validation loss = 0.0008306442177854478
Validation loss = 0.0008185249753296375
Validation loss = 0.0005896501825191081
Validation loss = 0.0005878062220290303
Validation loss = 0.0005930669722147286
Validation loss = 0.0006760905380360782
Validation loss = 0.0005433797487057745
Validation loss = 0.0006131781847216189
Validation loss = 0.0006049195653758943
Validation loss = 0.0013598579680547118
Validation loss = 0.0005657702567987144
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0007595625356771052
Validation loss = 0.0007150958408601582
Validation loss = 0.0006096121505834162
Validation loss = 0.000640713784378022
Validation loss = 0.0006493409746326506
Validation loss = 0.000742135860491544
Validation loss = 0.0011408907594159245
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0006386968889273703
Validation loss = 0.0006211131112650037
Validation loss = 0.0009550294489599764
Validation loss = 0.0005675260908901691
Validation loss = 0.0005930481129325926
Validation loss = 0.000596276018768549
Validation loss = 0.0005800760700367391
Validation loss = 0.0007791509269736707
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0009761955589056015
Validation loss = 0.0005882451077923179
Validation loss = 0.0007245160522870719
Validation loss = 0.0005721932393498719
Validation loss = 0.0006709318840876222
Validation loss = 0.0021030132193118334
Validation loss = 0.0005743413348682225
Validation loss = 0.0009960600873455405
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 16       |
| MaximumReturn | 199      |
| MinimumReturn | 122      |
| TotalSamples  | 59994    |
----------------------------
itr #17 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0008276953012682498
Validation loss = 0.0006908833747729659
Validation loss = 0.0007529605063609779
Validation loss = 0.0006525397766381502
Validation loss = 0.0005870077293366194
Validation loss = 0.0006364479195326567
Validation loss = 0.0006188077968545258
Validation loss = 0.0007322702440433204
Validation loss = 0.0007577458745799959
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006009437493048608
Validation loss = 0.0006799438851885498
Validation loss = 0.0008521821582689881
Validation loss = 0.0006080828607082367
Validation loss = 0.0007356416899710894
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0010171630419790745
Validation loss = 0.000693935842718929
Validation loss = 0.0009897182462736964
Validation loss = 0.0007156765786930919
Validation loss = 0.0006187687977217138
Validation loss = 0.0006154291331768036
Validation loss = 0.0010251926723867655
Validation loss = 0.0007486289250664413
Validation loss = 0.0007883639773353934
Validation loss = 0.0009044647449627519
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.000647324079181999
Validation loss = 0.0006081783794797957
Validation loss = 0.0006100793252699077
Validation loss = 0.0006737163057550788
Validation loss = 0.0007172835757955909
Validation loss = 0.0005868376465514302
Validation loss = 0.0006655976758338511
Validation loss = 0.000687400228343904
Validation loss = 0.0007618239615112543
Validation loss = 0.0005941946292296052
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0008803022792562842
Validation loss = 0.0006404315354302526
Validation loss = 0.0008026154828257859
Validation loss = 0.0006627134280279279
Validation loss = 0.0005894198548048735
Validation loss = 0.0006705306586809456
Validation loss = 0.0006497502909041941
Validation loss = 0.0006544478819705546
Validation loss = 0.0006596653838641942
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 178      |
| Iteration     | 17       |
| MaximumReturn | 198      |
| MinimumReturn | 124      |
| TotalSamples  | 63327    |
----------------------------
itr #18 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0008397880592383444
Validation loss = 0.0006306964787654579
Validation loss = 0.0008893524063751101
Validation loss = 0.0006945247296243906
Validation loss = 0.0006741785909980536
Validation loss = 0.000696624512784183
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0007367012440226972
Validation loss = 0.000620319799054414
Validation loss = 0.0006723136175423861
Validation loss = 0.0005623989854939282
Validation loss = 0.0006168591789901257
Validation loss = 0.000563053647056222
Validation loss = 0.0008197880233637989
Validation loss = 0.000652449787594378
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0006058941944502294
Validation loss = 0.0007922325748950243
Validation loss = 0.0006635258323512971
Validation loss = 0.0006681642844341695
Validation loss = 0.0006545475334860384
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007374131819233298
Validation loss = 0.0006174730369821191
Validation loss = 0.0006176162278279662
Validation loss = 0.0005914959474466741
Validation loss = 0.0007215781370177865
Validation loss = 0.0006725469138473272
Validation loss = 0.0006001335568726063
Validation loss = 0.0006264614057727158
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0007111513405106962
Validation loss = 0.000724856392480433
Validation loss = 0.0009250222356058657
Validation loss = 0.0007235496887005866
Validation loss = 0.0006054500699974597
Validation loss = 0.000621617422439158
Validation loss = 0.0009516293066553771
Validation loss = 0.0006297133513726294
Validation loss = 0.0005945402081124485
Validation loss = 0.0006281026871874928
Validation loss = 0.0007184870191849768
Validation loss = 0.0005864560953341424
Validation loss = 0.0006564751965925097
Validation loss = 0.0006223230739124119
Validation loss = 0.0007111649611033499
Validation loss = 0.0006103299674578011
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 18       |
| MaximumReturn | 199      |
| MinimumReturn | 133      |
| TotalSamples  | 66660    |
----------------------------
itr #19 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006446636980399489
Validation loss = 0.0006795470835641026
Validation loss = 0.0007174142519943416
Validation loss = 0.0006580690969713032
Validation loss = 0.0007017343305051327
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0007274744566529989
Validation loss = 0.000679820601362735
Validation loss = 0.0006906649214215577
Validation loss = 0.0010039503686130047
Validation loss = 0.0006827819743193686
Validation loss = 0.000683354155626148
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0007219979306682944
Validation loss = 0.0008743149810470641
Validation loss = 0.0006641033687628806
Validation loss = 0.0006701874081045389
Validation loss = 0.0007393492269329727
Validation loss = 0.0007503936067223549
Validation loss = 0.0006866405019536614
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007185876602306962
Validation loss = 0.0006871824734844267
Validation loss = 0.0008707923116162419
Validation loss = 0.0009575756266713142
Validation loss = 0.0007090967847034335
Validation loss = 0.0007409015670418739
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0006741906981915236
Validation loss = 0.000711429282091558
Validation loss = 0.0007162382826209068
Validation loss = 0.0007402410847134888
Validation loss = 0.0006981408223509789
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 19       |
| MaximumReturn | 199      |
| MinimumReturn | 112      |
| TotalSamples  | 69993    |
----------------------------
itr #20 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0008392586023546755
Validation loss = 0.0009184390073642135
Validation loss = 0.000718825263902545
Validation loss = 0.0007651281193830073
Validation loss = 0.0007984237745404243
Validation loss = 0.0007409443496726453
Validation loss = 0.0007376290741376579
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.000771301391068846
Validation loss = 0.0008137051481753588
Validation loss = 0.0007462843204848468
Validation loss = 0.0006496921996586025
Validation loss = 0.0007680713897570968
Validation loss = 0.0007453403668478131
Validation loss = 0.0007052766159176826
Validation loss = 0.0009446398471482098
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00070755131309852
Validation loss = 0.0007273238734342158
Validation loss = 0.0006960936589166522
Validation loss = 0.0007734302780590951
Validation loss = 0.00440960330888629
Validation loss = 0.0007249322370626032
Validation loss = 0.0006895742262713611
Validation loss = 0.0006948640220798552
Validation loss = 0.0007240829872898757
Validation loss = 0.0007413073908537626
Validation loss = 0.0006861925940029323
Validation loss = 0.0007219757535494864
Validation loss = 0.0007310208748094738
Validation loss = 0.0007760576554574072
Validation loss = 0.0007126741111278534
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0006943157059140503
Validation loss = 0.000652021262794733
Validation loss = 0.0006913886754773557
Validation loss = 0.000932302325963974
Validation loss = 0.0007274490199051797
Validation loss = 0.0007310815271921456
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.000702174031175673
Validation loss = 0.0006639788043685257
Validation loss = 0.0007138615474104881
Validation loss = 0.0007574944174848497
Validation loss = 0.0008149249479174614
Validation loss = 0.0007017832831479609
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 20       |
| MaximumReturn | 199      |
| MinimumReturn | 122      |
| TotalSamples  | 73326    |
----------------------------
itr #21 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006994061404839158
Validation loss = 0.0006704377010464668
Validation loss = 0.0007634200155735016
Validation loss = 0.0007018327596597373
Validation loss = 0.0007434733561240137
Validation loss = 0.0006823612493462861
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.001009043538942933
Validation loss = 0.0006858411943539977
Validation loss = 0.0007889121770858765
Validation loss = 0.0007905092206783593
Validation loss = 0.0007314881659112871
Validation loss = 0.0006805171724408865
Validation loss = 0.0008931016200222075
Validation loss = 0.0006937349680811167
Validation loss = 0.0006925440393388271
Validation loss = 0.0007513298187404871
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0007255295640788972
Validation loss = 0.0007583856349810958
Validation loss = 0.0009314355556853116
Validation loss = 0.0007165022543631494
Validation loss = 0.0007349476800300181
Validation loss = 0.0006762912962585688
Validation loss = 0.0014594243839383125
Validation loss = 0.0007122906390577555
Validation loss = 0.0008343730587512255
Validation loss = 0.0007220538100227714
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007140290108509362
Validation loss = 0.0006864820024929941
Validation loss = 0.000864731555338949
Validation loss = 0.0008098296821117401
Validation loss = 0.0006785871228203177
Validation loss = 0.0007387084187939763
Validation loss = 0.0007423554779961705
Validation loss = 0.0007465819362550974
Validation loss = 0.0008138237753883004
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0007769943331368268
Validation loss = 0.0007790710660628974
Validation loss = 0.0007620510878041387
Validation loss = 0.0013199133099988103
Validation loss = 0.0007612749468535185
Validation loss = 0.0007305736653506756
Validation loss = 0.0008115125820040703
Validation loss = 0.0007217293605208397
Validation loss = 0.0007389608072116971
Validation loss = 0.0007428969256579876
Validation loss = 0.0007777099963277578
Validation loss = 0.0007060745847411454
Validation loss = 0.0007933003944344819
Validation loss = 0.0008388537680730224
Validation loss = 0.0006848740740679204
Validation loss = 0.0006895017577335238
Validation loss = 0.0007252342766150832
Validation loss = 0.000732923683244735
Validation loss = 0.0006904909387230873
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 21       |
| MaximumReturn | 199      |
| MinimumReturn | 117      |
| TotalSamples  | 76659    |
----------------------------
itr #22 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0007408541860058904
Validation loss = 0.0007547336281277239
Validation loss = 0.0007850058609619737
Validation loss = 0.0008367022383026779
Validation loss = 0.000764144235290587
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0007990160374902189
Validation loss = 0.0007765855989418924
Validation loss = 0.0007705521420575678
Validation loss = 0.0008875402272678912
Validation loss = 0.0008084086002781987
Validation loss = 0.0007934426539577544
Validation loss = 0.0007660800474695861
Validation loss = 0.0007464887457899749
Validation loss = 0.0007410839316435158
Validation loss = 0.0007257253164425492
Validation loss = 0.0007047991384752095
Validation loss = 0.0007095773471519351
Validation loss = 0.0007289748755283654
Validation loss = 0.0007715187966823578
Validation loss = 0.0007223371649160981
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0008175768307410181
Validation loss = 0.000913750845938921
Validation loss = 0.0008567452314309776
Validation loss = 0.0008229325176216662
Validation loss = 0.0008299819310195744
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007474126177839935
Validation loss = 0.0007531757582910359
Validation loss = 0.0007735936087556183
Validation loss = 0.0007757299463264644
Validation loss = 0.0008022434776648879
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0007313212263397872
Validation loss = 0.0007386634242720902
Validation loss = 0.0009073998662643135
Validation loss = 0.0007055760361254215
Validation loss = 0.0008772147120907903
Validation loss = 0.0007664919248782098
Validation loss = 0.0007324072066694498
Validation loss = 0.0007859824108891189
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 22       |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 79992    |
----------------------------
itr #23 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0008374868193641305
Validation loss = 0.0007961475057527423
Validation loss = 0.0008197507122531533
Validation loss = 0.0007713453378528357
Validation loss = 0.0008134061354212463
Validation loss = 0.0009407101315446198
Validation loss = 0.0008242601761594415
Validation loss = 0.0008778064511716366
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0008425350533798337
Validation loss = 0.0007983626564964652
Validation loss = 0.0008291130652651191
Validation loss = 0.0007678030524402857
Validation loss = 0.0008929269388318062
Validation loss = 0.0008042618865147233
Validation loss = 0.0007718493579886854
Validation loss = 0.0008745644008740783
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0008330304408445954
Validation loss = 0.0010729269124567509
Validation loss = 0.0008523275027982891
Validation loss = 0.0008058432722464204
Validation loss = 0.0010201407130807638
Validation loss = 0.0008787031983956695
Validation loss = 0.0008181597804650664
Validation loss = 0.0008293265709653497
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0008035353384912014
Validation loss = 0.000908524903934449
Validation loss = 0.0009227104601450264
Validation loss = 0.0008038867963477969
Validation loss = 0.0008225878700613976
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0007910100976005197
Validation loss = 0.0009863398736342788
Validation loss = 0.000797935004811734
Validation loss = 0.000852723780553788
Validation loss = 0.0009640324860811234
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 160      |
| Iteration     | 23       |
| MaximumReturn | 199      |
| MinimumReturn | 115      |
| TotalSamples  | 83325    |
----------------------------
itr #24 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0008109373738989234
Validation loss = 0.0009450005018152297
Validation loss = 0.000860084779560566
Validation loss = 0.0008486869046464562
Validation loss = 0.0008020910900086164
Validation loss = 0.0008646852802485228
Validation loss = 0.0011202930472791195
Validation loss = 0.0008008499280549586
Validation loss = 0.0009177683969028294
Validation loss = 0.0007926288526505232
Validation loss = 0.0008080021943897009
Validation loss = 0.0008463650592602789
Validation loss = 0.001116132945753634
Validation loss = 0.001068544457666576
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0008189786458387971
Validation loss = 0.000862402084749192
Validation loss = 0.0008130985079333186
Validation loss = 0.0008152847876772285
Validation loss = 0.0010026964591816068
Validation loss = 0.0008204247569665313
Validation loss = 0.0008231634274125099
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0008902271511033177
Validation loss = 0.0007924324600026011
Validation loss = 0.0008412111783400178
Validation loss = 0.0008499236428178847
Validation loss = 0.0008034674101509154
Validation loss = 0.0008016034262254834
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0009067996288649738
Validation loss = 0.0008548290934413671
Validation loss = 0.000827677664346993
Validation loss = 0.0008052877383306623
Validation loss = 0.0009876744588837028
Validation loss = 0.0008247457444667816
Validation loss = 0.0007999544613994658
Validation loss = 0.0008332472061738372
Validation loss = 0.0009298091754317284
Validation loss = 0.0008383085369132459
Validation loss = 0.0008076643571257591
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0009118101443164051
Validation loss = 0.0008344899979420006
Validation loss = 0.0008628928335383534
Validation loss = 0.0008351757423952222
Validation loss = 0.0008064409485086799
Validation loss = 0.0008806055411696434
Validation loss = 0.0010174834169447422
Validation loss = 0.0008240087772719562
Validation loss = 0.0007916840841062367
Validation loss = 0.0008657137514092028
Validation loss = 0.0008531010244041681
Validation loss = 0.0007840064354240894
Validation loss = 0.0007995504420250654
Validation loss = 0.0008282726630568504
Validation loss = 0.0008513224893249571
Validation loss = 0.0010155942291021347
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 24       |
| MaximumReturn | 199      |
| MinimumReturn | 128      |
| TotalSamples  | 86658    |
----------------------------
itr #25 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.000951874244492501
Validation loss = 0.0015582364285364747
Validation loss = 0.0009479839936830103
Validation loss = 0.0009528716327622533
Validation loss = 0.0009675472392700613
Validation loss = 0.0010092497104778886
Validation loss = 0.0009442631271667778
Validation loss = 0.0010418284218758345
Validation loss = 0.001000243821181357
Validation loss = 0.0010086564579978585
Validation loss = 0.000948586966842413
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.001207624445669353
Validation loss = 0.0009302487596869469
Validation loss = 0.0010104886023327708
Validation loss = 0.0009572287672199309
Validation loss = 0.0009367598686367273
Validation loss = 0.0009359288378618658
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0009613352594897151
Validation loss = 0.0009428882040083408
Validation loss = 0.0009721545502543449
Validation loss = 0.001021749572828412
Validation loss = 0.000960431236308068
Validation loss = 0.001047399127855897
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0009580499026924372
Validation loss = 0.0009463196620345116
Validation loss = 0.0009438233682885766
Validation loss = 0.0010225811274722219
Validation loss = 0.001055338652804494
Validation loss = 0.0009274527546949685
Validation loss = 0.0009677117923274636
Validation loss = 0.0009301409008912742
Validation loss = 0.0009700554655864835
Validation loss = 0.0009679702343419194
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0009652161388657987
Validation loss = 0.0012884933967143297
Validation loss = 0.0009125726064667106
Validation loss = 0.0014012500178068876
Validation loss = 0.0010262279538437724
Validation loss = 0.0009245854453183711
Validation loss = 0.0009882423328235745
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 25       |
| MaximumReturn | 199      |
| MinimumReturn | 125      |
| TotalSamples  | 89991    |
----------------------------
itr #26 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0009922666940838099
Validation loss = 0.001026793266646564
Validation loss = 0.0011190596269443631
Validation loss = 0.0009801494888961315
Validation loss = 0.0010046189418062568
Validation loss = 0.0010021566413342953
Validation loss = 0.0010788821382448077
Validation loss = 0.0010137223871424794
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0009953358676284552
Validation loss = 0.001046953140757978
Validation loss = 0.0010371783282607794
Validation loss = 0.001072355080395937
Validation loss = 0.0010391480755060911
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.001000091666355729
Validation loss = 0.0010289964266121387
Validation loss = 0.0010262301657348871
Validation loss = 0.0009758727392181754
Validation loss = 0.0009741532267071307
Validation loss = 0.001023713848553598
Validation loss = 0.0011545037850737572
Validation loss = 0.0009997643064707518
Validation loss = 0.00098231784068048
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0010261040879413486
Validation loss = 0.0009736264473758638
Validation loss = 0.0009822766296565533
Validation loss = 0.0010110093280673027
Validation loss = 0.0010150864254683256
Validation loss = 0.0010145348496735096
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0011014145566150546
Validation loss = 0.0010329984361305833
Validation loss = 0.0010103335371240973
Validation loss = 0.0010256597306579351
Validation loss = 0.0009903998579829931
Validation loss = 0.0010676388628780842
Validation loss = 0.0012554164277389646
Validation loss = 0.0009839624399319291
Validation loss = 0.001003362238407135
Validation loss = 0.0011980220442637801
Validation loss = 0.0010250966297462583
Validation loss = 0.0011198034044355154
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 155      |
| Iteration     | 26       |
| MaximumReturn | 198      |
| MinimumReturn | 119      |
| TotalSamples  | 93324    |
----------------------------
itr #27 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0010511429281905293
Validation loss = 0.001052472391165793
Validation loss = 0.0010691171046346426
Validation loss = 0.0011451090686023235
Validation loss = 0.0011094979709014297
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0011038320371881127
Validation loss = 0.0010914802551269531
Validation loss = 0.0010241324780508876
Validation loss = 0.001100581488572061
Validation loss = 0.00110914115794003
Validation loss = 0.0010710811475291848
Validation loss = 0.0011833691969513893
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0011378312483429909
Validation loss = 0.0010459686163812876
Validation loss = 0.001019440358504653
Validation loss = 0.0010480462806299329
Validation loss = 0.0010597818763926625
Validation loss = 0.0011550893541425467
Validation loss = 0.001088772900402546
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0010994723998010159
Validation loss = 0.0010807913495227695
Validation loss = 0.0010796128772199154
Validation loss = 0.001077474094927311
Validation loss = 0.00108667544554919
Validation loss = 0.001163580804131925
Validation loss = 0.0010460768826305866
Validation loss = 0.0010476869065314531
Validation loss = 0.0010697151301428676
Validation loss = 0.0010685360757634044
Validation loss = 0.00112481031101197
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0010581365786492825
Validation loss = 0.0010399140883237123
Validation loss = 0.0010883848881348968
Validation loss = 0.0011437012581154704
Validation loss = 0.0010442397324368358
Validation loss = 0.0010389008093625307
Validation loss = 0.0010222066193819046
Validation loss = 0.0010097973281517625
Validation loss = 0.0011173371458426118
Validation loss = 0.001057416433468461
Validation loss = 0.001054737833328545
Validation loss = 0.0010167979635298252
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 27       |
| MaximumReturn | 199      |
| MinimumReturn | 124      |
| TotalSamples  | 96657    |
----------------------------
itr #28 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0011352193541824818
Validation loss = 0.0014009318547323346
Validation loss = 0.0011763632064685225
Validation loss = 0.0011955058434978127
Validation loss = 0.0011797334300354123
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0011980700073763728
Validation loss = 0.0011844808468595147
Validation loss = 0.001173560624010861
Validation loss = 0.0011858311481773853
Validation loss = 0.0012215805472806096
Validation loss = 0.0011791716096922755
Validation loss = 0.0011737229069694877
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0011328380787745118
Validation loss = 0.0011494524078443646
Validation loss = 0.001131724682636559
Validation loss = 0.001169398776255548
Validation loss = 0.00115737144369632
Validation loss = 0.0011517253005877137
Validation loss = 0.0012117704609408975
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0013233778299763799
Validation loss = 0.0013555176556110382
Validation loss = 0.0012583237839862704
Validation loss = 0.0011520832777023315
Validation loss = 0.0011309932451695204
Validation loss = 0.0012090641539543867
Validation loss = 0.0011319349287077785
Validation loss = 0.0011466355063021183
Validation loss = 0.0015354007482528687
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0012609997065737844
Validation loss = 0.0011831134324893355
Validation loss = 0.001161351683549583
Validation loss = 0.0013268940383568406
Validation loss = 0.001134722027927637
Validation loss = 0.001125688781030476
Validation loss = 0.0011718360474333167
Validation loss = 0.0011451700702309608
Validation loss = 0.0011428624857217073
Validation loss = 0.0012012372026219964
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 28       |
| MaximumReturn | 199      |
| MinimumReturn | 113      |
| TotalSamples  | 99990    |
----------------------------
itr #29 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0011983639560639858
Validation loss = 0.0012223919620737433
Validation loss = 0.0011954973451793194
Validation loss = 0.0012276727939024568
Validation loss = 0.0012151928385719657
Validation loss = 0.001291233696974814
Validation loss = 0.0012351807672530413
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0011938732350245118
Validation loss = 0.0012306459248065948
Validation loss = 0.0012157801538705826
Validation loss = 0.0013236702652648091
Validation loss = 0.0012774631613865495
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0015682157827541232
Validation loss = 0.0012217017356306314
Validation loss = 0.0011829574359580874
Validation loss = 0.0012480317382141948
Validation loss = 0.001200036145746708
Validation loss = 0.0012039002031087875
Validation loss = 0.0012391692725941539
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0012258890783414245
Validation loss = 0.0012776842340826988
Validation loss = 0.0012080055894330144
Validation loss = 0.0011909190798178315
Validation loss = 0.0012697417987510562
Validation loss = 0.0012547451769933105
Validation loss = 0.001153641613200307
Validation loss = 0.0012253489112481475
Validation loss = 0.001165863941423595
Validation loss = 0.0011711162514984608
Validation loss = 0.001207738067023456
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.001209320849739015
Validation loss = 0.0012021256843581796
Validation loss = 0.0011908778687939048
Validation loss = 0.0011875875061377883
Validation loss = 0.0012154283467680216
Validation loss = 0.0013903257204219699
Validation loss = 0.0011959255207329988
Validation loss = 0.0012552045518532395
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 29       |
| MaximumReturn | 199      |
| MinimumReturn | 119      |
| TotalSamples  | 103323   |
----------------------------
itr #30 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.001311872387304902
Validation loss = 0.0014048099983483553
Validation loss = 0.0013071121647953987
Validation loss = 0.0013252203352749348
Validation loss = 0.0012972898548468947
Validation loss = 0.001291613094508648
Validation loss = 0.0013201666297391057
Validation loss = 0.0012555605499073863
Validation loss = 0.001284696045331657
Validation loss = 0.0013281297869980335
Validation loss = 0.001278952695429325
Validation loss = 0.0013026556698605418
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0013209717581048608
Validation loss = 0.0012652912409976125
Validation loss = 0.0013973296154290438
Validation loss = 0.0012993875425308943
Validation loss = 0.0013046549865975976
Validation loss = 0.001305223093368113
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0014016364002600312
Validation loss = 0.0013030602131038904
Validation loss = 0.0013225049478933215
Validation loss = 0.0013029886176809669
Validation loss = 0.0012626589741557837
Validation loss = 0.0013496405445039272
Validation loss = 0.0012772339396178722
Validation loss = 0.0012999559985473752
Validation loss = 0.0012567236553877592
Validation loss = 0.001339697977527976
Validation loss = 0.0012994519202038646
Validation loss = 0.0013394590932875872
Validation loss = 0.0013740714639425278
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.001284330035559833
Validation loss = 0.001303424476645887
Validation loss = 0.0012769349850714207
Validation loss = 0.0012619023909792304
Validation loss = 0.0012899443972855806
Validation loss = 0.0012457463890314102
Validation loss = 0.0013130954466760159
Validation loss = 0.0012882760493084788
Validation loss = 0.0013370612869039178
Validation loss = 0.0012488140491768718
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0012893092352896929
Validation loss = 0.0012750073801726103
Validation loss = 0.001298611401580274
Validation loss = 0.001288700383156538
Validation loss = 0.0012707568239420652
Validation loss = 0.0013133813627064228
Validation loss = 0.0013105584075674415
Validation loss = 0.0012453139061108232
Validation loss = 0.001283136080019176
Validation loss = 0.001254748785868287
Validation loss = 0.0012770920293405652
Validation loss = 0.0012498711002990603
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 30       |
| MaximumReturn | 199      |
| MinimumReturn | 111      |
| TotalSamples  | 106656   |
----------------------------
itr #31 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0014358280459418893
Validation loss = 0.0015021522995084524
Validation loss = 0.0015099139418452978
Validation loss = 0.0015429535415023565
Validation loss = 0.0015111127868294716
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0015518669970333576
Validation loss = 0.0014740220503881574
Validation loss = 0.0014802414225414395
Validation loss = 0.0016784952022135258
Validation loss = 0.0014417062047868967
Validation loss = 0.0015258933417499065
Validation loss = 0.0015292285243049264
Validation loss = 0.0014716038713231683
Validation loss = 0.0015302790561690927
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0015482245944440365
Validation loss = 0.0014334475854411721
Validation loss = 0.0014387158444151282
Validation loss = 0.0014248932711780071
Validation loss = 0.0014861966483294964
Validation loss = 0.0014978067483752966
Validation loss = 0.0014555018860846758
Validation loss = 0.0014419868821278214
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0014138444093987346
Validation loss = 0.0014261750038713217
Validation loss = 0.0014450737508013844
Validation loss = 0.0014282428892329335
Validation loss = 0.0014341406058520079
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0014973358483985066
Validation loss = 0.0015567756490781903
Validation loss = 0.0014291706029325724
Validation loss = 0.0014645805349573493
Validation loss = 0.001450142590329051
Validation loss = 0.0014394614845514297
Validation loss = 0.0015458478592336178
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 31       |
| MaximumReturn | 199      |
| MinimumReturn | 114      |
| TotalSamples  | 109989   |
----------------------------
itr #32 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0017327170353382826
Validation loss = 0.0017130911583080888
Validation loss = 0.0016861777985468507
Validation loss = 0.001792030525393784
Validation loss = 0.0018005740130320191
Validation loss = 0.0016893460415303707
Validation loss = 0.0016736957477405667
Validation loss = 0.0016970905708149076
Validation loss = 0.0016829547239467502
Validation loss = 0.0017878992948681116
Validation loss = 0.0016835042042657733
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0016718339174985886
Validation loss = 0.0017335868906229734
Validation loss = 0.0017227483913302422
Validation loss = 0.0016929787816479802
Validation loss = 0.0017078256933018565
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.001768690999597311
Validation loss = 0.0017019404331222177
Validation loss = 0.0017143741715699434
Validation loss = 0.0017334885196760297
Validation loss = 0.0017560679698362947
Validation loss = 0.0017191381193697453
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0017925624269992113
Validation loss = 0.0018073908286169171
Validation loss = 0.0017426811391487718
Validation loss = 0.001694084843620658
Validation loss = 0.001729344017803669
Validation loss = 0.0017538518877699971
Validation loss = 0.0017883848631754518
Validation loss = 0.0018212036229670048
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0018677813932299614
Validation loss = 0.001765011576935649
Validation loss = 0.0016903996001929045
Validation loss = 0.0018309777369722724
Validation loss = 0.0017641442827880383
Validation loss = 0.0016690446063876152
Validation loss = 0.0016878221649676561
Validation loss = 0.0017028965521603823
Validation loss = 0.001697526779025793
Validation loss = 0.001713210018351674
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 32       |
| MaximumReturn | 199      |
| MinimumReturn | 117      |
| TotalSamples  | 113322   |
----------------------------
itr #33 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.001864926889538765
Validation loss = 0.0017561347922310233
Validation loss = 0.001755562494508922
Validation loss = 0.0017389233689755201
Validation loss = 0.0017334123840555549
Validation loss = 0.0018327344441786408
Validation loss = 0.0017449840670451522
Validation loss = 0.0017404978862032294
Validation loss = 0.001759199658408761
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0018230924615636468
Validation loss = 0.00179319828748703
Validation loss = 0.001806295127607882
Validation loss = 0.0017555606318637729
Validation loss = 0.0018774977652356029
Validation loss = 0.0017522983253002167
Validation loss = 0.0017659428995102644
Validation loss = 0.0018067576456815004
Validation loss = 0.001811766647733748
Validation loss = 0.0018283389508724213
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0017835973994806409
Validation loss = 0.0017710317624732852
Validation loss = 0.001857958734035492
Validation loss = 0.002064356580376625
Validation loss = 0.001765451510436833
Validation loss = 0.00179857958573848
Validation loss = 0.0018823243444785476
Validation loss = 0.0017912609037011862
Validation loss = 0.0017571902135387063
Validation loss = 0.0017517410451546311
Validation loss = 0.001773645868524909
Validation loss = 0.0017598718404769897
Validation loss = 0.0018062081653624773
Validation loss = 0.0017329482361674309
Validation loss = 0.001762128435075283
Validation loss = 0.0018274163594469428
Validation loss = 0.0017736684530973434
Validation loss = 0.001749383402056992
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0017393454909324646
Validation loss = 0.0017814015736803412
Validation loss = 0.0017303784843534231
Validation loss = 0.0017153037479147315
Validation loss = 0.0017491623293608427
Validation loss = 0.0017617041012272239
Validation loss = 0.0019129562424495816
Validation loss = 0.0017267164075747132
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0017885415581986308
Validation loss = 0.0017526845913380384
Validation loss = 0.001742284162901342
Validation loss = 0.0017685039201751351
Validation loss = 0.0017268192023038864
Validation loss = 0.0017262583132833242
Validation loss = 0.0018070191144943237
Validation loss = 0.0018053969833999872
Validation loss = 0.001733216573484242
Validation loss = 0.0018705332186073065
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 33       |
| MaximumReturn | 199      |
| MinimumReturn | 120      |
| TotalSamples  | 116655   |
----------------------------
itr #34 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0021617854945361614
Validation loss = 0.00208553415723145
Validation loss = 0.0020741205662488937
Validation loss = 0.0020977261010557413
Validation loss = 0.0020447573624551296
Validation loss = 0.0020466779824346304
Validation loss = 0.002127004787325859
Validation loss = 0.002127033192664385
Validation loss = 0.0020408572163432837
Validation loss = 0.0021174498833715916
Validation loss = 0.0020917931105941534
Validation loss = 0.0022187973372638226
Validation loss = 0.0020754020661115646
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.002059350023046136
Validation loss = 0.002147909952327609
Validation loss = 0.002116787014529109
Validation loss = 0.002112183952704072
Validation loss = 0.002060519065707922
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.002079062396660447
Validation loss = 0.0020995554514229298
Validation loss = 0.002201187890022993
Validation loss = 0.0021866019815206528
Validation loss = 0.002172257285565138
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.002047242596745491
Validation loss = 0.0021191476844251156
Validation loss = 0.002078972291201353
Validation loss = 0.0020936387591063976
Validation loss = 0.002099019940942526
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0021398025564849377
Validation loss = 0.0020727107767015696
Validation loss = 0.0020437845960259438
Validation loss = 0.0020771122071892023
Validation loss = 0.0021098025608807802
Validation loss = 0.0021941072773188353
Validation loss = 0.002109742956236005
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 34       |
| MaximumReturn | 199      |
| MinimumReturn | 126      |
| TotalSamples  | 119988   |
----------------------------
itr #35 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0023907574359327555
Validation loss = 0.0023894456680864096
Validation loss = 0.0023936887737363577
Validation loss = 0.002446624683216214
Validation loss = 0.0024259903002530336
Validation loss = 0.0023511680774390697
Validation loss = 0.002388672437518835
Validation loss = 0.002323910128325224
Validation loss = 0.002422966994345188
Validation loss = 0.002362001221626997
Validation loss = 0.002483977470546961
Validation loss = 0.002360171638429165
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.00244365306571126
Validation loss = 0.0024274869356304407
Validation loss = 0.002393478062003851
Validation loss = 0.0024485315661877394
Validation loss = 0.0024670916609466076
Validation loss = 0.0023630147334188223
Validation loss = 0.0024071030784398317
Validation loss = 0.002382267266511917
Validation loss = 0.0024392777122557163
Validation loss = 0.002365407068282366
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.002512675942853093
Validation loss = 0.0023850335273891687
Validation loss = 0.002403730060905218
Validation loss = 0.002433008048683405
Validation loss = 0.002374225063249469
Validation loss = 0.00240138778463006
Validation loss = 0.0023975407239049673
Validation loss = 0.0023703421466052532
Validation loss = 0.002422565361484885
Validation loss = 0.0023692327085882425
Validation loss = 0.0024055680260062218
Validation loss = 0.0024174435529857874
Validation loss = 0.0024323647376149893
Validation loss = 0.002378570381551981
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0023685942869633436
Validation loss = 0.0027081191074103117
Validation loss = 0.002426534192636609
Validation loss = 0.002381965285167098
Validation loss = 0.002393909962847829
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.002486238721758127
Validation loss = 0.0024211027193814516
Validation loss = 0.002335600322112441
Validation loss = 0.0023920394014567137
Validation loss = 0.002365164691582322
Validation loss = 0.0024075934197753668
Validation loss = 0.0023720439057797194
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 35       |
| MaximumReturn | 199      |
| MinimumReturn | 124      |
| TotalSamples  | 123321   |
----------------------------
itr #36 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0028991324361413717
Validation loss = 0.002936028642579913
Validation loss = 0.0029181528370827436
Validation loss = 0.002900119638070464
Validation loss = 0.0030153137631714344
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.003179163672029972
Validation loss = 0.002947137225419283
Validation loss = 0.0029565575532615185
Validation loss = 0.0030086084734648466
Validation loss = 0.003011807333678007
Validation loss = 0.003020608564838767
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0030081733129918575
Validation loss = 0.0030728699639439583
Validation loss = 0.002984590595588088
Validation loss = 0.003013110952451825
Validation loss = 0.0028943230863660574
Validation loss = 0.0028869910165667534
Validation loss = 0.0030077307019382715
Validation loss = 0.002989218570291996
Validation loss = 0.00295716873370111
Validation loss = 0.002960264217108488
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0031098395120352507
Validation loss = 0.00295160710811615
Validation loss = 0.0030546854250133038
Validation loss = 0.002929229522123933
Validation loss = 0.0029392128344625235
Validation loss = 0.002915778895840049
Validation loss = 0.0029258164577186108
Validation loss = 0.0029471456073224545
Validation loss = 0.003012726316228509
Validation loss = 0.0030247257091104984
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.002916542813181877
Validation loss = 0.0029311436228454113
Validation loss = 0.0029557303059846163
Validation loss = 0.0029787342064082623
Validation loss = 0.0029580036643892527
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 36       |
| MaximumReturn | 199      |
| MinimumReturn | 129      |
| TotalSamples  | 126654   |
----------------------------
itr #37 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0034586028195917606
Validation loss = 0.003474250202998519
Validation loss = 0.0034482351038604975
Validation loss = 0.0034620417281985283
Validation loss = 0.003437262959778309
Validation loss = 0.0034655199851840734
Validation loss = 0.003477808553725481
Validation loss = 0.0034585832618176937
Validation loss = 0.0034489159006625414
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.003494532313197851
Validation loss = 0.0035297598224133253
Validation loss = 0.003604001132771373
Validation loss = 0.0036744510289281607
Validation loss = 0.0035823199432343245
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0034887385554611683
Validation loss = 0.00352835631929338
Validation loss = 0.003471994772553444
Validation loss = 0.0035030574072152376
Validation loss = 0.0034563508816063404
Validation loss = 0.0034058773890137672
Validation loss = 0.0035041705705225468
Validation loss = 0.0034325711894780397
Validation loss = 0.0035455951001495123
Validation loss = 0.003462966065853834
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0036233754362910986
Validation loss = 0.003524571191519499
Validation loss = 0.0034523962531238794
Validation loss = 0.003449977608397603
Validation loss = 0.0034902491606771946
Validation loss = 0.003548529464751482
Validation loss = 0.0034410483203828335
Validation loss = 0.0034663400147110224
Validation loss = 0.0034108799882233143
Validation loss = 0.0034609066788107157
Validation loss = 0.003472162876278162
Validation loss = 0.003488832153379917
Validation loss = 0.0034808465279638767
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.003414897946640849
Validation loss = 0.003474879777058959
Validation loss = 0.003539383877068758
Validation loss = 0.0034902661573141813
Validation loss = 0.0034178902860730886
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 37       |
| MaximumReturn | 197      |
| MinimumReturn | 129      |
| TotalSamples  | 129987   |
----------------------------
itr #38 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0038725961931049824
Validation loss = 0.0038369193207472563
Validation loss = 0.0037768851034343243
Validation loss = 0.0038070434238761663
Validation loss = 0.003817778080701828
Validation loss = 0.003904359182342887
Validation loss = 0.0037726531736552715
Validation loss = 0.003794490359723568
Validation loss = 0.003806912573054433
Validation loss = 0.0038182325661182404
Validation loss = 0.003802911378443241
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0039044544100761414
Validation loss = 0.003921799827367067
Validation loss = 0.0038396641612052917
Validation loss = 0.0038143377751111984
Validation loss = 0.0037929946556687355
Validation loss = 0.0038242926821112633
Validation loss = 0.003785009030252695
Validation loss = 0.003770293900743127
Validation loss = 0.003857308067381382
Validation loss = 0.0038310980889946222
Validation loss = 0.0037998652551323175
Validation loss = 0.003931713290512562
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.003783404128625989
Validation loss = 0.0037738957908004522
Validation loss = 0.0037840232253074646
Validation loss = 0.003748477203771472
Validation loss = 0.0037706296425312757
Validation loss = 0.003785473294556141
Validation loss = 0.003946929704397917
Validation loss = 0.003770579816773534
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.003936134744435549
Validation loss = 0.0038902873639017344
Validation loss = 0.003907929174602032
Validation loss = 0.0038074778858572245
Validation loss = 0.003868150757625699
Validation loss = 0.003768504597246647
Validation loss = 0.003807916771620512
Validation loss = 0.003818425815552473
Validation loss = 0.0038909045979380608
Validation loss = 0.003801514394581318
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0038361819460988045
Validation loss = 0.0037334077060222626
Validation loss = 0.0038834824226796627
Validation loss = 0.0037839608266949654
Validation loss = 0.003806980326771736
Validation loss = 0.0037272446788847446
Validation loss = 0.003908464219421148
Validation loss = 0.003801628714427352
Validation loss = 0.003829433349892497
Validation loss = 0.0037532499991357327
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 38       |
| MaximumReturn | 199      |
| MinimumReturn | 130      |
| TotalSamples  | 133320   |
----------------------------
itr #39 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.003970130812376738
Validation loss = 0.003938449081033468
Validation loss = 0.00397981284186244
Validation loss = 0.0039438530802726746
Validation loss = 0.004027827177196741
Validation loss = 0.0039734505116939545
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.004011667333543301
Validation loss = 0.003969588316977024
Validation loss = 0.0041008018888533115
Validation loss = 0.0040730577893555164
Validation loss = 0.0040950048714876175
Validation loss = 0.004021467640995979
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.004013666417449713
Validation loss = 0.00396951986476779
Validation loss = 0.00395525898784399
Validation loss = 0.004006037954241037
Validation loss = 0.003966392483562231
Validation loss = 0.0039605870842933655
Validation loss = 0.004041193053126335
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.003965758718550205
Validation loss = 0.004036066588014364
Validation loss = 0.003947494085878134
Validation loss = 0.00406420323997736
Validation loss = 0.003944015130400658
Validation loss = 0.004048284143209457
Validation loss = 0.003967634867876768
Validation loss = 0.0039538913406431675
Validation loss = 0.003960066474974155
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0040688710287213326
Validation loss = 0.004006959497928619
Validation loss = 0.004156550858169794
Validation loss = 0.00401580985635519
Validation loss = 0.003932645078748465
Validation loss = 0.003989584743976593
Validation loss = 0.004038514103740454
Validation loss = 0.004052502568811178
Validation loss = 0.003996293060481548
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 39       |
| MaximumReturn | 199      |
| MinimumReturn | 126      |
| TotalSamples  | 136653   |
----------------------------
