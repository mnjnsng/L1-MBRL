Logging to experiments/pendulum/Pendulum_Action_Noise/w100e0.5_seed2231
Print configuration .....
{'env_name': 'pendulum', 'random_seeds': [3214, 2431, 2531, 2231], 'save_variables': False, 'model_save_dir': '/tmp/pendulum_models/', 'restore_variables': False, 'start_onpol_iter': 0, 'onpol_iters': 40, 'num_path_random': 25, 'num_path_onpol': 25, 'env_horizon': 200, 'max_train_data': 200000, 'max_val_data': 100000, 'discard_ratio': 0.0, 'dynamics': {'pre_training': {'mode': 'intrinsic_reward', 'itr': 0, 'policy_itr': 20}, 'model': 'nn', 'ensemble': True, 'ensemble_model_count': 5, 'enable_particle_ensemble': True, 'particles': 5, 'obs_var': 1.0, 'intrinsic_reward_coeff': 1.0, 'ita': 1.0, 'mode': 'random', 'val': True, 'n_layers': 4, 'hidden_size': 1000, 'activation': 'relu', 'batch_size': 1000, 'learning_rate': 0.001, 'reg_coeff': 0.0, 'epochs': 200, 'kfac_params': {'learning_rate': 0.1, 'damping': 0.001, 'momentum': 0.9, 'kl_clip': 0.0001, 'cov_ema_decay': 0.99}}, 'policy': {'network_shape': [64, 64], 'init_logstd': 0.0, 'activation': 'tanh', 'reinitialize_every_itr': False}, 'trpo': {'horizon': 200, 'gamma': 0.99, 'step_size': 0.01, 'iterations': 20, 'batch_size': 50000, 'gae': 0.95, 'visualization': False, 'visualize_iterations': [0]}, 'algo': 'trpo'}
Generating random rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating random rollouts.
Creating normalization for training data.
Done creating normalization for training data.
Particle ensemble enabled? True
An ensemble of 5 dynamics model <class 'model.dynamics.NNDynamicsModel'> initialized
Train dynamics model with intrinsic reward only? False
Pre-training enabled. Using only intrinsic reward.
Pre-training dynamics model for 0 iterations...
Done pre-training dynamics model.
Using external reward only.
itr #0 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.2911631166934967
Validation loss = 0.03145540505647659
Validation loss = 0.011750481091439724
Validation loss = 0.0038653952069580555
Validation loss = 0.0018043677555397153
Validation loss = 0.001237501041032374
Validation loss = 0.0009898190619423985
Validation loss = 0.0009280219674110413
Validation loss = 0.0009240066865459085
Validation loss = 0.0009197845356538892
Validation loss = 0.0008459676173515618
Validation loss = 0.0008247272344306111
Validation loss = 0.0008033124613575637
Validation loss = 0.0009554974385537207
Validation loss = 0.001809102832339704
Validation loss = 0.001002768985927105
Validation loss = 0.0009306262363679707
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.29317399859428406
Validation loss = 0.021609527990221977
Validation loss = 0.007265022955834866
Validation loss = 0.002602004213258624
Validation loss = 0.0013141067465767264
Validation loss = 0.0009968345984816551
Validation loss = 0.0008511369815096259
Validation loss = 0.0008746779640205204
Validation loss = 0.0008809819119051099
Validation loss = 0.00877290591597557
Validation loss = 0.001251412439160049
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.2640317380428314
Validation loss = 0.03927363455295563
Validation loss = 0.010220570489764214
Validation loss = 0.003209143877029419
Validation loss = 0.0016611820319667459
Validation loss = 0.001099244924262166
Validation loss = 0.0009975507855415344
Validation loss = 0.0008757718605920672
Validation loss = 0.0008740415796637535
Validation loss = 0.0008739212644286454
Validation loss = 0.0013369470834732056
Validation loss = 0.0020032173488289118
Validation loss = 0.0008408479625359178
Validation loss = 0.0008892445475794375
Validation loss = 0.0012837923131883144
Validation loss = 0.0016454001888632774
Validation loss = 0.0008528765174560249
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.26086893677711487
Validation loss = 0.027401108294725418
Validation loss = 0.00713831651955843
Validation loss = 0.0026920537929981947
Validation loss = 0.001365070347674191
Validation loss = 0.001013518893159926
Validation loss = 0.0008895781938917935
Validation loss = 0.0008637894061394036
Validation loss = 0.0008545771706849337
Validation loss = 0.0008962114807218313
Validation loss = 0.0010174381313845515
Validation loss = 0.0009853565134108067
Validation loss = 0.0009191025164909661
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.291927695274353
Validation loss = 0.024758076295256615
Validation loss = 0.008859380148351192
Validation loss = 0.0032276769634336233
Validation loss = 0.001510852831415832
Validation loss = 0.0010701549472287297
Validation loss = 0.0009202170185744762
Validation loss = 0.0008764665690250695
Validation loss = 0.0008855105843394995
Validation loss = 0.0008349748677574098
Validation loss = 0.000902149360626936
Validation loss = 0.003821822814643383
Validation loss = 0.0010823949705809355
Validation loss = 0.0009023188031278551
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 163      |
| Iteration     | 0        |
| MaximumReturn | 198      |
| MinimumReturn | 110      |
| TotalSamples  | 6666     |
----------------------------
itr #1 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.01460233610123396
Validation loss = 0.0019645027350634336
Validation loss = 0.0008066024747677147
Validation loss = 0.0007305718609131873
Validation loss = 0.0006849709898233414
Validation loss = 0.000823462672997266
Validation loss = 0.0007332843379117548
Validation loss = 0.0011526342714205384
Validation loss = 0.001104797120206058
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.012515977956354618
Validation loss = 0.0018097107531502843
Validation loss = 0.0009387717582285404
Validation loss = 0.0009306066203862429
Validation loss = 0.0007412030827254057
Validation loss = 0.0009523739572614431
Validation loss = 0.0012110084062442183
Validation loss = 0.0006384057342074811
Validation loss = 0.0007798593142069876
Validation loss = 0.0008537489920854568
Validation loss = 0.0013569811126217246
Validation loss = 0.0033386023715138435
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.014732379466295242
Validation loss = 0.0017471933970227838
Validation loss = 0.0008058820967562497
Validation loss = 0.0007208944880403578
Validation loss = 0.0014866115525364876
Validation loss = 0.0006740142125636339
Validation loss = 0.0007003925857134163
Validation loss = 0.0010363301262259483
Validation loss = 0.001746825990267098
Validation loss = 0.0015191863058134913
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.01802189089357853
Validation loss = 0.0025884362403303385
Validation loss = 0.0008880384266376495
Validation loss = 0.0006634313031099737
Validation loss = 0.0006567484233528376
Validation loss = 0.0007117164204828441
Validation loss = 0.00802686344832182
Validation loss = 0.0015021493891254067
Validation loss = 0.0007070667925290763
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.02004983462393284
Validation loss = 0.0026664556935429573
Validation loss = 0.0009363344870507717
Validation loss = 0.0006751805194653571
Validation loss = 0.0007891771383583546
Validation loss = 0.002594497986137867
Validation loss = 0.0007588494918309152
Validation loss = 0.0008816611371003091
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 1        |
| MaximumReturn | 199      |
| MinimumReturn | 135      |
| TotalSamples  | 9999     |
----------------------------
itr #2 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.004906247835606337
Validation loss = 0.0008443398401141167
Validation loss = 0.0006094148266129196
Validation loss = 0.0007364759803749621
Validation loss = 0.0026684915646910667
Validation loss = 0.0007604722632095218
Validation loss = 0.0007137232460081577
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.006321035325527191
Validation loss = 0.0006329613970592618
Validation loss = 0.000983283156529069
Validation loss = 0.0008681323379278183
Validation loss = 0.001024945406243205
Validation loss = 0.0009760740213096142
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.005351886618882418
Validation loss = 0.0011376285692676902
Validation loss = 0.0006096800207160413
Validation loss = 0.0014410221483558416
Validation loss = 0.000713291869033128
Validation loss = 0.0017579194391146302
Validation loss = 0.0012269261060282588
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.006438466254621744
Validation loss = 0.0008224562625400722
Validation loss = 0.0009719150839373469
Validation loss = 0.0007226188899949193
Validation loss = 0.0027299760840833187
Validation loss = 0.001407706760801375
Validation loss = 0.0007617574883624911
Validation loss = 0.0007872901624068618
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.006439493503421545
Validation loss = 0.0008786924881860614
Validation loss = 0.0006807318422943354
Validation loss = 0.0015576418954879045
Validation loss = 0.0012297176290303469
Validation loss = 0.0009290194138884544
Validation loss = 0.002418766263872385
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 2        |
| MaximumReturn | 199      |
| MinimumReturn | 109      |
| TotalSamples  | 13332    |
----------------------------
itr #3 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.003721958724781871
Validation loss = 0.0008670646348036826
Validation loss = 0.0011547424364835024
Validation loss = 0.0013135840417817235
Validation loss = 0.0014946538722142577
Validation loss = 0.0011733928695321083
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.008397819474339485
Validation loss = 0.0008102040737867355
Validation loss = 0.0008957439567893744
Validation loss = 0.0006075824494473636
Validation loss = 0.0013037921162322164
Validation loss = 0.0013076855102553964
Validation loss = 0.0012895645340904593
Validation loss = 0.0011392654851078987
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.004055025987327099
Validation loss = 0.0015851681819185615
Validation loss = 0.0005702046910300851
Validation loss = 0.0014679556479677558
Validation loss = 0.0008728312677703798
Validation loss = 0.0007301612640731037
Validation loss = 0.0016329459613189101
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0022787367925047874
Validation loss = 0.0009984798962250352
Validation loss = 0.00421467749401927
Validation loss = 0.000959210388828069
Validation loss = 0.0010860624024644494
Validation loss = 0.0008067445014603436
Validation loss = 0.002091759815812111
Validation loss = 0.0005025237915106118
Validation loss = 0.0011348784901201725
Validation loss = 0.0005785874673165381
Validation loss = 0.0020091449841856956
Validation loss = 0.000996079877950251
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0048314970917999744
Validation loss = 0.0006758854724466801
Validation loss = 0.0009458024869672954
Validation loss = 0.001362791284918785
Validation loss = 0.001192062278278172
Validation loss = 0.0009954829001799226
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 3        |
| MaximumReturn | 199      |
| MinimumReturn | 117      |
| TotalSamples  | 16665    |
----------------------------
itr #4 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0025273559149354696
Validation loss = 0.0007175116916187108
Validation loss = 0.0014426407869905233
Validation loss = 0.0008709825342521071
Validation loss = 0.0014281219337135553
Validation loss = 0.001155191333964467
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0012570098042488098
Validation loss = 0.0012314078630879521
Validation loss = 0.0022904458455741405
Validation loss = 0.0008136401884257793
Validation loss = 0.0017834813334047794
Validation loss = 0.0014536003582179546
Validation loss = 0.0005502298008650541
Validation loss = 0.0010517018381506205
Validation loss = 0.0009585816878825426
Validation loss = 0.000679534103255719
Validation loss = 0.0005026338621973991
Validation loss = 0.000900982937309891
Validation loss = 0.0016574080800637603
Validation loss = 0.0005997314001433551
Validation loss = 0.0006723965634591877
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0031378723215311766
Validation loss = 0.0006889795768074691
Validation loss = 0.001243604812771082
Validation loss = 0.0006693862960673869
Validation loss = 0.0012731777969747782
Validation loss = 0.001041848910972476
Validation loss = 0.001238140743225813
Validation loss = 0.0014261427568271756
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.006694898009300232
Validation loss = 0.0009625808452256024
Validation loss = 0.0009756811778061092
Validation loss = 0.0006449282518588006
Validation loss = 0.0007543496321886778
Validation loss = 0.0008512651547789574
Validation loss = 0.0009395547676831484
Validation loss = 0.000882084365002811
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.017509816214442253
Validation loss = 0.0005445580463856459
Validation loss = 0.0006103767664171755
Validation loss = 0.0007986439159139991
Validation loss = 0.0013928397092968225
Validation loss = 0.0014582397416234016
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 4        |
| MaximumReturn | 199      |
| MinimumReturn | 124      |
| TotalSamples  | 19998    |
----------------------------
itr #5 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0017687128856778145
Validation loss = 0.0012192359426990151
Validation loss = 0.0014226797502487898
Validation loss = 0.0008183379541151226
Validation loss = 0.0006023570313118398
Validation loss = 0.000673967064358294
Validation loss = 0.001294427434913814
Validation loss = 0.0008527023601345718
Validation loss = 0.0010075161699205637
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0015166933881118894
Validation loss = 0.0009931892855092883
Validation loss = 0.0005562552832998335
Validation loss = 0.0007760378648526967
Validation loss = 0.0010940228821709752
Validation loss = 0.0013037691824138165
Validation loss = 0.0004977030912414193
Validation loss = 0.0012281967792659998
Validation loss = 0.0013450243277475238
Validation loss = 0.0008424366824328899
Validation loss = 0.0006084978231228888
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0011070908512920141
Validation loss = 0.0008840759983286262
Validation loss = 0.0005938020767644048
Validation loss = 0.000799557485152036
Validation loss = 0.0015305856941267848
Validation loss = 0.0018595395376905799
Validation loss = 0.001075690845027566
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0009760114480741322
Validation loss = 0.0016822507604956627
Validation loss = 0.0006835564854554832
Validation loss = 0.0005170608637854457
Validation loss = 0.0012167742243036628
Validation loss = 0.0007294517708942294
Validation loss = 0.0010878890752792358
Validation loss = 0.0008572593214921653
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0037393649108707905
Validation loss = 0.0005922824493609369
Validation loss = 0.0009157871827483177
Validation loss = 0.0009047403000295162
Validation loss = 0.0008393693715333939
Validation loss = 0.0013148606522008777
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 5        |
| MaximumReturn | 199      |
| MinimumReturn | 125      |
| TotalSamples  | 23331    |
----------------------------
itr #6 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0012960458407178521
Validation loss = 0.0024096856359392405
Validation loss = 0.0009529395611025393
Validation loss = 0.0007758474093861878
Validation loss = 0.0009596614399924874
Validation loss = 0.00070013856748119
Validation loss = 0.0006208497798070312
Validation loss = 0.00101273856125772
Validation loss = 0.0005802117520943284
Validation loss = 0.0007139568915590644
Validation loss = 0.0007473274599760771
Validation loss = 0.0012307658325880766
Validation loss = 0.0009255351615138352
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.001110175042413175
Validation loss = 0.0007753192912787199
Validation loss = 0.0005172481178306043
Validation loss = 0.001005534897558391
Validation loss = 0.002328932750970125
Validation loss = 0.0008724162471480668
Validation loss = 0.001020097522996366
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0005624146433547139
Validation loss = 0.0006056310958229005
Validation loss = 0.0005579293938353658
Validation loss = 0.0009439563727937639
Validation loss = 0.0011404583929106593
Validation loss = 0.000555769947823137
Validation loss = 0.001076218904927373
Validation loss = 0.0006134137511253357
Validation loss = 0.001438095816411078
Validation loss = 0.0007149936864152551
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0016374480910599232
Validation loss = 0.000738062197342515
Validation loss = 0.0008282585185952485
Validation loss = 0.000807473377790302
Validation loss = 0.0009169306140393019
Validation loss = 0.0009104166529141366
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0015344732673838735
Validation loss = 0.0007567446446046233
Validation loss = 0.001136934501118958
Validation loss = 0.0023853143211454153
Validation loss = 0.001125660608522594
Validation loss = 0.0006692152819596231
Validation loss = 0.001316045061685145
Validation loss = 0.0011904578423127532
Validation loss = 0.0008746206876821816
Validation loss = 0.0009124355856329203
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 179      |
| Iteration     | 6        |
| MaximumReturn | 199      |
| MinimumReturn | 132      |
| TotalSamples  | 26664    |
----------------------------
itr #7 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0007871185662224889
Validation loss = 0.0006033441750332713
Validation loss = 0.001335299457423389
Validation loss = 0.0005123758455738425
Validation loss = 0.0009108531521633267
Validation loss = 0.0005576157127507031
Validation loss = 0.0008798509370535612
Validation loss = 0.0009754851926118135
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.001302578253671527
Validation loss = 0.0008623155299574137
Validation loss = 0.0004938757629133761
Validation loss = 0.0011628986103460193
Validation loss = 0.0005843512481078506
Validation loss = 0.0007780861924402416
Validation loss = 0.0009771832264959812
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0014789607375860214
Validation loss = 0.0007041748613119125
Validation loss = 0.0007485201931558549
Validation loss = 0.000629064510576427
Validation loss = 0.0009316652431152761
Validation loss = 0.0006181048811413348
Validation loss = 0.0008525730227120221
Validation loss = 0.0009169457480311394
Validation loss = 0.0007006911328062415
Validation loss = 0.0005273089627735317
Validation loss = 0.0005848127766512334
Validation loss = 0.0007138996734283864
Validation loss = 0.00041839320329017937
Validation loss = 0.001522109960205853
Validation loss = 0.0006241981172934175
Validation loss = 0.0010190110187977552
Validation loss = 0.0009968301746994257
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0010582396062090993
Validation loss = 0.001048794249072671
Validation loss = 0.0008480770047754049
Validation loss = 0.0010212443303316832
Validation loss = 0.0008839927613735199
Validation loss = 0.0004986781859770417
Validation loss = 0.0011100771371275187
Validation loss = 0.0009885106701403856
Validation loss = 0.0006405409658327699
Validation loss = 0.0007506686961278319
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0011387517442926764
Validation loss = 0.0008035731152631342
Validation loss = 0.0005951850907877088
Validation loss = 0.0007104785763658583
Validation loss = 0.0005847143474966288
Validation loss = 0.0008039170643314719
Validation loss = 0.0008737460593692958
Validation loss = 0.0009595294250175357
Validation loss = 0.0004776637360919267
Validation loss = 0.002599315019324422
Validation loss = 0.0008786434773355722
Validation loss = 0.0006266803829930723
Validation loss = 0.0009047106141224504
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 7        |
| MaximumReturn | 199      |
| MinimumReturn | 118      |
| TotalSamples  | 29997    |
----------------------------
itr #8 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.001627987832762301
Validation loss = 0.001119744498282671
Validation loss = 0.0008757283794693649
Validation loss = 0.0021760996896773577
Validation loss = 0.00042985708569176495
Validation loss = 0.0012530036037787795
Validation loss = 0.0007884034421294928
Validation loss = 0.0005708251846954226
Validation loss = 0.001889951410703361
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006913244142197073
Validation loss = 0.001178294187411666
Validation loss = 0.0007233120268210769
Validation loss = 0.0007097788620740175
Validation loss = 0.0006427967455238104
Validation loss = 0.000677334435749799
Validation loss = 0.0005196685087867081
Validation loss = 0.0006164356018416584
Validation loss = 0.0008842799579724669
Validation loss = 0.0006227156263776124
Validation loss = 0.0006879972061142325
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0008838432258926332
Validation loss = 0.0008050633477978408
Validation loss = 0.0006087496294640005
Validation loss = 0.00063492072513327
Validation loss = 0.000879953324329108
Validation loss = 0.0008533947984687984
Validation loss = 0.000742200470995158
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007731126970611513
Validation loss = 0.0005998265114612877
Validation loss = 0.0012319142697378993
Validation loss = 0.0005422915564849973
Validation loss = 0.0012071909150108695
Validation loss = 0.0005613128305412829
Validation loss = 0.0005093683721497655
Validation loss = 0.0005806606495752931
Validation loss = 0.0011340552009642124
Validation loss = 0.0007259121048264205
Validation loss = 0.0010508585255593061
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0013219622196629643
Validation loss = 0.0005211380193941295
Validation loss = 0.0009200095082633197
Validation loss = 0.0004732087254524231
Validation loss = 0.0007786779897287488
Validation loss = 0.000883418892044574
Validation loss = 0.0012671038275584579
Validation loss = 0.0008713704883120954
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 8        |
| MaximumReturn | 199      |
| MinimumReturn | 126      |
| TotalSamples  | 33330    |
----------------------------
itr #9 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006231500301510096
Validation loss = 0.0009294158080592752
Validation loss = 0.0010352699318900704
Validation loss = 0.000565964262932539
Validation loss = 0.0008105521555989981
Validation loss = 0.0005857813521288335
Validation loss = 0.0005483379354700446
Validation loss = 0.0010559752117842436
Validation loss = 0.0005643648328259587
Validation loss = 0.0006402106373570859
Validation loss = 0.001635046093724668
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005314293084666133
Validation loss = 0.0009352192864753306
Validation loss = 0.000817564083263278
Validation loss = 0.0009009386412799358
Validation loss = 0.0005170697113499045
Validation loss = 0.0005920031107962132
Validation loss = 0.0011337078176438808
Validation loss = 0.0013934802263975143
Validation loss = 0.0006835487438365817
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0007415184518322349
Validation loss = 0.0006538731977343559
Validation loss = 0.000994545640423894
Validation loss = 0.0007648338796570897
Validation loss = 0.0013441848568618298
Validation loss = 0.0004387479857541621
Validation loss = 0.0005266923690214753
Validation loss = 0.0006868516793474555
Validation loss = 0.0014444736298173666
Validation loss = 0.0005729499971494079
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0009104764321818948
Validation loss = 0.0005234690615907311
Validation loss = 0.0011796732433140278
Validation loss = 0.0006032707169651985
Validation loss = 0.0005469926400110126
Validation loss = 0.0009871807415038347
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0007901945500634611
Validation loss = 0.0006345805013552308
Validation loss = 0.0009873744565993547
Validation loss = 0.0006571264239028096
Validation loss = 0.0006828265613876283
Validation loss = 0.0007825496722944081
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 177      |
| Iteration     | 9        |
| MaximumReturn | 199      |
| MinimumReturn | 124      |
| TotalSamples  | 36663    |
----------------------------
itr #10 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0007873638533055782
Validation loss = 0.0010782027384266257
Validation loss = 0.0007199638057500124
Validation loss = 0.0007045574020594358
Validation loss = 0.0012005469761788845
Validation loss = 0.0007044768426567316
Validation loss = 0.0007983880932442844
Validation loss = 0.000967250729445368
Validation loss = 0.001010590116493404
Validation loss = 0.0009282443206757307
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0007779289735481143
Validation loss = 0.0007606158615089953
Validation loss = 0.0008944073342718184
Validation loss = 0.0008152866503223777
Validation loss = 0.000850691634695977
Validation loss = 0.0006920139421708882
Validation loss = 0.0009767310693860054
Validation loss = 0.00073713791789487
Validation loss = 0.000698870571795851
Validation loss = 0.000689710199367255
Validation loss = 0.0007576415664516389
Validation loss = 0.0008504448924213648
Validation loss = 0.000804262119345367
Validation loss = 0.0006634809542447329
Validation loss = 0.0006374127115122974
Validation loss = 0.000654378964100033
Validation loss = 0.0007546889828518033
Validation loss = 0.0010056679602712393
Validation loss = 0.0009134456049650908
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0008442493854090571
Validation loss = 0.0013697075191885233
Validation loss = 0.0006960371392779052
Validation loss = 0.0007133758626878262
Validation loss = 0.0008569387719035149
Validation loss = 0.0007247838075272739
Validation loss = 0.0010492050787433982
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0008970252820290625
Validation loss = 0.000929525529500097
Validation loss = 0.0009914174443110824
Validation loss = 0.0008131774375215173
Validation loss = 0.000849834643304348
Validation loss = 0.0007984373951330781
Validation loss = 0.001065601478330791
Validation loss = 0.0008611435769125819
Validation loss = 0.0008217894937843084
Validation loss = 0.0009808423928916454
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00111569173168391
Validation loss = 0.0007903635851107538
Validation loss = 0.0006679524085484445
Validation loss = 0.0012755183270201087
Validation loss = 0.0015437747351825237
Validation loss = 0.0009889333741739392
Validation loss = 0.0009677038760855794
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 10       |
| MaximumReturn | 199      |
| MinimumReturn | 122      |
| TotalSamples  | 39996    |
----------------------------
itr #11 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0009367993916384876
Validation loss = 0.0010019948240369558
Validation loss = 0.0012129158712923527
Validation loss = 0.0010119287762790918
Validation loss = 0.0010668017202988267
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0009707932476885617
Validation loss = 0.000954160001128912
Validation loss = 0.0008018998196348548
Validation loss = 0.0007248827023431659
Validation loss = 0.000868361268658191
Validation loss = 0.0008125078747980297
Validation loss = 0.000870650343131274
Validation loss = 0.0010866547236219049
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0014233645051717758
Validation loss = 0.00077499431790784
Validation loss = 0.000971382309217006
Validation loss = 0.0010666821617633104
Validation loss = 0.0009879035642370582
Validation loss = 0.0009553767740726471
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0010354443220421672
Validation loss = 0.0009047942003235221
Validation loss = 0.000802790280431509
Validation loss = 0.0010717903496697545
Validation loss = 0.0008836804772727191
Validation loss = 0.0007714630919508636
Validation loss = 0.000893201504368335
Validation loss = 0.000798721972387284
Validation loss = 0.0009156206506304443
Validation loss = 0.0009350894251838326
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0009230697760358453
Validation loss = 0.0015613387804478407
Validation loss = 0.000982587574981153
Validation loss = 0.0008827672572806478
Validation loss = 0.0008464292623102665
Validation loss = 0.0008379222126677632
Validation loss = 0.0011896456126123667
Validation loss = 0.0010180729441344738
Validation loss = 0.0015733797335997224
Validation loss = 0.0009603613871149719
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 164      |
| Iteration     | 11       |
| MaximumReturn | 198      |
| MinimumReturn | 123      |
| TotalSamples  | 43329    |
----------------------------
itr #12 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0012290043523535132
Validation loss = 0.0009661194635555148
Validation loss = 0.0011223913170397282
Validation loss = 0.0009253737516701221
Validation loss = 0.0008983516017906368
Validation loss = 0.0009086887584999204
Validation loss = 0.0009326216531917453
Validation loss = 0.0011153332889080048
Validation loss = 0.0010915680322796106
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0010440042242407799
Validation loss = 0.0012158869067206979
Validation loss = 0.0009077326976694167
Validation loss = 0.000964950944762677
Validation loss = 0.0011242303298786283
Validation loss = 0.0008867670549079776
Validation loss = 0.0009916661074385047
Validation loss = 0.001124404021538794
Validation loss = 0.0010520309442654252
Validation loss = 0.0010406149085611105
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0012372963828966022
Validation loss = 0.0014974477235227823
Validation loss = 0.001106633455492556
Validation loss = 0.0010394623968750238
Validation loss = 0.0009546629153192043
Validation loss = 0.0012229018611833453
Validation loss = 0.0012143919011577964
Validation loss = 0.0012627068208530545
Validation loss = 0.0010694412048906088
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.001205492066219449
Validation loss = 0.001025757985189557
Validation loss = 0.0010961859952658415
Validation loss = 0.0011593018425628543
Validation loss = 0.0011018665973097086
Validation loss = 0.0012633909936994314
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0010132944444194436
Validation loss = 0.0010621147230267525
Validation loss = 0.0015777633525431156
Validation loss = 0.0010136885102838278
Validation loss = 0.0010156893404200673
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 12       |
| MaximumReturn | 199      |
| MinimumReturn | 124      |
| TotalSamples  | 46662    |
----------------------------
itr #13 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0014757715398445725
Validation loss = 0.0010790482629090548
Validation loss = 0.0013238448882475495
Validation loss = 0.0011238792212679982
Validation loss = 0.0011514159850776196
Validation loss = 0.0009748045704327524
Validation loss = 0.0010754192480817437
Validation loss = 0.0011108673643320799
Validation loss = 0.0011080179829150438
Validation loss = 0.0012434134259819984
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0011537899263203144
Validation loss = 0.0010654573561623693
Validation loss = 0.0009706589626148343
Validation loss = 0.0011645474005490541
Validation loss = 0.0010108787100762129
Validation loss = 0.0010470905108377337
Validation loss = 0.0009607556858099997
Validation loss = 0.0012369792675599456
Validation loss = 0.0009631231660023332
Validation loss = 0.0010022020433098078
Validation loss = 0.001177528640255332
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0011063164565712214
Validation loss = 0.001033649081364274
Validation loss = 0.0022942267823964357
Validation loss = 0.0013846771325916052
Validation loss = 0.0010760078439489007
Validation loss = 0.0010209890315309167
Validation loss = 0.0012397542595863342
Validation loss = 0.0010548752034083009
Validation loss = 0.0010703877778723836
Validation loss = 0.0010878558969125152
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0010408483212813735
Validation loss = 0.0010891379788517952
Validation loss = 0.001115578692406416
Validation loss = 0.0011117092799395323
Validation loss = 0.0013896223390474916
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0018383861752226949
Validation loss = 0.0010178694501519203
Validation loss = 0.0013998536160215735
Validation loss = 0.001026364043354988
Validation loss = 0.0013960065552964807
Validation loss = 0.0010870841797441244
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 13       |
| MaximumReturn | 198      |
| MinimumReturn | 108      |
| TotalSamples  | 49995    |
----------------------------
itr #14 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.001270725973881781
Validation loss = 0.0012460286961868405
Validation loss = 0.0013527579139918089
Validation loss = 0.0014611624646931887
Validation loss = 0.0014339976478368044
Validation loss = 0.0012376640224829316
Validation loss = 0.0013862891355529428
Validation loss = 0.0014591335784643888
Validation loss = 0.0012112691765651107
Validation loss = 0.001540832337923348
Validation loss = 0.0011663390323519707
Validation loss = 0.0012234783498570323
Validation loss = 0.0013620950048789382
Validation loss = 0.0013113718014210463
Validation loss = 0.0012501138262450695
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0012386918533593416
Validation loss = 0.0011107833124697208
Validation loss = 0.0011925846338272095
Validation loss = 0.0012027635239064693
Validation loss = 0.0011526687303557992
Validation loss = 0.0014283113414421678
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0013327243505045772
Validation loss = 0.0013342401944100857
Validation loss = 0.0011997093679383397
Validation loss = 0.001286159735172987
Validation loss = 0.0015481997979804873
Validation loss = 0.0012649984564632177
Validation loss = 0.001482637831941247
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00157349300570786
Validation loss = 0.0014108753530308604
Validation loss = 0.00235003768466413
Validation loss = 0.0013788894284516573
Validation loss = 0.0013115223264321685
Validation loss = 0.001175637706182897
Validation loss = 0.0013718211557716131
Validation loss = 0.0019649239256978035
Validation loss = 0.0012743615079671144
Validation loss = 0.0012446941109374166
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0016886696685105562
Validation loss = 0.0012328632874414325
Validation loss = 0.0013580025406554341
Validation loss = 0.0013226558221504092
Validation loss = 0.001218233024701476
Validation loss = 0.0016265108715742826
Validation loss = 0.001540621044114232
Validation loss = 0.0012562978081405163
Validation loss = 0.0015902238665148616
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 14       |
| MaximumReturn | 199      |
| MinimumReturn | 122      |
| TotalSamples  | 53328    |
----------------------------
itr #15 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0013897728640586138
Validation loss = 0.0014845795230939984
Validation loss = 0.0014585449825972319
Validation loss = 0.001402323367074132
Validation loss = 0.0014544230652973056
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0016494891606271267
Validation loss = 0.001336997956968844
Validation loss = 0.0014189716894179583
Validation loss = 0.0013696540845558047
Validation loss = 0.0013968708226457238
Validation loss = 0.0013913222355768085
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0014234638074412942
Validation loss = 0.0014172049704939127
Validation loss = 0.001364685595035553
Validation loss = 0.0014904304407536983
Validation loss = 0.001546358224004507
Validation loss = 0.0013626081636175513
Validation loss = 0.0016654619248583913
Validation loss = 0.001317492569796741
Validation loss = 0.0014137463876977563
Validation loss = 0.0013899027835577726
Validation loss = 0.0013275089440867305
Validation loss = 0.0015309022273868322
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0015974895795807242
Validation loss = 0.0028578313067555428
Validation loss = 0.0013486983953043818
Validation loss = 0.0015379430260509253
Validation loss = 0.0013667928287759423
Validation loss = 0.0014713137643411756
Validation loss = 0.0017781408969312906
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0017842688830569386
Validation loss = 0.001365936128422618
Validation loss = 0.0013760996516793966
Validation loss = 0.0016241570701822639
Validation loss = 0.0015125932404771447
Validation loss = 0.001416421728208661
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 15       |
| MaximumReturn | 199      |
| MinimumReturn | 135      |
| TotalSamples  | 56661    |
----------------------------
itr #16 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0017522439593449235
Validation loss = 0.001534616807475686
Validation loss = 0.0016467133536934853
Validation loss = 0.001583696692250669
Validation loss = 0.001627658843062818
Validation loss = 0.0015367019223049283
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0016889075050130486
Validation loss = 0.0016022773925215006
Validation loss = 0.0017567322356626391
Validation loss = 0.0015731750754639506
Validation loss = 0.0017208810895681381
Validation loss = 0.0017917949007824063
Validation loss = 0.0015224225353449583
Validation loss = 0.0017275236314162612
Validation loss = 0.001489959075115621
Validation loss = 0.0015374701470136642
Validation loss = 0.0015176784945651889
Validation loss = 0.0015539154410362244
Validation loss = 0.0018896611873060465
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0016988974530249834
Validation loss = 0.0015565891517326236
Validation loss = 0.0017087104497477412
Validation loss = 0.0015121821779757738
Validation loss = 0.0018881043652072549
Validation loss = 0.0015513045946136117
Validation loss = 0.0016041885828599334
Validation loss = 0.0015496808337047696
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0016044771764427423
Validation loss = 0.0015537189319729805
Validation loss = 0.0015705859987065196
Validation loss = 0.0015685632824897766
Validation loss = 0.0015692824963480234
Validation loss = 0.001761781983077526
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0015497151762247086
Validation loss = 0.001644209143705666
Validation loss = 0.0016617592191323638
Validation loss = 0.0016777247656136751
Validation loss = 0.0016100945649668574
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 159      |
| Iteration     | 16       |
| MaximumReturn | 199      |
| MinimumReturn | 119      |
| TotalSamples  | 59994    |
----------------------------
itr #17 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0018511263187974691
Validation loss = 0.0019611301831901073
Validation loss = 0.0019161110976710916
Validation loss = 0.0018871998181566596
Validation loss = 0.0018820090917870402
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.002084291074424982
Validation loss = 0.0019915394950658083
Validation loss = 0.0019538376946002245
Validation loss = 0.0019846512004733086
Validation loss = 0.0018669053679332137
Validation loss = 0.002089225919917226
Validation loss = 0.0018688581185415387
Validation loss = 0.0020231243688613176
Validation loss = 0.0018494552932679653
Validation loss = 0.0019378780853003263
Validation loss = 0.0018915857654064894
Validation loss = 0.001843119622208178
Validation loss = 0.0018898759735748172
Validation loss = 0.0019806271884590387
Validation loss = 0.0018529155058786273
Validation loss = 0.0020407207775861025
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0019861306063830853
Validation loss = 0.001996102510020137
Validation loss = 0.001821545884013176
Validation loss = 0.001909367274492979
Validation loss = 0.00188009487465024
Validation loss = 0.0018347731092944741
Validation loss = 0.002342412481084466
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.001960013061761856
Validation loss = 0.001959713874384761
Validation loss = 0.0018404613947495818
Validation loss = 0.0018656711326912045
Validation loss = 0.001976429484784603
Validation loss = 0.001830866327509284
Validation loss = 0.0021562264300882816
Validation loss = 0.0019127590348944068
Validation loss = 0.0019304496236145496
Validation loss = 0.0021480033174157143
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.001934363623149693
Validation loss = 0.0019173923647031188
Validation loss = 0.0019734653178602457
Validation loss = 0.002352967858314514
Validation loss = 0.002179072005674243
Validation loss = 0.002030330477282405
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 177      |
| Iteration     | 17       |
| MaximumReturn | 199      |
| MinimumReturn | 130      |
| TotalSamples  | 63327    |
----------------------------
itr #18 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0019536828622221947
Validation loss = 0.0021016697864979506
Validation loss = 0.002006943104788661
Validation loss = 0.0021010846830904484
Validation loss = 0.0020060776732861996
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0023395174648612738
Validation loss = 0.0019830064848065376
Validation loss = 0.002089753281325102
Validation loss = 0.0019800278823822737
Validation loss = 0.0020560806151479483
Validation loss = 0.0019012570846825838
Validation loss = 0.001953383442014456
Validation loss = 0.002032945165410638
Validation loss = 0.002098304685205221
Validation loss = 0.002124088816344738
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.002010517055168748
Validation loss = 0.0022750466596335173
Validation loss = 0.002022749977186322
Validation loss = 0.0020492535550147295
Validation loss = 0.001917766174301505
Validation loss = 0.0020865576807409525
Validation loss = 0.0020288426894694567
Validation loss = 0.002023232402279973
Validation loss = 0.0019172325264662504
Validation loss = 0.0020288368687033653
Validation loss = 0.002014602068811655
Validation loss = 0.002022676169872284
Validation loss = 0.001962126698344946
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0020836382173001766
Validation loss = 0.0020018238574266434
Validation loss = 0.0020776414312422276
Validation loss = 0.0019996026530861855
Validation loss = 0.0019528778502717614
Validation loss = 0.0020285628270357847
Validation loss = 0.002491207327693701
Validation loss = 0.002031901851296425
Validation loss = 0.002001663204282522
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0019824239425361156
Validation loss = 0.002071367111057043
Validation loss = 0.0020232589449733496
Validation loss = 0.002268577925860882
Validation loss = 0.0019411372486501932
Validation loss = 0.001995453843846917
Validation loss = 0.002243519527837634
Validation loss = 0.002285168506205082
Validation loss = 0.0020699535962194204
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 178      |
| Iteration     | 18       |
| MaximumReturn | 199      |
| MinimumReturn | 119      |
| TotalSamples  | 66660    |
----------------------------
itr #19 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0027930380310863256
Validation loss = 0.0027295115869492292
Validation loss = 0.0028194631449878216
Validation loss = 0.002835677471011877
Validation loss = 0.0026670615188777447
Validation loss = 0.0027525732293725014
Validation loss = 0.002789398655295372
Validation loss = 0.0027383004780858755
Validation loss = 0.0028852950781583786
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.002845213981345296
Validation loss = 0.002743888646364212
Validation loss = 0.002680779667571187
Validation loss = 0.002798025496304035
Validation loss = 0.002783488715067506
Validation loss = 0.0031974122393876314
Validation loss = 0.0027513685636222363
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.002823642222210765
Validation loss = 0.0027752500027418137
Validation loss = 0.0030170546378940344
Validation loss = 0.0026308964006602764
Validation loss = 0.002821770729497075
Validation loss = 0.0027874077204614878
Validation loss = 0.0027683249209076166
Validation loss = 0.002832316095009446
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.002869053976610303
Validation loss = 0.002932473784312606
Validation loss = 0.0029741760808974504
Validation loss = 0.0027876324020326138
Validation loss = 0.0028461578767746687
Validation loss = 0.0027152474503964186
Validation loss = 0.002667124615982175
Validation loss = 0.002767139347270131
Validation loss = 0.002717832801863551
Validation loss = 0.0028205295093357563
Validation loss = 0.002773173851892352
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0031647374853491783
Validation loss = 0.002831794787198305
Validation loss = 0.0029559358954429626
Validation loss = 0.0028737636748701334
Validation loss = 0.0027246768586337566
Validation loss = 0.002790489699691534
Validation loss = 0.002791051985695958
Validation loss = 0.0027125240303575993
Validation loss = 0.002765401965007186
Validation loss = 0.002740822732448578
Validation loss = 0.002866889350116253
Validation loss = 0.002712556626647711
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 164      |
| Iteration     | 19       |
| MaximumReturn | 197      |
| MinimumReturn | 104      |
| TotalSamples  | 69993    |
----------------------------
itr #20 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0034629066940397024
Validation loss = 0.003468056209385395
Validation loss = 0.0033830306492745876
Validation loss = 0.0034228467848151922
Validation loss = 0.003506667912006378
Validation loss = 0.003308873390778899
Validation loss = 0.0032395499292761087
Validation loss = 0.003336474299430847
Validation loss = 0.003344661323353648
Validation loss = 0.0033445656299591064
Validation loss = 0.0032762871123850346
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0034080527257174253
Validation loss = 0.003329345490783453
Validation loss = 0.003254805924370885
Validation loss = 0.003406364703550935
Validation loss = 0.0034381160512566566
Validation loss = 0.003314647590741515
Validation loss = 0.003420200664550066
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0034121093340218067
Validation loss = 0.003527774242684245
Validation loss = 0.0033192047849297523
Validation loss = 0.0035048113204538822
Validation loss = 0.0036320933140814304
Validation loss = 0.003298041643574834
Validation loss = 0.0033981571905314922
Validation loss = 0.0033594556152820587
Validation loss = 0.0033763565588742495
Validation loss = 0.0034066231455653906
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0034486136864870787
Validation loss = 0.003540390869602561
Validation loss = 0.003341499948874116
Validation loss = 0.003308958373963833
Validation loss = 0.0033240090124309063
Validation loss = 0.0034572600852698088
Validation loss = 0.0034228856675326824
Validation loss = 0.003426257288083434
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0036237528547644615
Validation loss = 0.003589864354580641
Validation loss = 0.003417815314605832
Validation loss = 0.00335977366194129
Validation loss = 0.003310890169814229
Validation loss = 0.003332877764478326
Validation loss = 0.0033535074908286333
Validation loss = 0.003313802182674408
Validation loss = 0.003425522241741419
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 20       |
| MaximumReturn | 199      |
| MinimumReturn | 115      |
| TotalSamples  | 73326    |
----------------------------
itr #21 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0036541707813739777
Validation loss = 0.003687193850055337
Validation loss = 0.0038417400792241096
Validation loss = 0.0037251918110996485
Validation loss = 0.003716937033459544
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.003765121102333069
Validation loss = 0.0036961825098842382
Validation loss = 0.003990962635725737
Validation loss = 0.0036422519478946924
Validation loss = 0.00370755884796381
Validation loss = 0.0037011983804404736
Validation loss = 0.00366780161857605
Validation loss = 0.003826513886451721
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.003716314909979701
Validation loss = 0.003797211917117238
Validation loss = 0.0040088435634970665
Validation loss = 0.003697218606248498
Validation loss = 0.003693484701216221
Validation loss = 0.0037292870692908764
Validation loss = 0.003660944290459156
Validation loss = 0.003691130317747593
Validation loss = 0.0036929650232195854
Validation loss = 0.004428285639733076
Validation loss = 0.0040230280719697475
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0037340084090828896
Validation loss = 0.003658125875517726
Validation loss = 0.003771734656766057
Validation loss = 0.0036645743530243635
Validation loss = 0.0037898991722613573
Validation loss = 0.0037765817251056433
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0038313891272991896
Validation loss = 0.003725035348907113
Validation loss = 0.004001494962722063
Validation loss = 0.003912616521120071
Validation loss = 0.0037719847168773413
Validation loss = 0.0037766459863632917
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 21       |
| MaximumReturn | 199      |
| MinimumReturn | 126      |
| TotalSamples  | 76659    |
----------------------------
itr #22 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.004381200764328241
Validation loss = 0.004285232629626989
Validation loss = 0.004374946001917124
Validation loss = 0.004390369169414043
Validation loss = 0.004508674144744873
Validation loss = 0.004533194005489349
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.004593574907630682
Validation loss = 0.004283674526959658
Validation loss = 0.004482239950448275
Validation loss = 0.004332161042839289
Validation loss = 0.004237222019582987
Validation loss = 0.004223980475217104
Validation loss = 0.00439055310562253
Validation loss = 0.0042382534593343735
Validation loss = 0.004262570291757584
Validation loss = 0.004231975879520178
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.004361187107861042
Validation loss = 0.0044500865042209625
Validation loss = 0.004243271425366402
Validation loss = 0.0042777289636433125
Validation loss = 0.004185782745480537
Validation loss = 0.004221814684569836
Validation loss = 0.004386450629681349
Validation loss = 0.0046134269796311855
Validation loss = 0.004329178482294083
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.004258342087268829
Validation loss = 0.004272578749805689
Validation loss = 0.0043321410194039345
Validation loss = 0.004572869278490543
Validation loss = 0.004337226040661335
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0048019797541201115
Validation loss = 0.00429943110793829
Validation loss = 0.004336511716246605
Validation loss = 0.004373469389975071
Validation loss = 0.00452747056260705
Validation loss = 0.004270629957318306
Validation loss = 0.004336253274232149
Validation loss = 0.004224821459501982
Validation loss = 0.004584755282849073
Validation loss = 0.004180234391242266
Validation loss = 0.004435271956026554
Validation loss = 0.0043373266234993935
Validation loss = 0.004294351674616337
Validation loss = 0.004420583136379719
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 22       |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 79992    |
----------------------------
itr #23 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.004885334521532059
Validation loss = 0.004863283596932888
Validation loss = 0.005398505367338657
Validation loss = 0.004899743013083935
Validation loss = 0.005051797721534967
Validation loss = 0.004934406839311123
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.004895564168691635
Validation loss = 0.004816698841750622
Validation loss = 0.004870768636465073
Validation loss = 0.00480203190818429
Validation loss = 0.004950591828674078
Validation loss = 0.004887693095952272
Validation loss = 0.004999529104679823
Validation loss = 0.00492874626070261
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.005002971738576889
Validation loss = 0.004827273543924093
Validation loss = 0.005101809743791819
Validation loss = 0.00493598124012351
Validation loss = 0.004969990812242031
Validation loss = 0.0048355283215641975
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0048796553164720535
Validation loss = 0.004924293607473373
Validation loss = 0.00496140168979764
Validation loss = 0.005156787112355232
Validation loss = 0.004895848222076893
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.005197015590965748
Validation loss = 0.004937346093356609
Validation loss = 0.005011687986552715
Validation loss = 0.004978895653039217
Validation loss = 0.005347932688891888
Validation loss = 0.004916919395327568
Validation loss = 0.004809717647731304
Validation loss = 0.004820960108190775
Validation loss = 0.00487339636310935
Validation loss = 0.004814495798200369
Validation loss = 0.004877625964581966
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 23       |
| MaximumReturn | 199      |
| MinimumReturn | 108      |
| TotalSamples  | 83325    |
----------------------------
itr #24 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.005189613439142704
Validation loss = 0.005250666290521622
Validation loss = 0.0052076540887355804
Validation loss = 0.005153940059244633
Validation loss = 0.005203785374760628
Validation loss = 0.005094197113066912
Validation loss = 0.0051786587573587894
Validation loss = 0.005276940297335386
Validation loss = 0.005288364831358194
Validation loss = 0.005200221668928862
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.005193818360567093
Validation loss = 0.005112445447593927
Validation loss = 0.005207692738622427
Validation loss = 0.005103108007460833
Validation loss = 0.0051659452728927135
Validation loss = 0.005138717591762543
Validation loss = 0.005256477277725935
Validation loss = 0.005110507365316153
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.005285962950438261
Validation loss = 0.005226305685937405
Validation loss = 0.005358126945793629
Validation loss = 0.005111691541969776
Validation loss = 0.005183376371860504
Validation loss = 0.005101134069263935
Validation loss = 0.00517656235024333
Validation loss = 0.005193652119487524
Validation loss = 0.005318946670740843
Validation loss = 0.00512755336239934
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.005208692979067564
Validation loss = 0.005242490209639072
Validation loss = 0.005205389112234116
Validation loss = 0.005100630689412355
Validation loss = 0.005199618171900511
Validation loss = 0.005321446340531111
Validation loss = 0.005349571816623211
Validation loss = 0.005163233727216721
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.005159738007932901
Validation loss = 0.005112148821353912
Validation loss = 0.005075986497104168
Validation loss = 0.005393174476921558
Validation loss = 0.005476197227835655
Validation loss = 0.005089926067739725
Validation loss = 0.005208526272326708
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 24       |
| MaximumReturn | 199      |
| MinimumReturn | 104      |
| TotalSamples  | 86658    |
----------------------------
itr #25 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.005601632408797741
Validation loss = 0.005572319496423006
Validation loss = 0.005710865370929241
Validation loss = 0.005634374916553497
Validation loss = 0.00547966780140996
Validation loss = 0.005427842028439045
Validation loss = 0.005497207399457693
Validation loss = 0.005592368543148041
Validation loss = 0.005514609161764383
Validation loss = 0.005455617792904377
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.005480965133756399
Validation loss = 0.005519973114132881
Validation loss = 0.00568417739123106
Validation loss = 0.005573130678385496
Validation loss = 0.0055015054531395435
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.005573852453380823
Validation loss = 0.0055758352391421795
Validation loss = 0.005560981575399637
Validation loss = 0.005571227055042982
Validation loss = 0.005578670185059309
Validation loss = 0.005491096526384354
Validation loss = 0.005545116029679775
Validation loss = 0.005667134653776884
Validation loss = 0.0056422981433570385
Validation loss = 0.0054992251098155975
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00549187371507287
Validation loss = 0.005627959500998259
Validation loss = 0.005541786085814238
Validation loss = 0.005546729080379009
Validation loss = 0.005613500252366066
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.005680616945028305
Validation loss = 0.005629960913211107
Validation loss = 0.005518801510334015
Validation loss = 0.005516545847058296
Validation loss = 0.005587959662079811
Validation loss = 0.005585449747741222
Validation loss = 0.005556371528655291
Validation loss = 0.00560085941106081
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 177      |
| Iteration     | 25       |
| MaximumReturn | 199      |
| MinimumReturn | 130      |
| TotalSamples  | 89991    |
----------------------------
itr #26 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.006436979863792658
Validation loss = 0.00646454282104969
Validation loss = 0.006487944163382053
Validation loss = 0.006626938469707966
Validation loss = 0.006499654147773981
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.006427715066820383
Validation loss = 0.006456130649894476
Validation loss = 0.0064348578453063965
Validation loss = 0.006508474703878164
Validation loss = 0.006503865588456392
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.006615097168833017
Validation loss = 0.006532564293593168
Validation loss = 0.0066300188191235065
Validation loss = 0.00659614522010088
Validation loss = 0.006861573550850153
Validation loss = 0.006534470245242119
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.006542929448187351
Validation loss = 0.006535437889397144
Validation loss = 0.006447779946029186
Validation loss = 0.006497969850897789
Validation loss = 0.006831869017332792
Validation loss = 0.0065939160995185375
Validation loss = 0.006502123083919287
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.006481708958745003
Validation loss = 0.006437706295400858
Validation loss = 0.006574899423867464
Validation loss = 0.006509633734822273
Validation loss = 0.00662232656031847
Validation loss = 0.006635686382651329
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 161      |
| Iteration     | 26       |
| MaximumReturn | 198      |
| MinimumReturn | 116      |
| TotalSamples  | 93324    |
----------------------------
itr #27 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.006556045729666948
Validation loss = 0.006459689233452082
Validation loss = 0.006398030556738377
Validation loss = 0.006595936603844166
Validation loss = 0.006534401327371597
Validation loss = 0.006467421539127827
Validation loss = 0.006603100802749395
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.006403105333447456
Validation loss = 0.0065216077491641045
Validation loss = 0.006454056594520807
Validation loss = 0.00648634135723114
Validation loss = 0.006438213400542736
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.006428542546927929
Validation loss = 0.006465512327849865
Validation loss = 0.006634005811065435
Validation loss = 0.006518098525702953
Validation loss = 0.006443412508815527
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.006387198809534311
Validation loss = 0.006595849525183439
Validation loss = 0.00646279938519001
Validation loss = 0.006532718893140554
Validation loss = 0.006601013708859682
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0066105080768466
Validation loss = 0.006556109059602022
Validation loss = 0.006541494280099869
Validation loss = 0.006750933360308409
Validation loss = 0.006573551334440708
Validation loss = 0.006443381309509277
Validation loss = 0.006378815975040197
Validation loss = 0.006561119109392166
Validation loss = 0.006428584456443787
Validation loss = 0.006523100659251213
Validation loss = 0.0065500931814312935
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 163      |
| Iteration     | 27       |
| MaximumReturn | 199      |
| MinimumReturn | 116      |
| TotalSamples  | 96657    |
----------------------------
itr #28 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.006859451532363892
Validation loss = 0.00685562239959836
Validation loss = 0.006937991827726364
Validation loss = 0.006729964632540941
Validation loss = 0.006798837333917618
Validation loss = 0.006900229025632143
Validation loss = 0.006889886688441038
Validation loss = 0.006953837815672159
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.006786688696593046
Validation loss = 0.00684397155418992
Validation loss = 0.006764934863895178
Validation loss = 0.00687688821926713
Validation loss = 0.006847310811281204
Validation loss = 0.006883766036480665
Validation loss = 0.006777708884328604
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.007200116291642189
Validation loss = 0.006812453269958496
Validation loss = 0.006941608618944883
Validation loss = 0.006901157554239035
Validation loss = 0.006769732106477022
Validation loss = 0.006872222293168306
Validation loss = 0.006790481973439455
Validation loss = 0.0070160687901079655
Validation loss = 0.0070015485398471355
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00696583604440093
Validation loss = 0.006902657449245453
Validation loss = 0.006870498415082693
Validation loss = 0.006821192800998688
Validation loss = 0.006777087692171335
Validation loss = 0.006895373109728098
Validation loss = 0.006787184160202742
Validation loss = 0.006944280117750168
Validation loss = 0.0069944835267961025
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00682652136310935
Validation loss = 0.006888361182063818
Validation loss = 0.006958145182579756
Validation loss = 0.006782373879104853
Validation loss = 0.0069597624242305756
Validation loss = 0.00681223114952445
Validation loss = 0.006811223458498716
Validation loss = 0.006845612078905106
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 28       |
| MaximumReturn | 199      |
| MinimumReturn | 114      |
| TotalSamples  | 99990    |
----------------------------
itr #29 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.007244471460580826
Validation loss = 0.00745447538793087
Validation loss = 0.0074617816135287285
Validation loss = 0.007399425376206636
Validation loss = 0.007353966124355793
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.007205953821539879
Validation loss = 0.007235413417220116
Validation loss = 0.007333250250667334
Validation loss = 0.0072368448600173
Validation loss = 0.007111992221325636
Validation loss = 0.007269205991178751
Validation loss = 0.007422465831041336
Validation loss = 0.0072319465689361095
Validation loss = 0.007213702891021967
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.007211153395473957
Validation loss = 0.007392161525785923
Validation loss = 0.007197640370577574
Validation loss = 0.007292518392205238
Validation loss = 0.007183985784649849
Validation loss = 0.007372909691184759
Validation loss = 0.007265974767506123
Validation loss = 0.007198766339570284
Validation loss = 0.007124141324311495
Validation loss = 0.007273165974766016
Validation loss = 0.00729859434068203
Validation loss = 0.007374521344900131
Validation loss = 0.007251395378261805
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0073379832319915295
Validation loss = 0.007442895323038101
Validation loss = 0.00733344629406929
Validation loss = 0.007226410321891308
Validation loss = 0.007165329996496439
Validation loss = 0.007227443624287844
Validation loss = 0.007190302480012178
Validation loss = 0.0072634173557162285
Validation loss = 0.007417800836265087
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.007369247265160084
Validation loss = 0.007419184315949678
Validation loss = 0.007198736071586609
Validation loss = 0.007163877598941326
Validation loss = 0.007330596912652254
Validation loss = 0.007413642480969429
Validation loss = 0.007345111574977636
Validation loss = 0.007411913480609655
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 29       |
| MaximumReturn | 199      |
| MinimumReturn | 128      |
| TotalSamples  | 103323   |
----------------------------
itr #30 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.007519491948187351
Validation loss = 0.007609598804265261
Validation loss = 0.007407045923173428
Validation loss = 0.007517262361943722
Validation loss = 0.007380867842584848
Validation loss = 0.007444160990417004
Validation loss = 0.007334768772125244
Validation loss = 0.007435327861458063
Validation loss = 0.007476417347788811
Validation loss = 0.007397248409688473
Validation loss = 0.00760666374117136
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.00749766593798995
Validation loss = 0.007442147005349398
Validation loss = 0.007564948871731758
Validation loss = 0.007512575481086969
Validation loss = 0.007504103239625692
Validation loss = 0.007536758203059435
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00812054704874754
Validation loss = 0.007461740635335445
Validation loss = 0.007533070631325245
Validation loss = 0.007464013062417507
Validation loss = 0.007419628091156483
Validation loss = 0.007455016952008009
Validation loss = 0.0074689919129014015
Validation loss = 0.007470696698874235
Validation loss = 0.007427392993122339
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.007481842767447233
Validation loss = 0.007582609076052904
Validation loss = 0.007647929713129997
Validation loss = 0.0075171273201704025
Validation loss = 0.007444426883012056
Validation loss = 0.007451907265931368
Validation loss = 0.007766152266412973
Validation loss = 0.007438690867275
Validation loss = 0.007523359265178442
Validation loss = 0.007503549102693796
Validation loss = 0.0076002515852451324
Validation loss = 0.007466515526175499
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.007463912013918161
Validation loss = 0.007558254990726709
Validation loss = 0.007814917713403702
Validation loss = 0.007487461902201176
Validation loss = 0.007478796876966953
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 156      |
| Iteration     | 30       |
| MaximumReturn | 199      |
| MinimumReturn | 108      |
| TotalSamples  | 106656   |
----------------------------
itr #31 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.008192189037799835
Validation loss = 0.008000769652426243
Validation loss = 0.00799295399338007
Validation loss = 0.007865055464208126
Validation loss = 0.007913696579635143
Validation loss = 0.007767249830067158
Validation loss = 0.007866484113037586
Validation loss = 0.00783746037632227
Validation loss = 0.007870443165302277
Validation loss = 0.007807503920048475
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.008032814599573612
Validation loss = 0.007995466701686382
Validation loss = 0.00814308412373066
Validation loss = 0.007982637733221054
Validation loss = 0.007881579920649529
Validation loss = 0.007978319190442562
Validation loss = 0.007828145287930965
Validation loss = 0.007830043323338032
Validation loss = 0.007834344170987606
Validation loss = 0.007818791083991528
Validation loss = 0.007868836633861065
Validation loss = 0.007929950021207333
Validation loss = 0.007942656055092812
Validation loss = 0.007909656502306461
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00788020994514227
Validation loss = 0.007815748453140259
Validation loss = 0.007831529714167118
Validation loss = 0.007933463901281357
Validation loss = 0.007862291298806667
Validation loss = 0.007932544685900211
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.007928826846182346
Validation loss = 0.007892235182225704
Validation loss = 0.007861648686230183
Validation loss = 0.007899301126599312
Validation loss = 0.008042405359447002
Validation loss = 0.007849049754440784
Validation loss = 0.007876229472458363
Validation loss = 0.007898134179413319
Validation loss = 0.00786275789141655
Validation loss = 0.007983539253473282
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00789972860366106
Validation loss = 0.008001049980521202
Validation loss = 0.007898075506091118
Validation loss = 0.007904740050435066
Validation loss = 0.007946749217808247
Validation loss = 0.007934321649372578
Validation loss = 0.007969530299305916
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 31       |
| MaximumReturn | 197      |
| MinimumReturn | 111      |
| TotalSamples  | 109989   |
----------------------------
itr #32 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.008228141814470291
Validation loss = 0.008249238133430481
Validation loss = 0.008179021067917347
Validation loss = 0.008328285999596119
Validation loss = 0.008295110426843166
Validation loss = 0.008180951699614525
Validation loss = 0.008483579382300377
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.008267119526863098
Validation loss = 0.008400180377066135
Validation loss = 0.008405464701354504
Validation loss = 0.00824611634016037
Validation loss = 0.008274820633232594
Validation loss = 0.008297096937894821
Validation loss = 0.008239584974944592
Validation loss = 0.008295703679323196
Validation loss = 0.008312715217471123
Validation loss = 0.00850702729076147
Validation loss = 0.008294932544231415
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00824939925223589
Validation loss = 0.008190599270164967
Validation loss = 0.008239957503974438
Validation loss = 0.008221839554607868
Validation loss = 0.008266725577414036
Validation loss = 0.008317835628986359
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00831794273108244
Validation loss = 0.008390107192099094
Validation loss = 0.008201715536415577
Validation loss = 0.00821676105260849
Validation loss = 0.008216309361159801
Validation loss = 0.008339016698300838
Validation loss = 0.008210862055420876
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.008329659700393677
Validation loss = 0.008520565927028656
Validation loss = 0.008247504942119122
Validation loss = 0.008263909257948399
Validation loss = 0.008220749907195568
Validation loss = 0.008293263614177704
Validation loss = 0.008279776200652122
Validation loss = 0.008393796160817146
Validation loss = 0.008309133350849152
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 32       |
| MaximumReturn | 198      |
| MinimumReturn | 109      |
| TotalSamples  | 113322   |
----------------------------
itr #33 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.008669626899063587
Validation loss = 0.008802357129752636
Validation loss = 0.00867969449609518
Validation loss = 0.008978926576673985
Validation loss = 0.008818106725811958
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.008839470334351063
Validation loss = 0.008799528703093529
Validation loss = 0.008706486783921719
Validation loss = 0.008655807934701443
Validation loss = 0.008761675097048283
Validation loss = 0.008694170974195004
Validation loss = 0.008670886047184467
Validation loss = 0.008758065290749073
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.008785241283476353
Validation loss = 0.008774081245064735
Validation loss = 0.008734731003642082
Validation loss = 0.008739566430449486
Validation loss = 0.008642701432108879
Validation loss = 0.00866769254207611
Validation loss = 0.008650084026157856
Validation loss = 0.008643326349556446
Validation loss = 0.00861281342804432
Validation loss = 0.008696009404957294
Validation loss = 0.008774744346737862
Validation loss = 0.008706411346793175
Validation loss = 0.00866479892283678
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.008817135356366634
Validation loss = 0.008853946812450886
Validation loss = 0.008907013572752476
Validation loss = 0.008680153638124466
Validation loss = 0.008944621309638023
Validation loss = 0.008778086863458157
Validation loss = 0.008659492246806622
Validation loss = 0.008946573361754417
Validation loss = 0.00869667250663042
Validation loss = 0.008652799762785435
Validation loss = 0.00869138352572918
Validation loss = 0.008664771914482117
Validation loss = 0.008864893577992916
Validation loss = 0.00875154323875904
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00873211957514286
Validation loss = 0.008676182478666306
Validation loss = 0.008879180066287518
Validation loss = 0.008602890186011791
Validation loss = 0.00877291988581419
Validation loss = 0.008776279166340828
Validation loss = 0.008823787793517113
Validation loss = 0.008694945834577084
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 164      |
| Iteration     | 33       |
| MaximumReturn | 199      |
| MinimumReturn | 122      |
| TotalSamples  | 116655   |
----------------------------
itr #34 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.009178776293992996
Validation loss = 0.009066993370652199
Validation loss = 0.00932698231190443
Validation loss = 0.009111172519624233
Validation loss = 0.009105859324336052
Validation loss = 0.00910221878439188
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.00930988509207964
Validation loss = 0.009155997075140476
Validation loss = 0.009208633564412594
Validation loss = 0.00924246571958065
Validation loss = 0.009089205414056778
Validation loss = 0.00913575291633606
Validation loss = 0.009257856756448746
Validation loss = 0.009219730272889137
Validation loss = 0.00922491867095232
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.009283937513828278
Validation loss = 0.009248904883861542
Validation loss = 0.009219415485858917
Validation loss = 0.009125087410211563
Validation loss = 0.009266814216971397
Validation loss = 0.009099743328988552
Validation loss = 0.009158497676253319
Validation loss = 0.00916944071650505
Validation loss = 0.009350525215268135
Validation loss = 0.009062303230166435
Validation loss = 0.009116324596107006
Validation loss = 0.0091879703104496
Validation loss = 0.009233326651155949
Validation loss = 0.009166526608169079
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.009092011488974094
Validation loss = 0.009164417162537575
Validation loss = 0.009278485551476479
Validation loss = 0.009211240336298943
Validation loss = 0.009099185466766357
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00910141970962286
Validation loss = 0.00909634493291378
Validation loss = 0.009248352609574795
Validation loss = 0.00920178648084402
Validation loss = 0.009309046901762486
Validation loss = 0.009127268567681313
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 164      |
| Iteration     | 34       |
| MaximumReturn | 198      |
| MinimumReturn | 123      |
| TotalSamples  | 119988   |
----------------------------
itr #35 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.010143707506358624
Validation loss = 0.010238265618681908
Validation loss = 0.01041411329060793
Validation loss = 0.009958535432815552
Validation loss = 0.009995349682867527
Validation loss = 0.009888237342238426
Validation loss = 0.009856799617409706
Validation loss = 0.009909613057971
Validation loss = 0.010012825019657612
Validation loss = 0.010087749920785427
Validation loss = 0.00993754155933857
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.009939298033714294
Validation loss = 0.009980225004255772
Validation loss = 0.009856571443378925
Validation loss = 0.010068217292428017
Validation loss = 0.010113663040101528
Validation loss = 0.009911194443702698
Validation loss = 0.010170643217861652
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.010043319314718246
Validation loss = 0.010274828411638737
Validation loss = 0.010107068344950676
Validation loss = 0.009893465787172318
Validation loss = 0.009952722117304802
Validation loss = 0.010189107619225979
Validation loss = 0.00989537313580513
Validation loss = 0.009958314709365368
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00989250186830759
Validation loss = 0.009881967678666115
Validation loss = 0.010112214833498001
Validation loss = 0.009937763214111328
Validation loss = 0.010058040730655193
Validation loss = 0.00994160771369934
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.010067562572658062
Validation loss = 0.0099945655092597
Validation loss = 0.010074079036712646
Validation loss = 0.010228602215647697
Validation loss = 0.00985625945031643
Validation loss = 0.00992338452488184
Validation loss = 0.009879710152745247
Validation loss = 0.009918684139847755
Validation loss = 0.009973375126719475
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 35       |
| MaximumReturn | 199      |
| MinimumReturn | 123      |
| TotalSamples  | 123321   |
----------------------------
itr #36 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.010458438657224178
Validation loss = 0.010432732291519642
Validation loss = 0.010589242912828922
Validation loss = 0.010401804931461811
Validation loss = 0.010554485954344273
Validation loss = 0.010561966337263584
Validation loss = 0.010475466027855873
Validation loss = 0.010546205565333366
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.010534269735217094
Validation loss = 0.010501009412109852
Validation loss = 0.010568096302449703
Validation loss = 0.010419568978250027
Validation loss = 0.010515931062400341
Validation loss = 0.010483819991350174
Validation loss = 0.010557559318840504
Validation loss = 0.010418268851935863
Validation loss = 0.010450446978211403
Validation loss = 0.010449470952153206
Validation loss = 0.010429688729345798
Validation loss = 0.01054051611572504
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.010466968640685081
Validation loss = 0.010598724707961082
Validation loss = 0.01057069655507803
Validation loss = 0.010442089289426804
Validation loss = 0.010438056662678719
Validation loss = 0.010399668477475643
Validation loss = 0.010569597594439983
Validation loss = 0.010487312451004982
Validation loss = 0.010517512448132038
Validation loss = 0.010582339018583298
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.010490529239177704
Validation loss = 0.010387859307229519
Validation loss = 0.010433847084641457
Validation loss = 0.010613745078444481
Validation loss = 0.01044111791998148
Validation loss = 0.010470756329596043
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.010505489073693752
Validation loss = 0.01057133637368679
Validation loss = 0.010567518882453442
Validation loss = 0.010532461106777191
Validation loss = 0.010542096570134163
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 36       |
| MaximumReturn | 199      |
| MinimumReturn | 123      |
| TotalSamples  | 126654   |
----------------------------
itr #37 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.011012335307896137
Validation loss = 0.010986262932419777
Validation loss = 0.011137750931084156
Validation loss = 0.011054198257625103
Validation loss = 0.011151788756251335
Validation loss = 0.011333008296787739
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.011041504330933094
Validation loss = 0.011060657911002636
Validation loss = 0.010991879738867283
Validation loss = 0.010964957065880299
Validation loss = 0.010964535176753998
Validation loss = 0.011115631088614464
Validation loss = 0.011030188761651516
Validation loss = 0.011099002324044704
Validation loss = 0.011019902303814888
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.011289786547422409
Validation loss = 0.011045379564166069
Validation loss = 0.010956678539514542
Validation loss = 0.011026071384549141
Validation loss = 0.011120332404971123
Validation loss = 0.011023749597370625
Validation loss = 0.010966762900352478
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.011003591120243073
Validation loss = 0.010973299853503704
Validation loss = 0.011132803745567799
Validation loss = 0.011105035431683064
Validation loss = 0.011027383618056774
Validation loss = 0.011143837124109268
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.01106587890535593
Validation loss = 0.01099469605833292
Validation loss = 0.011113399639725685
Validation loss = 0.011149601079523563
Validation loss = 0.010994897224009037
Validation loss = 0.011317635886371136
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 157      |
| Iteration     | 37       |
| MaximumReturn | 198      |
| MinimumReturn | 123      |
| TotalSamples  | 129987   |
----------------------------
itr #38 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.011178136803209782
Validation loss = 0.01148837897926569
Validation loss = 0.011352531611919403
Validation loss = 0.011486923322081566
Validation loss = 0.011226318776607513
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.011341988109052181
Validation loss = 0.011295631527900696
Validation loss = 0.011318445205688477
Validation loss = 0.011148544028401375
Validation loss = 0.011248071677982807
Validation loss = 0.011173376813530922
Validation loss = 0.011267978698015213
Validation loss = 0.011194711551070213
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.011320764198899269
Validation loss = 0.011378338560461998
Validation loss = 0.011240706779062748
Validation loss = 0.011362823657691479
Validation loss = 0.01125701516866684
Validation loss = 0.01124108862131834
Validation loss = 0.011212268844246864
Validation loss = 0.01136899646371603
Validation loss = 0.011215465143322945
Validation loss = 0.011338919401168823
Validation loss = 0.01128154806792736
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.011353939771652222
Validation loss = 0.011299360543489456
Validation loss = 0.011341742239892483
Validation loss = 0.011299247853457928
Validation loss = 0.01126051601022482
Validation loss = 0.011245745234191418
Validation loss = 0.011188644915819168
Validation loss = 0.011316546238958836
Validation loss = 0.011167483404278755
Validation loss = 0.011286081746220589
Validation loss = 0.011465215124189854
Validation loss = 0.011282495222985744
Validation loss = 0.011271550320088863
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.01137748546898365
Validation loss = 0.011412863619625568
Validation loss = 0.011304433457553387
Validation loss = 0.011433213949203491
Validation loss = 0.01141201239079237
Validation loss = 0.011288478039205074
Validation loss = 0.01135061401873827
Validation loss = 0.011196741834282875
Validation loss = 0.011341508477926254
Validation loss = 0.011295441538095474
Validation loss = 0.011424921452999115
Validation loss = 0.011201588436961174
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 38       |
| MaximumReturn | 198      |
| MinimumReturn | 123      |
| TotalSamples  | 133320   |
----------------------------
itr #39 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.011485841125249863
Validation loss = 0.01153810415416956
Validation loss = 0.011623273603618145
Validation loss = 0.011574970558285713
Validation loss = 0.011822475120425224
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.01157359965145588
Validation loss = 0.011501865461468697
Validation loss = 0.01153072714805603
Validation loss = 0.011468213982880116
Validation loss = 0.011459358967840672
Validation loss = 0.011752309277653694
Validation loss = 0.01143193244934082
Validation loss = 0.011626208201050758
Validation loss = 0.011496046558022499
Validation loss = 0.011452498845756054
Validation loss = 0.011468909680843353
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.01138917077332735
Validation loss = 0.011614205315709114
Validation loss = 0.011788461357355118
Validation loss = 0.011383535340428352
Validation loss = 0.011452375911176205
Validation loss = 0.011579586192965508
Validation loss = 0.011586933396756649
Validation loss = 0.011614247225224972
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.011525760404765606
Validation loss = 0.011490928940474987
Validation loss = 0.011463779024779797
Validation loss = 0.01141733955591917
Validation loss = 0.01147982943803072
Validation loss = 0.011520642787218094
Validation loss = 0.011419366113841534
Validation loss = 0.011519331485033035
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.011507080867886543
Validation loss = 0.012208212167024612
Validation loss = 0.011564165353775024
Validation loss = 0.011407238431274891
Validation loss = 0.011742528527975082
Validation loss = 0.011511932127177715
Validation loss = 0.011477911844849586
Validation loss = 0.011487266048789024
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 39       |
| MaximumReturn | 198      |
| MinimumReturn | 123      |
| TotalSamples  | 136653   |
----------------------------
