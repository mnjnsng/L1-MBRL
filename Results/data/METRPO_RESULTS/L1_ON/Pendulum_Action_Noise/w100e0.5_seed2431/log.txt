Logging to experiments/pendulum/Pendulum_Action_Noise/w100e0.5_seed2431
Print configuration .....
{'env_name': 'pendulum', 'random_seeds': [3214, 2431, 2531, 2231], 'save_variables': False, 'model_save_dir': '/tmp/pendulum_models/', 'restore_variables': False, 'start_onpol_iter': 0, 'onpol_iters': 40, 'num_path_random': 25, 'num_path_onpol': 25, 'env_horizon': 200, 'max_train_data': 200000, 'max_val_data': 100000, 'discard_ratio': 0.0, 'dynamics': {'pre_training': {'mode': 'intrinsic_reward', 'itr': 0, 'policy_itr': 20}, 'model': 'nn', 'ensemble': True, 'ensemble_model_count': 5, 'enable_particle_ensemble': True, 'particles': 5, 'obs_var': 1.0, 'intrinsic_reward_coeff': 1.0, 'ita': 1.0, 'mode': 'random', 'val': True, 'n_layers': 4, 'hidden_size': 1000, 'activation': 'relu', 'batch_size': 1000, 'learning_rate': 0.001, 'reg_coeff': 0.0, 'epochs': 200, 'kfac_params': {'learning_rate': 0.1, 'damping': 0.001, 'momentum': 0.9, 'kl_clip': 0.0001, 'cov_ema_decay': 0.99}}, 'policy': {'network_shape': [64, 64], 'init_logstd': 0.0, 'activation': 'tanh', 'reinitialize_every_itr': False}, 'trpo': {'horizon': 200, 'gamma': 0.99, 'step_size': 0.01, 'iterations': 20, 'batch_size': 50000, 'gae': 0.95, 'visualization': False, 'visualize_iterations': [0]}, 'algo': 'trpo'}
Generating random rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating random rollouts.
Creating normalization for training data.
Done creating normalization for training data.
Particle ensemble enabled? True
An ensemble of 5 dynamics model <class 'model.dynamics.NNDynamicsModel'> initialized
Train dynamics model with intrinsic reward only? False
Pre-training enabled. Using only intrinsic reward.
Pre-training dynamics model for 0 iterations...
Done pre-training dynamics model.
Using external reward only.
itr #0 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.1958906203508377
Validation loss = 0.024460094049572945
Validation loss = 0.0067243753001093864
Validation loss = 0.0029391723219305277
Validation loss = 0.0016965953400358558
Validation loss = 0.0012502308236435056
Validation loss = 0.0011431077728047967
Validation loss = 0.0010712554212659597
Validation loss = 0.0010513360612094402
Validation loss = 0.0010361869353801012
Validation loss = 0.001439544023014605
Validation loss = 0.0009838494006544352
Validation loss = 0.004717085976153612
Validation loss = 0.0023433067835867405
Validation loss = 0.0011220723390579224
Validation loss = 0.0009154572617262602
Validation loss = 0.0008959248079918325
Validation loss = 0.0009261936065740883
Validation loss = 0.0022345215547829866
Validation loss = 0.0012770576868206263
Validation loss = 0.0008776668692007661
Validation loss = 0.0010256823152303696
Validation loss = 0.011917391791939735
Validation loss = 0.002013157354667783
Validation loss = 0.001622986514121294
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.21368294954299927
Validation loss = 0.018357213586568832
Validation loss = 0.006283042952418327
Validation loss = 0.0025922770146280527
Validation loss = 0.0014681128086522222
Validation loss = 0.0011853346368297935
Validation loss = 0.0010674940422177315
Validation loss = 0.0010659274412319064
Validation loss = 0.0013185595162212849
Validation loss = 0.001357236527837813
Validation loss = 0.0010666289599612355
Validation loss = 0.0010235286317765713
Validation loss = 0.0013505304232239723
Validation loss = 0.003922791685909033
Validation loss = 0.0011882911203429103
Validation loss = 0.0011863273102790117
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.21704819798469543
Validation loss = 0.023561611771583557
Validation loss = 0.008292319253087044
Validation loss = 0.003070133738219738
Validation loss = 0.001635812921449542
Validation loss = 0.0011817835038527846
Validation loss = 0.0010693761287257075
Validation loss = 0.0009949456434696913
Validation loss = 0.0009405834716744721
Validation loss = 0.0009251703158952296
Validation loss = 0.0009478075662627816
Validation loss = 0.0009860396385192871
Validation loss = 0.00098177429754287
Validation loss = 0.0009275091579183936
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.20787972211837769
Validation loss = 0.031939685344696045
Validation loss = 0.007930319756269455
Validation loss = 0.0028233597986400127
Validation loss = 0.0015271765878424048
Validation loss = 0.0011541825952008367
Validation loss = 0.0010379591258242726
Validation loss = 0.0009567217784933746
Validation loss = 0.0009368231985718012
Validation loss = 0.000869481940753758
Validation loss = 0.0009253250900655985
Validation loss = 0.0010570511221885681
Validation loss = 0.0009854412637650967
Validation loss = 0.004513158928602934
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.24413003027439117
Validation loss = 0.019460968673229218
Validation loss = 0.008003110066056252
Validation loss = 0.002802737755700946
Validation loss = 0.0017301187617704272
Validation loss = 0.001194443553686142
Validation loss = 0.0011593400267884135
Validation loss = 0.0010346631752327085
Validation loss = 0.000968107080552727
Validation loss = 0.0010080847423523664
Validation loss = 0.003970907535403967
Validation loss = 0.0011463811388239264
Validation loss = 0.0009460618603043258
Validation loss = 0.0025070379488170147
Validation loss = 0.002013259334489703
Validation loss = 0.001044450793415308
Validation loss = 0.0015212841099128127
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 0        |
| MaximumReturn | 199      |
| MinimumReturn | 107      |
| TotalSamples  | 6666     |
----------------------------
itr #1 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.01528744027018547
Validation loss = 0.0017710061511024833
Validation loss = 0.0006966654327698052
Validation loss = 0.0005770681309513748
Validation loss = 0.000512569269631058
Validation loss = 0.0005222623003646731
Validation loss = 0.001347134355455637
Validation loss = 0.0005069322069175541
Validation loss = 0.0015534916892647743
Validation loss = 0.0005226708599366248
Validation loss = 0.007481660693883896
Validation loss = 0.0009664017125032842
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.01972479186952114
Validation loss = 0.0029510806780308485
Validation loss = 0.0007234062650240958
Validation loss = 0.0005616075941361487
Validation loss = 0.0005460318643599749
Validation loss = 0.0032064029946923256
Validation loss = 0.00064929632935673
Validation loss = 0.0005495539517141879
Validation loss = 0.001249684952199459
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.007868406362831593
Validation loss = 0.0016753339441493154
Validation loss = 0.000637738557998091
Validation loss = 0.0012575890868902206
Validation loss = 0.0005699957837350667
Validation loss = 0.0014688256196677685
Validation loss = 0.0006898830179125071
Validation loss = 0.0005617636488750577
Validation loss = 0.001080334302969277
Validation loss = 0.0007059795316308737
Validation loss = 0.001066769938915968
Validation loss = 0.0005259276949800551
Validation loss = 0.002609924180433154
Validation loss = 0.0004766752535942942
Validation loss = 0.003484320128336549
Validation loss = 0.0010442977072671056
Validation loss = 0.0005574461538344622
Validation loss = 0.0007489756681025028
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.006574665661901236
Validation loss = 0.0017984943697229028
Validation loss = 0.00072660663863644
Validation loss = 0.000535945117007941
Validation loss = 0.0007196934311650693
Validation loss = 0.0005830505397170782
Validation loss = 0.0016424224013462663
Validation loss = 0.001200038124807179
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.015554380603134632
Validation loss = 0.002779412781819701
Validation loss = 0.0008513137581758201
Validation loss = 0.0005646005156449974
Validation loss = 0.000529492215719074
Validation loss = 0.0005583434249274433
Validation loss = 0.0007639029063284397
Validation loss = 0.000846880313474685
Validation loss = 0.0009970733663067222
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 1        |
| MaximumReturn | 198      |
| MinimumReturn | 119      |
| TotalSamples  | 9999     |
----------------------------
itr #2 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.002391054527834058
Validation loss = 0.0005538258119486272
Validation loss = 0.0009395998786203563
Validation loss = 0.000688400526996702
Validation loss = 0.0004654117801692337
Validation loss = 0.00046694319462403655
Validation loss = 0.0008098160033114254
Validation loss = 0.0009855906246230006
Validation loss = 0.0007189248572103679
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.003243766725063324
Validation loss = 0.0008377634803764522
Validation loss = 0.000562537694349885
Validation loss = 0.0005874604685232043
Validation loss = 0.0017287833616137505
Validation loss = 0.001021535019390285
Validation loss = 0.0005611687665805221
Validation loss = 0.0011322915088385344
Validation loss = 0.001913728192448616
Validation loss = 0.0007825302891433239
Validation loss = 0.0006611408316530287
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0029240550938993692
Validation loss = 0.0005941643030382693
Validation loss = 0.0004605321155395359
Validation loss = 0.0021747765131294727
Validation loss = 0.0004730997607111931
Validation loss = 0.0008263709023594856
Validation loss = 0.0005272446433082223
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00440761586651206
Validation loss = 0.0007159456727094948
Validation loss = 0.00046372608630917966
Validation loss = 0.0010304473107680678
Validation loss = 0.0006907092756591737
Validation loss = 0.0009132454870268703
Validation loss = 0.000763293937779963
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.003999765031039715
Validation loss = 0.0007124548428691924
Validation loss = 0.000578843813855201
Validation loss = 0.0009512867545709014
Validation loss = 0.0010714178206399083
Validation loss = 0.0005817304481752217
Validation loss = 0.0008915695361793041
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 2        |
| MaximumReturn | 199      |
| MinimumReturn | 96.9     |
| TotalSamples  | 13332    |
----------------------------
itr #3 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0018650548299774528
Validation loss = 0.0006014693644829094
Validation loss = 0.000406950042815879
Validation loss = 0.0004918845952488482
Validation loss = 0.00043893689871765673
Validation loss = 0.0005488245515152812
Validation loss = 0.0009359707473777235
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0021313668694347143
Validation loss = 0.00041235986282117665
Validation loss = 0.0018198498291894794
Validation loss = 0.0007103310781531036
Validation loss = 0.0005821262602694333
Validation loss = 0.0016691040946170688
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.002867568051442504
Validation loss = 0.0006082372856326401
Validation loss = 0.0008205634658224881
Validation loss = 0.0006352781783789396
Validation loss = 0.0007012439309619367
Validation loss = 0.0008916591759771109
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.008414585143327713
Validation loss = 0.0005681681795977056
Validation loss = 0.0005010463064536452
Validation loss = 0.0005579992430284619
Validation loss = 0.0009818297112360597
Validation loss = 0.0014888049336150289
Validation loss = 0.0010654143989086151
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0012078258441761136
Validation loss = 0.0006495644920505583
Validation loss = 0.0008493713685311377
Validation loss = 0.0006875558174215257
Validation loss = 0.0005336151807568967
Validation loss = 0.0013080262579023838
Validation loss = 0.0009019203134812415
Validation loss = 0.0019125196849927306
Validation loss = 0.0004870338598266244
Validation loss = 0.0007275628740899265
Validation loss = 0.0017117330571636558
Validation loss = 0.0010715618263930082
Validation loss = 0.0009742018301039934
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 3        |
| MaximumReturn | 199      |
| MinimumReturn | 132      |
| TotalSamples  | 16665    |
----------------------------
itr #4 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.009931144304573536
Validation loss = 0.0004606599686667323
Validation loss = 0.0003689784789457917
Validation loss = 0.0003833369701169431
Validation loss = 0.00047712057130411267
Validation loss = 0.002141323173418641
Validation loss = 0.0012310711899772286
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0019962904043495655
Validation loss = 0.0006327868322841823
Validation loss = 0.0008859525551088154
Validation loss = 0.0004601904365699738
Validation loss = 0.0006333761848509312
Validation loss = 0.0008702345658093691
Validation loss = 0.00044698038254864514
Validation loss = 0.0010163180995732546
Validation loss = 0.0003755401121452451
Validation loss = 0.0006607191753573716
Validation loss = 0.0020665270276367664
Validation loss = 0.0011596621479839087
Validation loss = 0.00043328723404556513
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0010463454527780414
Validation loss = 0.0005330161075107753
Validation loss = 0.0005002813995815814
Validation loss = 0.0015939439181238413
Validation loss = 0.0007711822399869561
Validation loss = 0.0007602036348544061
Validation loss = 0.0033659166656434536
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.003144331043586135
Validation loss = 0.0008080242550931871
Validation loss = 0.0008286542142741382
Validation loss = 0.0006920010200701654
Validation loss = 0.0010817933361977339
Validation loss = 0.0005701855989173055
Validation loss = 0.0014986672904342413
Validation loss = 0.0006824745214544237
Validation loss = 0.000804953626357019
Validation loss = 0.0010203078854829073
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0015938755823299289
Validation loss = 0.0011310054687783122
Validation loss = 0.0008027427829802036
Validation loss = 0.0003506657376419753
Validation loss = 0.0005880482494831085
Validation loss = 0.0005947170429863036
Validation loss = 0.0011162010487169027
Validation loss = 0.000517442706041038
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 4        |
| MaximumReturn | 199      |
| MinimumReturn | 122      |
| TotalSamples  | 19998    |
----------------------------
itr #5 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006611525896005332
Validation loss = 0.0003494502161629498
Validation loss = 0.0010004641953855753
Validation loss = 0.0014639918226748705
Validation loss = 0.0004922233638353646
Validation loss = 0.0006076862337067723
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0009226003894582391
Validation loss = 0.0006082170875743032
Validation loss = 0.0006051835371181369
Validation loss = 0.001307406579144299
Validation loss = 0.0007194731151685119
Validation loss = 0.0003381404385436326
Validation loss = 0.0004881629138253629
Validation loss = 0.002106474246829748
Validation loss = 0.0005203660693950951
Validation loss = 0.0004066661058459431
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0012450668727979064
Validation loss = 0.00032950149034149945
Validation loss = 0.0005473042838275433
Validation loss = 0.0005694314604625106
Validation loss = 0.0004199018294457346
Validation loss = 0.0011773905716836452
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0017666325438767672
Validation loss = 0.0018663003575056791
Validation loss = 0.0004802334005944431
Validation loss = 0.001277576433494687
Validation loss = 0.0004037733597215265
Validation loss = 0.0004969915607944131
Validation loss = 0.0002866927534341812
Validation loss = 0.0003556563169695437
Validation loss = 0.00044415527372621
Validation loss = 0.0004284206370357424
Validation loss = 0.0008468786254525185
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00173182412981987
Validation loss = 0.000396972696762532
Validation loss = 0.0004777945578098297
Validation loss = 0.0005933585925959051
Validation loss = 0.000567592796869576
Validation loss = 0.00037959226756356657
Validation loss = 0.0007936671609058976
Validation loss = 0.0007222688291221857
Validation loss = 0.0011514521902427077
Validation loss = 0.0007893979782238603
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 5        |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 23331    |
----------------------------
itr #6 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0014154916862025857
Validation loss = 0.000448752281954512
Validation loss = 0.0007214649231173098
Validation loss = 0.0004901490756310523
Validation loss = 0.0007244859007187188
Validation loss = 0.0011771447025239468
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0007016753661446273
Validation loss = 0.0003532695409376174
Validation loss = 0.0007630565669387579
Validation loss = 0.00044397960300557315
Validation loss = 0.0007856467273086309
Validation loss = 0.0003999904729425907
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.002030332339927554
Validation loss = 0.0005190995871089399
Validation loss = 0.0022210038732737303
Validation loss = 0.0006377935060299933
Validation loss = 0.00040123582584783435
Validation loss = 0.0009764429414644837
Validation loss = 0.000544755021110177
Validation loss = 0.0005388528807088733
Validation loss = 0.0007672099163755774
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007805995410308242
Validation loss = 0.00036474893568083644
Validation loss = 0.0005188382929190993
Validation loss = 0.0005668849335052073
Validation loss = 0.0019824763294309378
Validation loss = 0.0004891460994258523
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0012737226206809282
Validation loss = 0.0006744960555806756
Validation loss = 0.0004158022638875991
Validation loss = 0.0005452492041513324
Validation loss = 0.0007827605586498976
Validation loss = 0.00046814640518277884
Validation loss = 0.0007924724486656487
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 179      |
| Iteration     | 6        |
| MaximumReturn | 199      |
| MinimumReturn | 136      |
| TotalSamples  | 26664    |
----------------------------
itr #7 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.002723943442106247
Validation loss = 0.0008460730314254761
Validation loss = 0.00040180457290261984
Validation loss = 0.0008788573322817683
Validation loss = 0.0006136671872809529
Validation loss = 0.0011352027067914605
Validation loss = 0.0005859611555933952
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0010850292164832354
Validation loss = 0.0005980918067507446
Validation loss = 0.0004666896420530975
Validation loss = 0.0003885205660481006
Validation loss = 0.0007114571635611355
Validation loss = 0.00042777048656716943
Validation loss = 0.0005403041141107678
Validation loss = 0.0004325408081058413
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0011034953640773892
Validation loss = 0.00036229973193258047
Validation loss = 0.0007536914781667292
Validation loss = 0.0003688006545417011
Validation loss = 0.0015902961604297161
Validation loss = 0.0004878164909314364
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.000531665631569922
Validation loss = 0.0009240557556040585
Validation loss = 0.0005009289598092437
Validation loss = 0.0006542978808283806
Validation loss = 0.0008637764840386808
Validation loss = 0.0005319297197274864
Validation loss = 0.00037746585439890623
Validation loss = 0.0007208063034340739
Validation loss = 0.0006891374359838665
Validation loss = 0.0004216277739033103
Validation loss = 0.0007941325311549008
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0007412650156766176
Validation loss = 0.00044443627120926976
Validation loss = 0.00033120179432444274
Validation loss = 0.0007989279692992568
Validation loss = 0.0005741380737163126
Validation loss = 0.0004463321529328823
Validation loss = 0.0005393805331550539
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 164      |
| Iteration     | 7        |
| MaximumReturn | 199      |
| MinimumReturn | 107      |
| TotalSamples  | 29997    |
----------------------------
itr #8 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0007997333304956555
Validation loss = 0.0005301033961586654
Validation loss = 0.0009910966036841273
Validation loss = 0.00034611811861395836
Validation loss = 0.0007545597618445754
Validation loss = 0.0003076993452850729
Validation loss = 0.0009026266052387655
Validation loss = 0.0013214844511821866
Validation loss = 0.0007328102365136147
Validation loss = 0.0007385698845610023
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006835160311311483
Validation loss = 0.0006754421046935022
Validation loss = 0.0006872952217236161
Validation loss = 0.0005124438321217895
Validation loss = 0.0007619978860020638
Validation loss = 0.00033246574457734823
Validation loss = 0.00041355282883159816
Validation loss = 0.0008569331839680672
Validation loss = 0.0006454404210671782
Validation loss = 0.000439847179222852
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0005492882337421179
Validation loss = 0.0005804558750241995
Validation loss = 0.0011805318063125014
Validation loss = 0.001009647618047893
Validation loss = 0.0005629962543025613
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0006569126271642745
Validation loss = 0.0005484442808665335
Validation loss = 0.0005428011645562947
Validation loss = 0.0009225074318237603
Validation loss = 0.0018125162459909916
Validation loss = 0.00043492374243214726
Validation loss = 0.00040272052865475416
Validation loss = 0.0007929829298518598
Validation loss = 0.00037298485403880477
Validation loss = 0.0008431053138338029
Validation loss = 0.0006243021925911307
Validation loss = 0.0018128405790776014
Validation loss = 0.0002960329002235085
Validation loss = 0.0005796243785880506
Validation loss = 0.0005826540291309357
Validation loss = 0.0005612720269709826
Validation loss = 0.00040244165575131774
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0007293307571671903
Validation loss = 0.0004405367944855243
Validation loss = 0.0004448354593478143
Validation loss = 0.0005455177742987871
Validation loss = 0.0006568608223460615
Validation loss = 0.0004776619316544384
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 8        |
| MaximumReturn | 199      |
| MinimumReturn | 124      |
| TotalSamples  | 33330    |
----------------------------
itr #9 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005310936830937862
Validation loss = 0.0005548875196836889
Validation loss = 0.0005267851520329714
Validation loss = 0.0005868051666766405
Validation loss = 0.000884553010109812
Validation loss = 0.000805875810328871
Validation loss = 0.0011026393622159958
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.000503351679071784
Validation loss = 0.0003683064423967153
Validation loss = 0.0005528166657313704
Validation loss = 0.0011529254261404276
Validation loss = 0.0009484069887548685
Validation loss = 0.000419865915318951
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0005269056418910623
Validation loss = 0.0007544336840510368
Validation loss = 0.0008440983365289867
Validation loss = 0.0007380089373327792
Validation loss = 0.0005109470803290606
Validation loss = 0.0007197614177130163
Validation loss = 0.0007185845752246678
Validation loss = 0.0007876083254814148
Validation loss = 0.0005015266360715032
Validation loss = 0.00041704735485836864
Validation loss = 0.00041549914749339223
Validation loss = 0.0004082343657501042
Validation loss = 0.0005756573518738151
Validation loss = 0.0006529944948852062
Validation loss = 0.0007639487157575786
Validation loss = 0.0006939026061445475
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.000923881190828979
Validation loss = 0.00031091683194972575
Validation loss = 0.000466345576569438
Validation loss = 0.0005057487869635224
Validation loss = 0.00047261020517908037
Validation loss = 0.0006953932461328804
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0006543329800479114
Validation loss = 0.0006573001155629754
Validation loss = 0.0004677330725826323
Validation loss = 0.0005949719925411046
Validation loss = 0.0005103221628814936
Validation loss = 0.0018939472502097487
Validation loss = 0.00037111376877874136
Validation loss = 0.0005289172404445708
Validation loss = 0.0004756499547511339
Validation loss = 0.000631632748991251
Validation loss = 0.0006345929577946663
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 9        |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 36663    |
----------------------------
itr #10 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0011439401423558593
Validation loss = 0.001038608606904745
Validation loss = 0.0006130053079687059
Validation loss = 0.00046558439498767257
Validation loss = 0.000840337947010994
Validation loss = 0.0006592721329070628
Validation loss = 0.0009236388723365963
Validation loss = 0.0008238289738073945
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005478840321302414
Validation loss = 0.0005630706436932087
Validation loss = 0.0005306366365402937
Validation loss = 0.0007391886319965124
Validation loss = 0.0004933300078846514
Validation loss = 0.0005234672571532428
Validation loss = 0.0006137374439276755
Validation loss = 0.000552037381567061
Validation loss = 0.0006847437471151352
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0012157284654676914
Validation loss = 0.0005993903614580631
Validation loss = 0.0012616526801139116
Validation loss = 0.0005443058325909078
Validation loss = 0.0006227829726412892
Validation loss = 0.000537211773917079
Validation loss = 0.0007301286095753312
Validation loss = 0.0005613703979179263
Validation loss = 0.0005484657012857497
Validation loss = 0.0004978092620149255
Validation loss = 0.0007323205354623497
Validation loss = 0.0005915823858231306
Validation loss = 0.0005788510898128152
Validation loss = 0.0005386711563915014
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0008231354877352715
Validation loss = 0.0007557935314252973
Validation loss = 0.0006411706563085318
Validation loss = 0.0009457305422984064
Validation loss = 0.0006739645614288747
Validation loss = 0.0005676923901773989
Validation loss = 0.0006201287033036351
Validation loss = 0.0010577882640063763
Validation loss = 0.00045577908167615533
Validation loss = 0.0009959792951121926
Validation loss = 0.000538814056199044
Validation loss = 0.0005364972748793662
Validation loss = 0.0005439228261820972
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0009436442633159459
Validation loss = 0.001014704117551446
Validation loss = 0.0005933868233114481
Validation loss = 0.0006592783029191196
Validation loss = 0.000596510770265013
Validation loss = 0.0009664336685091257
Validation loss = 0.000618810998275876
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 177      |
| Iteration     | 10       |
| MaximumReturn | 199      |
| MinimumReturn | 116      |
| TotalSamples  | 39996    |
----------------------------
itr #11 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0007069900748319924
Validation loss = 0.001125215901993215
Validation loss = 0.0007318208809010684
Validation loss = 0.000851945485919714
Validation loss = 0.0008344684028998017
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.000723426986951381
Validation loss = 0.0007835965370759368
Validation loss = 0.0010879253968596458
Validation loss = 0.0006949020316824317
Validation loss = 0.000743220909498632
Validation loss = 0.000676927505992353
Validation loss = 0.0015421111602336168
Validation loss = 0.0007248983602039516
Validation loss = 0.000700517266523093
Validation loss = 0.0008955182274803519
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0008200281299650669
Validation loss = 0.0006814181106165051
Validation loss = 0.0014856630004942417
Validation loss = 0.0007117180502973497
Validation loss = 0.0006355491932481527
Validation loss = 0.0006740680546499789
Validation loss = 0.0006158084725029767
Validation loss = 0.0005835148622281849
Validation loss = 0.0006379210390150547
Validation loss = 0.0008689025416970253
Validation loss = 0.0006602475768886507
Validation loss = 0.0006907725473865867
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.000702740391716361
Validation loss = 0.0008225676720030606
Validation loss = 0.0006826437311246991
Validation loss = 0.0006388588808476925
Validation loss = 0.0009517053258605301
Validation loss = 0.0007097257766872644
Validation loss = 0.0011126985773444176
Validation loss = 0.0006667682901024818
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00078585249138996
Validation loss = 0.0007844691863283515
Validation loss = 0.0006988573586568236
Validation loss = 0.0009697343921288848
Validation loss = 0.0012687814887613058
Validation loss = 0.0006461165030486882
Validation loss = 0.0007115956395864487
Validation loss = 0.0006275455234572291
Validation loss = 0.0007230351329781115
Validation loss = 0.0006099817110225558
Validation loss = 0.0008771721040830016
Validation loss = 0.0006090363021939993
Validation loss = 0.0006036353297531605
Validation loss = 0.0011064719874411821
Validation loss = 0.0010018786415457726
Validation loss = 0.0007194428471848369
Validation loss = 0.0007501582731492817
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 11       |
| MaximumReturn | 199      |
| MinimumReturn | 123      |
| TotalSamples  | 43329    |
----------------------------
itr #12 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0007785671623423696
Validation loss = 0.0010602519614621997
Validation loss = 0.0008415031479671597
Validation loss = 0.0009573367424309254
Validation loss = 0.0008243708871304989
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.00142692681401968
Validation loss = 0.0007570653688162565
Validation loss = 0.0013036398449912667
Validation loss = 0.000989498570561409
Validation loss = 0.0008607007330283523
Validation loss = 0.0007631973130628467
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0008600762812420726
Validation loss = 0.0008522366406396031
Validation loss = 0.0008378570782952011
Validation loss = 0.0008704125066287816
Validation loss = 0.001017235335893929
Validation loss = 0.0007696688990108669
Validation loss = 0.0007434022845700383
Validation loss = 0.0007777462597005069
Validation loss = 0.0007943768869154155
Validation loss = 0.0009084432385861874
Validation loss = 0.0007518325001001358
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007854955620132387
Validation loss = 0.0007883330690674484
Validation loss = 0.0009143445640802383
Validation loss = 0.000798648688942194
Validation loss = 0.0010726189939305186
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0007425045478157699
Validation loss = 0.00098630681168288
Validation loss = 0.0012531602988019586
Validation loss = 0.0008365812827832997
Validation loss = 0.0010050309356302023
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 163      |
| Iteration     | 12       |
| MaximumReturn | 198      |
| MinimumReturn | 115      |
| TotalSamples  | 46662    |
----------------------------
itr #13 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0013142152456566691
Validation loss = 0.0011201270390301943
Validation loss = 0.0013981529045850039
Validation loss = 0.001154246972873807
Validation loss = 0.0010696991812437773
Validation loss = 0.0010935015743598342
Validation loss = 0.0009665903053246439
Validation loss = 0.0010180685203522444
Validation loss = 0.001027437043376267
Validation loss = 0.001141020911745727
Validation loss = 0.0010418589226901531
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0010832985863089561
Validation loss = 0.0011420517694205046
Validation loss = 0.001113686477765441
Validation loss = 0.0011310550617054105
Validation loss = 0.0010937005281448364
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0010669301263988018
Validation loss = 0.0011706226505339146
Validation loss = 0.0010188888991251588
Validation loss = 0.0010820213938131928
Validation loss = 0.0011346971150487661
Validation loss = 0.0010125727858394384
Validation loss = 0.0009961784817278385
Validation loss = 0.0010171541944146156
Validation loss = 0.0010824231430888176
Validation loss = 0.0011146018514409661
Validation loss = 0.0010999758960679173
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0011706782970577478
Validation loss = 0.0009952594991773367
Validation loss = 0.0009821814019232988
Validation loss = 0.0009566129301674664
Validation loss = 0.00128868967294693
Validation loss = 0.001333739492110908
Validation loss = 0.0012660385109484196
Validation loss = 0.0010543186217546463
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0010302183218300343
Validation loss = 0.001126352814026177
Validation loss = 0.0013373802648857236
Validation loss = 0.001018485170789063
Validation loss = 0.0010466662934049964
Validation loss = 0.0015902271261438727
Validation loss = 0.0015225025126710534
Validation loss = 0.0010266804601997137
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 13       |
| MaximumReturn | 199      |
| MinimumReturn | 115      |
| TotalSamples  | 49995    |
----------------------------
itr #14 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0012231100117787719
Validation loss = 0.001503183739259839
Validation loss = 0.001311431173235178
Validation loss = 0.0012498658616095781
Validation loss = 0.0015290240989997983
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0012575351865962148
Validation loss = 0.0012384643778204918
Validation loss = 0.0012078923173248768
Validation loss = 0.0012109453091397882
Validation loss = 0.0012401248095557094
Validation loss = 0.0011657088762149215
Validation loss = 0.0011863255640491843
Validation loss = 0.0012310609454289079
Validation loss = 0.0011228597722947598
Validation loss = 0.001483425498008728
Validation loss = 0.0011370319407433271
Validation loss = 0.0011592359514907002
Validation loss = 0.0013076129835098982
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0011857763165608048
Validation loss = 0.0011169933713972569
Validation loss = 0.0012270391453057528
Validation loss = 0.0012451780494302511
Validation loss = 0.0011903195409104228
Validation loss = 0.0011847841087728739
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0011866106651723385
Validation loss = 0.0011461572721600533
Validation loss = 0.001286557293497026
Validation loss = 0.001366775599308312
Validation loss = 0.0014357604086399078
Validation loss = 0.0011483405251055956
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0012882542796432972
Validation loss = 0.0015588972019031644
Validation loss = 0.0011359630152583122
Validation loss = 0.0011390126310288906
Validation loss = 0.0012815722730010748
Validation loss = 0.0013480773195624352
Validation loss = 0.0012040006695315242
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 14       |
| MaximumReturn | 199      |
| MinimumReturn | 129      |
| TotalSamples  | 53328    |
----------------------------
itr #15 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.001392208505421877
Validation loss = 0.0013188947923481464
Validation loss = 0.0013719541020691395
Validation loss = 0.0012559002498164773
Validation loss = 0.0013616376090794802
Validation loss = 0.0013950759312137961
Validation loss = 0.0012304787524044514
Validation loss = 0.0012922356836497784
Validation loss = 0.001344706630334258
Validation loss = 0.0012821070849895477
Validation loss = 0.0012475773692131042
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0016373295802623034
Validation loss = 0.001344302436336875
Validation loss = 0.0013254868099465966
Validation loss = 0.0012730733724310994
Validation loss = 0.0013860078761354089
Validation loss = 0.0013485937379300594
Validation loss = 0.0012413172516971827
Validation loss = 0.0013825888745486736
Validation loss = 0.0013048253022134304
Validation loss = 0.001241960097104311
Validation loss = 0.0013155397027730942
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0015496679116040468
Validation loss = 0.001315438305027783
Validation loss = 0.0012750185560435057
Validation loss = 0.0012975421268492937
Validation loss = 0.0012897534761577845
Validation loss = 0.0013075633905828
Validation loss = 0.0012913388200104237
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0012997003505006433
Validation loss = 0.0013821746688336134
Validation loss = 0.001380503410473466
Validation loss = 0.0014020855305716395
Validation loss = 0.001487479079514742
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0014838201459497213
Validation loss = 0.0014595359098166227
Validation loss = 0.0013066697865724564
Validation loss = 0.001317376270890236
Validation loss = 0.0014129711780697107
Validation loss = 0.0012630204437300563
Validation loss = 0.0014165821485221386
Validation loss = 0.0014007223071530461
Validation loss = 0.0013105631805956364
Validation loss = 0.0015055512776598334
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 15       |
| MaximumReturn | 199      |
| MinimumReturn | 122      |
| TotalSamples  | 56661    |
----------------------------
itr #16 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0014892623294144869
Validation loss = 0.0015596842858940363
Validation loss = 0.0014950092881917953
Validation loss = 0.0014992021024227142
Validation loss = 0.0016266327584162354
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0014963740250095725
Validation loss = 0.0015234284801408648
Validation loss = 0.0015378097305074334
Validation loss = 0.0015647260006517172
Validation loss = 0.0016265753656625748
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0016083518275991082
Validation loss = 0.001807829481549561
Validation loss = 0.0014772361610084772
Validation loss = 0.0014819987118244171
Validation loss = 0.0014934863429516554
Validation loss = 0.001520176068879664
Validation loss = 0.0014923370908945799
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0014981074491515756
Validation loss = 0.001887751743197441
Validation loss = 0.0014981484273448586
Validation loss = 0.0015332149341702461
Validation loss = 0.0015277975471690297
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0015892386436462402
Validation loss = 0.001458425191231072
Validation loss = 0.0015997013542801142
Validation loss = 0.0014358863700181246
Validation loss = 0.0014526768354699016
Validation loss = 0.001519360695965588
Validation loss = 0.0015808072639629245
Validation loss = 0.0015303136315196753
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 177      |
| Iteration     | 16       |
| MaximumReturn | 199      |
| MinimumReturn | 139      |
| TotalSamples  | 59994    |
----------------------------
itr #17 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0016769126523286104
Validation loss = 0.001803956925868988
Validation loss = 0.0020501851104199886
Validation loss = 0.0019482594216242433
Validation loss = 0.001693405327387154
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.001963605172932148
Validation loss = 0.0017626994522288442
Validation loss = 0.0017282979097217321
Validation loss = 0.001709461910650134
Validation loss = 0.0016950203571468592
Validation loss = 0.0021299037616699934
Validation loss = 0.0018168485257774591
Validation loss = 0.0017358687473461032
Validation loss = 0.0017232042737305164
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0017474072519689798
Validation loss = 0.0017155633540824056
Validation loss = 0.001970889512449503
Validation loss = 0.0018018013797700405
Validation loss = 0.0018398574320599437
Validation loss = 0.0016713808290660381
Validation loss = 0.001754634315147996
Validation loss = 0.0039044551085680723
Validation loss = 0.001662984723225236
Validation loss = 0.001649982645176351
Validation loss = 0.0018431335920467973
Validation loss = 0.001680439105257392
Validation loss = 0.0016367746284231544
Validation loss = 0.0017528870375826955
Validation loss = 0.0016505628591403365
Validation loss = 0.0017395955510437489
Validation loss = 0.001711529796011746
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.002216394292190671
Validation loss = 0.0017522104317322373
Validation loss = 0.0018056660192087293
Validation loss = 0.0018163323402404785
Validation loss = 0.0017000780208036304
Validation loss = 0.001707737217657268
Validation loss = 0.0017386036925017834
Validation loss = 0.0016646216390654445
Validation loss = 0.001802380895242095
Validation loss = 0.0017502974951639771
Validation loss = 0.0017483755946159363
Validation loss = 0.0017697422299534082
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.001959443325176835
Validation loss = 0.0017144265584647655
Validation loss = 0.0018702594097703695
Validation loss = 0.0017081702826544642
Validation loss = 0.0017208864446729422
Validation loss = 0.0016578254289925098
Validation loss = 0.0018925537588074803
Validation loss = 0.0017364795785397291
Validation loss = 0.0017139342380687594
Validation loss = 0.0017024477710947394
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 17       |
| MaximumReturn | 199      |
| MinimumReturn | 123      |
| TotalSamples  | 63327    |
----------------------------
itr #18 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0019549066200852394
Validation loss = 0.0016994138713926077
Validation loss = 0.0017541198758408427
Validation loss = 0.0016856677830219269
Validation loss = 0.001689393655396998
Validation loss = 0.0016729255439713597
Validation loss = 0.0017005667323246598
Validation loss = 0.0019019086612388492
Validation loss = 0.0017160711577162147
Validation loss = 0.0017243489855900407
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0017467187717556953
Validation loss = 0.00167435547336936
Validation loss = 0.0017535482766106725
Validation loss = 0.0019119970966130495
Validation loss = 0.0018356209620833397
Validation loss = 0.0016854829154908657
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0017770117847248912
Validation loss = 0.0018216430908069015
Validation loss = 0.0017042512772604823
Validation loss = 0.0019100297940894961
Validation loss = 0.0017244642367586493
Validation loss = 0.0017602697480469942
Validation loss = 0.0018009250052273273
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0020011740271002054
Validation loss = 0.0017148166662082076
Validation loss = 0.0017795409075915813
Validation loss = 0.0016992745222523808
Validation loss = 0.001675521139986813
Validation loss = 0.0018152885604649782
Validation loss = 0.0017579570412635803
Validation loss = 0.0017707815859466791
Validation loss = 0.0017263962654396892
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0018142287153750658
Validation loss = 0.0016752437222748995
Validation loss = 0.0017247939249500632
Validation loss = 0.0017266473732888699
Validation loss = 0.0016653143102303147
Validation loss = 0.0018007304752245545
Validation loss = 0.0017302617197856307
Validation loss = 0.001729639945551753
Validation loss = 0.0023267322685569525
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 18       |
| MaximumReturn | 199      |
| MinimumReturn | 121      |
| TotalSamples  | 66660    |
----------------------------
itr #19 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.002152200322598219
Validation loss = 0.0021440736018121243
Validation loss = 0.002117503434419632
Validation loss = 0.0025710896588861942
Validation loss = 0.0022108934354037046
Validation loss = 0.0021533395629376173
Validation loss = 0.002408979693427682
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.002140744822099805
Validation loss = 0.0022283755242824554
Validation loss = 0.002292612800374627
Validation loss = 0.0026338384486734867
Validation loss = 0.0024304520338773727
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.002169908257201314
Validation loss = 0.0021304371766746044
Validation loss = 0.0021522114984691143
Validation loss = 0.0025515423621982336
Validation loss = 0.002297766739502549
Validation loss = 0.00218565808609128
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00226403190754354
Validation loss = 0.002251049969345331
Validation loss = 0.002264021197333932
Validation loss = 0.002257468644529581
Validation loss = 0.002219590125605464
Validation loss = 0.0022406093776226044
Validation loss = 0.0021933026146143675
Validation loss = 0.0021421974524855614
Validation loss = 0.0023510868195444345
Validation loss = 0.0022940088529139757
Validation loss = 0.0021317885257303715
Validation loss = 0.002270455937832594
Validation loss = 0.0021298706997185946
Validation loss = 0.0021315854974091053
Validation loss = 0.002166748046875
Validation loss = 0.0022487861569970846
Validation loss = 0.0021624830551445484
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.002293080324307084
Validation loss = 0.0021747825667262077
Validation loss = 0.0021387736778706312
Validation loss = 0.0021523688919842243
Validation loss = 0.0022323518060147762
Validation loss = 0.0023238970898091793
Validation loss = 0.0022068123798817396
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 183      |
| Iteration     | 19       |
| MaximumReturn | 199      |
| MinimumReturn | 135      |
| TotalSamples  | 69993    |
----------------------------
itr #20 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0028510047122836113
Validation loss = 0.0029802131466567516
Validation loss = 0.002846672898158431
Validation loss = 0.002902101958170533
Validation loss = 0.003040280193090439
Validation loss = 0.0029342996422201395
Validation loss = 0.002819812623783946
Validation loss = 0.002835499122738838
Validation loss = 0.0027906682807952166
Validation loss = 0.0028225828427821398
Validation loss = 0.0028573148883879185
Validation loss = 0.002855555387213826
Validation loss = 0.0028900709003210068
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.002894375240430236
Validation loss = 0.0028706109151244164
Validation loss = 0.002789904363453388
Validation loss = 0.0029406449757516384
Validation loss = 0.0030106634367257357
Validation loss = 0.00291289109736681
Validation loss = 0.002889863448217511
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.002958974801003933
Validation loss = 0.0029131444171071053
Validation loss = 0.0028025878127664328
Validation loss = 0.0030341146048158407
Validation loss = 0.0028271961491554976
Validation loss = 0.002849171170964837
Validation loss = 0.002795049687847495
Validation loss = 0.002816414460539818
Validation loss = 0.0028563451487571
Validation loss = 0.0028437271248549223
Validation loss = 0.002959243953227997
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0028339424170553684
Validation loss = 0.0028452856931835413
Validation loss = 0.002898943843320012
Validation loss = 0.0028710090555250645
Validation loss = 0.00279342383146286
Validation loss = 0.0033457891549915075
Validation loss = 0.0028373338282108307
Validation loss = 0.002776489360257983
Validation loss = 0.0028498873580247164
Validation loss = 0.00289457431063056
Validation loss = 0.0028610331937670708
Validation loss = 0.002777286572381854
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.002975824987515807
Validation loss = 0.003179658902809024
Validation loss = 0.002794957486912608
Validation loss = 0.002950334455817938
Validation loss = 0.0028472270350903273
Validation loss = 0.0028225418645888567
Validation loss = 0.0029845023527741432
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 20       |
| MaximumReturn | 199      |
| MinimumReturn | 140      |
| TotalSamples  | 73326    |
----------------------------
itr #21 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0031506428495049477
Validation loss = 0.0032182224094867706
Validation loss = 0.003062768839299679
Validation loss = 0.0030113758984953165
Validation loss = 0.0030734017491340637
Validation loss = 0.003023722441866994
Validation loss = 0.003226904897019267
Validation loss = 0.003042627824470401
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.003201772691681981
Validation loss = 0.0030093281529843807
Validation loss = 0.003227947046980262
Validation loss = 0.0030403307173401117
Validation loss = 0.0029800264164805412
Validation loss = 0.003097521374002099
Validation loss = 0.0032033196184784174
Validation loss = 0.0030572114046663046
Validation loss = 0.003081734525039792
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.003086063079535961
Validation loss = 0.002989149885252118
Validation loss = 0.003105676732957363
Validation loss = 0.0029975164216011763
Validation loss = 0.003015871625393629
Validation loss = 0.003033699467778206
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0032398325856775045
Validation loss = 0.00307383481413126
Validation loss = 0.0030686103273183107
Validation loss = 0.0031413568649441004
Validation loss = 0.0029812005814164877
Validation loss = 0.003009492764249444
Validation loss = 0.003041028743609786
Validation loss = 0.003123335540294647
Validation loss = 0.002984782215207815
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.003180288476869464
Validation loss = 0.0031317686662077904
Validation loss = 0.003136115148663521
Validation loss = 0.0030698617920279503
Validation loss = 0.003045877208933234
Validation loss = 0.0030379402451217175
Validation loss = 0.003026012796908617
Validation loss = 0.0029441597871482372
Validation loss = 0.00315176323056221
Validation loss = 0.0030203659553080797
Validation loss = 0.003030861262232065
Validation loss = 0.003058767644688487
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 21       |
| MaximumReturn | 199      |
| MinimumReturn | 125      |
| TotalSamples  | 76659    |
----------------------------
itr #22 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.003657028777524829
Validation loss = 0.003709709970280528
Validation loss = 0.0035927193239331245
Validation loss = 0.003590451320633292
Validation loss = 0.003564937273040414
Validation loss = 0.00357960001565516
Validation loss = 0.0036957343108952045
Validation loss = 0.0035856508184224367
Validation loss = 0.0036046544555574656
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0038330366369336843
Validation loss = 0.0036260983906686306
Validation loss = 0.00369569705799222
Validation loss = 0.003508471418172121
Validation loss = 0.0035689680371433496
Validation loss = 0.0035339095629751682
Validation loss = 0.003608609549701214
Validation loss = 0.0036170314997434616
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.003735168604180217
Validation loss = 0.0035496389027684927
Validation loss = 0.003712814999744296
Validation loss = 0.003649438964203
Validation loss = 0.0035412197466939688
Validation loss = 0.0035457834601402283
Validation loss = 0.0035707566421478987
Validation loss = 0.003571233944967389
Validation loss = 0.003590009640902281
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0038595281075686216
Validation loss = 0.003624711884185672
Validation loss = 0.003532219910994172
Validation loss = 0.0037837130948901176
Validation loss = 0.0035506703425198793
Validation loss = 0.0035757478326559067
Validation loss = 0.003547749714925885
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0036545819602906704
Validation loss = 0.0036177749279886484
Validation loss = 0.003615590976551175
Validation loss = 0.003709200071170926
Validation loss = 0.0036300690844655037
Validation loss = 0.0036381883546710014
Validation loss = 0.0035679892171174288
Validation loss = 0.003768719732761383
Validation loss = 0.003590428503230214
Validation loss = 0.0036707876715809107
Validation loss = 0.003571768756955862
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 22       |
| MaximumReturn | 199      |
| MinimumReturn | 128      |
| TotalSamples  | 79992    |
----------------------------
itr #23 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.004514345433562994
Validation loss = 0.00437126262113452
Validation loss = 0.004328247159719467
Validation loss = 0.004416090901941061
Validation loss = 0.004388684872537851
Validation loss = 0.0043441494926810265
Validation loss = 0.004365522414445877
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.004376999102532864
Validation loss = 0.004357413854449987
Validation loss = 0.004354382865130901
Validation loss = 0.004339106380939484
Validation loss = 0.004407613538205624
Validation loss = 0.004416088107973337
Validation loss = 0.0043449439108371735
Validation loss = 0.0043595111928880215
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.004383332561701536
Validation loss = 0.0044164410792291164
Validation loss = 0.004338751547038555
Validation loss = 0.0045187631621956825
Validation loss = 0.004370708949863911
Validation loss = 0.0043545374646782875
Validation loss = 0.004338196013122797
Validation loss = 0.004437577910721302
Validation loss = 0.00444765156134963
Validation loss = 0.004278966691344976
Validation loss = 0.004568313714116812
Validation loss = 0.004365121480077505
Validation loss = 0.004349200986325741
Validation loss = 0.004369023256003857
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.004363839514553547
Validation loss = 0.004575512371957302
Validation loss = 0.004397546406835318
Validation loss = 0.004523979965597391
Validation loss = 0.004402686841785908
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.004399021156132221
Validation loss = 0.00454226229339838
Validation loss = 0.004417044576257467
Validation loss = 0.00433875760063529
Validation loss = 0.004435277543962002
Validation loss = 0.004419880919158459
Validation loss = 0.004344765096902847
Validation loss = 0.004430027212947607
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 23       |
| MaximumReturn | 199      |
| MinimumReturn | 126      |
| TotalSamples  | 83325    |
----------------------------
itr #24 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.004839339759200811
Validation loss = 0.004788123071193695
Validation loss = 0.004667928442358971
Validation loss = 0.004635835997760296
Validation loss = 0.004802432842552662
Validation loss = 0.004951828625053167
Validation loss = 0.004698678385466337
Validation loss = 0.004598639439791441
Validation loss = 0.0046271937899291515
Validation loss = 0.004565550480037928
Validation loss = 0.004636593163013458
Validation loss = 0.004785536322742701
Validation loss = 0.004659535828977823
Validation loss = 0.004632425028830767
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.004678054247051477
Validation loss = 0.004676511511206627
Validation loss = 0.004687516018748283
Validation loss = 0.004674552474170923
Validation loss = 0.004726090934127569
Validation loss = 0.004640941973775625
Validation loss = 0.004623262211680412
Validation loss = 0.0045799147337675095
Validation loss = 0.004720388446003199
Validation loss = 0.004611510317772627
Validation loss = 0.004649093374609947
Validation loss = 0.004675392527133226
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.004771654028445482
Validation loss = 0.004630512557923794
Validation loss = 0.004618976730853319
Validation loss = 0.004645426291972399
Validation loss = 0.0046164533123373985
Validation loss = 0.004893193021416664
Validation loss = 0.004596510902047157
Validation loss = 0.0046231867745518684
Validation loss = 0.0045847902074456215
Validation loss = 0.00456542894244194
Validation loss = 0.004608903545886278
Validation loss = 0.004732735455036163
Validation loss = 0.00472608907148242
Validation loss = 0.00463775685057044
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0047885156236588955
Validation loss = 0.004648535046726465
Validation loss = 0.004914436023682356
Validation loss = 0.004704632796347141
Validation loss = 0.0050304122269153595
Validation loss = 0.004700704477727413
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.004730416461825371
Validation loss = 0.004655448254197836
Validation loss = 0.004545434378087521
Validation loss = 0.0046449159272015095
Validation loss = 0.004606404807418585
Validation loss = 0.004602642729878426
Validation loss = 0.004591576289385557
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 24       |
| MaximumReturn | 198      |
| MinimumReturn | 98.6     |
| TotalSamples  | 86658    |
----------------------------
itr #25 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.005192826502025127
Validation loss = 0.005165289621800184
Validation loss = 0.005202522501349449
Validation loss = 0.005175669677555561
Validation loss = 0.005080223549157381
Validation loss = 0.005209886934608221
Validation loss = 0.005266490392386913
Validation loss = 0.00511601148173213
Validation loss = 0.00511572090908885
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.005226342473179102
Validation loss = 0.005182146560400724
Validation loss = 0.005167226307094097
Validation loss = 0.005235457327216864
Validation loss = 0.005235116463154554
Validation loss = 0.005194509401917458
Validation loss = 0.005158335901796818
Validation loss = 0.005244951229542494
Validation loss = 0.00510360999032855
Validation loss = 0.005185423884540796
Validation loss = 0.00509398989379406
Validation loss = 0.005137654487043619
Validation loss = 0.00535972835496068
Validation loss = 0.0051231374964118
Validation loss = 0.005107478704303503
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.005172060802578926
Validation loss = 0.0051634665578603745
Validation loss = 0.005062139127403498
Validation loss = 0.005179346073418856
Validation loss = 0.005120443180203438
Validation loss = 0.005220275837928057
Validation loss = 0.00523205054923892
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.005185607820749283
Validation loss = 0.0054282839410007
Validation loss = 0.005299469456076622
Validation loss = 0.005281833931803703
Validation loss = 0.005237366072833538
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.005177199840545654
Validation loss = 0.005146725103259087
Validation loss = 0.005161629524081945
Validation loss = 0.0052068172954022884
Validation loss = 0.005186847411096096
Validation loss = 0.005149874836206436
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 25       |
| MaximumReturn | 198      |
| MinimumReturn | 92.5     |
| TotalSamples  | 89991    |
----------------------------
itr #26 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00542947743088007
Validation loss = 0.005460672080516815
Validation loss = 0.005321165081113577
Validation loss = 0.005593993701040745
Validation loss = 0.005408231168985367
Validation loss = 0.005319502204656601
Validation loss = 0.005332073662430048
Validation loss = 0.005257779732346535
Validation loss = 0.005280990619212389
Validation loss = 0.005421383306384087
Validation loss = 0.00529834907501936
Validation loss = 0.0053420159965753555
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0052591790445148945
Validation loss = 0.0053614783100783825
Validation loss = 0.00545593211427331
Validation loss = 0.0053083524107933044
Validation loss = 0.005337706301361322
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00528352428227663
Validation loss = 0.005410703830420971
Validation loss = 0.005312182009220123
Validation loss = 0.005348104052245617
Validation loss = 0.005253988318145275
Validation loss = 0.0053041353821754456
Validation loss = 0.0052728913724422455
Validation loss = 0.005245917942374945
Validation loss = 0.0053212009370327
Validation loss = 0.005279685836285353
Validation loss = 0.005256074946373701
Validation loss = 0.00527444202452898
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.005357659421861172
Validation loss = 0.005408585537225008
Validation loss = 0.005516269709914923
Validation loss = 0.005310212727636099
Validation loss = 0.005545981228351593
Validation loss = 0.005441571120172739
Validation loss = 0.005303151905536652
Validation loss = 0.005310640670359135
Validation loss = 0.005288473796099424
Validation loss = 0.005344221834093332
Validation loss = 0.0053507792763412
Validation loss = 0.00530234444886446
Validation loss = 0.005273460876196623
Validation loss = 0.005430607125163078
Validation loss = 0.005298119969666004
Validation loss = 0.005279126577079296
Validation loss = 0.005531060975044966
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.005377903115004301
Validation loss = 0.005356010515242815
Validation loss = 0.005283172242343426
Validation loss = 0.005398576147854328
Validation loss = 0.005418338347226381
Validation loss = 0.005307670682668686
Validation loss = 0.005348229315131903
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 26       |
| MaximumReturn | 198      |
| MinimumReturn | 105      |
| TotalSamples  | 93324    |
----------------------------
itr #27 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.005460151471197605
Validation loss = 0.005463244393467903
Validation loss = 0.005709032528102398
Validation loss = 0.005387242883443832
Validation loss = 0.005470456089824438
Validation loss = 0.005518510937690735
Validation loss = 0.005440960172563791
Validation loss = 0.005456356331706047
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.005456321872770786
Validation loss = 0.005486996844410896
Validation loss = 0.005399630405008793
Validation loss = 0.00543928612023592
Validation loss = 0.00566647294908762
Validation loss = 0.0058075436390936375
Validation loss = 0.005488684866577387
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.005458826664835215
Validation loss = 0.005407997872680426
Validation loss = 0.005615129601210356
Validation loss = 0.005426374264061451
Validation loss = 0.005513519048690796
Validation loss = 0.005378815345466137
Validation loss = 0.005573514383286238
Validation loss = 0.005488159134984016
Validation loss = 0.005429164506494999
Validation loss = 0.0054044960997998714
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00551077863201499
Validation loss = 0.005523578729480505
Validation loss = 0.005530403461307287
Validation loss = 0.005407789256423712
Validation loss = 0.005410491023212671
Validation loss = 0.0058266338892281055
Validation loss = 0.0055310907773673534
Validation loss = 0.005502161104232073
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.005505991168320179
Validation loss = 0.00543166371062398
Validation loss = 0.005633928347378969
Validation loss = 0.0054525998421013355
Validation loss = 0.005436018109321594
Validation loss = 0.00542557705193758
Validation loss = 0.005484360735863447
Validation loss = 0.00545924250036478
Validation loss = 0.005463480483740568
Validation loss = 0.005453848280012608
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 27       |
| MaximumReturn | 199      |
| MinimumReturn | 103      |
| TotalSamples  | 96657    |
----------------------------
itr #28 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00549524137750268
Validation loss = 0.005594281945377588
Validation loss = 0.005610776599496603
Validation loss = 0.005685036536306143
Validation loss = 0.005581094417721033
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0055494592525064945
Validation loss = 0.005533228162676096
Validation loss = 0.005587584804743528
Validation loss = 0.005673140753060579
Validation loss = 0.005667399615049362
Validation loss = 0.005537102464586496
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.005565434694290161
Validation loss = 0.005636326968669891
Validation loss = 0.005592490080744028
Validation loss = 0.0055775814689695835
Validation loss = 0.005637399852275848
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00560084730386734
Validation loss = 0.006140662357211113
Validation loss = 0.00551622873172164
Validation loss = 0.0055962963961064816
Validation loss = 0.005785632412880659
Validation loss = 0.005600569769740105
Validation loss = 0.005543476436287165
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.005612984299659729
Validation loss = 0.005576504860073328
Validation loss = 0.005637405905872583
Validation loss = 0.005608497187495232
Validation loss = 0.005831874907016754
Validation loss = 0.005741402506828308
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 28       |
| MaximumReturn | 199      |
| MinimumReturn | 130      |
| TotalSamples  | 99990    |
----------------------------
itr #29 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.005488227121531963
Validation loss = 0.005539322271943092
Validation loss = 0.005605577956885099
Validation loss = 0.0054832505993545055
Validation loss = 0.005482024978846312
Validation loss = 0.005511866882443428
Validation loss = 0.005503377877175808
Validation loss = 0.0055759684182703495
Validation loss = 0.005516533739864826
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0055151814594864845
Validation loss = 0.0055804504081606865
Validation loss = 0.005522374529391527
Validation loss = 0.005585487000644207
Validation loss = 0.00550502073019743
Validation loss = 0.0055718268267810345
Validation loss = 0.005735230632126331
Validation loss = 0.005620005540549755
Validation loss = 0.005513644311577082
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.005497933831065893
Validation loss = 0.005504889413714409
Validation loss = 0.005579793825745583
Validation loss = 0.005493949167430401
Validation loss = 0.005573077592998743
Validation loss = 0.0055741723626852036
Validation loss = 0.005541207734495401
Validation loss = 0.005477114114910364
Validation loss = 0.0056788744404911995
Validation loss = 0.005543038249015808
Validation loss = 0.005595581140369177
Validation loss = 0.005729073658585548
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0056328619830310345
Validation loss = 0.005645228084176779
Validation loss = 0.00555356265977025
Validation loss = 0.0055141388438642025
Validation loss = 0.00552713219076395
Validation loss = 0.005516217555850744
Validation loss = 0.005702442955225706
Validation loss = 0.0055564153008162975
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.005531661678105593
Validation loss = 0.00552038848400116
Validation loss = 0.005570101086050272
Validation loss = 0.005615970119833946
Validation loss = 0.005720878019928932
Validation loss = 0.0055465251207351685
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 29       |
| MaximumReturn | 199      |
| MinimumReturn | 112      |
| TotalSamples  | 103323   |
----------------------------
itr #30 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.005695424973964691
Validation loss = 0.005672762170433998
Validation loss = 0.005786164663732052
Validation loss = 0.005744710098952055
Validation loss = 0.005730773322284222
Validation loss = 0.005658536683768034
Validation loss = 0.005700141191482544
Validation loss = 0.005973462015390396
Validation loss = 0.005742570850998163
Validation loss = 0.005710002966225147
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.005748063325881958
Validation loss = 0.0058951652608811855
Validation loss = 0.006041666027158499
Validation loss = 0.00577549496665597
Validation loss = 0.00569303659722209
Validation loss = 0.0058568730019032955
Validation loss = 0.005792228505015373
Validation loss = 0.005910116247832775
Validation loss = 0.005689218174666166
Validation loss = 0.005726449657231569
Validation loss = 0.005725707393139601
Validation loss = 0.00568600557744503
Validation loss = 0.0057020788080990314
Validation loss = 0.005741626024246216
Validation loss = 0.00588954146951437
Validation loss = 0.005789144430309534
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.005768452771008015
Validation loss = 0.005676280707120895
Validation loss = 0.005654373671859503
Validation loss = 0.005814692471176386
Validation loss = 0.005739457905292511
Validation loss = 0.005694494117051363
Validation loss = 0.005719225853681564
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.005730599630624056
Validation loss = 0.005749215837568045
Validation loss = 0.005764614790678024
Validation loss = 0.0056725735776126385
Validation loss = 0.005782696418464184
Validation loss = 0.005827495362609625
Validation loss = 0.005820573773235083
Validation loss = 0.00579690421000123
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.005727981682866812
Validation loss = 0.005701316986232996
Validation loss = 0.005694213323295116
Validation loss = 0.005766004323959351
Validation loss = 0.005721621681004763
Validation loss = 0.00588522432371974
Validation loss = 0.00572980334982276
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 182      |
| Iteration     | 30       |
| MaximumReturn | 199      |
| MinimumReturn | 110      |
| TotalSamples  | 106656   |
----------------------------
itr #31 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00590403750538826
Validation loss = 0.005895137321203947
Validation loss = 0.005897476337850094
Validation loss = 0.005939682479947805
Validation loss = 0.006011451594531536
Validation loss = 0.005949611775577068
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.005943723022937775
Validation loss = 0.006012325175106525
Validation loss = 0.0059222085401415825
Validation loss = 0.005892592947930098
Validation loss = 0.005924844648689032
Validation loss = 0.005848275031894445
Validation loss = 0.005914819892495871
Validation loss = 0.005983041599392891
Validation loss = 0.005903179757297039
Validation loss = 0.005867179483175278
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00586366094648838
Validation loss = 0.005913951899856329
Validation loss = 0.0059441085904836655
Validation loss = 0.005965577904134989
Validation loss = 0.005877259187400341
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.005978857632726431
Validation loss = 0.0060970657505095005
Validation loss = 0.006082824897021055
Validation loss = 0.0059000286273658276
Validation loss = 0.005884644575417042
Validation loss = 0.005954419262707233
Validation loss = 0.005930136423557997
Validation loss = 0.005959006026387215
Validation loss = 0.005886879749596119
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.005998731590807438
Validation loss = 0.005979552399367094
Validation loss = 0.0058642481453716755
Validation loss = 0.0059430403634905815
Validation loss = 0.005979353561997414
Validation loss = 0.006048773881047964
Validation loss = 0.005895249545574188
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 163      |
| Iteration     | 31       |
| MaximumReturn | 199      |
| MinimumReturn | 87.3     |
| TotalSamples  | 109989   |
----------------------------
itr #32 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.006315231788903475
Validation loss = 0.006435252260416746
Validation loss = 0.006355812773108482
Validation loss = 0.006333737634122372
Validation loss = 0.006209128070622683
Validation loss = 0.006239072419703007
Validation loss = 0.006329694762825966
Validation loss = 0.006331172771751881
Validation loss = 0.006253696512430906
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.006255016196519136
Validation loss = 0.006381423678249121
Validation loss = 0.006255795247852802
Validation loss = 0.006251261103898287
Validation loss = 0.006302029825747013
Validation loss = 0.006163714919239283
Validation loss = 0.00629059225320816
Validation loss = 0.006297165062278509
Validation loss = 0.006352712865918875
Validation loss = 0.006279384717345238
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.006378041580319405
Validation loss = 0.006244894582778215
Validation loss = 0.0062797171995043755
Validation loss = 0.006247744895517826
Validation loss = 0.006254248321056366
Validation loss = 0.006292575504630804
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0062772296369075775
Validation loss = 0.006325374823063612
Validation loss = 0.006262557581067085
Validation loss = 0.00632358156144619
Validation loss = 0.006273788865655661
Validation loss = 0.006333751138299704
Validation loss = 0.006736095529049635
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0062276944518089294
Validation loss = 0.006271360907703638
Validation loss = 0.006266733631491661
Validation loss = 0.006266475655138493
Validation loss = 0.006299472413957119
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 32       |
| MaximumReturn | 199      |
| MinimumReturn | 110      |
| TotalSamples  | 113322   |
----------------------------
itr #33 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.006320710759609938
Validation loss = 0.006338487844914198
Validation loss = 0.006399436853826046
Validation loss = 0.00634356215596199
Validation loss = 0.006296741310507059
Validation loss = 0.006310769356787205
Validation loss = 0.0062648537568748
Validation loss = 0.006298338528722525
Validation loss = 0.006292551755905151
Validation loss = 0.006304706912487745
Validation loss = 0.006322466768324375
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.006494575180113316
Validation loss = 0.006297391373664141
Validation loss = 0.006402061320841312
Validation loss = 0.00641560647636652
Validation loss = 0.006363753229379654
Validation loss = 0.006292473059147596
Validation loss = 0.006381708662956953
Validation loss = 0.006317439489066601
Validation loss = 0.006287250202149153
Validation loss = 0.006276699714362621
Validation loss = 0.006360712926834822
Validation loss = 0.006479253061115742
Validation loss = 0.006463280878961086
Validation loss = 0.006358203012496233
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.006291246972978115
Validation loss = 0.006363932508975267
Validation loss = 0.0063622999005019665
Validation loss = 0.006307657342404127
Validation loss = 0.006306041497737169
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.006359341088682413
Validation loss = 0.006415327545255423
Validation loss = 0.006478738039731979
Validation loss = 0.00631440756842494
Validation loss = 0.006341221276670694
Validation loss = 0.006277837790548801
Validation loss = 0.006346952635794878
Validation loss = 0.0063263061456382275
Validation loss = 0.006422787439078093
Validation loss = 0.006321176886558533
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.006324872374534607
Validation loss = 0.0063095358200371265
Validation loss = 0.006409632973372936
Validation loss = 0.006295584607869387
Validation loss = 0.006344265304505825
Validation loss = 0.006428143475204706
Validation loss = 0.006447702646255493
Validation loss = 0.0063543496653437614
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 161      |
| Iteration     | 33       |
| MaximumReturn | 199      |
| MinimumReturn | 109      |
| TotalSamples  | 116655   |
----------------------------
itr #34 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.006583624519407749
Validation loss = 0.006599912419915199
Validation loss = 0.00652843015268445
Validation loss = 0.006577927153557539
Validation loss = 0.006620133761316538
Validation loss = 0.006605507340282202
Validation loss = 0.006735592149198055
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.006580929271876812
Validation loss = 0.006579332984983921
Validation loss = 0.006566982716321945
Validation loss = 0.006658518686890602
Validation loss = 0.006626601330935955
Validation loss = 0.006559084169566631
Validation loss = 0.0065841227769851685
Validation loss = 0.006585760973393917
Validation loss = 0.006586416158825159
Validation loss = 0.0065469383262097836
Validation loss = 0.006793559994548559
Validation loss = 0.006662824656814337
Validation loss = 0.006609045434743166
Validation loss = 0.006566646974533796
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00652788020670414
Validation loss = 0.006601252593100071
Validation loss = 0.006538044661283493
Validation loss = 0.006518888287246227
Validation loss = 0.006603581830859184
Validation loss = 0.006537371315062046
Validation loss = 0.006608570460230112
Validation loss = 0.00667526014149189
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0066304500214755535
Validation loss = 0.006626724265515804
Validation loss = 0.0065488205291330814
Validation loss = 0.006645786575973034
Validation loss = 0.006615910679101944
Validation loss = 0.006597951054573059
Validation loss = 0.006789619568735361
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0065573533065617085
Validation loss = 0.006594878155738115
Validation loss = 0.006628908682614565
Validation loss = 0.006682415492832661
Validation loss = 0.0065960451029241085
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 34       |
| MaximumReturn | 199      |
| MinimumReturn | 114      |
| TotalSamples  | 119988   |
----------------------------
itr #35 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.006541065406054258
Validation loss = 0.006507626734673977
Validation loss = 0.006608504336327314
Validation loss = 0.006534103769809008
Validation loss = 0.0065341247245669365
Validation loss = 0.0066343629732728004
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.006567514501512051
Validation loss = 0.006573811173439026
Validation loss = 0.006534371059387922
Validation loss = 0.006560455542057753
Validation loss = 0.006595078390091658
Validation loss = 0.0065366472117602825
Validation loss = 0.006497705355286598
Validation loss = 0.006619330961257219
Validation loss = 0.0064985984936356544
Validation loss = 0.006540211383253336
Validation loss = 0.006547475233674049
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.006780417636036873
Validation loss = 0.006603704299777746
Validation loss = 0.0065222554840147495
Validation loss = 0.006533872801810503
Validation loss = 0.006569094490259886
Validation loss = 0.006547180935740471
Validation loss = 0.006537012755870819
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.006506161764264107
Validation loss = 0.006525710225105286
Validation loss = 0.006657968740910292
Validation loss = 0.0066488864831626415
Validation loss = 0.006615808233618736
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.006736272480338812
Validation loss = 0.00654101837426424
Validation loss = 0.006602467969059944
Validation loss = 0.0064955600537359715
Validation loss = 0.006520959082990885
Validation loss = 0.006660826504230499
Validation loss = 0.006602296605706215
Validation loss = 0.0065176221542060375
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 35       |
| MaximumReturn | 199      |
| MinimumReturn | 112      |
| TotalSamples  | 123321   |
----------------------------
itr #36 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.006759874988347292
Validation loss = 0.006642087362706661
Validation loss = 0.006825875490903854
Validation loss = 0.006839739624410868
Validation loss = 0.006647290661931038
Validation loss = 0.0067532737739384174
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.006657135207206011
Validation loss = 0.006703718099743128
Validation loss = 0.0067509813234210014
Validation loss = 0.00670658890157938
Validation loss = 0.0068128714337944984
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.006690029986202717
Validation loss = 0.006701887119561434
Validation loss = 0.006685364525765181
Validation loss = 0.00665355334058404
Validation loss = 0.006711611989885569
Validation loss = 0.006933840457350016
Validation loss = 0.0066350591368973255
Validation loss = 0.006665958557277918
Validation loss = 0.006726522464305162
Validation loss = 0.006645571440458298
Validation loss = 0.006693227216601372
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.006743249483406544
Validation loss = 0.006674791686236858
Validation loss = 0.006805028300732374
Validation loss = 0.006969792302697897
Validation loss = 0.006738116964697838
Validation loss = 0.006908442359417677
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.006685267202556133
Validation loss = 0.007026029750704765
Validation loss = 0.006654384545981884
Validation loss = 0.006811222992837429
Validation loss = 0.006686601787805557
Validation loss = 0.006645258981734514
Validation loss = 0.006731083150953054
Validation loss = 0.006787830498069525
Validation loss = 0.0067378561943769455
Validation loss = 0.006790664978325367
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 159      |
| Iteration     | 36       |
| MaximumReturn | 199      |
| MinimumReturn | 83.5     |
| TotalSamples  | 126654   |
----------------------------
itr #37 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.007140276953577995
Validation loss = 0.007088016718626022
Validation loss = 0.007031058426946402
Validation loss = 0.0070398650132119656
Validation loss = 0.00703465985134244
Validation loss = 0.007082072086632252
Validation loss = 0.007080055307596922
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0070788986049592495
Validation loss = 0.006972119677811861
Validation loss = 0.007035831920802593
Validation loss = 0.007045823149383068
Validation loss = 0.007062697317451239
Validation loss = 0.007006428670138121
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0070170448161661625
Validation loss = 0.00715452479198575
Validation loss = 0.007107250392436981
Validation loss = 0.007127799093723297
Validation loss = 0.007210840005427599
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.007054412271827459
Validation loss = 0.007068478502333164
Validation loss = 0.007071387954056263
Validation loss = 0.007067772559821606
Validation loss = 0.0072884876281023026
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00706991134211421
Validation loss = 0.0070551978424191475
Validation loss = 0.007032544352114201
Validation loss = 0.006998018827289343
Validation loss = 0.007074382621794939
Validation loss = 0.007055211346596479
Validation loss = 0.007070548366755247
Validation loss = 0.007066062185913324
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 37       |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 129987   |
----------------------------
itr #38 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.007499434053897858
Validation loss = 0.007250505033880472
Validation loss = 0.007252149283885956
Validation loss = 0.00725267268717289
Validation loss = 0.007188500836491585
Validation loss = 0.007199140731245279
Validation loss = 0.007260349579155445
Validation loss = 0.007258763071149588
Validation loss = 0.007298337295651436
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.007148712873458862
Validation loss = 0.007375981658697128
Validation loss = 0.007223798427730799
Validation loss = 0.00735347718000412
Validation loss = 0.007223434280604124
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.007158920168876648
Validation loss = 0.007302520330995321
Validation loss = 0.007167235016822815
Validation loss = 0.007198021747171879
Validation loss = 0.007196796126663685
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.007212735712528229
Validation loss = 0.007172550540417433
Validation loss = 0.007210070267319679
Validation loss = 0.007226521149277687
Validation loss = 0.007244860753417015
Validation loss = 0.007208762690424919
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.007206892594695091
Validation loss = 0.007169811055064201
Validation loss = 0.007250228431075811
Validation loss = 0.007532949093729258
Validation loss = 0.007187490817159414
Validation loss = 0.007167867850512266
Validation loss = 0.007193494588136673
Validation loss = 0.007151832338422537
Validation loss = 0.007244025357067585
Validation loss = 0.007248287089169025
Validation loss = 0.007271953392773867
Validation loss = 0.007155659608542919
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 38       |
| MaximumReturn | 199      |
| MinimumReturn | 121      |
| TotalSamples  | 133320   |
----------------------------
itr #39 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.007506498135626316
Validation loss = 0.007481649983674288
Validation loss = 0.007530511356890202
Validation loss = 0.007497394923120737
Validation loss = 0.0075186071917414665
Validation loss = 0.007555627729743719
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0075241271406412125
Validation loss = 0.007505262270569801
Validation loss = 0.007700532674789429
Validation loss = 0.007477755658328533
Validation loss = 0.007502864580601454
Validation loss = 0.007722374051809311
Validation loss = 0.007549905683845282
Validation loss = 0.007524481043219566
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.007530327886343002
Validation loss = 0.007637249771505594
Validation loss = 0.007526571396738291
Validation loss = 0.0076402029953897
Validation loss = 0.007474589627236128
Validation loss = 0.0075621334835886955
Validation loss = 0.0076849814504384995
Validation loss = 0.00755248311907053
Validation loss = 0.007515133824199438
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00754862604662776
Validation loss = 0.007556316442787647
Validation loss = 0.007507895585149527
Validation loss = 0.007505413610488176
Validation loss = 0.0075090681202709675
Validation loss = 0.007571936585009098
Validation loss = 0.0075872838497161865
Validation loss = 0.0076864841394126415
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.007628800347447395
Validation loss = 0.007661765441298485
Validation loss = 0.007579358294606209
Validation loss = 0.007508676964789629
Validation loss = 0.007510313298553228
Validation loss = 0.007487372495234013
Validation loss = 0.007567076478153467
Validation loss = 0.007494383957237005
Validation loss = 0.007537005469202995
Validation loss = 0.007541762199252844
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 180      |
| Iteration     | 39       |
| MaximumReturn | 199      |
| MinimumReturn | 131      |
| TotalSamples  | 136653   |
----------------------------
