Logging to experiments/pendulum/Pendulum/w100e0.5_seed3214
Print configuration .....
{'env_name': 'pendulum', 'random_seeds': [3214, 2431, 2531, 2231], 'save_variables': False, 'model_save_dir': '/tmp/pendulum_models/', 'restore_variables': False, 'start_onpol_iter': 0, 'onpol_iters': 40, 'num_path_random': 25, 'num_path_onpol': 25, 'env_horizon': 200, 'max_train_data': 200000, 'max_val_data': 100000, 'discard_ratio': 0.0, 'dynamics': {'pre_training': {'mode': 'intrinsic_reward', 'itr': 0, 'policy_itr': 20}, 'model': 'nn', 'ensemble': True, 'ensemble_model_count': 5, 'enable_particle_ensemble': True, 'particles': 5, 'obs_var': 1.0, 'intrinsic_reward_coeff': 1.0, 'ita': 1.0, 'mode': 'random', 'val': True, 'n_layers': 4, 'hidden_size': 1000, 'activation': 'relu', 'batch_size': 1000, 'learning_rate': 0.001, 'reg_coeff': 0.0, 'epochs': 200, 'kfac_params': {'learning_rate': 0.1, 'damping': 0.001, 'momentum': 0.9, 'kl_clip': 0.0001, 'cov_ema_decay': 0.99}}, 'policy': {'network_shape': [64, 64], 'init_logstd': 0.0, 'activation': 'tanh', 'reinitialize_every_itr': False}, 'trpo': {'horizon': 200, 'gamma': 0.99, 'step_size': 0.01, 'iterations': 20, 'batch_size': 50000, 'gae': 0.95, 'visualization': False, 'visualize_iterations': [0]}, 'algo': 'trpo'}
Generating random rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating random rollouts.
Creating normalization for training data.
Done creating normalization for training data.
Particle ensemble enabled? True
An ensemble of 5 dynamics model <class 'model.dynamics.NNDynamicsModel'> initialized
Train dynamics model with intrinsic reward only? False
Pre-training enabled. Using only intrinsic reward.
Pre-training dynamics model for 0 iterations...
Done pre-training dynamics model.
Using external reward only.
itr #0 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.41331973671913147
Validation loss = 0.031202275305986404
Validation loss = 0.008588519878685474
Validation loss = 0.003894902765750885
Validation loss = 0.002165182027965784
Validation loss = 0.0018032609950751066
Validation loss = 0.0018135497812181711
Validation loss = 0.005871259607374668
Validation loss = 0.0022437337320297956
Validation loss = 0.0015932020032778382
Validation loss = 0.002534079598262906
Validation loss = 0.0022676733788102865
Validation loss = 0.005719383247196674
Validation loss = 0.0022003839258104563
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3602522313594818
Validation loss = 0.05049796402454376
Validation loss = 0.011508374474942684
Validation loss = 0.004333621822297573
Validation loss = 0.002596573904156685
Validation loss = 0.0018685933900997043
Validation loss = 0.0017688714433461428
Validation loss = 0.0016581171657890081
Validation loss = 0.0016075816238299012
Validation loss = 0.0015806942246854305
Validation loss = 0.001596186775714159
Validation loss = 0.001666574040427804
Validation loss = 0.0020782870706170797
Validation loss = 0.0017548580653965473
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.31649455428123474
Validation loss = 0.04086587578058243
Validation loss = 0.010139488615095615
Validation loss = 0.004263423383235931
Validation loss = 0.0023652808740735054
Validation loss = 0.0017878777580335736
Validation loss = 0.001671993755735457
Validation loss = 0.0015446426114067435
Validation loss = 0.0015872804215177894
Validation loss = 0.001835495000705123
Validation loss = 0.0014880017843097448
Validation loss = 0.005570241715759039
Validation loss = 0.00215346971526742
Validation loss = 0.0016019297763705254
Validation loss = 0.001392761361785233
Validation loss = 0.0023253161925822496
Validation loss = 0.0019956750329583883
Validation loss = 0.0017748018726706505
Validation loss = 0.0034852500539273024
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.34049779176712036
Validation loss = 0.03670486807823181
Validation loss = 0.010016869753599167
Validation loss = 0.003930127248167992
Validation loss = 0.002377632074058056
Validation loss = 0.001852606888860464
Validation loss = 0.0016500194324180484
Validation loss = 0.0016129766590893269
Validation loss = 0.0018617995083332062
Validation loss = 0.0022674871142953634
Validation loss = 0.0016123091336339712
Validation loss = 0.0015527193900197744
Validation loss = 0.00967260543256998
Validation loss = 0.0039307535625994205
Validation loss = 0.001633823849260807
Validation loss = 0.0016998790670186281
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.34840062260627747
Validation loss = 0.03952222317457199
Validation loss = 0.013102278113365173
Validation loss = 0.005272066220641136
Validation loss = 0.002664179541170597
Validation loss = 0.0020544594153761864
Validation loss = 0.0017016613855957985
Validation loss = 0.00162507186178118
Validation loss = 0.0015506275231018662
Validation loss = 0.001578988740220666
Validation loss = 0.0015616430900990963
Validation loss = 0.0015596583252772689
Validation loss = 0.003965866286307573
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 178      |
| Iteration     | 0        |
| MaximumReturn | 199      |
| MinimumReturn | 120      |
| TotalSamples  | 6666     |
----------------------------
itr #1 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.010857989080250263
Validation loss = 0.0026987632736563683
Validation loss = 0.0011251737596467137
Validation loss = 0.0011452181497588754
Validation loss = 0.0019442979246377945
Validation loss = 0.0010639074025675654
Validation loss = 0.0017722399206832051
Validation loss = 0.0026932775508612394
Validation loss = 0.001157284714281559
Validation loss = 0.004138363990932703
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.014539174735546112
Validation loss = 0.0027772237081080675
Validation loss = 0.0011025952408090234
Validation loss = 0.0011236348655074835
Validation loss = 0.004936572629958391
Validation loss = 0.0017922684783115983
Validation loss = 0.001380503992550075
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.02493622899055481
Validation loss = 0.0030895874369889498
Validation loss = 0.0011848584981635213
Validation loss = 0.0009923111647367477
Validation loss = 0.0013423538766801357
Validation loss = 0.0012769008753821254
Validation loss = 0.00116287125274539
Validation loss = 0.0019668799359351397
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.011432553641498089
Validation loss = 0.002401466714218259
Validation loss = 0.0011664741905406117
Validation loss = 0.0011529272887855768
Validation loss = 0.000912244024220854
Validation loss = 0.0015555109130218625
Validation loss = 0.0009943951154127717
Validation loss = 0.0011438234942033887
Validation loss = 0.0016384236514568329
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.015435452573001385
Validation loss = 0.0022874469868838787
Validation loss = 0.0011760195484384894
Validation loss = 0.0016437377780675888
Validation loss = 0.001036083442158997
Validation loss = 0.001081914291717112
Validation loss = 0.002239106222987175
Validation loss = 0.0018282540841028094
Validation loss = 0.001269449363462627
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 1        |
| MaximumReturn | 199      |
| MinimumReturn | 118      |
| TotalSamples  | 9999     |
----------------------------
itr #2 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00608689384534955
Validation loss = 0.0012215252500027418
Validation loss = 0.001335514010861516
Validation loss = 0.0019475467270240188
Validation loss = 0.0015071352245286107
Validation loss = 0.0019266738090664148
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.006112047005444765
Validation loss = 0.0018651006976142526
Validation loss = 0.0016567467246204615
Validation loss = 0.0008064339053817093
Validation loss = 0.001429312047548592
Validation loss = 0.001314727938733995
Validation loss = 0.001804777653887868
Validation loss = 0.0010921885259449482
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.006331539712846279
Validation loss = 0.0008151287911459804
Validation loss = 0.0010443866485729814
Validation loss = 0.0008526447927579284
Validation loss = 0.0009168935939669609
Validation loss = 0.005684254691004753
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00450718542560935
Validation loss = 0.0007850565016269684
Validation loss = 0.0009112920379266143
Validation loss = 0.004705236759036779
Validation loss = 0.0018924286123365164
Validation loss = 0.0008088467875495553
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.006089814472943544
Validation loss = 0.0012405595043674111
Validation loss = 0.0010708934860303998
Validation loss = 0.001117923529818654
Validation loss = 0.00230458565056324
Validation loss = 0.0010037727188318968
Validation loss = 0.001765137305483222
Validation loss = 0.001260014483705163
Validation loss = 0.001471238792873919
Validation loss = 0.0013835662975907326
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 181      |
| Iteration     | 2        |
| MaximumReturn | 199      |
| MinimumReturn | 138      |
| TotalSamples  | 13332    |
----------------------------
itr #3 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.003928445745259523
Validation loss = 0.0020506775472313166
Validation loss = 0.0009139508474618196
Validation loss = 0.00372295081615448
Validation loss = 0.0013753656530752778
Validation loss = 0.0017972321948036551
Validation loss = 0.0019666936714202166
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.002483398886397481
Validation loss = 0.0014029376907274127
Validation loss = 0.0014390386641025543
Validation loss = 0.0018224784871563315
Validation loss = 0.002116777468472719
Validation loss = 0.0009781725239008665
Validation loss = 0.0023367826361209154
Validation loss = 0.0014964427100494504
Validation loss = 0.0014620827278122306
Validation loss = 0.002491287188604474
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.006361574400216341
Validation loss = 0.0010737479897215962
Validation loss = 0.0008279667817987502
Validation loss = 0.0010999583173543215
Validation loss = 0.0009546378278173506
Validation loss = 0.002922842977568507
Validation loss = 0.0007441421621479094
Validation loss = 0.0009099935996346176
Validation loss = 0.001714073121547699
Validation loss = 0.0031559148337692022
Validation loss = 0.0006681755185127258
Validation loss = 0.002479917136952281
Validation loss = 0.0021215018350631
Validation loss = 0.0020574850495904684
Validation loss = 0.0013675001682713628
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0033367525320500135
Validation loss = 0.0010664397850632668
Validation loss = 0.0012905679177492857
Validation loss = 0.0010851995320990682
Validation loss = 0.0011092551285400987
Validation loss = 0.002551174722611904
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.004289171192795038
Validation loss = 0.001109369215555489
Validation loss = 0.0010964155662804842
Validation loss = 0.001510710921138525
Validation loss = 0.0011475515784695745
Validation loss = 0.0015307219000533223
Validation loss = 0.0022774646058678627
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 178      |
| Iteration     | 3        |
| MaximumReturn | 199      |
| MinimumReturn | 120      |
| TotalSamples  | 16665    |
----------------------------
itr #4 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.006485867314040661
Validation loss = 0.0011058044619858265
Validation loss = 0.0011302587809041142
Validation loss = 0.0008475396316498518
Validation loss = 0.0006662929081358016
Validation loss = 0.0014045459683984518
Validation loss = 0.0017706772778183222
Validation loss = 0.0010667616734281182
Validation loss = 0.0014306330122053623
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.004046812653541565
Validation loss = 0.0008712836424820125
Validation loss = 0.0007503214292228222
Validation loss = 0.0007099322392605245
Validation loss = 0.002191753825172782
Validation loss = 0.0008548831101506948
Validation loss = 0.0024611065164208412
Validation loss = 0.0011778383050113916
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.004117782227694988
Validation loss = 0.0006824785959906876
Validation loss = 0.0037730697076767683
Validation loss = 0.0006901125889271498
Validation loss = 0.0006737562944181263
Validation loss = 0.001807418535463512
Validation loss = 0.0009529126109555364
Validation loss = 0.0007883592043071985
Validation loss = 0.0008993029478006065
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0020223348401486874
Validation loss = 0.0007473438163287938
Validation loss = 0.0008115525706671178
Validation loss = 0.0014783168444409966
Validation loss = 0.0011022795224562287
Validation loss = 0.0012622731737792492
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.009199089370667934
Validation loss = 0.0010121256345883012
Validation loss = 0.0009137686574831605
Validation loss = 0.0012506920611485839
Validation loss = 0.0012430462520569563
Validation loss = 0.0010829950915649533
Validation loss = 0.0006706742569804192
Validation loss = 0.0012931596720591187
Validation loss = 0.0010249081533402205
Validation loss = 0.001118165673688054
Validation loss = 0.000628124107606709
Validation loss = 0.0008834903128445148
Validation loss = 0.00084879802307114
Validation loss = 0.000753351952880621
Validation loss = 0.0012168578105047345
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 4        |
| MaximumReturn | 198      |
| MinimumReturn | 128      |
| TotalSamples  | 19998    |
----------------------------
itr #5 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.001359653426334262
Validation loss = 0.0011336926836520433
Validation loss = 0.0007651993655599654
Validation loss = 0.0008485711878165603
Validation loss = 0.0011315234005451202
Validation loss = 0.0016716995742172003
Validation loss = 0.0013093568850308657
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0014346020761877298
Validation loss = 0.0009581962367519736
Validation loss = 0.0009848007466644049
Validation loss = 0.0007959802751429379
Validation loss = 0.001116667641326785
Validation loss = 0.0011581169674172997
Validation loss = 0.001612265594303608
Validation loss = 0.0006981612532399595
Validation loss = 0.0032573030330240726
Validation loss = 0.0006234636530280113
Validation loss = 0.0009271627059206367
Validation loss = 0.0009237265912815928
Validation loss = 0.001441093860194087
Validation loss = 0.0006536656874231994
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.005877185147255659
Validation loss = 0.0007511599687859416
Validation loss = 0.0007706719916313887
Validation loss = 0.000687500461935997
Validation loss = 0.0006036015693098307
Validation loss = 0.0015591747360303998
Validation loss = 0.0012342074187472463
Validation loss = 0.0007176323560997844
Validation loss = 0.0009134912979789078
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0014274943387135863
Validation loss = 0.0008564867894165218
Validation loss = 0.0008830685401335359
Validation loss = 0.0018574172863736749
Validation loss = 0.0013081447687000036
Validation loss = 0.0015796771040186286
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0023241869639605284
Validation loss = 0.0006883508758619428
Validation loss = 0.0010734020033851266
Validation loss = 0.0010388203663751483
Validation loss = 0.002029835246503353
Validation loss = 0.0009966131765395403
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 5        |
| MaximumReturn | 199      |
| MinimumReturn | 132      |
| TotalSamples  | 23331    |
----------------------------
itr #6 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.004222540650516748
Validation loss = 0.001702542300336063
Validation loss = 0.000677629781421274
Validation loss = 0.0010358982253819704
Validation loss = 0.000744761317037046
Validation loss = 0.000620284816250205
Validation loss = 0.0013076115865260363
Validation loss = 0.0013034783769398928
Validation loss = 0.0005528648034669459
Validation loss = 0.0009932040702551603
Validation loss = 0.0008220461895689368
Validation loss = 0.0015059966826811433
Validation loss = 0.0005096753593534231
Validation loss = 0.0007196891820058227
Validation loss = 0.0009371360647492111
Validation loss = 0.001691189012490213
Validation loss = 0.0007277248078025877
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0009857281111180782
Validation loss = 0.0007682960131205618
Validation loss = 0.0008136734832078218
Validation loss = 0.000981576507911086
Validation loss = 0.001769208931364119
Validation loss = 0.0008104518055915833
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0012475637486204505
Validation loss = 0.0005735011654905975
Validation loss = 0.0015229828422889113
Validation loss = 0.0020753638818860054
Validation loss = 0.0008544293814338744
Validation loss = 0.0005434096674434841
Validation loss = 0.0006531288963742554
Validation loss = 0.0025559214409440756
Validation loss = 0.0005346126272343099
Validation loss = 0.000702513090800494
Validation loss = 0.0006172958528622985
Validation loss = 0.0010234789224341512
Validation loss = 0.0011336472816765308
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0038994974456727505
Validation loss = 0.00048024056013673544
Validation loss = 0.0007184816640801728
Validation loss = 0.0013026637025177479
Validation loss = 0.000610398652497679
Validation loss = 0.0014943205751478672
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.003335065906867385
Validation loss = 0.0007248818292282522
Validation loss = 0.0007609439780935645
Validation loss = 0.0016737367259338498
Validation loss = 0.0008158889249898493
Validation loss = 0.005767780356109142
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 6        |
| MaximumReturn | 199      |
| MinimumReturn | 107      |
| TotalSamples  | 26664    |
----------------------------
itr #7 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0008594734827056527
Validation loss = 0.0005656109424307942
Validation loss = 0.0005742748035117984
Validation loss = 0.0009883412858471274
Validation loss = 0.0050962138921022415
Validation loss = 0.0005866044666618109
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0007146515999920666
Validation loss = 0.0008627399220131338
Validation loss = 0.0006454564863815904
Validation loss = 0.0008668636437505484
Validation loss = 0.0011893861228600144
Validation loss = 0.0006506609497591853
Validation loss = 0.0006915424019098282
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.001332589308731258
Validation loss = 0.0007542638340964913
Validation loss = 0.0010952833108603954
Validation loss = 0.0004654637014027685
Validation loss = 0.0007656783564016223
Validation loss = 0.0005950125632807612
Validation loss = 0.0010543221142143011
Validation loss = 0.0007808581576682627
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0008806323166936636
Validation loss = 0.0014161162544041872
Validation loss = 0.0007306921761482954
Validation loss = 0.0013247387250885367
Validation loss = 0.0019799123983830214
Validation loss = 0.0005928262835368514
Validation loss = 0.0011328781256452203
Validation loss = 0.0005769662093371153
Validation loss = 0.0007885280647315085
Validation loss = 0.0004519686917774379
Validation loss = 0.000529329408891499
Validation loss = 0.0012799110263586044
Validation loss = 0.0005467115552164614
Validation loss = 0.0007300158613361418
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.002243836410343647
Validation loss = 0.0007582101388834417
Validation loss = 0.0005384677788242698
Validation loss = 0.0008021583780646324
Validation loss = 0.0008923190762288868
Validation loss = 0.0006062663742341101
Validation loss = 0.0007621821714565158
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 7        |
| MaximumReturn | 199      |
| MinimumReturn | 116      |
| TotalSamples  | 29997    |
----------------------------
itr #8 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0012889407807961106
Validation loss = 0.0004515521868597716
Validation loss = 0.00048506396706216037
Validation loss = 0.0011359698837623
Validation loss = 0.0007617739611305296
Validation loss = 0.0007236726814880967
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006295338389463723
Validation loss = 0.0020790009293705225
Validation loss = 0.0004323965695220977
Validation loss = 0.0006332743214443326
Validation loss = 0.000500742404256016
Validation loss = 0.001572695909999311
Validation loss = 0.00041733396938070655
Validation loss = 0.0005916038062423468
Validation loss = 0.0007490558782592416
Validation loss = 0.0008328368421643972
Validation loss = 0.0010618676897138357
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0008445152780041099
Validation loss = 0.0013377275317907333
Validation loss = 0.0004716910480055958
Validation loss = 0.0004734224930871278
Validation loss = 0.0015031195944175124
Validation loss = 0.0004422493220772594
Validation loss = 0.0007123230025172234
Validation loss = 0.0005033105844631791
Validation loss = 0.0008166908519342542
Validation loss = 0.0010984730906784534
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0006819915142841637
Validation loss = 0.001011724234558642
Validation loss = 0.0010584404226392508
Validation loss = 0.0006280774250626564
Validation loss = 0.0019607364665716887
Validation loss = 0.0004636448866222054
Validation loss = 0.001579405041411519
Validation loss = 0.0005134160164743662
Validation loss = 0.001398452091962099
Validation loss = 0.0004415513831190765
Validation loss = 0.00040809749043546617
Validation loss = 0.0017657108837738633
Validation loss = 0.000660461257211864
Validation loss = 0.0006385984015651047
Validation loss = 0.0004471958673093468
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0007722526206634939
Validation loss = 0.0006851622019894421
Validation loss = 0.0005422420217655599
Validation loss = 0.0007429290562868118
Validation loss = 0.0008549937629140913
Validation loss = 0.0008639927837066352
Validation loss = 0.0006432982045225799
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 8        |
| MaximumReturn | 199      |
| MinimumReturn | 118      |
| TotalSamples  | 33330    |
----------------------------
itr #9 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.000933654373511672
Validation loss = 0.0005369331338442862
Validation loss = 0.0006763673154637218
Validation loss = 0.001301220036111772
Validation loss = 0.0009925225749611855
Validation loss = 0.0008744216756895185
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0007339889416471124
Validation loss = 0.0006410207133740187
Validation loss = 0.0004666707827709615
Validation loss = 0.0013065229868516326
Validation loss = 0.0011057653464376926
Validation loss = 0.0004739812866318971
Validation loss = 0.0006657427875325084
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.001658139517530799
Validation loss = 0.0006067679496482015
Validation loss = 0.0005520086269825697
Validation loss = 0.000491284066811204
Validation loss = 0.0005156390252523124
Validation loss = 0.0005485121509991586
Validation loss = 0.0006448457133956254
Validation loss = 0.0006892080418765545
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007535743061453104
Validation loss = 0.0007774296100251377
Validation loss = 0.0007291520014405251
Validation loss = 0.00201016990467906
Validation loss = 0.0007074451423250139
Validation loss = 0.0004762081371154636
Validation loss = 0.0011169805657118559
Validation loss = 0.0005960427224636078
Validation loss = 0.00042834170744754374
Validation loss = 0.0004952398012392223
Validation loss = 0.00054201390594244
Validation loss = 0.0015668183332309127
Validation loss = 0.0015512554673478007
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0009131236001849174
Validation loss = 0.0004197459784336388
Validation loss = 0.0010248400503769517
Validation loss = 0.0005378510686568916
Validation loss = 0.0006760925753042102
Validation loss = 0.0005799205391667783
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 9        |
| MaximumReturn | 199      |
| MinimumReturn | 124      |
| TotalSamples  | 36663    |
----------------------------
itr #10 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0008137023542076349
Validation loss = 0.000617441488429904
Validation loss = 0.0005028758896514773
Validation loss = 0.0007463697693310678
Validation loss = 0.0020370190031826496
Validation loss = 0.00043041291064582765
Validation loss = 0.0006495873676612973
Validation loss = 0.0004845087823923677
Validation loss = 0.0005465071299113333
Validation loss = 0.00045697877067141235
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0012959741288796067
Validation loss = 0.0005965030286461115
Validation loss = 0.00044391516712494195
Validation loss = 0.0005817320779897273
Validation loss = 0.0005638168659061193
Validation loss = 0.00106035394128412
Validation loss = 0.001210052170790732
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0006784994038753211
Validation loss = 0.0006641485379077494
Validation loss = 0.0005889395251870155
Validation loss = 0.0005901165422983468
Validation loss = 0.0005300152697600424
Validation loss = 0.0005912554333917797
Validation loss = 0.0005001986864954233
Validation loss = 0.0005905943689867854
Validation loss = 0.001012446009553969
Validation loss = 0.0004996487987227738
Validation loss = 0.000481771829072386
Validation loss = 0.0010494607267901301
Validation loss = 0.0003654749598354101
Validation loss = 0.0006771755870431662
Validation loss = 0.0005076911766082048
Validation loss = 0.0009373193606734276
Validation loss = 0.00045567465713247657
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0009246826521120965
Validation loss = 0.00040786867612041533
Validation loss = 0.0004798912850674242
Validation loss = 0.00041941588278859854
Validation loss = 0.0005848498549312353
Validation loss = 0.0005485429428517818
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0015829832991585135
Validation loss = 0.00041036627953872085
Validation loss = 0.0006486105266958475
Validation loss = 0.001732705277390778
Validation loss = 0.00041817917372100055
Validation loss = 0.0005031561013311148
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 10       |
| MaximumReturn | 199      |
| MinimumReturn | 115      |
| TotalSamples  | 39996    |
----------------------------
itr #11 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0007815542630851269
Validation loss = 0.0008589589851908386
Validation loss = 0.0005113899242132902
Validation loss = 0.0004741208686027676
Validation loss = 0.00046072970144450665
Validation loss = 0.0004945470136590302
Validation loss = 0.0004994490882381797
Validation loss = 0.0006714219925925136
Validation loss = 0.00038734011468477547
Validation loss = 0.0008626228081993759
Validation loss = 0.0005725934752263129
Validation loss = 0.0007329288637265563
Validation loss = 0.0009873707313090563
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0008164955070242286
Validation loss = 0.0004908993723802269
Validation loss = 0.0005956856766715646
Validation loss = 0.000519997498486191
Validation loss = 0.000635554373729974
Validation loss = 0.0005049764877185225
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0006058685830794275
Validation loss = 0.0016510639106854796
Validation loss = 0.00041393734863959253
Validation loss = 0.0004729115462396294
Validation loss = 0.0008943822467699647
Validation loss = 0.00041015021270141006
Validation loss = 0.0004900785279460251
Validation loss = 0.0007268781773746014
Validation loss = 0.0006802700809203088
Validation loss = 0.0010621619876474142
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.001123951282352209
Validation loss = 0.0007404732168652117
Validation loss = 0.0005898875533603132
Validation loss = 0.0005718867178075016
Validation loss = 0.0005574498791247606
Validation loss = 0.0004425567458383739
Validation loss = 0.00044110105955041945
Validation loss = 0.0005082337884232402
Validation loss = 0.0005952272331342101
Validation loss = 0.0006535873981192708
Validation loss = 0.0004175413341727108
Validation loss = 0.0012936023995280266
Validation loss = 0.0006893818499520421
Validation loss = 0.0015496975975111127
Validation loss = 0.0004924970562569797
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0009507330250926316
Validation loss = 0.001582797267474234
Validation loss = 0.0005304697551764548
Validation loss = 0.0004962373641319573
Validation loss = 0.0006107377703301609
Validation loss = 0.0008854775805957615
Validation loss = 0.001635638764128089
Validation loss = 0.00042333098826929927
Validation loss = 0.0023913513869047165
Validation loss = 0.0004859882756136358
Validation loss = 0.0004401891492307186
Validation loss = 0.0005024202400818467
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 11       |
| MaximumReturn | 198      |
| MinimumReturn | 117      |
| TotalSamples  | 43329    |
----------------------------
itr #12 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005080786067992449
Validation loss = 0.0006808208418078721
Validation loss = 0.0004592745390255004
Validation loss = 0.0005535957170650363
Validation loss = 0.0005640641902573407
Validation loss = 0.0004410086548887193
Validation loss = 0.00045347289415076375
Validation loss = 0.0007660607225261629
Validation loss = 0.0005058268434368074
Validation loss = 0.0006320772808976471
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005686563672497869
Validation loss = 0.0005471651093102992
Validation loss = 0.0004992830799892545
Validation loss = 0.0005161038716323674
Validation loss = 0.0007263557054102421
Validation loss = 0.0006367862806655467
Validation loss = 0.0006011986988596618
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0009355554939247668
Validation loss = 0.0005209276569075882
Validation loss = 0.0010894951410591602
Validation loss = 0.00043865502811968327
Validation loss = 0.0006777263479307294
Validation loss = 0.0009140294860117137
Validation loss = 0.00048225527280010283
Validation loss = 0.0011462593683972955
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0004885941161774099
Validation loss = 0.0006162424106150866
Validation loss = 0.0021595542784780264
Validation loss = 0.0004073953896295279
Validation loss = 0.00045344303362071514
Validation loss = 0.0007166539435274899
Validation loss = 0.0007910284912213683
Validation loss = 0.0004712509398814291
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00041319633601233363
Validation loss = 0.0005531010101549327
Validation loss = 0.0005725711234845221
Validation loss = 0.0005290928529575467
Validation loss = 0.0005860468954779208
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 12       |
| MaximumReturn | 199      |
| MinimumReturn | 116      |
| TotalSamples  | 46662    |
----------------------------
itr #13 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005838338984176517
Validation loss = 0.0011905302526429296
Validation loss = 0.0005907863960601389
Validation loss = 0.0007083188975229859
Validation loss = 0.00044068318675272167
Validation loss = 0.00045645592035725713
Validation loss = 0.0005161535227671266
Validation loss = 0.0007964804535731673
Validation loss = 0.0004648183530662209
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005949875921942294
Validation loss = 0.0005519718397408724
Validation loss = 0.0005481072003021836
Validation loss = 0.0005270217661745846
Validation loss = 0.0004736197297461331
Validation loss = 0.0005188049981370568
Validation loss = 0.0015402212738990784
Validation loss = 0.00047950714360922575
Validation loss = 0.0006010475335642695
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0007675461820326746
Validation loss = 0.00042389269219711423
Validation loss = 0.0005864883423782885
Validation loss = 0.0005052852793596685
Validation loss = 0.0008361693471670151
Validation loss = 0.00042466423474252224
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0008643192704766989
Validation loss = 0.00044062829692848027
Validation loss = 0.000834684818983078
Validation loss = 0.00042613191180862486
Validation loss = 0.0004995710914954543
Validation loss = 0.000414842099417001
Validation loss = 0.0005318490439094603
Validation loss = 0.0008074011420831084
Validation loss = 0.00047037441981956363
Validation loss = 0.00043707730947062373
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00047119767987169325
Validation loss = 0.0006423650775104761
Validation loss = 0.0005711676785722375
Validation loss = 0.0009106401121243834
Validation loss = 0.0005570019711740315
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 13       |
| MaximumReturn | 199      |
| MinimumReturn | 131      |
| TotalSamples  | 49995    |
----------------------------
itr #14 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005346102989278734
Validation loss = 0.0005158368148840964
Validation loss = 0.0008952238131314516
Validation loss = 0.0004994425689801574
Validation loss = 0.0008734285947866738
Validation loss = 0.0007374110864475369
Validation loss = 0.000524693401530385
Validation loss = 0.0006203533848747611
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006305626011453569
Validation loss = 0.000555036123842001
Validation loss = 0.000517658656463027
Validation loss = 0.0012102844193577766
Validation loss = 0.0006721807876601815
Validation loss = 0.0005387397250160575
Validation loss = 0.0005753592704422772
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.001996260602027178
Validation loss = 0.0007466709357686341
Validation loss = 0.0007960749207995832
Validation loss = 0.0005384757532738149
Validation loss = 0.0005211937241256237
Validation loss = 0.0007633806671947241
Validation loss = 0.0005312026478350163
Validation loss = 0.0006455047405324876
Validation loss = 0.0005447669536806643
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005385135882534087
Validation loss = 0.0005129050114192069
Validation loss = 0.0007848191889934242
Validation loss = 0.0007102423696778715
Validation loss = 0.0005336417816579342
Validation loss = 0.0006354141514748335
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0010202168487012386
Validation loss = 0.0004882845678366721
Validation loss = 0.000718233291991055
Validation loss = 0.0005747229442931712
Validation loss = 0.0006349629256874323
Validation loss = 0.0009157135500572622
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 14       |
| MaximumReturn | 199      |
| MinimumReturn | 116      |
| TotalSamples  | 53328    |
----------------------------
itr #15 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.000517993641551584
Validation loss = 0.0005914829671382904
Validation loss = 0.000719307514373213
Validation loss = 0.0006951543618924916
Validation loss = 0.0050239404663443565
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005871759494766593
Validation loss = 0.0008129349444061518
Validation loss = 0.0005602504825219512
Validation loss = 0.0005524713196791708
Validation loss = 0.0006522026378661394
Validation loss = 0.0008862692047841847
Validation loss = 0.0005798446363769472
Validation loss = 0.0015091410605236888
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0005491093033924699
Validation loss = 0.0010261210845783353
Validation loss = 0.0005649476079270244
Validation loss = 0.0006013289676047862
Validation loss = 0.0005689572426490486
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0006285181734710932
Validation loss = 0.00052873813547194
Validation loss = 0.0005018579540774226
Validation loss = 0.0004904986708424985
Validation loss = 0.0008385684341192245
Validation loss = 0.0005073430947959423
Validation loss = 0.0005880978424102068
Validation loss = 0.00043890340020880103
Validation loss = 0.0005089141777716577
Validation loss = 0.0004890121053904295
Validation loss = 0.0006349888863041997
Validation loss = 0.0009353561908937991
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0009278637007810175
Validation loss = 0.0005692902486771345
Validation loss = 0.001064274343661964
Validation loss = 0.0007164367707446218
Validation loss = 0.0005721246707253158
Validation loss = 0.0006496149580925703
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 163      |
| Iteration     | 15       |
| MaximumReturn | 199      |
| MinimumReturn | 114      |
| TotalSamples  | 56661    |
----------------------------
itr #16 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.001156345009803772
Validation loss = 0.0006170629640109837
Validation loss = 0.0007358049624599516
Validation loss = 0.000713516550604254
Validation loss = 0.0007677571848034859
Validation loss = 0.0009721717215143144
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006805328303016722
Validation loss = 0.0006071209209039807
Validation loss = 0.0006679833750240505
Validation loss = 0.0006946452194824815
Validation loss = 0.0006216421606950462
Validation loss = 0.0007049032137729228
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0006386115564964712
Validation loss = 0.0011339773191139102
Validation loss = 0.0006622817018069327
Validation loss = 0.0006691119051538408
Validation loss = 0.0008083992288447917
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0006740986718796194
Validation loss = 0.0006475156405940652
Validation loss = 0.000637464108876884
Validation loss = 0.0006711274036206305
Validation loss = 0.0006899754516780376
Validation loss = 0.0006452323286794126
Validation loss = 0.0010389367816969752
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0011372969020158052
Validation loss = 0.0008865463314577937
Validation loss = 0.0008122255676425993
Validation loss = 0.0006442511221393943
Validation loss = 0.0006326133152469993
Validation loss = 0.0009525111527182162
Validation loss = 0.0006121080950833857
Validation loss = 0.000726758036762476
Validation loss = 0.0006930416566319764
Validation loss = 0.0006293850601650774
Validation loss = 0.0008558130939491093
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 163      |
| Iteration     | 16       |
| MaximumReturn | 198      |
| MinimumReturn | 112      |
| TotalSamples  | 59994    |
----------------------------
itr #17 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0008010851452127099
Validation loss = 0.0007460478809662163
Validation loss = 0.0006229690043255687
Validation loss = 0.0006914285477250814
Validation loss = 0.0007470056298188865
Validation loss = 0.0006671790033578873
Validation loss = 0.000800018897280097
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0007984476978890598
Validation loss = 0.0007397877634502947
Validation loss = 0.0006780694820918143
Validation loss = 0.0006896181148476899
Validation loss = 0.0007020254852250218
Validation loss = 0.0006657434278167784
Validation loss = 0.001527102431282401
Validation loss = 0.0006401005084626377
Validation loss = 0.0006586465751752257
Validation loss = 0.0008075935184024274
Validation loss = 0.0007280318532139063
Validation loss = 0.0007876977324485779
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0007059313938952982
Validation loss = 0.0006797994719818234
Validation loss = 0.000780223635956645
Validation loss = 0.0006759511888958514
Validation loss = 0.0007772225653752685
Validation loss = 0.0007391926483251154
Validation loss = 0.0007780360756441951
Validation loss = 0.0006883071619085968
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0006921681924723089
Validation loss = 0.00060769118135795
Validation loss = 0.0006697346107102931
Validation loss = 0.0006458535208366811
Validation loss = 0.0008615973056294024
Validation loss = 0.0006404219893738627
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0010079329367727041
Validation loss = 0.0006876752595417202
Validation loss = 0.0008134304662235081
Validation loss = 0.0006984139326959848
Validation loss = 0.0007984099793247879
Validation loss = 0.0009104026248678565
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 17       |
| MaximumReturn | 199      |
| MinimumReturn | 117      |
| TotalSamples  | 63327    |
----------------------------
itr #18 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0009801830165088177
Validation loss = 0.0008211006061173975
Validation loss = 0.0006806975579820573
Validation loss = 0.0007308654021471739
Validation loss = 0.0008569022174924612
Validation loss = 0.0007175502832978964
Validation loss = 0.0010758745484054089
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0007382939220406115
Validation loss = 0.0009188026888296008
Validation loss = 0.0007345019839704037
Validation loss = 0.0008536091772839427
Validation loss = 0.0007454440346919
Validation loss = 0.0007507261470891535
Validation loss = 0.0008857423672452569
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0007469023112207651
Validation loss = 0.0006926385103724897
Validation loss = 0.0007715035462751985
Validation loss = 0.0007004841463640332
Validation loss = 0.0007043280638754368
Validation loss = 0.0008675541612319648
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007140787784010172
Validation loss = 0.0006853208760730922
Validation loss = 0.0006733390036970377
Validation loss = 0.0007588606094941497
Validation loss = 0.0007289990317076445
Validation loss = 0.0007230935734696686
Validation loss = 0.000767678371630609
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0008467041188850999
Validation loss = 0.000787470955401659
Validation loss = 0.000971959438174963
Validation loss = 0.0008234501583501697
Validation loss = 0.0007182068657130003
Validation loss = 0.000765270262490958
Validation loss = 0.0008149812347255647
Validation loss = 0.000656530843116343
Validation loss = 0.0007890712004154921
Validation loss = 0.0007677082321606576
Validation loss = 0.0007753586978651583
Validation loss = 0.0007308200583793223
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 18       |
| MaximumReturn | 199      |
| MinimumReturn | 103      |
| TotalSamples  | 66660    |
----------------------------
itr #19 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0008760535856708884
Validation loss = 0.0008714564610272646
Validation loss = 0.0007453386206179857
Validation loss = 0.0009174702572636306
Validation loss = 0.0008043896523304284
Validation loss = 0.000757174042519182
Validation loss = 0.0009601066703908145
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0008327183895744383
Validation loss = 0.0007637322414666414
Validation loss = 0.0007769354269839823
Validation loss = 0.0014118454419076443
Validation loss = 0.0007865510997362435
Validation loss = 0.0007458961918018758
Validation loss = 0.0007200015243142843
Validation loss = 0.0008884845301508904
Validation loss = 0.0008989270427264273
Validation loss = 0.0008580528083257377
Validation loss = 0.0008320672786794603
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0007430676487274468
Validation loss = 0.0008049829630181193
Validation loss = 0.000789825280662626
Validation loss = 0.0008842481183819473
Validation loss = 0.000848246505483985
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0017196161206811666
Validation loss = 0.0007610110333189368
Validation loss = 0.0009139705216512084
Validation loss = 0.0007479518535546958
Validation loss = 0.0007278078119270504
Validation loss = 0.000753493164665997
Validation loss = 0.0007710197241976857
Validation loss = 0.0007172253099270165
Validation loss = 0.0008946458692662418
Validation loss = 0.0008661292376928031
Validation loss = 0.0010944942478090525
Validation loss = 0.0007309894426725805
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0007672233041375875
Validation loss = 0.0010157247306779027
Validation loss = 0.0007844964857213199
Validation loss = 0.0007730443030595779
Validation loss = 0.0007463252986781299
Validation loss = 0.000710036139935255
Validation loss = 0.0008064374560490251
Validation loss = 0.0010759622091427445
Validation loss = 0.000770501559600234
Validation loss = 0.0007682329160161316
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 177      |
| Iteration     | 19       |
| MaximumReturn | 199      |
| MinimumReturn | 120      |
| TotalSamples  | 69993    |
----------------------------
itr #20 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0012249662540853024
Validation loss = 0.0011854635085910559
Validation loss = 0.0009310346795246005
Validation loss = 0.0010279318084940314
Validation loss = 0.0009641900542192161
Validation loss = 0.0010008583776652813
Validation loss = 0.0011329371482133865
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0011038826778531075
Validation loss = 0.001089054741896689
Validation loss = 0.0010040354682132602
Validation loss = 0.0011278061429038644
Validation loss = 0.0008847643621265888
Validation loss = 0.001201770850457251
Validation loss = 0.0009704788099043071
Validation loss = 0.0010443502105772495
Validation loss = 0.0009003772283904254
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0009448195341974497
Validation loss = 0.0009749574237503111
Validation loss = 0.0011471427278593183
Validation loss = 0.0010131563758477569
Validation loss = 0.0010309917852282524
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0009184293448925018
Validation loss = 0.0010082956869155169
Validation loss = 0.0009824942098930478
Validation loss = 0.000946022046264261
Validation loss = 0.0009548034868203104
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0010212591150775552
Validation loss = 0.0013150700833648443
Validation loss = 0.00095637101912871
Validation loss = 0.0010565510019659996
Validation loss = 0.0009396608220413327
Validation loss = 0.0009389835759066045
Validation loss = 0.000983134494163096
Validation loss = 0.0009805683512240648
Validation loss = 0.0009965818608179688
Validation loss = 0.0011105660814791918
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 20       |
| MaximumReturn | 199      |
| MinimumReturn | 125      |
| TotalSamples  | 73326    |
----------------------------
itr #21 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0012034650426357985
Validation loss = 0.0009308886947110295
Validation loss = 0.0010128229623660445
Validation loss = 0.0012374317739158869
Validation loss = 0.0011192183010280132
Validation loss = 0.001014960347674787
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0010875724256038666
Validation loss = 0.0009610156994313002
Validation loss = 0.0009605884552001953
Validation loss = 0.0011261575855314732
Validation loss = 0.0010198329109698534
Validation loss = 0.0010112456511706114
Validation loss = 0.0010812142863869667
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.001165012246929109
Validation loss = 0.0013312902301549911
Validation loss = 0.0010705663589760661
Validation loss = 0.0009882372105494142
Validation loss = 0.0009876224212348461
Validation loss = 0.0012031268561258912
Validation loss = 0.0010786668863147497
Validation loss = 0.001148313982412219
Validation loss = 0.0011281617917120457
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0009948952356353402
Validation loss = 0.0010788112413138151
Validation loss = 0.0009626130340620875
Validation loss = 0.0010103646200150251
Validation loss = 0.0009643702651374042
Validation loss = 0.0010797737631946802
Validation loss = 0.0011952086351811886
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0010172139154747128
Validation loss = 0.000997764989733696
Validation loss = 0.0009811933850869536
Validation loss = 0.0009950383100658655
Validation loss = 0.000963060068897903
Validation loss = 0.0009782463312149048
Validation loss = 0.0012292424216866493
Validation loss = 0.001044603530317545
Validation loss = 0.0009749253513291478
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 21       |
| MaximumReturn | 199      |
| MinimumReturn | 117      |
| TotalSamples  | 76659    |
----------------------------
itr #22 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0012580053880810738
Validation loss = 0.001211562310345471
Validation loss = 0.0012813496869057417
Validation loss = 0.0011555502424016595
Validation loss = 0.0012058953288942575
Validation loss = 0.0011599865974858403
Validation loss = 0.0011785312090069056
Validation loss = 0.0011721458286046982
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0012444908497855067
Validation loss = 0.0011756792664527893
Validation loss = 0.004864185582846403
Validation loss = 0.0011198640568181872
Validation loss = 0.0012084246845915914
Validation loss = 0.0013154582120478153
Validation loss = 0.0011967013124376535
Validation loss = 0.0012061422457918525
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0012897634878754616
Validation loss = 0.0011639976873993874
Validation loss = 0.0012529585510492325
Validation loss = 0.0011465155985206366
Validation loss = 0.0012025056639686227
Validation loss = 0.001215376891195774
Validation loss = 0.0012318406952545047
Validation loss = 0.0013230174081400037
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.001469564507715404
Validation loss = 0.0013950146967545152
Validation loss = 0.0013085779501125216
Validation loss = 0.0020348653197288513
Validation loss = 0.001214558258652687
Validation loss = 0.0012148699024692178
Validation loss = 0.0012849245686084032
Validation loss = 0.001405388698913157
Validation loss = 0.0012692970922216773
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.001488440902903676
Validation loss = 0.0011360666248947382
Validation loss = 0.001404174487106502
Validation loss = 0.0013394817942753434
Validation loss = 0.0012226831167936325
Validation loss = 0.0011402018135413527
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 155      |
| Iteration     | 22       |
| MaximumReturn | 199      |
| MinimumReturn | 99.1     |
| TotalSamples  | 79992    |
----------------------------
itr #23 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.001335890730842948
Validation loss = 0.0012881994480267167
Validation loss = 0.0014227887149900198
Validation loss = 0.0013630669564008713
Validation loss = 0.0013463462237268686
Validation loss = 0.0014761568745598197
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0014773678267374635
Validation loss = 0.0016069842968136072
Validation loss = 0.0014829420251771808
Validation loss = 0.0013563723769038916
Validation loss = 0.0014089507749304175
Validation loss = 0.0013294024392962456
Validation loss = 0.0014242117758840322
Validation loss = 0.0013511227443814278
Validation loss = 0.0012752522015944123
Validation loss = 0.0014252627734094858
Validation loss = 0.0012978442246094346
Validation loss = 0.0013300160644575953
Validation loss = 0.0014933624770492315
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0013497198233380914
Validation loss = 0.001369136618450284
Validation loss = 0.001492556999437511
Validation loss = 0.0013510618591681123
Validation loss = 0.001460306579247117
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.001307463040575385
Validation loss = 0.0012674344470724463
Validation loss = 0.0013204396236687899
Validation loss = 0.0013394308043643832
Validation loss = 0.0013467561220750213
Validation loss = 0.0013848866801708937
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00127986550796777
Validation loss = 0.0012923416215926409
Validation loss = 0.0015447211917489767
Validation loss = 0.0014839298091828823
Validation loss = 0.0013050431152805686
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 177      |
| Iteration     | 23       |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 83325    |
----------------------------
itr #24 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.001384487608447671
Validation loss = 0.0012662207009270787
Validation loss = 0.0014270915417000651
Validation loss = 0.0014152952935546637
Validation loss = 0.0015645279781892896
Validation loss = 0.0013774383114650846
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0013247447786852717
Validation loss = 0.0013407268561422825
Validation loss = 0.0013225823640823364
Validation loss = 0.0014552691718563437
Validation loss = 0.0013146146666258574
Validation loss = 0.0013263521250337362
Validation loss = 0.0013818362494930625
Validation loss = 0.0014655644772574306
Validation loss = 0.0013375648995861411
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0020126833114773035
Validation loss = 0.0013469415716826916
Validation loss = 0.0013043771032243967
Validation loss = 0.0013512453297153115
Validation loss = 0.0013373999390751123
Validation loss = 0.0013329355278983712
Validation loss = 0.0013973820023238659
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0014502640115097165
Validation loss = 0.001326667028479278
Validation loss = 0.001290027634240687
Validation loss = 0.001390746096149087
Validation loss = 0.001366875134408474
Validation loss = 0.0013350755907595158
Validation loss = 0.001324926968663931
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0013205412542447448
Validation loss = 0.0015263145323842764
Validation loss = 0.001520082470960915
Validation loss = 0.0013989058788865805
Validation loss = 0.0013188341399654746
Validation loss = 0.001339969108812511
Validation loss = 0.0014760673511773348
Validation loss = 0.0012751736212521791
Validation loss = 0.0015016506658867002
Validation loss = 0.0013690621126443148
Validation loss = 0.0013005196815356612
Validation loss = 0.0013180000241845846
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 24       |
| MaximumReturn | 199      |
| MinimumReturn | 118      |
| TotalSamples  | 86658    |
----------------------------
itr #25 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.001413245452567935
Validation loss = 0.0014152497751638293
Validation loss = 0.0015362610574811697
Validation loss = 0.0015622942009940743
Validation loss = 0.0015156088629737496
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0016672710189595819
Validation loss = 0.0014935079962015152
Validation loss = 0.001539534656330943
Validation loss = 0.001483407337218523
Validation loss = 0.0013916799798607826
Validation loss = 0.001502486877143383
Validation loss = 0.0014784943778067827
Validation loss = 0.0014039656380191445
Validation loss = 0.0015220273053273559
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0014298742171376944
Validation loss = 0.0015295562334358692
Validation loss = 0.0014103640569373965
Validation loss = 0.0014868545113131404
Validation loss = 0.001482392312027514
Validation loss = 0.0022889587562531233
Validation loss = 0.001668541575782001
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0014994017546996474
Validation loss = 0.0017923674313351512
Validation loss = 0.001408287207596004
Validation loss = 0.0015565287321805954
Validation loss = 0.0014160224236547947
Validation loss = 0.0014960120897740126
Validation loss = 0.0014676910359412432
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0014490480534732342
Validation loss = 0.0014295694418251514
Validation loss = 0.0015654813032597303
Validation loss = 0.0013910842826589942
Validation loss = 0.001431322656571865
Validation loss = 0.0014642082387581468
Validation loss = 0.001558904186822474
Validation loss = 0.0015499797882512212
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 25       |
| MaximumReturn | 199      |
| MinimumReturn | 125      |
| TotalSamples  | 89991    |
----------------------------
itr #26 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.001646514399908483
Validation loss = 0.0016970043070614338
Validation loss = 0.0017370041459798813
Validation loss = 0.001621263101696968
Validation loss = 0.0016377799911424518
Validation loss = 0.0017182598821818829
Validation loss = 0.0016569405561313033
Validation loss = 0.0016145287081599236
Validation loss = 0.0017165683675557375
Validation loss = 0.0017236275598406792
Validation loss = 0.0018880048301070929
Validation loss = 0.001701605273410678
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0016539966454729438
Validation loss = 0.001602805801667273
Validation loss = 0.0016764990286901593
Validation loss = 0.001605124562047422
Validation loss = 0.0017997786635532975
Validation loss = 0.0017557661049067974
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0016518945340067148
Validation loss = 0.0016140417428687215
Validation loss = 0.001663604867644608
Validation loss = 0.0018734493060037494
Validation loss = 0.0016277562826871872
Validation loss = 0.0016360428417101502
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0017050807364284992
Validation loss = 0.0018864553421735764
Validation loss = 0.0016509706620126963
Validation loss = 0.0017437738133594394
Validation loss = 0.0017969153122976422
Validation loss = 0.0016617876244708896
Validation loss = 0.0016738945851102471
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.001683804439380765
Validation loss = 0.0016568866558372974
Validation loss = 0.0016210017492994666
Validation loss = 0.0016550351865589619
Validation loss = 0.0016556567279621959
Validation loss = 0.0016691015334799886
Validation loss = 0.0016707375179976225
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 182      |
| Iteration     | 26       |
| MaximumReturn | 199      |
| MinimumReturn | 145      |
| TotalSamples  | 93324    |
----------------------------
itr #27 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.001661787973716855
Validation loss = 0.0017264054622501135
Validation loss = 0.0017124704318121076
Validation loss = 0.0017166489269584417
Validation loss = 0.0017730183899402618
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0017223316244781017
Validation loss = 0.001691341050900519
Validation loss = 0.0017968378961086273
Validation loss = 0.0017419018549844623
Validation loss = 0.0016958204796537757
Validation loss = 0.002040359890088439
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0018818266689777374
Validation loss = 0.0016544919926673174
Validation loss = 0.0018242300720885396
Validation loss = 0.0017001311061903834
Validation loss = 0.0016877858433872461
Validation loss = 0.0017043479019775987
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0017522127600386739
Validation loss = 0.001754374592565
Validation loss = 0.001726401038467884
Validation loss = 0.0018776118522509933
Validation loss = 0.0018145310459658504
Validation loss = 0.0017040984239429235
Validation loss = 0.0017261301400139928
Validation loss = 0.0017357132164761424
Validation loss = 0.001701553352177143
Validation loss = 0.0016519312048330903
Validation loss = 0.0017214695690199733
Validation loss = 0.0017536233644932508
Validation loss = 0.0017293309792876244
Validation loss = 0.0018274560570716858
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00167369213886559
Validation loss = 0.0016647676238790154
Validation loss = 0.001833519316278398
Validation loss = 0.0017778965411707759
Validation loss = 0.00168406474404037
Validation loss = 0.001707779010757804
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 27       |
| MaximumReturn | 199      |
| MinimumReturn | 109      |
| TotalSamples  | 96657    |
----------------------------
itr #28 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.002034683944657445
Validation loss = 0.002045044442638755
Validation loss = 0.0019423087360337377
Validation loss = 0.0019337773555889726
Validation loss = 0.0019321799045428634
Validation loss = 0.0019201971590518951
Validation loss = 0.0019326506881043315
Validation loss = 0.001982121029868722
Validation loss = 0.001995278522372246
Validation loss = 0.0019106011604890227
Validation loss = 0.0019431235268712044
Validation loss = 0.002152924658730626
Validation loss = 0.0021381278056651354
Validation loss = 0.0020530878100544214
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0020327128004282713
Validation loss = 0.002047990681603551
Validation loss = 0.0019338363781571388
Validation loss = 0.0022174532059580088
Validation loss = 0.0020031174644827843
Validation loss = 0.001935988082550466
Validation loss = 0.0019183992408216
Validation loss = 0.001907489262521267
Validation loss = 0.0018868617480620742
Validation loss = 0.001953758765012026
Validation loss = 0.001928186509758234
Validation loss = 0.0019776765257120132
Validation loss = 0.0018890096107497811
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0019753710366785526
Validation loss = 0.0021972411777824163
Validation loss = 0.0019137590425089002
Validation loss = 0.002637200290337205
Validation loss = 0.001921523711644113
Validation loss = 0.0019023517379537225
Validation loss = 0.0019949746783822775
Validation loss = 0.0019510407000780106
Validation loss = 0.0019474861910566688
Validation loss = 0.001960363471880555
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0019312059739604592
Validation loss = 0.001974963815882802
Validation loss = 0.0019089188426733017
Validation loss = 0.001900554052554071
Validation loss = 0.0019215447828173637
Validation loss = 0.001919336966238916
Validation loss = 0.0019917769823223352
Validation loss = 0.0019589283037930727
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0019216086948290467
Validation loss = 0.001957058208063245
Validation loss = 0.00194490363355726
Validation loss = 0.0019194806227460504
Validation loss = 0.0019441628828644753
Validation loss = 0.0019900454208254814
Validation loss = 0.001890824525617063
Validation loss = 0.0019854484125971794
Validation loss = 0.002006928203627467
Validation loss = 0.001910403952933848
Validation loss = 0.0019317438127472997
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 181      |
| Iteration     | 28       |
| MaximumReturn | 199      |
| MinimumReturn | 134      |
| TotalSamples  | 99990    |
----------------------------
itr #29 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.002754911547526717
Validation loss = 0.0026639585848897696
Validation loss = 0.0026436287444084883
Validation loss = 0.002627790905535221
Validation loss = 0.0027306885458528996
Validation loss = 0.002609266433864832
Validation loss = 0.0026616596151143312
Validation loss = 0.0027913704980164766
Validation loss = 0.002650601789355278
Validation loss = 0.002657742705196142
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0029241228476166725
Validation loss = 0.0026613199152052402
Validation loss = 0.0025812953244894743
Validation loss = 0.0026553685311228037
Validation loss = 0.0026113081257790327
Validation loss = 0.0028025612700730562
Validation loss = 0.0026083183474838734
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0027025085873901844
Validation loss = 0.002654716605320573
Validation loss = 0.00268710614182055
Validation loss = 0.0026087313890457153
Validation loss = 0.0026179132983088493
Validation loss = 0.002608807059004903
Validation loss = 0.002652176422998309
Validation loss = 0.002598501043394208
Validation loss = 0.002630693605169654
Validation loss = 0.0026085644494742155
Validation loss = 0.002703313482925296
Validation loss = 0.0026032431051135063
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.002684313338249922
Validation loss = 0.002667840337380767
Validation loss = 0.0026642954908311367
Validation loss = 0.0026355355512350798
Validation loss = 0.002599935047328472
Validation loss = 0.0025937494356185198
Validation loss = 0.0026704699266701937
Validation loss = 0.0027006736490875483
Validation loss = 0.002707738894969225
Validation loss = 0.0026310861576348543
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0026291226968169212
Validation loss = 0.0026769693940877914
Validation loss = 0.0025981352664530277
Validation loss = 0.0028797215782105923
Validation loss = 0.0027287930715829134
Validation loss = 0.002647619228810072
Validation loss = 0.0026039916556328535
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 29       |
| MaximumReturn | 199      |
| MinimumReturn | 118      |
| TotalSamples  | 103323   |
----------------------------
itr #30 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.002785163000226021
Validation loss = 0.002882369328290224
Validation loss = 0.002762414049357176
Validation loss = 0.002796413376927376
Validation loss = 0.002747905906289816
Validation loss = 0.0028238953091204166
Validation loss = 0.0031361007131636143
Validation loss = 0.0027171755209565163
Validation loss = 0.0027644827496260405
Validation loss = 0.0027377018705010414
Validation loss = 0.0027919672429561615
Validation loss = 0.002746650716289878
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0027905015740543604
Validation loss = 0.0029905037954449654
Validation loss = 0.002726755104959011
Validation loss = 0.0027123880572617054
Validation loss = 0.0028856100980192423
Validation loss = 0.0028315396048128605
Validation loss = 0.002728558611124754
Validation loss = 0.002708506304770708
Validation loss = 0.002785704331472516
Validation loss = 0.002843041904270649
Validation loss = 0.002753622131422162
Validation loss = 0.0027157538570463657
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.002840394387021661
Validation loss = 0.0027910161297768354
Validation loss = 0.0027888957411050797
Validation loss = 0.002754468470811844
Validation loss = 0.0027647754177451134
Validation loss = 0.002745291218161583
Validation loss = 0.0031835860572755337
Validation loss = 0.0027581185568124056
Validation loss = 0.002792556770145893
Validation loss = 0.002870975760743022
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0028697208035737276
Validation loss = 0.0029118098318576813
Validation loss = 0.002767497906461358
Validation loss = 0.002755100140348077
Validation loss = 0.0028511076234281063
Validation loss = 0.0027031830977648497
Validation loss = 0.002875985112041235
Validation loss = 0.002803608076646924
Validation loss = 0.0027947057969868183
Validation loss = 0.0027623444329947233
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0028821169398725033
Validation loss = 0.002872392302379012
Validation loss = 0.002731927437707782
Validation loss = 0.002832753350958228
Validation loss = 0.0027390685863792896
Validation loss = 0.0027510831132531166
Validation loss = 0.0029097304213792086
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 179      |
| Iteration     | 30       |
| MaximumReturn | 199      |
| MinimumReturn | 109      |
| TotalSamples  | 106656   |
----------------------------
itr #31 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0034372450318187475
Validation loss = 0.0034849592484533787
Validation loss = 0.0034559499472379684
Validation loss = 0.003499126760289073
Validation loss = 0.0034251390025019646
Validation loss = 0.003590376116335392
Validation loss = 0.003415770595893264
Validation loss = 0.0035194989759474993
Validation loss = 0.003393746679648757
Validation loss = 0.0033965278416872025
Validation loss = 0.0034271515905857086
Validation loss = 0.0033814217895269394
Validation loss = 0.0034110327251255512
Validation loss = 0.003387829288840294
Validation loss = 0.0034064953215420246
Validation loss = 0.00346161937341094
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0035447305999696255
Validation loss = 0.003494704607874155
Validation loss = 0.0034202230162918568
Validation loss = 0.003401897381991148
Validation loss = 0.003543619764968753
Validation loss = 0.0034189403522759676
Validation loss = 0.003626861609518528
Validation loss = 0.0034854495897889137
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.003397482680156827
Validation loss = 0.0034779678098857403
Validation loss = 0.0034061104524880648
Validation loss = 0.003536539850756526
Validation loss = 0.003421810222789645
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.003454178338870406
Validation loss = 0.0034577783662825823
Validation loss = 0.0033631138503551483
Validation loss = 0.0034247818402945995
Validation loss = 0.0034089400433003902
Validation loss = 0.0034207969438284636
Validation loss = 0.0035199038684368134
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0034103828947991133
Validation loss = 0.0034562258515506983
Validation loss = 0.0034262933768332005
Validation loss = 0.0035823655780404806
Validation loss = 0.0034106357488781214
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 31       |
| MaximumReturn | 199      |
| MinimumReturn | 125      |
| TotalSamples  | 109989   |
----------------------------
itr #32 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.004257592838257551
Validation loss = 0.004046694375574589
Validation loss = 0.0040649487636983395
Validation loss = 0.004121937323361635
Validation loss = 0.00433559762313962
Validation loss = 0.0041731661185622215
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.004036396276205778
Validation loss = 0.00409731874242425
Validation loss = 0.004047490190714598
Validation loss = 0.004021769389510155
Validation loss = 0.004029556177556515
Validation loss = 0.004049630835652351
Validation loss = 0.004085584077984095
Validation loss = 0.004094563890248537
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.004218391142785549
Validation loss = 0.004092507995665073
Validation loss = 0.004311001393944025
Validation loss = 0.004141827113926411
Validation loss = 0.004080269951373339
Validation loss = 0.004074259661138058
Validation loss = 0.004030654672533274
Validation loss = 0.004161452874541283
Validation loss = 0.003993410617113113
Validation loss = 0.00421915715560317
Validation loss = 0.0040580108761787415
Validation loss = 0.00402987701818347
Validation loss = 0.003995194099843502
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.004132654517889023
Validation loss = 0.004162447061389685
Validation loss = 0.004049388691782951
Validation loss = 0.004235067870467901
Validation loss = 0.004002759698778391
Validation loss = 0.004199401941150427
Validation loss = 0.004288711119443178
Validation loss = 0.004083808045834303
Validation loss = 0.004036745056509972
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.004071234725415707
Validation loss = 0.004102949518710375
Validation loss = 0.004081680439412594
Validation loss = 0.004257730208337307
Validation loss = 0.004186756443232298
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 32       |
| MaximumReturn | 199      |
| MinimumReturn | 128      |
| TotalSamples  | 113322   |
----------------------------
itr #33 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.004504682961851358
Validation loss = 0.0044097076170146465
Validation loss = 0.004477962851524353
Validation loss = 0.0046131969429552555
Validation loss = 0.0044591245241463184
Validation loss = 0.0043731555342674255
Validation loss = 0.004394145682454109
Validation loss = 0.004420362878590822
Validation loss = 0.004539537709206343
Validation loss = 0.004431058187037706
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.004395951982587576
Validation loss = 0.004456757567822933
Validation loss = 0.0044443849474191666
Validation loss = 0.004462000913918018
Validation loss = 0.004470993299037218
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0045365882106125355
Validation loss = 0.00444479426369071
Validation loss = 0.00453399121761322
Validation loss = 0.004469685722142458
Validation loss = 0.00446111848577857
Validation loss = 0.004480533301830292
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.004509010352194309
Validation loss = 0.004407200962305069
Validation loss = 0.004398330580443144
Validation loss = 0.004461613949388266
Validation loss = 0.00453342916443944
Validation loss = 0.004582318477332592
Validation loss = 0.00447500916197896
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.004526985343545675
Validation loss = 0.0044793859124183655
Validation loss = 0.004454833455383778
Validation loss = 0.004453227389603853
Validation loss = 0.004393655341118574
Validation loss = 0.004513854626566172
Validation loss = 0.004450016189366579
Validation loss = 0.004408746026456356
Validation loss = 0.004491657018661499
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 33       |
| MaximumReturn | 198      |
| MinimumReturn | 136      |
| TotalSamples  | 116655   |
----------------------------
itr #34 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.005063640885055065
Validation loss = 0.005036154296249151
Validation loss = 0.0049707177095115185
Validation loss = 0.0049965521320700645
Validation loss = 0.004932058975100517
Validation loss = 0.005033256486058235
Validation loss = 0.005138768348842859
Validation loss = 0.004947437439113855
Validation loss = 0.005009983200579882
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.004959115292876959
Validation loss = 0.005101468414068222
Validation loss = 0.0049297260120511055
Validation loss = 0.004984622821211815
Validation loss = 0.005056637339293957
Validation loss = 0.004995272494852543
Validation loss = 0.005001526791602373
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0050558047369122505
Validation loss = 0.005058851093053818
Validation loss = 0.004951977636665106
Validation loss = 0.004924857057631016
Validation loss = 0.005086231976747513
Validation loss = 0.005152115598320961
Validation loss = 0.005069574806839228
Validation loss = 0.005069064907729626
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00490203034132719
Validation loss = 0.004969378933310509
Validation loss = 0.004950567614287138
Validation loss = 0.005040172021836042
Validation loss = 0.004983719438314438
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.004908515606075525
Validation loss = 0.004869232885539532
Validation loss = 0.004956506658345461
Validation loss = 0.005014599766582251
Validation loss = 0.00494335638359189
Validation loss = 0.005136745981872082
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 34       |
| MaximumReturn | 199      |
| MinimumReturn | 116      |
| TotalSamples  | 119988   |
----------------------------
itr #35 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00515864510089159
Validation loss = 0.005276371259242296
Validation loss = 0.005271060857921839
Validation loss = 0.005250400863587856
Validation loss = 0.005204066168516874
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.005209254566580057
Validation loss = 0.005178262013942003
Validation loss = 0.005165490787476301
Validation loss = 0.005129783879965544
Validation loss = 0.0051429858431220055
Validation loss = 0.005182601045817137
Validation loss = 0.005177078768610954
Validation loss = 0.005205706227570772
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.005119101144373417
Validation loss = 0.005256797652691603
Validation loss = 0.005150905344635248
Validation loss = 0.005253539886325598
Validation loss = 0.0052255867049098015
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.005396902095526457
Validation loss = 0.005238538142293692
Validation loss = 0.005270548164844513
Validation loss = 0.005171557888388634
Validation loss = 0.005123603157699108
Validation loss = 0.00522538973018527
Validation loss = 0.0052641090005636215
Validation loss = 0.005159784574061632
Validation loss = 0.005262213293462992
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.005228650756180286
Validation loss = 0.005140307359397411
Validation loss = 0.0052588507533073425
Validation loss = 0.005193549208343029
Validation loss = 0.005249321460723877
Validation loss = 0.005191461648792028
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 35       |
| MaximumReturn | 198      |
| MinimumReturn | 108      |
| TotalSamples  | 123321   |
----------------------------
itr #36 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.005415189545601606
Validation loss = 0.005581219214946032
Validation loss = 0.00554426247254014
Validation loss = 0.0054519339464604855
Validation loss = 0.0053361221216619015
Validation loss = 0.005346792750060558
Validation loss = 0.005492976401001215
Validation loss = 0.0054039182141423225
Validation loss = 0.006135914474725723
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.005469407420605421
Validation loss = 0.005303983110934496
Validation loss = 0.00547552015632391
Validation loss = 0.005459423176944256
Validation loss = 0.005407293792814016
Validation loss = 0.005446793977171183
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00542116304859519
Validation loss = 0.005486120004206896
Validation loss = 0.005414100829511881
Validation loss = 0.005429796874523163
Validation loss = 0.005460538901388645
Validation loss = 0.005320338532328606
Validation loss = 0.005395517218858004
Validation loss = 0.00563389528542757
Validation loss = 0.005387493409216404
Validation loss = 0.005356539040803909
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.005479787476360798
Validation loss = 0.005638499744236469
Validation loss = 0.005491447169333696
Validation loss = 0.005425009410828352
Validation loss = 0.005401259288191795
Validation loss = 0.0054860468953847885
Validation loss = 0.005363515578210354
Validation loss = 0.005427731201052666
Validation loss = 0.005426154006272554
Validation loss = 0.005401060450822115
Validation loss = 0.006000140681862831
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.005432316102087498
Validation loss = 0.005626025144010782
Validation loss = 0.005456235259771347
Validation loss = 0.005326992832124233
Validation loss = 0.00545906089246273
Validation loss = 0.0054871817119419575
Validation loss = 0.005395606625825167
Validation loss = 0.005341098178178072
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 164      |
| Iteration     | 36       |
| MaximumReturn | 198      |
| MinimumReturn | 112      |
| TotalSamples  | 126654   |
----------------------------
itr #37 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00573428301140666
Validation loss = 0.005598495248705149
Validation loss = 0.005596544127911329
Validation loss = 0.005859251134097576
Validation loss = 0.005731513723731041
Validation loss = 0.005564672872424126
Validation loss = 0.005573045928031206
Validation loss = 0.005693890154361725
Validation loss = 0.00577514385804534
Validation loss = 0.005611132830381393
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.005639635492116213
Validation loss = 0.005610854830592871
Validation loss = 0.005603112280368805
Validation loss = 0.00559008726850152
Validation loss = 0.005762387532740831
Validation loss = 0.005552812945097685
Validation loss = 0.005632274318486452
Validation loss = 0.00578074948862195
Validation loss = 0.0055999127216637135
Validation loss = 0.005712046753615141
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.005583833437412977
Validation loss = 0.0055785588920116425
Validation loss = 0.005861361511051655
Validation loss = 0.00561890471726656
Validation loss = 0.00555789191275835
Validation loss = 0.005990711040794849
Validation loss = 0.005710240453481674
Validation loss = 0.00561110582202673
Validation loss = 0.0055837854743003845
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.005642585456371307
Validation loss = 0.00558637548238039
Validation loss = 0.005567966029047966
Validation loss = 0.005603804253041744
Validation loss = 0.005716754123568535
Validation loss = 0.005622697528451681
Validation loss = 0.005586675833910704
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0055979774333536625
Validation loss = 0.00562701839953661
Validation loss = 0.005621404852718115
Validation loss = 0.005789346527308226
Validation loss = 0.005618956871330738
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 37       |
| MaximumReturn | 199      |
| MinimumReturn | 108      |
| TotalSamples  | 129987   |
----------------------------
itr #38 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.006312832701951265
Validation loss = 0.006284796632826328
Validation loss = 0.006291051860898733
Validation loss = 0.006390508264303207
Validation loss = 0.006344639230519533
Validation loss = 0.006370158866047859
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.006623368244618177
Validation loss = 0.006395342294126749
Validation loss = 0.006253223400563002
Validation loss = 0.006503880023956299
Validation loss = 0.0063909972086548805
Validation loss = 0.006416988093405962
Validation loss = 0.0063148546032607555
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.006374124437570572
Validation loss = 0.006312706973403692
Validation loss = 0.00633457163348794
Validation loss = 0.006403560284525156
Validation loss = 0.006280687637627125
Validation loss = 0.006245236378163099
Validation loss = 0.00643504923209548
Validation loss = 0.006375371944159269
Validation loss = 0.006434732582420111
Validation loss = 0.006324032787233591
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0064327772706747055
Validation loss = 0.00646306574344635
Validation loss = 0.0062842038460075855
Validation loss = 0.006283406168222427
Validation loss = 0.006297924090176821
Validation loss = 0.006325681693851948
Validation loss = 0.006311351899057627
Validation loss = 0.00638121971860528
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.006260179448872805
Validation loss = 0.006336374208331108
Validation loss = 0.006335343234241009
Validation loss = 0.0063494788482785225
Validation loss = 0.006350203882902861
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 38       |
| MaximumReturn | 199      |
| MinimumReturn | 114      |
| TotalSamples  | 133320   |
----------------------------
itr #39 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.006498380564153194
Validation loss = 0.0063738832250237465
Validation loss = 0.006447766907513142
Validation loss = 0.006490740925073624
Validation loss = 0.006534605752676725
Validation loss = 0.006701007951050997
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.006404174026101828
Validation loss = 0.006416067015379667
Validation loss = 0.006353604607284069
Validation loss = 0.006627514027059078
Validation loss = 0.00637521967291832
Validation loss = 0.006458894349634647
Validation loss = 0.006350215990096331
Validation loss = 0.006332645658403635
Validation loss = 0.006466224789619446
Validation loss = 0.006427116692066193
Validation loss = 0.00657716253772378
Validation loss = 0.006361893378198147
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.006379285827279091
Validation loss = 0.0066991993226110935
Validation loss = 0.0064225210808217525
Validation loss = 0.006419471465051174
Validation loss = 0.00644657900556922
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00646431976929307
Validation loss = 0.00644644582644105
Validation loss = 0.006421483121812344
Validation loss = 0.006601408589631319
Validation loss = 0.006464453414082527
Validation loss = 0.006422511301934719
Validation loss = 0.006418868899345398
Validation loss = 0.0064695714972913265
Validation loss = 0.006390745285898447
Validation loss = 0.00651229964569211
Validation loss = 0.006505016703158617
Validation loss = 0.006391356233507395
Validation loss = 0.006545881740748882
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.006421627942472696
Validation loss = 0.0064202528446912766
Validation loss = 0.006448256317526102
Validation loss = 0.006438992917537689
Validation loss = 0.006628943607211113
Validation loss = 0.006588582880795002
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 39       |
| MaximumReturn | 198      |
| MinimumReturn | 106      |
| TotalSamples  | 136653   |
----------------------------
