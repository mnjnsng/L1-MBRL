Logging to experiments/pendulum/Pendulum/w100e0.5_seed2431
Print configuration .....
{'env_name': 'pendulum', 'random_seeds': [3214, 2431, 2531, 2231], 'save_variables': False, 'model_save_dir': '/tmp/pendulum_models/', 'restore_variables': False, 'start_onpol_iter': 0, 'onpol_iters': 40, 'num_path_random': 25, 'num_path_onpol': 25, 'env_horizon': 200, 'max_train_data': 200000, 'max_val_data': 100000, 'discard_ratio': 0.0, 'dynamics': {'pre_training': {'mode': 'intrinsic_reward', 'itr': 0, 'policy_itr': 20}, 'model': 'nn', 'ensemble': True, 'ensemble_model_count': 5, 'enable_particle_ensemble': True, 'particles': 5, 'obs_var': 1.0, 'intrinsic_reward_coeff': 1.0, 'ita': 1.0, 'mode': 'random', 'val': True, 'n_layers': 4, 'hidden_size': 1000, 'activation': 'relu', 'batch_size': 1000, 'learning_rate': 0.001, 'reg_coeff': 0.0, 'epochs': 200, 'kfac_params': {'learning_rate': 0.1, 'damping': 0.001, 'momentum': 0.9, 'kl_clip': 0.0001, 'cov_ema_decay': 0.99}}, 'policy': {'network_shape': [64, 64], 'init_logstd': 0.0, 'activation': 'tanh', 'reinitialize_every_itr': False}, 'trpo': {'horizon': 200, 'gamma': 0.99, 'step_size': 0.01, 'iterations': 20, 'batch_size': 50000, 'gae': 0.95, 'visualization': False, 'visualize_iterations': [0]}, 'algo': 'trpo'}
Generating random rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating random rollouts.
Creating normalization for training data.
Done creating normalization for training data.
Particle ensemble enabled? True
An ensemble of 5 dynamics model <class 'model.dynamics.NNDynamicsModel'> initialized
Train dynamics model with intrinsic reward only? False
Pre-training enabled. Using only intrinsic reward.
Pre-training dynamics model for 0 iterations...
Done pre-training dynamics model.
Using external reward only.
itr #0 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.21664077043533325
Validation loss = 0.022886907681822777
Validation loss = 0.006086871027946472
Validation loss = 0.0015828553587198257
Validation loss = 0.0006830437341704965
Validation loss = 0.0004460241470951587
Validation loss = 0.00034891077666543424
Validation loss = 0.00029337601154111326
Validation loss = 0.0002492334460839629
Validation loss = 0.0002876039070542902
Validation loss = 0.00024605178623460233
Validation loss = 0.00022861678735353053
Validation loss = 0.0002549329656176269
Validation loss = 0.00032815171289257705
Validation loss = 0.0005388077697716653
Validation loss = 0.00018446669855620712
Validation loss = 0.000364715262548998
Validation loss = 0.0005961740971542895
Validation loss = 0.00033589458325877786
Validation loss = 0.0009728603181429207
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.20053401589393616
Validation loss = 0.020099105313420296
Validation loss = 0.005780647974461317
Validation loss = 0.001429287251085043
Validation loss = 0.0007089051650837064
Validation loss = 0.0003375337109901011
Validation loss = 0.00028782672598026693
Validation loss = 0.00021233387815300375
Validation loss = 0.0002390661247773096
Validation loss = 0.00020934776694048196
Validation loss = 0.00024965821648947895
Validation loss = 0.0003064000920858234
Validation loss = 0.0004885903326794505
Validation loss = 0.0003305569407530129
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.19140000641345978
Validation loss = 0.028705108910799026
Validation loss = 0.008287599310278893
Validation loss = 0.0021041524596512318
Validation loss = 0.0007448845426551998
Validation loss = 0.00040426821215078235
Validation loss = 0.00030963399331085384
Validation loss = 0.00025925211957655847
Validation loss = 0.0002560581488069147
Validation loss = 0.000264607893768698
Validation loss = 0.0002247820666525513
Validation loss = 0.0002680363832041621
Validation loss = 0.0002448161831125617
Validation loss = 0.00023628764029126614
Validation loss = 0.0011683584889397025
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.19734561443328857
Validation loss = 0.021251248195767403
Validation loss = 0.007163454778492451
Validation loss = 0.0017512013437226415
Validation loss = 0.0008920644177123904
Validation loss = 0.0005059890099801123
Validation loss = 0.0003145067603327334
Validation loss = 0.0002661355829332024
Validation loss = 0.00026890257140621543
Validation loss = 0.0002365865366300568
Validation loss = 0.0002510639315005392
Validation loss = 0.0003164762747474015
Validation loss = 0.00034612740273587406
Validation loss = 0.0005805905093438923
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.21326324343681335
Validation loss = 0.01949421688914299
Validation loss = 0.006207243539392948
Validation loss = 0.002060740254819393
Validation loss = 0.0006733603659085929
Validation loss = 0.0005435369675979018
Validation loss = 0.00026697106659412384
Validation loss = 0.0003029626968782395
Validation loss = 0.00028803650639019907
Validation loss = 0.0002761194482445717
Validation loss = 0.0002510573831386864
Validation loss = 0.0003353065112605691
Validation loss = 0.0005193616379983723
Validation loss = 0.00041156887891702354
Validation loss = 0.0018693364690989256
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 160      |
| Iteration     | 0        |
| MaximumReturn | 199      |
| MinimumReturn | 88       |
| TotalSamples  | 6666     |
----------------------------
itr #1 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.020388079807162285
Validation loss = 0.002262423513457179
Validation loss = 0.0010905573144555092
Validation loss = 0.0010339926229789853
Validation loss = 0.001370755024254322
Validation loss = 0.0011629598448053002
Validation loss = 0.0011500382097437978
Validation loss = 0.002441982040181756
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.01308701653033495
Validation loss = 0.002749021165072918
Validation loss = 0.0013096389593556523
Validation loss = 0.001018321723677218
Validation loss = 0.001448427326977253
Validation loss = 0.0010791221866384149
Validation loss = 0.0015797604573890567
Validation loss = 0.001424659974873066
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.012182892300188541
Validation loss = 0.001742242369800806
Validation loss = 0.001220196601934731
Validation loss = 0.0010188402375206351
Validation loss = 0.0009704611147753894
Validation loss = 0.0013596474891528487
Validation loss = 0.0012745537096634507
Validation loss = 0.0013123234966769814
Validation loss = 0.0012764964485540986
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00822922307997942
Validation loss = 0.0023175294045358896
Validation loss = 0.0012159609468653798
Validation loss = 0.0010190448956564069
Validation loss = 0.0010577845387160778
Validation loss = 0.0012005078606307507
Validation loss = 0.0012427136534824967
Validation loss = 0.0012996793957427144
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.010431595146656036
Validation loss = 0.0015676397597417235
Validation loss = 0.0010395426070317626
Validation loss = 0.00333423656411469
Validation loss = 0.001927347737364471
Validation loss = 0.0012983187334612012
Validation loss = 0.0011271809926256537
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 1        |
| MaximumReturn | 199      |
| MinimumReturn | 124      |
| TotalSamples  | 9999     |
----------------------------
itr #2 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.004069630056619644
Validation loss = 0.0011382984230294824
Validation loss = 0.0014658358413726091
Validation loss = 0.0012527587823569775
Validation loss = 0.001087335986085236
Validation loss = 0.0013768032658845186
Validation loss = 0.0012077582068741322
Validation loss = 0.0014211770612746477
Validation loss = 0.0010939622297883034
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0072615682147443295
Validation loss = 0.0010566508863121271
Validation loss = 0.0010383244371041656
Validation loss = 0.0008912865305319428
Validation loss = 0.0027500162832438946
Validation loss = 0.0012363450368866324
Validation loss = 0.000879027764312923
Validation loss = 0.002917527686804533
Validation loss = 0.0015140760224312544
Validation loss = 0.0016086373943835497
Validation loss = 0.0014286476653069258
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.004609249532222748
Validation loss = 0.0013222071575000882
Validation loss = 0.0008467005682177842
Validation loss = 0.0008722582133486867
Validation loss = 0.0011489659082144499
Validation loss = 0.002738980110734701
Validation loss = 0.001187182730063796
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.003449823707342148
Validation loss = 0.0008979795384220779
Validation loss = 0.0009398759575560689
Validation loss = 0.001278364914469421
Validation loss = 0.0009206499671563506
Validation loss = 0.0011790278367698193
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.004137551877647638
Validation loss = 0.0012217011535540223
Validation loss = 0.0012110256357118487
Validation loss = 0.0012683357344940305
Validation loss = 0.001406608847901225
Validation loss = 0.001610632985830307
Validation loss = 0.0019397089490666986
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 2        |
| MaximumReturn | 199      |
| MinimumReturn | 125      |
| TotalSamples  | 13332    |
----------------------------
itr #3 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0020451124291867018
Validation loss = 0.0008956925012171268
Validation loss = 0.0014230977976694703
Validation loss = 0.0025126717519015074
Validation loss = 0.0010605568531900644
Validation loss = 0.0024271104484796524
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.002647239016368985
Validation loss = 0.0010786879574880004
Validation loss = 0.0030996075365692377
Validation loss = 0.0012193655129522085
Validation loss = 0.0016635050997138023
Validation loss = 0.0013919812627136707
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.002027659909799695
Validation loss = 0.0010463256621733308
Validation loss = 0.0008774609304964542
Validation loss = 0.0009801987325772643
Validation loss = 0.0012427372857928276
Validation loss = 0.0009678700007498264
Validation loss = 0.0009791050106287003
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0035915228072553873
Validation loss = 0.001274141133762896
Validation loss = 0.0012001889990642667
Validation loss = 0.0014112385688349605
Validation loss = 0.002079096157103777
Validation loss = 0.0012783750426024199
Validation loss = 0.0012046328047290444
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0022508841939270496
Validation loss = 0.0010180467506870627
Validation loss = 0.0009917027782648802
Validation loss = 0.001362815499305725
Validation loss = 0.0010651310440152884
Validation loss = 0.0014123833971098065
Validation loss = 0.0014637975255027413
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 160      |
| Iteration     | 3        |
| MaximumReturn | 199      |
| MinimumReturn | 113      |
| TotalSamples  | 16665    |
----------------------------
itr #4 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0015177488094195724
Validation loss = 0.0007275667740032077
Validation loss = 0.0013940276112407446
Validation loss = 0.0011162285227328539
Validation loss = 0.002305449452251196
Validation loss = 0.0010438242461532354
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0015778562519699335
Validation loss = 0.0008495092042721808
Validation loss = 0.0018762993859127164
Validation loss = 0.0010048565454781055
Validation loss = 0.0025260141119360924
Validation loss = 0.0010649797040969133
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.002318313345313072
Validation loss = 0.0011805088724941015
Validation loss = 0.0012073569232597947
Validation loss = 0.0011939209653064609
Validation loss = 0.002499534282833338
Validation loss = 0.0010732868686318398
Validation loss = 0.0009683551033958793
Validation loss = 0.0011411106679588556
Validation loss = 0.00107263820245862
Validation loss = 0.0008099815458990633
Validation loss = 0.0008563256706111133
Validation loss = 0.0015715141780674458
Validation loss = 0.0010079344501718879
Validation loss = 0.0007700140122324228
Validation loss = 0.0010596346110105515
Validation loss = 0.001451342599466443
Validation loss = 0.0009478825377300382
Validation loss = 0.0012984185013920069
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.001537573174573481
Validation loss = 0.0016341727459803224
Validation loss = 0.0014400710351765156
Validation loss = 0.001181208062916994
Validation loss = 0.0007225071894936264
Validation loss = 0.0011434979969635606
Validation loss = 0.0010495157912373543
Validation loss = 0.002269085729494691
Validation loss = 0.0008630810771137476
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.003512670984491706
Validation loss = 0.0010003034258261323
Validation loss = 0.001879785442724824
Validation loss = 0.001224988023750484
Validation loss = 0.0011806132970377803
Validation loss = 0.0007774931145831943
Validation loss = 0.0008916993974708021
Validation loss = 0.0007600014214403927
Validation loss = 0.0016463857609778643
Validation loss = 0.0008776410249993205
Validation loss = 0.0009458580170758069
Validation loss = 0.000852042983751744
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 4        |
| MaximumReturn | 199      |
| MinimumReturn | 125      |
| TotalSamples  | 19998    |
----------------------------
itr #5 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.001552261528559029
Validation loss = 0.0015594896394759417
Validation loss = 0.0010294807143509388
Validation loss = 0.00103950931224972
Validation loss = 0.0008214663830585778
Validation loss = 0.0011481719557195902
Validation loss = 0.0012870412319898605
Validation loss = 0.001519619021564722
Validation loss = 0.0008040543762035668
Validation loss = 0.0015182469505816698
Validation loss = 0.0007312620873562992
Validation loss = 0.000815622799564153
Validation loss = 0.001029809471219778
Validation loss = 0.0010221808915957808
Validation loss = 0.0007268062909133732
Validation loss = 0.0023844894021749496
Validation loss = 0.0007765832124277949
Validation loss = 0.0012608205433934927
Validation loss = 0.0006798204267397523
Validation loss = 0.0005645145429298282
Validation loss = 0.0006815933156758547
Validation loss = 0.0013257341925054789
Validation loss = 0.001071418751962483
Validation loss = 0.002489787060767412
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.002736134920269251
Validation loss = 0.0006544643547385931
Validation loss = 0.0008176310802809894
Validation loss = 0.0008021147805266082
Validation loss = 0.0024626892991364002
Validation loss = 0.000672206049785018
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0020986783783882856
Validation loss = 0.0008570514619350433
Validation loss = 0.0016466515371575952
Validation loss = 0.0010426370427012444
Validation loss = 0.0008264359785243869
Validation loss = 0.0016094937454909086
Validation loss = 0.0008046240545809269
Validation loss = 0.0010039897169917822
Validation loss = 0.0006514198030345142
Validation loss = 0.003225242020562291
Validation loss = 0.0006114299176260829
Validation loss = 0.000651898852083832
Validation loss = 0.0007643965072929859
Validation loss = 0.0008947034366428852
Validation loss = 0.002183168660849333
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0009029768407344818
Validation loss = 0.0013683317229151726
Validation loss = 0.001011487445794046
Validation loss = 0.0008701441111043096
Validation loss = 0.0009213665616698563
Validation loss = 0.0012467940105125308
Validation loss = 0.0009043909376487136
Validation loss = 0.0008422242244705558
Validation loss = 0.0011076460359618068
Validation loss = 0.0007172142504714429
Validation loss = 0.0012218535412102938
Validation loss = 0.0006408283370546997
Validation loss = 0.0024110761005431414
Validation loss = 0.0008765946258790791
Validation loss = 0.001030742540024221
Validation loss = 0.0007335641421377659
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.001602640375494957
Validation loss = 0.0007095049950294197
Validation loss = 0.00100512127391994
Validation loss = 0.001321112271398306
Validation loss = 0.0010700293350964785
Validation loss = 0.0007953994791023433
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 5        |
| MaximumReturn | 198      |
| MinimumReturn | 124      |
| TotalSamples  | 23331    |
----------------------------
itr #6 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0010033309226855636
Validation loss = 0.000694428978022188
Validation loss = 0.0007387849036604166
Validation loss = 0.0007780112791806459
Validation loss = 0.0007797617581672966
Validation loss = 0.0010395414428785443
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0011460465611889958
Validation loss = 0.0010974789038300514
Validation loss = 0.0011855090269818902
Validation loss = 0.000777200039010495
Validation loss = 0.001165138091892004
Validation loss = 0.0010214458452537656
Validation loss = 0.0008302109781652689
Validation loss = 0.0007663255673833191
Validation loss = 0.0017070648027583957
Validation loss = 0.0009590262197889388
Validation loss = 0.001316108275204897
Validation loss = 0.0007748394855298102
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0016492881113663316
Validation loss = 0.0007975252810865641
Validation loss = 0.0006020419532433152
Validation loss = 0.0020391137804836035
Validation loss = 0.0006187353865243495
Validation loss = 0.0011703410418704152
Validation loss = 0.0008653800468891859
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0013029592810198665
Validation loss = 0.0009492026874795556
Validation loss = 0.0007354675326496363
Validation loss = 0.0071721673011779785
Validation loss = 0.0006552590639330447
Validation loss = 0.0007966194534674287
Validation loss = 0.0008012615726329386
Validation loss = 0.0006032923120073974
Validation loss = 0.0007096806075423956
Validation loss = 0.0005504609434865415
Validation loss = 0.0013934026937931776
Validation loss = 0.0006783695425838232
Validation loss = 0.003216108772903681
Validation loss = 0.0008003236725926399
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0008200333104468882
Validation loss = 0.0016830548411235213
Validation loss = 0.001261565601453185
Validation loss = 0.0006985842483118176
Validation loss = 0.0007457269821316004
Validation loss = 0.0007481409120373428
Validation loss = 0.000776052416767925
Validation loss = 0.0007912786677479744
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 6        |
| MaximumReturn | 199      |
| MinimumReturn | 128      |
| TotalSamples  | 26664    |
----------------------------
itr #7 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0008800312643870711
Validation loss = 0.0009930005762726068
Validation loss = 0.0005582937737926841
Validation loss = 0.000584760622587055
Validation loss = 0.000873059849254787
Validation loss = 0.0014639516593888402
Validation loss = 0.0005205454654060304
Validation loss = 0.0009461287409067154
Validation loss = 0.0004989230656065047
Validation loss = 0.0005687265074811876
Validation loss = 0.0007314261747524142
Validation loss = 0.0006877814885228872
Validation loss = 0.0006998914177529514
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0016537222545593977
Validation loss = 0.0015959963202476501
Validation loss = 0.0018421964487060905
Validation loss = 0.0006300694076344371
Validation loss = 0.0010130322771146894
Validation loss = 0.0006787301390431821
Validation loss = 0.0014517410891130567
Validation loss = 0.0013423020718619227
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0012197219766676426
Validation loss = 0.0006784328725188971
Validation loss = 0.0005470625474117696
Validation loss = 0.0011573208030313253
Validation loss = 0.0007087992271408439
Validation loss = 0.001026978949084878
Validation loss = 0.0008434471092186868
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0008856584900058806
Validation loss = 0.000533321057446301
Validation loss = 0.0007024951628409326
Validation loss = 0.0008328694384545088
Validation loss = 0.0008916444494388998
Validation loss = 0.0005156522965990007
Validation loss = 0.0009295266354456544
Validation loss = 0.0023703905753791332
Validation loss = 0.0005039930110797286
Validation loss = 0.0006339518586173654
Validation loss = 0.000621685991063714
Validation loss = 0.0010230484185740352
Validation loss = 0.0005365943070501089
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0006416070973500609
Validation loss = 0.0017545035807415843
Validation loss = 0.0008924555731937289
Validation loss = 0.0010535754263401031
Validation loss = 0.0006296324427239597
Validation loss = 0.0011699816677719355
Validation loss = 0.0007421286427415907
Validation loss = 0.0005107254255563021
Validation loss = 0.0015117698349058628
Validation loss = 0.0006292647449299693
Validation loss = 0.0007795979618094862
Validation loss = 0.0007196270162239671
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 7        |
| MaximumReturn | 199      |
| MinimumReturn | 122      |
| TotalSamples  | 29997    |
----------------------------
itr #8 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006157260504551232
Validation loss = 0.0008788775303401053
Validation loss = 0.0007282383739948273
Validation loss = 0.0006043087923899293
Validation loss = 0.0007525557302869856
Validation loss = 0.0007192330085672438
Validation loss = 0.0007197067025117576
Validation loss = 0.000750805193092674
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0012471252121031284
Validation loss = 0.0007962907548062503
Validation loss = 0.0005755542078986764
Validation loss = 0.0007288247579708695
Validation loss = 0.0006296563078649342
Validation loss = 0.0013305158354341984
Validation loss = 0.0006577851017937064
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0008902528206817806
Validation loss = 0.00056959263747558
Validation loss = 0.00048175948904827237
Validation loss = 0.000852358469273895
Validation loss = 0.0007339735748246312
Validation loss = 0.0009451451478525996
Validation loss = 0.0006977812154218554
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007307166233658791
Validation loss = 0.0009875671239569783
Validation loss = 0.0004504072421696037
Validation loss = 0.0006018280400894582
Validation loss = 0.0007690161000937223
Validation loss = 0.0005609778454527259
Validation loss = 0.0008969776099547744
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.000815034203696996
Validation loss = 0.0012342602713033557
Validation loss = 0.0005922686541453004
Validation loss = 0.0006645490066148341
Validation loss = 0.00048598009743727744
Validation loss = 0.0007704598247073591
Validation loss = 0.00044766307109966874
Validation loss = 0.0009020261350087821
Validation loss = 0.0004878546460531652
Validation loss = 0.0014872559113427997
Validation loss = 0.0006055719568394125
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 162      |
| Iteration     | 8        |
| MaximumReturn | 198      |
| MinimumReturn | 131      |
| TotalSamples  | 33330    |
----------------------------
itr #9 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.000536808161996305
Validation loss = 0.0005236862925812602
Validation loss = 0.0004123679827898741
Validation loss = 0.0005025509162805974
Validation loss = 0.0007323329919017851
Validation loss = 0.0005567385233007371
Validation loss = 0.000495332176797092
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0008033817866817117
Validation loss = 0.000539659580681473
Validation loss = 0.0013087012339383364
Validation loss = 0.0004931807634420693
Validation loss = 0.0005335853202268481
Validation loss = 0.0008329281699843705
Validation loss = 0.000950787216424942
Validation loss = 0.0006087254732847214
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0006188410334289074
Validation loss = 0.0005010287859477103
Validation loss = 0.0005402634851634502
Validation loss = 0.0007278018747456372
Validation loss = 0.0005713692517019808
Validation loss = 0.0005486365407705307
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0015255881007760763
Validation loss = 0.0007642963901162148
Validation loss = 0.000510810874402523
Validation loss = 0.000816557789221406
Validation loss = 0.0013142222305759788
Validation loss = 0.0005176023114472628
Validation loss = 0.000512477068696171
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0006467957282438874
Validation loss = 0.0014615231193602085
Validation loss = 0.0006615531165152788
Validation loss = 0.000587601913139224
Validation loss = 0.0005145234754309058
Validation loss = 0.0004959260113537312
Validation loss = 0.0016063312068581581
Validation loss = 0.000479783455375582
Validation loss = 0.0006284149712882936
Validation loss = 0.0004482783260755241
Validation loss = 0.0014884485863149166
Validation loss = 0.0003982743655797094
Validation loss = 0.0007071271538734436
Validation loss = 0.0006931208772584796
Validation loss = 0.0005873409099876881
Validation loss = 0.00047056435141712427
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 164      |
| Iteration     | 9        |
| MaximumReturn | 199      |
| MinimumReturn | 106      |
| TotalSamples  | 36663    |
----------------------------
itr #10 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005423069815151393
Validation loss = 0.0006843457813374698
Validation loss = 0.0006238005589693785
Validation loss = 0.00041715597035363317
Validation loss = 0.0005729998229071498
Validation loss = 0.000727704435121268
Validation loss = 0.00047785218339413404
Validation loss = 0.00041565258288756013
Validation loss = 0.0005153014790266752
Validation loss = 0.0006395673844963312
Validation loss = 0.000586845213547349
Validation loss = 0.0007019247277639806
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.00041397204040549695
Validation loss = 0.0004946726839989424
Validation loss = 0.001691774232313037
Validation loss = 0.0005342631484381855
Validation loss = 0.0008132642251439393
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0007746923365630209
Validation loss = 0.0005091376951895654
Validation loss = 0.0005763541557826102
Validation loss = 0.0015581791521981359
Validation loss = 0.0006082240724936128
Validation loss = 0.0005084900185465813
Validation loss = 0.0004921989166177809
Validation loss = 0.0007555843330919743
Validation loss = 0.0015055458061397076
Validation loss = 0.0004215777444187552
Validation loss = 0.0005597418639808893
Validation loss = 0.0006759180105291307
Validation loss = 0.0006604610243812203
Validation loss = 0.0009971876861527562
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007968185236677527
Validation loss = 0.0006047291099093854
Validation loss = 0.0011246537324041128
Validation loss = 0.0004882460052613169
Validation loss = 0.000407995015848428
Validation loss = 0.0004423949867486954
Validation loss = 0.0006539947935380042
Validation loss = 0.000761644565500319
Validation loss = 0.0006321949185803533
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0006190494168549776
Validation loss = 0.00044655133388005197
Validation loss = 0.0007971840677782893
Validation loss = 0.0005213501281104982
Validation loss = 0.000673959031701088
Validation loss = 0.0005348945269361138
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 182      |
| Iteration     | 10       |
| MaximumReturn | 199      |
| MinimumReturn | 128      |
| TotalSamples  | 39996    |
----------------------------
itr #11 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0004989186418242753
Validation loss = 0.00047090803855098784
Validation loss = 0.00045196362771093845
Validation loss = 0.0009958648588508368
Validation loss = 0.00041543535189703107
Validation loss = 0.0005574451643042266
Validation loss = 0.0005717371241189539
Validation loss = 0.0008044910500757396
Validation loss = 0.0004197373054921627
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.000620282138697803
Validation loss = 0.0006086309440433979
Validation loss = 0.0007007491658441722
Validation loss = 0.0005630661034956574
Validation loss = 0.0006291955942288041
Validation loss = 0.00040727504529058933
Validation loss = 0.0005357891204766929
Validation loss = 0.0004979277146048844
Validation loss = 0.0009712839382700622
Validation loss = 0.00047992970212362707
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0006186825921759009
Validation loss = 0.0004070000723004341
Validation loss = 0.00041570988832972944
Validation loss = 0.0006770424079149961
Validation loss = 0.0004935916513204575
Validation loss = 0.0010963407112285495
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.002449360443279147
Validation loss = 0.0005212596151977777
Validation loss = 0.00045834860065951943
Validation loss = 0.0004594108904711902
Validation loss = 0.00036049738992005587
Validation loss = 0.000510325247887522
Validation loss = 0.0004891280550509691
Validation loss = 0.00038381683407351375
Validation loss = 0.0008351708529517055
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0007246921304613352
Validation loss = 0.0005144376773387194
Validation loss = 0.0005356715410016477
Validation loss = 0.0006872270605526865
Validation loss = 0.0005483357817865908
Validation loss = 0.00047338116564787924
Validation loss = 0.0005530383205041289
Validation loss = 0.0005483460263349116
Validation loss = 0.0005135988467372954
Validation loss = 0.0004263820592314005
Validation loss = 0.0005559305427595973
Validation loss = 0.0004427445528563112
Validation loss = 0.0006334448698908091
Validation loss = 0.0005170148215256631
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 11       |
| MaximumReturn | 199      |
| MinimumReturn | 125      |
| TotalSamples  | 43329    |
----------------------------
itr #12 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005889431340619922
Validation loss = 0.0005548852495849133
Validation loss = 0.0006164216320030391
Validation loss = 0.00048277544556185603
Validation loss = 0.00046366191236302257
Validation loss = 0.0005001445533707738
Validation loss = 0.00042769312858581543
Validation loss = 0.0006998663884587586
Validation loss = 0.0005516528035514057
Validation loss = 0.00040423276368528605
Validation loss = 0.00063705985667184
Validation loss = 0.0004194909706711769
Validation loss = 0.0005723695503547788
Validation loss = 0.0004945190739817917
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005446557188406587
Validation loss = 0.0004621116095222533
Validation loss = 0.0005909031606279314
Validation loss = 0.0009177759056910872
Validation loss = 0.0005001177778467536
Validation loss = 0.0004729678330477327
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0007088423590175807
Validation loss = 0.0006427310290746391
Validation loss = 0.000505832489579916
Validation loss = 0.0006372896605171263
Validation loss = 0.0010095417965203524
Validation loss = 0.00042690473492257297
Validation loss = 0.00042781714000739157
Validation loss = 0.0005948100006207824
Validation loss = 0.0005761432112194598
Validation loss = 0.0006015529506839812
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.000751695828512311
Validation loss = 0.0003826409229077399
Validation loss = 0.0007686727913096547
Validation loss = 0.0005672381957992911
Validation loss = 0.0004292489611543715
Validation loss = 0.0005831624148413539
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0018964704358950257
Validation loss = 0.00035730766830965877
Validation loss = 0.0006181015633046627
Validation loss = 0.0005221645114943385
Validation loss = 0.0007860454497858882
Validation loss = 0.00048088538460433483
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 12       |
| MaximumReturn | 199      |
| MinimumReturn | 121      |
| TotalSamples  | 46662    |
----------------------------
itr #13 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005746776005253196
Validation loss = 0.00041796802543103695
Validation loss = 0.0005408970173448324
Validation loss = 0.000431110238423571
Validation loss = 0.0010680359555408359
Validation loss = 0.00035153815406374633
Validation loss = 0.0005283628706820309
Validation loss = 0.0004595553036779165
Validation loss = 0.00045908906031399965
Validation loss = 0.00041341944597661495
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005139218992553651
Validation loss = 0.0004243511357344687
Validation loss = 0.0008960175327956676
Validation loss = 0.0005315240705385804
Validation loss = 0.0010867343517020345
Validation loss = 0.00045205376227386296
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0005454262136481702
Validation loss = 0.000607637339271605
Validation loss = 0.0006336591322906315
Validation loss = 0.00047193330829031765
Validation loss = 0.0006839376292191446
Validation loss = 0.0005434350459836423
Validation loss = 0.0005179379368200898
Validation loss = 0.00043382999137975276
Validation loss = 0.0004372652911115438
Validation loss = 0.0005103122675791383
Validation loss = 0.00090216746320948
Validation loss = 0.0005127970362082124
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005335657624527812
Validation loss = 0.00038879550993442535
Validation loss = 0.0005744733498431742
Validation loss = 0.0006689217989332974
Validation loss = 0.0005406205309554935
Validation loss = 0.0007118423236533999
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.000536107923835516
Validation loss = 0.00036725495010614395
Validation loss = 0.0006343392888084054
Validation loss = 0.0004694429808296263
Validation loss = 0.000632700277492404
Validation loss = 0.0005852290196344256
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 160      |
| Iteration     | 13       |
| MaximumReturn | 198      |
| MinimumReturn | 122      |
| TotalSamples  | 49995    |
----------------------------
itr #14 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006156899034976959
Validation loss = 0.0005076604429632425
Validation loss = 0.0007027555839158595
Validation loss = 0.0007752769161015749
Validation loss = 0.00043496384751051664
Validation loss = 0.0004621199914254248
Validation loss = 0.0005011182511225343
Validation loss = 0.0006994552677497268
Validation loss = 0.00047404348151758313
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005011808243580163
Validation loss = 0.0012433683732524514
Validation loss = 0.0006094424170441926
Validation loss = 0.0004601801629178226
Validation loss = 0.000617940619122237
Validation loss = 0.0005095778615213931
Validation loss = 0.0004235437954775989
Validation loss = 0.0004113719332963228
Validation loss = 0.0004994755145162344
Validation loss = 0.00048211784451268613
Validation loss = 0.0004745800397358835
Validation loss = 0.0005407665739767253
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0010578094515949488
Validation loss = 0.0004358134465292096
Validation loss = 0.0005064038559794426
Validation loss = 0.0004592097830027342
Validation loss = 0.0007932784501463175
Validation loss = 0.0004296216065995395
Validation loss = 0.0004170212778262794
Validation loss = 0.0006330758333206177
Validation loss = 0.0004160567477811128
Validation loss = 0.0003938594018109143
Validation loss = 0.0004287293995730579
Validation loss = 0.0005703869392164052
Validation loss = 0.00039390329038724303
Validation loss = 0.000687472871504724
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0004122740065213293
Validation loss = 0.0005780206411145627
Validation loss = 0.0005252452683635056
Validation loss = 0.0004064735257998109
Validation loss = 0.0006730782915838063
Validation loss = 0.00046299854875542223
Validation loss = 0.0008053782512433827
Validation loss = 0.000445388606749475
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0004615148063749075
Validation loss = 0.0006589593249373138
Validation loss = 0.000510925950948149
Validation loss = 0.0005547957844100893
Validation loss = 0.00034778250847011805
Validation loss = 0.0004801162285730243
Validation loss = 0.0004101917438674718
Validation loss = 0.0004901223001070321
Validation loss = 0.0005085930461063981
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 14       |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 53328    |
----------------------------
itr #15 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00040934939170256257
Validation loss = 0.0003945793432649225
Validation loss = 0.000541730085387826
Validation loss = 0.00043149307020939887
Validation loss = 0.00042812881292775273
Validation loss = 0.00042677915189415216
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0004225913144182414
Validation loss = 0.0005798250203952193
Validation loss = 0.00060971116181463
Validation loss = 0.0005377558409236372
Validation loss = 0.0004824780626222491
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0004459545307327062
Validation loss = 0.00041416703606955707
Validation loss = 0.0006684178952127695
Validation loss = 0.00043666252167895436
Validation loss = 0.0005389646394178271
Validation loss = 0.0004746679915115237
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007407820085063577
Validation loss = 0.0008607454365119338
Validation loss = 0.00039311169530265033
Validation loss = 0.00042008148739114404
Validation loss = 0.0005316849565133452
Validation loss = 0.00045265170047059655
Validation loss = 0.0005852685426361859
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0004151157336309552
Validation loss = 0.00041235610842704773
Validation loss = 0.0005969222984276712
Validation loss = 0.0004968237481079996
Validation loss = 0.0009496057173237205
Validation loss = 0.0004979511140845716
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 15       |
| MaximumReturn | 197      |
| MinimumReturn | 135      |
| TotalSamples  | 56661    |
----------------------------
itr #16 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0004278241249267012
Validation loss = 0.0005874658818356693
Validation loss = 0.00037917919689789414
Validation loss = 0.00045572526869364083
Validation loss = 0.00044936788617633283
Validation loss = 0.0005298302858136594
Validation loss = 0.00036882972926832736
Validation loss = 0.0004604503628797829
Validation loss = 0.00047138339141383767
Validation loss = 0.0005174071993678808
Validation loss = 0.0004525051626842469
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0007907380349934101
Validation loss = 0.0005412406171672046
Validation loss = 0.0005179789732210338
Validation loss = 0.0006734777125529945
Validation loss = 0.0006282555987127125
Validation loss = 0.0011420174269005656
Validation loss = 0.0004915927420370281
Validation loss = 0.0004978193319402635
Validation loss = 0.0009595061419531703
Validation loss = 0.0005122889415360987
Validation loss = 0.0005867303116247058
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00041743539622984827
Validation loss = 0.0004804608761332929
Validation loss = 0.0004217808891553432
Validation loss = 0.00045085977762937546
Validation loss = 0.0005807512206956744
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0004352648393251002
Validation loss = 0.0004896487225778401
Validation loss = 0.0004347696958575398
Validation loss = 0.0004854720609728247
Validation loss = 0.00042962486622855067
Validation loss = 0.0004400463658384979
Validation loss = 0.0006048345239832997
Validation loss = 0.0006578111206181347
Validation loss = 0.00042456199298612773
Validation loss = 0.0007032290450297296
Validation loss = 0.0005281814956106246
Validation loss = 0.0004917053156532347
Validation loss = 0.0004319369327276945
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005404056864790618
Validation loss = 0.0007406953372992575
Validation loss = 0.00044607368181459606
Validation loss = 0.0009082573233172297
Validation loss = 0.0004016767779830843
Validation loss = 0.0004325946792960167
Validation loss = 0.0004269415803719312
Validation loss = 0.0008525734883733094
Validation loss = 0.00039268340333364904
Validation loss = 0.00040932331467047334
Validation loss = 0.00046640264918096364
Validation loss = 0.0006480089505203068
Validation loss = 0.00046050039236433804
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 16       |
| MaximumReturn | 199      |
| MinimumReturn | 122      |
| TotalSamples  | 59994    |
----------------------------
itr #17 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.000528772477991879
Validation loss = 0.0004684923915192485
Validation loss = 0.0004994205664843321
Validation loss = 0.0005219705053605139
Validation loss = 0.000450751802418381
Validation loss = 0.0005216065910644829
Validation loss = 0.0005758351762779057
Validation loss = 0.0005005004932172596
Validation loss = 0.0004051727883052081
Validation loss = 0.0007218002574518323
Validation loss = 0.0008062217966653407
Validation loss = 0.0004961513332091272
Validation loss = 0.0003886920749209821
Validation loss = 0.000521500245667994
Validation loss = 0.00045841297833248973
Validation loss = 0.0004456393944565207
Validation loss = 0.00048747315304353833
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005803937092423439
Validation loss = 0.0005433386540971696
Validation loss = 0.0005122215952724218
Validation loss = 0.000521915266290307
Validation loss = 0.000496782420668751
Validation loss = 0.001031075487844646
Validation loss = 0.0004611410840880126
Validation loss = 0.0005760134081356227
Validation loss = 0.0004505635588429868
Validation loss = 0.0006422296864911914
Validation loss = 0.00044913875171914697
Validation loss = 0.00049737369408831
Validation loss = 0.0008018744993023574
Validation loss = 0.0006244267569854856
Validation loss = 0.0005766184185631573
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0004910752759315073
Validation loss = 0.00043241848470643163
Validation loss = 0.0004932045703753829
Validation loss = 0.0005025492864660919
Validation loss = 0.0004986717831343412
Validation loss = 0.0009533154661767185
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007013609865680337
Validation loss = 0.0004533341561909765
Validation loss = 0.0005959657719358802
Validation loss = 0.0004905000096186996
Validation loss = 0.0004378274898044765
Validation loss = 0.0004940814105793834
Validation loss = 0.0005298692849464715
Validation loss = 0.0005942944553680718
Validation loss = 0.0004970952868461609
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005286926752887666
Validation loss = 0.0004173564084339887
Validation loss = 0.00047233246732503176
Validation loss = 0.0021698337513953447
Validation loss = 0.0005679226014763117
Validation loss = 0.0006907042698003352
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 17       |
| MaximumReturn | 199      |
| MinimumReturn | 132      |
| TotalSamples  | 63327    |
----------------------------
itr #18 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0004567932919599116
Validation loss = 0.00044210662599653006
Validation loss = 0.0005411759484559298
Validation loss = 0.0004804506606888026
Validation loss = 0.0005823430838063359
Validation loss = 0.0005893344641663134
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005801846273243427
Validation loss = 0.0010427219094708562
Validation loss = 0.0005091329803690314
Validation loss = 0.0006045905756764114
Validation loss = 0.0004365843487903476
Validation loss = 0.0006934606353752315
Validation loss = 0.0005630495143122971
Validation loss = 0.0009497121791355312
Validation loss = 0.00046390402712859213
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0006757011287845671
Validation loss = 0.0005384696996770799
Validation loss = 0.0005062942509539425
Validation loss = 0.0004757287970278412
Validation loss = 0.0004533031315077096
Validation loss = 0.0005515039665624499
Validation loss = 0.0004206704325042665
Validation loss = 0.00045343220699578524
Validation loss = 0.0006617965991608799
Validation loss = 0.0009001361904665828
Validation loss = 0.0004883731016889215
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0004913012380711734
Validation loss = 0.0006420128047466278
Validation loss = 0.0005195405683480203
Validation loss = 0.0005783605738542974
Validation loss = 0.0005375971086323261
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005948400357738137
Validation loss = 0.0005473757046274841
Validation loss = 0.0005188126815482974
Validation loss = 0.0005289447144605219
Validation loss = 0.0005512126954272389
Validation loss = 0.0005178123828954995
Validation loss = 0.00048690245603211224
Validation loss = 0.000467994948849082
Validation loss = 0.00041429733391851187
Validation loss = 0.00047833481221459806
Validation loss = 0.0010436547454446554
Validation loss = 0.0005328728002496064
Validation loss = 0.00043052961700595915
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 18       |
| MaximumReturn | 199      |
| MinimumReturn | 129      |
| TotalSamples  | 66660    |
----------------------------
itr #19 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005117846303619444
Validation loss = 0.0005461086984723806
Validation loss = 0.0004791029787156731
Validation loss = 0.0005569423665292561
Validation loss = 0.00045193819096311927
Validation loss = 0.0004336859274189919
Validation loss = 0.0004270479257684201
Validation loss = 0.000503137765917927
Validation loss = 0.00043405097676441073
Validation loss = 0.0005044854478910565
Validation loss = 0.00042068128823302686
Validation loss = 0.00044507611892186105
Validation loss = 0.0004933900781907141
Validation loss = 0.0004538099165074527
Validation loss = 0.0008558463887311518
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005547209293581545
Validation loss = 0.0005368547863326967
Validation loss = 0.0005383531679399312
Validation loss = 0.0012958385050296783
Validation loss = 0.00043489725794643164
Validation loss = 0.0004927059635519981
Validation loss = 0.0005526301683858037
Validation loss = 0.0004846433294005692
Validation loss = 0.0005009162123315036
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.000474961765576154
Validation loss = 0.0004549252043943852
Validation loss = 0.0005542885628528893
Validation loss = 0.0005177160492166877
Validation loss = 0.00044186360901221633
Validation loss = 0.0006793331122025847
Validation loss = 0.0006145753432065248
Validation loss = 0.0005946430028416216
Validation loss = 0.0005177418934181333
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.000503932882566005
Validation loss = 0.0005108353798277676
Validation loss = 0.0004986166022717953
Validation loss = 0.0005945670418441296
Validation loss = 0.0004440425836946815
Validation loss = 0.0004319310828577727
Validation loss = 0.0005972526269033551
Validation loss = 0.0005950506310909986
Validation loss = 0.00045996427070349455
Validation loss = 0.0004925171378999949
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0006203188095241785
Validation loss = 0.0005805519176647067
Validation loss = 0.00048825505655258894
Validation loss = 0.0005892886547371745
Validation loss = 0.0005949342157691717
Validation loss = 0.0005561294383369386
Validation loss = 0.0005658590816892684
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 19       |
| MaximumReturn | 199      |
| MinimumReturn | 126      |
| TotalSamples  | 69993    |
----------------------------
itr #20 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.000886924855876714
Validation loss = 0.0004864717775490135
Validation loss = 0.0004915930330753326
Validation loss = 0.0005210142699070275
Validation loss = 0.00048167110071517527
Validation loss = 0.0005264815990813076
Validation loss = 0.0004947639536112547
Validation loss = 0.0005114442319609225
Validation loss = 0.0005047119338996708
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005541516584344208
Validation loss = 0.00052049127407372
Validation loss = 0.000580988940782845
Validation loss = 0.0006182502256706357
Validation loss = 0.0005282970960251987
Validation loss = 0.0005841161473654211
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.000701583456248045
Validation loss = 0.0005076577072031796
Validation loss = 0.0004916787147521973
Validation loss = 0.000564649875741452
Validation loss = 0.0005145137547515333
Validation loss = 0.0005443992558866739
Validation loss = 0.0005679422174580395
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007580021629109979
Validation loss = 0.0005816822522319853
Validation loss = 0.0005067140446044505
Validation loss = 0.0006961441249586642
Validation loss = 0.0005523361614905298
Validation loss = 0.0006004816968925297
Validation loss = 0.0008228455553762615
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.000493620871566236
Validation loss = 0.0006073630065657198
Validation loss = 0.000535240804310888
Validation loss = 0.0004828321689274162
Validation loss = 0.0006674985634163022
Validation loss = 0.0006988545646890998
Validation loss = 0.0004969824803993106
Validation loss = 0.0004921015352010727
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 20       |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 73326    |
----------------------------
itr #21 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005420732777565718
Validation loss = 0.0007109845755621791
Validation loss = 0.000482463336084038
Validation loss = 0.00045078631956130266
Validation loss = 0.0004937837366014719
Validation loss = 0.0005115933017805219
Validation loss = 0.000466405414044857
Validation loss = 0.0005407539429143071
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0008312837453559041
Validation loss = 0.0005950169288553298
Validation loss = 0.0006049289368093014
Validation loss = 0.0006548476521857083
Validation loss = 0.0005345498211681843
Validation loss = 0.0005138667766004801
Validation loss = 0.0005775726167485118
Validation loss = 0.0005555828101933002
Validation loss = 0.0005032537737861276
Validation loss = 0.0006627150578424335
Validation loss = 0.0005325789097696543
Validation loss = 0.0007091597653925419
Validation loss = 0.0005315177259035408
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.000554797938093543
Validation loss = 0.0007884886581450701
Validation loss = 0.0004976848140358925
Validation loss = 0.0005680687027052045
Validation loss = 0.000508275639731437
Validation loss = 0.0005400843801908195
Validation loss = 0.0005260860198177397
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005049182218499482
Validation loss = 0.000688245112542063
Validation loss = 0.000521965092048049
Validation loss = 0.0006622493383474648
Validation loss = 0.0006119246827438474
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005269722896628082
Validation loss = 0.0005741736968047917
Validation loss = 0.0004957971978001297
Validation loss = 0.0005074847140349448
Validation loss = 0.0005350381834432483
Validation loss = 0.0005579302087426186
Validation loss = 0.0004980749799869955
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 21       |
| MaximumReturn | 199      |
| MinimumReturn | 113      |
| TotalSamples  | 76659    |
----------------------------
itr #22 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.000558846746571362
Validation loss = 0.000516784843057394
Validation loss = 0.0004903313820250332
Validation loss = 0.0005417857319116592
Validation loss = 0.0004981437814421952
Validation loss = 0.0005130807403475046
Validation loss = 0.00047648255713284016
Validation loss = 0.0004942971863783896
Validation loss = 0.0007894600857980549
Validation loss = 0.0005027852603234351
Validation loss = 0.0005900454707443714
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005968658369965851
Validation loss = 0.0007412345730699599
Validation loss = 0.0005207866197451949
Validation loss = 0.0008874509949237108
Validation loss = 0.0005153173697181046
Validation loss = 0.0008061948465183377
Validation loss = 0.0005336027825251222
Validation loss = 0.0007741244626231492
Validation loss = 0.0005203781765885651
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0005144714377820492
Validation loss = 0.0005214253906160593
Validation loss = 0.0007108040153980255
Validation loss = 0.0005507752066478133
Validation loss = 0.0009079611045308411
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005499473190866411
Validation loss = 0.0005814358009956777
Validation loss = 0.0006082154577597976
Validation loss = 0.0006032027304172516
Validation loss = 0.0006961416220292449
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005767553229816258
Validation loss = 0.0005229520029388368
Validation loss = 0.0005683769704774022
Validation loss = 0.0005054834764450788
Validation loss = 0.0006570820696651936
Validation loss = 0.0008410809678025544
Validation loss = 0.0005482370033860207
Validation loss = 0.0006156560848467052
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 22       |
| MaximumReturn | 199      |
| MinimumReturn | 130      |
| TotalSamples  | 79992    |
----------------------------
itr #23 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005859846132807434
Validation loss = 0.0006349381292238832
Validation loss = 0.0006176033057272434
Validation loss = 0.0004940027138218284
Validation loss = 0.0006036803824827075
Validation loss = 0.000547771982382983
Validation loss = 0.0005335021996870637
Validation loss = 0.00046751779154874384
Validation loss = 0.0005749081028625369
Validation loss = 0.000501624308526516
Validation loss = 0.0006714662304148078
Validation loss = 0.0005111916689202189
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006585270166397095
Validation loss = 0.0005992195219732821
Validation loss = 0.0005618578288704157
Validation loss = 0.0005056437221355736
Validation loss = 0.0006176907336339355
Validation loss = 0.0005577324773184955
Validation loss = 0.0005776251200586557
Validation loss = 0.0005813335883431137
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0009417118271812797
Validation loss = 0.0005101865972392261
Validation loss = 0.0007730126962997019
Validation loss = 0.000595823978073895
Validation loss = 0.0005109047633595765
Validation loss = 0.0004884874215349555
Validation loss = 0.0004904245142824948
Validation loss = 0.0007732707308605313
Validation loss = 0.0005355403991416097
Validation loss = 0.0005006037536077201
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.000555155798792839
Validation loss = 0.000555578910280019
Validation loss = 0.0005173199460841715
Validation loss = 0.0005375450709834695
Validation loss = 0.000780341331847012
Validation loss = 0.0006121954647824168
Validation loss = 0.0005335260066203773
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0004723654128611088
Validation loss = 0.000718227936886251
Validation loss = 0.000553095422219485
Validation loss = 0.0005039845709688962
Validation loss = 0.0005198594299145043
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 23       |
| MaximumReturn | 199      |
| MinimumReturn | 115      |
| TotalSamples  | 83325    |
----------------------------
itr #24 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005270635010674596
Validation loss = 0.0005727967363782227
Validation loss = 0.0005147912888787687
Validation loss = 0.0005821579252369702
Validation loss = 0.0004918775521218777
Validation loss = 0.0005037264782004058
Validation loss = 0.000622381514403969
Validation loss = 0.0005417136126197875
Validation loss = 0.0005331768770702183
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005406660493463278
Validation loss = 0.0006497647846117616
Validation loss = 0.0006022700690664351
Validation loss = 0.0005518370890058577
Validation loss = 0.0006034934194758534
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0005481719854287803
Validation loss = 0.0007332176901400089
Validation loss = 0.0005361043731682003
Validation loss = 0.0006855958490632474
Validation loss = 0.0005417463253252208
Validation loss = 0.0005981100257486105
Validation loss = 0.0005239855381660163
Validation loss = 0.00048638659063726664
Validation loss = 0.0005784103996120393
Validation loss = 0.0005272572743706405
Validation loss = 0.0007214517681859434
Validation loss = 0.0006420271238312125
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005491896881721914
Validation loss = 0.0005300325574353337
Validation loss = 0.0007073397864587605
Validation loss = 0.0005647977814078331
Validation loss = 0.0005708981771022081
Validation loss = 0.0005346460966393352
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0006165344966575503
Validation loss = 0.0005548779154196382
Validation loss = 0.0005413792678155005
Validation loss = 0.0008198972209356725
Validation loss = 0.0009456809493713081
Validation loss = 0.0005527818575501442
Validation loss = 0.0007858229801058769
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 179      |
| Iteration     | 24       |
| MaximumReturn | 199      |
| MinimumReturn | 125      |
| TotalSamples  | 86658    |
----------------------------
itr #25 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006034227553755045
Validation loss = 0.0005792599404230714
Validation loss = 0.0005914366338402033
Validation loss = 0.0005867914878763258
Validation loss = 0.0005453088670037687
Validation loss = 0.0006540542235597968
Validation loss = 0.0005424318369477987
Validation loss = 0.000797454034909606
Validation loss = 0.0005986098549328744
Validation loss = 0.00070507189957425
Validation loss = 0.0005836053751409054
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0007113465690053999
Validation loss = 0.000557261926587671
Validation loss = 0.000604179163929075
Validation loss = 0.0005700328620150685
Validation loss = 0.0005591966910287738
Validation loss = 0.0008399675134569407
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0006683495594188571
Validation loss = 0.0005393922328948975
Validation loss = 0.0005576047115027905
Validation loss = 0.0006279851659201086
Validation loss = 0.0006863318267278373
Validation loss = 0.0005597589770331979
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005772649892605841
Validation loss = 0.0006114240968599916
Validation loss = 0.0006071312236599624
Validation loss = 0.0006215202156454325
Validation loss = 0.0005408283905126154
Validation loss = 0.0005437746294774115
Validation loss = 0.0005657289293594658
Validation loss = 0.0005997586995363235
Validation loss = 0.0006547982338815928
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005489017930813134
Validation loss = 0.000568439660128206
Validation loss = 0.000578925246372819
Validation loss = 0.000634672527667135
Validation loss = 0.0006424137391149998
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 25       |
| MaximumReturn | 199      |
| MinimumReturn | 124      |
| TotalSamples  | 89991    |
----------------------------
itr #26 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005745163653045893
Validation loss = 0.0005647122161462903
Validation loss = 0.000595746620092541
Validation loss = 0.0005585898761637509
Validation loss = 0.0005756726604886353
Validation loss = 0.0006056968704797328
Validation loss = 0.0005943962605670094
Validation loss = 0.0009217571932822466
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005938661633990705
Validation loss = 0.0006523557240143418
Validation loss = 0.0006334885838441551
Validation loss = 0.0006146241212263703
Validation loss = 0.0008307420648634434
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0005873634363524616
Validation loss = 0.0005975126405246556
Validation loss = 0.0006757995579391718
Validation loss = 0.0006185622769407928
Validation loss = 0.0007373808184638619
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0006888883071951568
Validation loss = 0.0006100617465563118
Validation loss = 0.0007166293216869235
Validation loss = 0.000566576374694705
Validation loss = 0.0007257729885168374
Validation loss = 0.0006587487296201289
Validation loss = 0.0006071702227927744
Validation loss = 0.0006848558550700545
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.000632518669590354
Validation loss = 0.0007134191691875458
Validation loss = 0.0006525884382426739
Validation loss = 0.0007174742640927434
Validation loss = 0.0006334574427455664
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 180      |
| Iteration     | 26       |
| MaximumReturn | 199      |
| MinimumReturn | 133      |
| TotalSamples  | 93324    |
----------------------------
itr #27 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006698328070342541
Validation loss = 0.0005792576703242958
Validation loss = 0.0006211979198269546
Validation loss = 0.0006497358554042876
Validation loss = 0.0005840478697791696
Validation loss = 0.0006373895448632538
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006377151003107429
Validation loss = 0.0006077118450775743
Validation loss = 0.0006117943557910621
Validation loss = 0.0007686672615818679
Validation loss = 0.0006072134710848331
Validation loss = 0.000621574348770082
Validation loss = 0.0005964647862128913
Validation loss = 0.0005769190029241145
Validation loss = 0.0006111051188781857
Validation loss = 0.0008581446018069983
Validation loss = 0.0008174192043952644
Validation loss = 0.0005622633034363389
Validation loss = 0.0006134858122095466
Validation loss = 0.0006134267314337194
Validation loss = 0.0006186582031659782
Validation loss = 0.0008173121605068445
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0006132215494289994
Validation loss = 0.000652767252177
Validation loss = 0.0006028799107298255
Validation loss = 0.0006200441857799888
Validation loss = 0.0005975211970508099
Validation loss = 0.0007682166760787368
Validation loss = 0.0005776321631856263
Validation loss = 0.0006949421367608011
Validation loss = 0.0006279911613091826
Validation loss = 0.0006374254007823765
Validation loss = 0.0006305866409093142
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0006713977199979126
Validation loss = 0.0006245678523555398
Validation loss = 0.000637343036942184
Validation loss = 0.0007044195081107318
Validation loss = 0.000679440563544631
Validation loss = 0.0006026765913702548
Validation loss = 0.0006575768929906189
Validation loss = 0.0006033068639226258
Validation loss = 0.0006133144488558173
Validation loss = 0.0006104061030782759
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0006133318529464304
Validation loss = 0.0005943004507571459
Validation loss = 0.0006218500202521682
Validation loss = 0.0007951976149342954
Validation loss = 0.000623242580331862
Validation loss = 0.0005802953965030611
Validation loss = 0.0005811885930597782
Validation loss = 0.0006407810724340379
Validation loss = 0.000635224801953882
Validation loss = 0.0007174767670221627
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 27       |
| MaximumReturn | 199      |
| MinimumReturn | 120      |
| TotalSamples  | 96657    |
----------------------------
itr #28 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0007544754189439118
Validation loss = 0.0006741405813954771
Validation loss = 0.0006139411125332117
Validation loss = 0.0006686164415441453
Validation loss = 0.0006755127687938511
Validation loss = 0.000649436900857836
Validation loss = 0.0006931861862540245
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006462823948822916
Validation loss = 0.0007174969650804996
Validation loss = 0.0007525337277911603
Validation loss = 0.0008179331780411303
Validation loss = 0.0006292114849202335
Validation loss = 0.0006487445207312703
Validation loss = 0.0006585982046090066
Validation loss = 0.0006227764533832669
Validation loss = 0.0007385596982203424
Validation loss = 0.0007571781170554459
Validation loss = 0.0007082885131239891
Validation loss = 0.0006067119538784027
Validation loss = 0.0006028414354659617
Validation loss = 0.0006246392731554806
Validation loss = 0.0007074372842907906
Validation loss = 0.0007211985066533089
Validation loss = 0.000658454664517194
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0007200432009994984
Validation loss = 0.0006096959696151316
Validation loss = 0.0009072371758520603
Validation loss = 0.0006433036760427058
Validation loss = 0.0006917497958056629
Validation loss = 0.0009528907830826938
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0006666773115284741
Validation loss = 0.0007153393817134202
Validation loss = 0.000633490679319948
Validation loss = 0.0007328810170292854
Validation loss = 0.0007248179172165692
Validation loss = 0.0008008337463252246
Validation loss = 0.0006956597790122032
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0007474906742572784
Validation loss = 0.0006558715249411762
Validation loss = 0.00065594696206972
Validation loss = 0.0006734117050655186
Validation loss = 0.000606267771217972
Validation loss = 0.0007128032739274204
Validation loss = 0.0007156854844652116
Validation loss = 0.0006720228120684624
Validation loss = 0.0006597337196581066
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 28       |
| MaximumReturn | 199      |
| MinimumReturn | 98.9     |
| TotalSamples  | 99990    |
----------------------------
itr #29 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0007347121136263013
Validation loss = 0.0006814057123847306
Validation loss = 0.0007580449455417693
Validation loss = 0.0006368421600200236
Validation loss = 0.0006717920186929405
Validation loss = 0.0006836927495896816
Validation loss = 0.0006507033249363303
Validation loss = 0.0007828162051737309
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0007422859780490398
Validation loss = 0.0007324176840484142
Validation loss = 0.000841668457724154
Validation loss = 0.0006768485764041543
Validation loss = 0.0006667278939858079
Validation loss = 0.0007040298660285771
Validation loss = 0.0007112612365745008
Validation loss = 0.000664878636598587
Validation loss = 0.0006821943097747862
Validation loss = 0.0006945989443920553
Validation loss = 0.0007062354125082493
Validation loss = 0.0006383506115525961
Validation loss = 0.0006487361970357597
Validation loss = 0.0006601610220968723
Validation loss = 0.0006537276785820723
Validation loss = 0.0006216567126102746
Validation loss = 0.00065045797964558
Validation loss = 0.0006474777474068105
Validation loss = 0.0007088948623277247
Validation loss = 0.0006839044508524239
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0006834396626800299
Validation loss = 0.0007212463533505797
Validation loss = 0.0007366654463112354
Validation loss = 0.0007515677716583014
Validation loss = 0.0007667589816264808
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0006868852651678026
Validation loss = 0.0006499164737761021
Validation loss = 0.0006942552281543612
Validation loss = 0.0007643447024747729
Validation loss = 0.0007561881793662906
Validation loss = 0.0006719339871779084
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0006727264844812453
Validation loss = 0.0007022101199254394
Validation loss = 0.000729001360014081
Validation loss = 0.0006758433301001787
Validation loss = 0.0006774680223315954
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 29       |
| MaximumReturn | 199      |
| MinimumReturn | 129      |
| TotalSamples  | 103323   |
----------------------------
itr #30 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006724460399709642
Validation loss = 0.0007220940315164626
Validation loss = 0.000901820370927453
Validation loss = 0.0006804638542234898
Validation loss = 0.0008011895115487278
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0007649495964869857
Validation loss = 0.0007907062536105514
Validation loss = 0.000706273305695504
Validation loss = 0.0006535167922265828
Validation loss = 0.0006935992278158665
Validation loss = 0.0007582588004879653
Validation loss = 0.0007130333688110113
Validation loss = 0.000780556641984731
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0007564869592897594
Validation loss = 0.0007909221458248794
Validation loss = 0.0007609310559928417
Validation loss = 0.0007119029760360718
Validation loss = 0.0007718440028838813
Validation loss = 0.0007068795966915786
Validation loss = 0.0007972277817316353
Validation loss = 0.0007234212243929505
Validation loss = 0.0007259406265802681
Validation loss = 0.0007434840081259608
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007013337453827262
Validation loss = 0.0008023600094020367
Validation loss = 0.0007210159092210233
Validation loss = 0.0007402657065540552
Validation loss = 0.0008239239105023444
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0006847508484497666
Validation loss = 0.0007260986603796482
Validation loss = 0.0007916766335256398
Validation loss = 0.0007056817994453013
Validation loss = 0.0006855175597593188
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 30       |
| MaximumReturn | 199      |
| MinimumReturn | 135      |
| TotalSamples  | 106656   |
----------------------------
itr #31 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0007655058288946748
Validation loss = 0.0007962629315443337
Validation loss = 0.0007816373836249113
Validation loss = 0.0007694120868109167
Validation loss = 0.0007315044640563428
Validation loss = 0.0008256477303802967
Validation loss = 0.0007544832769781351
Validation loss = 0.0007411877741105855
Validation loss = 0.0007696761167608202
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.000886871013790369
Validation loss = 0.0007613029447384179
Validation loss = 0.000814590195659548
Validation loss = 0.0008577107219025493
Validation loss = 0.0007861186750233173
Validation loss = 0.0007582611870020628
Validation loss = 0.0007556429482065141
Validation loss = 0.0007383637130260468
Validation loss = 0.0007294967654161155
Validation loss = 0.0007778642466291785
Validation loss = 0.0007976219058036804
Validation loss = 0.000777270644903183
Validation loss = 0.0007593440823256969
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.000768845493439585
Validation loss = 0.0008171831723302603
Validation loss = 0.000848469790071249
Validation loss = 0.0008622296736575663
Validation loss = 0.0007819657330401242
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007959247450344265
Validation loss = 0.000807873613666743
Validation loss = 0.0008147331536747515
Validation loss = 0.0007791167590767145
Validation loss = 0.0007683390285819769
Validation loss = 0.0008656549616716802
Validation loss = 0.0008327232790179551
Validation loss = 0.0008220969466492534
Validation loss = 0.0008561305003240705
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0007769766380079091
Validation loss = 0.0007490890566259623
Validation loss = 0.0009667840786278248
Validation loss = 0.0008386143017560244
Validation loss = 0.0007578132208436728
Validation loss = 0.0007542006205767393
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 31       |
| MaximumReturn | 199      |
| MinimumReturn | 131      |
| TotalSamples  | 109989   |
----------------------------
itr #32 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0008364724344573915
Validation loss = 0.0008207791252061725
Validation loss = 0.0008447040454484522
Validation loss = 0.0008791272412054241
Validation loss = 0.0008130913483910263
Validation loss = 0.0009123904746957123
Validation loss = 0.0007901756325736642
Validation loss = 0.0008460741955786943
Validation loss = 0.0008416783530265093
Validation loss = 0.00082306785043329
Validation loss = 0.0008096466190181673
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0008033422636799514
Validation loss = 0.0008861370151862502
Validation loss = 0.0008482852717861533
Validation loss = 0.000804893730673939
Validation loss = 0.0009277393110096455
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.000880195468198508
Validation loss = 0.0008748119580559433
Validation loss = 0.0008188802166841924
Validation loss = 0.0008281721384264529
Validation loss = 0.0008426764979958534
Validation loss = 0.0008477151859551668
Validation loss = 0.0008221031166613102
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0008482919074594975
Validation loss = 0.0009324935381300747
Validation loss = 0.0008953300421126187
Validation loss = 0.0008620871230959892
Validation loss = 0.0009069737861864269
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.000929542351514101
Validation loss = 0.0008681958424858749
Validation loss = 0.0008749667322263122
Validation loss = 0.0008639414445497096
Validation loss = 0.0008531583589501679
Validation loss = 0.0009749638265930116
Validation loss = 0.0008099285187199712
Validation loss = 0.0008250033133663237
Validation loss = 0.0008086770540103316
Validation loss = 0.0008100304985418916
Validation loss = 0.0008218252914957702
Validation loss = 0.000821388908661902
Validation loss = 0.0008953811484389007
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 177      |
| Iteration     | 32       |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 113322   |
----------------------------
itr #33 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0008425203268416226
Validation loss = 0.0008442915859632194
Validation loss = 0.000852852244861424
Validation loss = 0.0008372380398213863
Validation loss = 0.0009438021224923432
Validation loss = 0.0009104763739742339
Validation loss = 0.0008532357169315219
Validation loss = 0.0008646834758110344
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0008849718724377453
Validation loss = 0.0009245114051736891
Validation loss = 0.0008727515814825892
Validation loss = 0.0008901628898456693
Validation loss = 0.0008759425254538655
Validation loss = 0.0008325421367771924
Validation loss = 0.0008859115769155324
Validation loss = 0.0008428763831034303
Validation loss = 0.0008201788295991719
Validation loss = 0.001011815038509667
Validation loss = 0.0008885423885658383
Validation loss = 0.0009618262993171811
Validation loss = 0.0008539569680579007
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0010491037974134088
Validation loss = 0.0008629728108644485
Validation loss = 0.0008722093771211803
Validation loss = 0.0008936749654822052
Validation loss = 0.0009476413251832128
Validation loss = 0.0009023931925185025
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0009023385937325656
Validation loss = 0.0008994386880658567
Validation loss = 0.0008998641860671341
Validation loss = 0.0009087828220799565
Validation loss = 0.000862899876665324
Validation loss = 0.0008714458090253174
Validation loss = 0.0010092193260788918
Validation loss = 0.0008944676374085248
Validation loss = 0.0009066401980817318
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0012458431301638484
Validation loss = 0.000881754036527127
Validation loss = 0.0008850970189087093
Validation loss = 0.000853148230817169
Validation loss = 0.0008766884566284716
Validation loss = 0.000828942924272269
Validation loss = 0.0008704651845619082
Validation loss = 0.0008581209694966674
Validation loss = 0.000884953245986253
Validation loss = 0.0010432300623506308
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 177      |
| Iteration     | 33       |
| MaximumReturn | 199      |
| MinimumReturn | 133      |
| TotalSamples  | 116655   |
----------------------------
itr #34 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0010097601916640997
Validation loss = 0.0009961794130504131
Validation loss = 0.0010219658724963665
Validation loss = 0.0010517026530578732
Validation loss = 0.0009976159781217575
Validation loss = 0.0010173380142077804
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0011096789967268705
Validation loss = 0.0010334508260712028
Validation loss = 0.0010027826065197587
Validation loss = 0.001034723361954093
Validation loss = 0.0010811993852257729
Validation loss = 0.0009960897732526064
Validation loss = 0.0010078834602609277
Validation loss = 0.0010220679687336087
Validation loss = 0.000983147881925106
Validation loss = 0.000994398957118392
Validation loss = 0.0010195184731855989
Validation loss = 0.0009965426288545132
Validation loss = 0.0011093554785475135
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0011060874676331878
Validation loss = 0.0010204053251072764
Validation loss = 0.0010392888216301799
Validation loss = 0.001204109750688076
Validation loss = 0.001020360621623695
Validation loss = 0.0010561966337263584
Validation loss = 0.0010272477520629764
Validation loss = 0.0010316924890503287
Validation loss = 0.0010281202849000692
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0010285356547683477
Validation loss = 0.0010653464123606682
Validation loss = 0.0010645489674061537
Validation loss = 0.001137154526077211
Validation loss = 0.0010485650273039937
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0009823163272812963
Validation loss = 0.0009944516932591796
Validation loss = 0.0009836324024945498
Validation loss = 0.0010340561857447028
Validation loss = 0.0010088268900290132
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 34       |
| MaximumReturn | 199      |
| MinimumReturn | 129      |
| TotalSamples  | 119988   |
----------------------------
itr #35 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0012032757513225079
Validation loss = 0.001206591841764748
Validation loss = 0.0012128109810873866
Validation loss = 0.0011914975475519896
Validation loss = 0.0011708729434758425
Validation loss = 0.001196458819322288
Validation loss = 0.0012297347420826554
Validation loss = 0.0012389365583658218
Validation loss = 0.0012522577308118343
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.001138487015850842
Validation loss = 0.0012057493440806866
Validation loss = 0.0011968649923801422
Validation loss = 0.0014196051051840186
Validation loss = 0.0011788919800892472
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0011822966625913978
Validation loss = 0.0012570354156196117
Validation loss = 0.0013922731159254909
Validation loss = 0.0012786523438990116
Validation loss = 0.0011949461186304688
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0012081036111339927
Validation loss = 0.0012376829981803894
Validation loss = 0.0012658827472478151
Validation loss = 0.0012516138376668096
Validation loss = 0.0012237565824761987
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0012103853514418006
Validation loss = 0.0011896569048985839
Validation loss = 0.0011638839496299624
Validation loss = 0.00126125558745116
Validation loss = 0.00128491863142699
Validation loss = 0.0013218051753938198
Validation loss = 0.0012773320777341723
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 35       |
| MaximumReturn | 199      |
| MinimumReturn | 115      |
| TotalSamples  | 123321   |
----------------------------
itr #36 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0012940815649926662
Validation loss = 0.0012522730976343155
Validation loss = 0.0012465465115383267
Validation loss = 0.001217211945913732
Validation loss = 0.001244424725882709
Validation loss = 0.001277833478525281
Validation loss = 0.0012381640262901783
Validation loss = 0.0014238677686080337
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.001267126644961536
Validation loss = 0.001348461490124464
Validation loss = 0.0012314433697611094
Validation loss = 0.0012266471749171615
Validation loss = 0.0012517353752627969
Validation loss = 0.0013805460184812546
Validation loss = 0.0012402536813169718
Validation loss = 0.00131699547637254
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0013018978061154485
Validation loss = 0.001267743413336575
Validation loss = 0.0013203984126448631
Validation loss = 0.0013091276632621884
Validation loss = 0.0012553106062114239
Validation loss = 0.0013308420311659575
Validation loss = 0.0012424082960933447
Validation loss = 0.001260064309462905
Validation loss = 0.0013346777996048331
Validation loss = 0.001301349257119
Validation loss = 0.0013185831485316157
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0012928623473271728
Validation loss = 0.0012897878186777234
Validation loss = 0.0012667134869843721
Validation loss = 0.0013321631122380495
Validation loss = 0.0013029000256210566
Validation loss = 0.0013235926162451506
Validation loss = 0.001346126082353294
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0013642533449456096
Validation loss = 0.0013662104029208422
Validation loss = 0.0012526334030553699
Validation loss = 0.0012537995353341103
Validation loss = 0.0013224993599578738
Validation loss = 0.0012796404771506786
Validation loss = 0.0013389120576903224
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 36       |
| MaximumReturn | 199      |
| MinimumReturn | 116      |
| TotalSamples  | 126654   |
----------------------------
itr #37 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.001463904744014144
Validation loss = 0.0014662682078778744
Validation loss = 0.0014557767426595092
Validation loss = 0.0014278888702392578
Validation loss = 0.0014337425818666816
Validation loss = 0.0014608502388000488
Validation loss = 0.0015260034706443548
Validation loss = 0.001479410333558917
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0014322033384814858
Validation loss = 0.0015676985494792461
Validation loss = 0.0015041683800518513
Validation loss = 0.001437476952560246
Validation loss = 0.0014354170998558402
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0015563881024718285
Validation loss = 0.0015949822263792157
Validation loss = 0.0014866120181977749
Validation loss = 0.0015484875766560435
Validation loss = 0.0014929897151887417
Validation loss = 0.0014498430537059903
Validation loss = 0.001465340843424201
Validation loss = 0.0015106775099411607
Validation loss = 0.0014905687421560287
Validation loss = 0.0015023545129224658
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0014888603473082185
Validation loss = 0.0014825512189418077
Validation loss = 0.0015385044971480966
Validation loss = 0.0015147713711485267
Validation loss = 0.001525425468571484
Validation loss = 0.0016024227952584624
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0015319199301302433
Validation loss = 0.0016074592713266611
Validation loss = 0.0014796495670452714
Validation loss = 0.0014932429185137153
Validation loss = 0.001539496355690062
Validation loss = 0.001484579755924642
Validation loss = 0.0014531259657815099
Validation loss = 0.0015466657932847738
Validation loss = 0.0015401759883388877
Validation loss = 0.0014588399790227413
Validation loss = 0.0018401057459414005
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 37       |
| MaximumReturn | 199      |
| MinimumReturn | 121      |
| TotalSamples  | 129987   |
----------------------------
itr #38 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0016940183704718947
Validation loss = 0.0016849894309416413
Validation loss = 0.0017125635640695691
Validation loss = 0.0017323369393125176
Validation loss = 0.0016559023642912507
Validation loss = 0.0016954750753939152
Validation loss = 0.001759537379257381
Validation loss = 0.001649408251978457
Validation loss = 0.001685913186520338
Validation loss = 0.0016815555281937122
Validation loss = 0.0017161666182801127
Validation loss = 0.001766599714756012
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0016744873719289899
Validation loss = 0.0016678017564117908
Validation loss = 0.00166949606500566
Validation loss = 0.0016820350429043174
Validation loss = 0.0016641588881611824
Validation loss = 0.0016987663693726063
Validation loss = 0.001672118902206421
Validation loss = 0.0016710622003301978
Validation loss = 0.0017675168346613646
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.001725539448671043
Validation loss = 0.0018137868028134108
Validation loss = 0.0016959744971245527
Validation loss = 0.0017504605930298567
Validation loss = 0.0016698547406122088
Validation loss = 0.0017366597894579172
Validation loss = 0.001744391629472375
Validation loss = 0.0016955082537606359
Validation loss = 0.0017170205246657133
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0019881478510797024
Validation loss = 0.0017701166216284037
Validation loss = 0.001792257186025381
Validation loss = 0.0017018348444253206
Validation loss = 0.0017627055058255792
Validation loss = 0.0017183050513267517
Validation loss = 0.001749337068758905
Validation loss = 0.0016956659965217113
Validation loss = 0.0016902837669476867
Validation loss = 0.0016737838741391897
Validation loss = 0.0016919190529733896
Validation loss = 0.001693666446954012
Validation loss = 0.0016996669583022594
Validation loss = 0.001732271397486329
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00169683201238513
Validation loss = 0.0016824794001877308
Validation loss = 0.0017130617052316666
Validation loss = 0.0016888004029169679
Validation loss = 0.001708114636130631
Validation loss = 0.0017314699944108725
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 38       |
| MaximumReturn | 199      |
| MinimumReturn | 119      |
| TotalSamples  | 133320   |
----------------------------
itr #39 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.002025588182732463
Validation loss = 0.0019267657771706581
Validation loss = 0.001937500899657607
Validation loss = 0.0019130564760416746
Validation loss = 0.0019303451990708709
Validation loss = 0.0019496839959174395
Validation loss = 0.001958919456228614
Validation loss = 0.0019583816174417734
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.001975694904103875
Validation loss = 0.0018990827957168221
Validation loss = 0.0019473731517791748
Validation loss = 0.002024473389610648
Validation loss = 0.001998365856707096
Validation loss = 0.001937074470333755
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00190664641559124
Validation loss = 0.0019307732582092285
Validation loss = 0.0019107398111373186
Validation loss = 0.0019175850320607424
Validation loss = 0.0019163066754117608
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0020063254050910473
Validation loss = 0.002001833636313677
Validation loss = 0.0019866765942424536
Validation loss = 0.0019181801471859217
Validation loss = 0.0019707782194018364
Validation loss = 0.0019595108460634947
Validation loss = 0.001981987152248621
Validation loss = 0.0018993411213159561
Validation loss = 0.001970290672034025
Validation loss = 0.0019369281362742186
Validation loss = 0.0019316627876833081
Validation loss = 0.0019292650977149606
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0019008873496204615
Validation loss = 0.0019348023924976587
Validation loss = 0.0020684534683823586
Validation loss = 0.0019121449440717697
Validation loss = 0.0019250771729275584
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 39       |
| MaximumReturn | 199      |
| MinimumReturn | 110      |
| TotalSamples  | 136653   |
----------------------------
