Logging to experiments/pendulum/Pendulum/w100e0.5_seed2231
Print configuration .....
{'env_name': 'pendulum', 'random_seeds': [3214, 2431, 2531, 2231], 'save_variables': False, 'model_save_dir': '/tmp/pendulum_models/', 'restore_variables': False, 'start_onpol_iter': 0, 'onpol_iters': 40, 'num_path_random': 25, 'num_path_onpol': 25, 'env_horizon': 200, 'max_train_data': 200000, 'max_val_data': 100000, 'discard_ratio': 0.0, 'dynamics': {'pre_training': {'mode': 'intrinsic_reward', 'itr': 0, 'policy_itr': 20}, 'model': 'nn', 'ensemble': True, 'ensemble_model_count': 5, 'enable_particle_ensemble': True, 'particles': 5, 'obs_var': 1.0, 'intrinsic_reward_coeff': 1.0, 'ita': 1.0, 'mode': 'random', 'val': True, 'n_layers': 4, 'hidden_size': 1000, 'activation': 'relu', 'batch_size': 1000, 'learning_rate': 0.001, 'reg_coeff': 0.0, 'epochs': 200, 'kfac_params': {'learning_rate': 0.1, 'damping': 0.001, 'momentum': 0.9, 'kl_clip': 0.0001, 'cov_ema_decay': 0.99}}, 'policy': {'network_shape': [64, 64], 'init_logstd': 0.0, 'activation': 'tanh', 'reinitialize_every_itr': False}, 'trpo': {'horizon': 200, 'gamma': 0.99, 'step_size': 0.01, 'iterations': 20, 'batch_size': 50000, 'gae': 0.95, 'visualization': False, 'visualize_iterations': [0]}, 'algo': 'trpo'}
Generating random rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating random rollouts.
Creating normalization for training data.
Done creating normalization for training data.
Particle ensemble enabled? True
An ensemble of 5 dynamics model <class 'model.dynamics.NNDynamicsModel'> initialized
Train dynamics model with intrinsic reward only? False
Pre-training enabled. Using only intrinsic reward.
Pre-training dynamics model for 0 iterations...
Done pre-training dynamics model.
Using external reward only.
itr #0 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.14125902950763702
Validation loss = 0.019246863201260567
Validation loss = 0.006983425933867693
Validation loss = 0.003091957652941346
Validation loss = 0.0024934217799454927
Validation loss = 0.001994034741073847
Validation loss = 0.0019121173536404967
Validation loss = 0.0019204238196834922
Validation loss = 0.00200325227342546
Validation loss = 0.0023651584051549435
Validation loss = 0.00187519786413759
Validation loss = 0.0025711371563374996
Validation loss = 0.0027594969142228365
Validation loss = 0.0022730266209691763
Validation loss = 0.007051053456962109
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.15320590138435364
Validation loss = 0.022026924416422844
Validation loss = 0.009805737994611263
Validation loss = 0.0036400780081748962
Validation loss = 0.002407074673101306
Validation loss = 0.0021239796187728643
Validation loss = 0.0019477279856801033
Validation loss = 0.0018669175915420055
Validation loss = 0.0018153843702748418
Validation loss = 0.001814916729927063
Validation loss = 0.0017968460451811552
Validation loss = 0.0018087946809828281
Validation loss = 0.0019255706574767828
Validation loss = 0.0018132060067728162
Validation loss = 0.013393174856901169
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.15556223690509796
Validation loss = 0.03943244367837906
Validation loss = 0.010259022936224937
Validation loss = 0.004989301785826683
Validation loss = 0.0029887030832469463
Validation loss = 0.002190089551731944
Validation loss = 0.0019275341182947159
Validation loss = 0.0018893068190664053
Validation loss = 0.001827460015192628
Validation loss = 0.0018290469888597727
Validation loss = 0.0018090178491547704
Validation loss = 0.0017566416645422578
Validation loss = 0.0017868329305201769
Validation loss = 0.0018408015603199601
Validation loss = 0.0017851166194304824
Validation loss = 0.0024637295864522457
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.1567838340997696
Validation loss = 0.021744120866060257
Validation loss = 0.006742588244378567
Validation loss = 0.003132083686068654
Validation loss = 0.002318102866411209
Validation loss = 0.002038758946582675
Validation loss = 0.001880607451312244
Validation loss = 0.0018967903451994061
Validation loss = 0.005761274136602879
Validation loss = 0.0028866406064480543
Validation loss = 0.00200883811339736
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.12224571406841278
Validation loss = 0.026453353464603424
Validation loss = 0.0084978137165308
Validation loss = 0.003574987640604377
Validation loss = 0.002391047775745392
Validation loss = 0.002027444774284959
Validation loss = 0.0019361353479325771
Validation loss = 0.0018578340532258153
Validation loss = 0.0018699574284255505
Validation loss = 0.0018264061072841287
Validation loss = 0.0018564681522548199
Validation loss = 0.0018627106910571456
Validation loss = 0.0019990585278719664
Validation loss = 0.00209804973565042
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 163      |
| Iteration     | 0        |
| MaximumReturn | 195      |
| MinimumReturn | 123      |
| TotalSamples  | 6666     |
----------------------------
itr #1 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.01459959801286459
Validation loss = 0.0028908385429531336
Validation loss = 0.0015567303635179996
Validation loss = 0.0014970781048759818
Validation loss = 0.0016573049360886216
Validation loss = 0.0013525551185011864
Validation loss = 0.0012635174207389355
Validation loss = 0.001955905230715871
Validation loss = 0.0015625449595972896
Validation loss = 0.0017488364828750491
Validation loss = 0.0013341869926080108
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.013012909330427647
Validation loss = 0.0029501148965209723
Validation loss = 0.001789224799722433
Validation loss = 0.001498798024840653
Validation loss = 0.001458224724046886
Validation loss = 0.0017698612064123154
Validation loss = 0.0013790993252769113
Validation loss = 0.0014193301321938634
Validation loss = 0.0017380040371790528
Validation loss = 0.0016342211747542024
Validation loss = 0.0014509629691019654
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.009310771711170673
Validation loss = 0.002528832061216235
Validation loss = 0.0015434209490194917
Validation loss = 0.0013446094235405326
Validation loss = 0.00137325597461313
Validation loss = 0.0012821502750739455
Validation loss = 0.0013499921187758446
Validation loss = 0.0017283433116972446
Validation loss = 0.0014253748813644052
Validation loss = 0.001285931677557528
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.014606225304305553
Validation loss = 0.003205740125849843
Validation loss = 0.001552780158817768
Validation loss = 0.003347141435369849
Validation loss = 0.0015350206522271037
Validation loss = 0.001290249521844089
Validation loss = 0.0013322195736691356
Validation loss = 0.002201874041929841
Validation loss = 0.0015015940880402923
Validation loss = 0.001438186620362103
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.012395977973937988
Validation loss = 0.003155988408252597
Validation loss = 0.0015839552506804466
Validation loss = 0.0013248184695839882
Validation loss = 0.0013297091936692595
Validation loss = 0.0014176280237734318
Validation loss = 0.0027095682453364134
Validation loss = 0.0016010083490982652
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 1        |
| MaximumReturn | 199      |
| MinimumReturn | 126      |
| TotalSamples  | 9999     |
----------------------------
itr #2 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.005862476769834757
Validation loss = 0.0014286462683230639
Validation loss = 0.0011880965903401375
Validation loss = 0.0013433771673589945
Validation loss = 0.0014183515449985862
Validation loss = 0.0016519026830792427
Validation loss = 0.0011977229733020067
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.010376540012657642
Validation loss = 0.0012676117476075888
Validation loss = 0.0011389118153601885
Validation loss = 0.0011162555310875177
Validation loss = 0.001087982440367341
Validation loss = 0.001830700202845037
Validation loss = 0.0011618691496551037
Validation loss = 0.0027914366219192743
Validation loss = 0.0012338516535237432
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00500862579792738
Validation loss = 0.001369392266497016
Validation loss = 0.0012535284040495753
Validation loss = 0.001436013844795525
Validation loss = 0.001270244363695383
Validation loss = 0.0030545531772077084
Validation loss = 0.0015528412768617272
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00430360808968544
Validation loss = 0.0011754827573895454
Validation loss = 0.002049192087724805
Validation loss = 0.001327307429164648
Validation loss = 0.0012912488309666514
Validation loss = 0.0013320802245289087
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.004591359756886959
Validation loss = 0.0011861545499414206
Validation loss = 0.0012695504119619727
Validation loss = 0.001256188377737999
Validation loss = 0.0013742067385464907
Validation loss = 0.001154965371824801
Validation loss = 0.0019077801844105124
Validation loss = 0.0011094941291958094
Validation loss = 0.0012517095310613513
Validation loss = 0.001571372733451426
Validation loss = 0.002595194149762392
Validation loss = 0.001624297583475709
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 2        |
| MaximumReturn | 199      |
| MinimumReturn | 117      |
| TotalSamples  | 13332    |
----------------------------
itr #3 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.004862046334892511
Validation loss = 0.001086513395421207
Validation loss = 0.0014135424280539155
Validation loss = 0.001281537115573883
Validation loss = 0.0011755937011912465
Validation loss = 0.002415639581158757
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0024484037421643734
Validation loss = 0.001175031764432788
Validation loss = 0.00128522131126374
Validation loss = 0.0042123571038246155
Validation loss = 0.001197552657686174
Validation loss = 0.0018068044446408749
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0020906159188598394
Validation loss = 0.0012149247340857983
Validation loss = 0.0021679529454559088
Validation loss = 0.0013990065781399608
Validation loss = 0.0015844254521653056
Validation loss = 0.0015140819596126676
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0025540762580931187
Validation loss = 0.002067392226308584
Validation loss = 0.00155129365157336
Validation loss = 0.0012326926225796342
Validation loss = 0.0020866896957159042
Validation loss = 0.0011044846614822745
Validation loss = 0.002117631956934929
Validation loss = 0.0010615974897518754
Validation loss = 0.0014394278405234218
Validation loss = 0.0013371744425967336
Validation loss = 0.001216287724673748
Validation loss = 0.0015029181959107518
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00341929798014462
Validation loss = 0.001378116081468761
Validation loss = 0.0014913496561348438
Validation loss = 0.001191084971651435
Validation loss = 0.0014183820458129048
Validation loss = 0.0014730097027495503
Validation loss = 0.0011997231049463153
Validation loss = 0.0019756683614104986
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 3        |
| MaximumReturn | 199      |
| MinimumReturn | 121      |
| TotalSamples  | 16665    |
----------------------------
itr #4 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.001535738236270845
Validation loss = 0.0011411517625674605
Validation loss = 0.0016520078061148524
Validation loss = 0.001324192387983203
Validation loss = 0.0012709044385701418
Validation loss = 0.0013417756417766213
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.003211365081369877
Validation loss = 0.001178682316094637
Validation loss = 0.0014125597663223743
Validation loss = 0.0012836207170039415
Validation loss = 0.0013226853916421533
Validation loss = 0.0010715831303969026
Validation loss = 0.0016779558500275016
Validation loss = 0.0012817580718547106
Validation loss = 0.0015462238807231188
Validation loss = 0.0012665730901062489
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.008908217772841454
Validation loss = 0.001222064602188766
Validation loss = 0.0009623820660635829
Validation loss = 0.00166526238899678
Validation loss = 0.0009666748810559511
Validation loss = 0.0011583955492824316
Validation loss = 0.0009713783510960639
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.003293604589998722
Validation loss = 0.0009972569532692432
Validation loss = 0.002788103185594082
Validation loss = 0.0010250327177345753
Validation loss = 0.0033171214163303375
Validation loss = 0.0010245491284877062
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.004329887684434652
Validation loss = 0.0010207858867943287
Validation loss = 0.0012166417436674237
Validation loss = 0.00125459348782897
Validation loss = 0.0012828924227505922
Validation loss = 0.0008775356691330671
Validation loss = 0.0010569010628387332
Validation loss = 0.0011348968837410212
Validation loss = 0.0011478536762297153
Validation loss = 0.0012795511865988374
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 4        |
| MaximumReturn | 199      |
| MinimumReturn | 125      |
| TotalSamples  | 19998    |
----------------------------
itr #5 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.002295092213898897
Validation loss = 0.0011580807622522116
Validation loss = 0.0009472818928770721
Validation loss = 0.0011095709633082151
Validation loss = 0.0009956203866750002
Validation loss = 0.0008924507419578731
Validation loss = 0.0011750649427995086
Validation loss = 0.0018541259923949838
Validation loss = 0.0013567970599979162
Validation loss = 0.0014251411193981767
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0019585993140935898
Validation loss = 0.001777790137566626
Validation loss = 0.0009415918029844761
Validation loss = 0.000890563998837024
Validation loss = 0.0010932402219623327
Validation loss = 0.0008680418832227588
Validation loss = 0.0008543097646906972
Validation loss = 0.0012934504775330424
Validation loss = 0.0010485730599611998
Validation loss = 0.0013838408049196005
Validation loss = 0.001132033416070044
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0012623550137504935
Validation loss = 0.0008818232454359531
Validation loss = 0.0014638373395428061
Validation loss = 0.0009845561580732465
Validation loss = 0.001065678196027875
Validation loss = 0.001198275713250041
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0011267380323261023
Validation loss = 0.0015801750123500824
Validation loss = 0.0008493901113979518
Validation loss = 0.001256068586371839
Validation loss = 0.000991346314549446
Validation loss = 0.0009050251683220267
Validation loss = 0.0019512418657541275
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0012804719153791666
Validation loss = 0.001278121955692768
Validation loss = 0.0012165033258497715
Validation loss = 0.000953974318690598
Validation loss = 0.0010904355440288782
Validation loss = 0.0009980674367398024
Validation loss = 0.001776213408447802
Validation loss = 0.0010491980938240886
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 5        |
| MaximumReturn | 199      |
| MinimumReturn | 126      |
| TotalSamples  | 23331    |
----------------------------
itr #6 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0013705444289371371
Validation loss = 0.0009800506522879004
Validation loss = 0.0021046774927526712
Validation loss = 0.0012310732854530215
Validation loss = 0.0014712520642206073
Validation loss = 0.0009978829184547067
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0021064155735075474
Validation loss = 0.0009963787160813808
Validation loss = 0.0009294828632846475
Validation loss = 0.0030876256059855223
Validation loss = 0.0008313391590490937
Validation loss = 0.0021340884268283844
Validation loss = 0.0009750280878506601
Validation loss = 0.0010455806041136384
Validation loss = 0.0010594509076327085
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0014017628272995353
Validation loss = 0.0009608861291781068
Validation loss = 0.0012954352423548698
Validation loss = 0.0012196552706882358
Validation loss = 0.0021019219420850277
Validation loss = 0.0009124040952883661
Validation loss = 0.001230379450134933
Validation loss = 0.0008807415142655373
Validation loss = 0.0010186686413362622
Validation loss = 0.0010507703991606832
Validation loss = 0.0009231664007529616
Validation loss = 0.0012821723939850926
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.001080842106603086
Validation loss = 0.0010496624745428562
Validation loss = 0.0015299477381631732
Validation loss = 0.0013851813273504376
Validation loss = 0.0008986677858047187
Validation loss = 0.00257604056969285
Validation loss = 0.0010315320687368512
Validation loss = 0.0009827609173953533
Validation loss = 0.0010121286613866687
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.000973053858615458
Validation loss = 0.0013409749371930957
Validation loss = 0.0012213067384436727
Validation loss = 0.0010907487012445927
Validation loss = 0.0010772746754810214
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 6        |
| MaximumReturn | 199      |
| MinimumReturn | 109      |
| TotalSamples  | 26664    |
----------------------------
itr #7 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0010790441883727908
Validation loss = 0.0007239412516355515
Validation loss = 0.00075005111284554
Validation loss = 0.0008050888427533209
Validation loss = 0.0008241345640271902
Validation loss = 0.0008183066383935511
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.000871089578140527
Validation loss = 0.0013064591912552714
Validation loss = 0.0009252289892174304
Validation loss = 0.0013729477068409324
Validation loss = 0.0010838800808414817
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.005445880815386772
Validation loss = 0.0008424636325798929
Validation loss = 0.0012596329906955361
Validation loss = 0.0007942781667225063
Validation loss = 0.001567143714055419
Validation loss = 0.0009500091546215117
Validation loss = 0.0007790618110448122
Validation loss = 0.000923186307772994
Validation loss = 0.0015073498943820596
Validation loss = 0.0013752988306805491
Validation loss = 0.0007303274469450116
Validation loss = 0.0007861406775191426
Validation loss = 0.0007502406369894743
Validation loss = 0.0008832286694087088
Validation loss = 0.0011281885672360659
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007926707621663809
Validation loss = 0.0009089086670428514
Validation loss = 0.0009640934295020998
Validation loss = 0.0009474242106080055
Validation loss = 0.0008218922303058207
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0009989439276978374
Validation loss = 0.0016188265290111303
Validation loss = 0.0007934417226351798
Validation loss = 0.0009518953738734126
Validation loss = 0.0009789005853235722
Validation loss = 0.0009706487180665135
Validation loss = 0.0011047620791941881
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 7        |
| MaximumReturn | 199      |
| MinimumReturn | 129      |
| TotalSamples  | 29997    |
----------------------------
itr #8 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0009506124188192189
Validation loss = 0.0008749310509301722
Validation loss = 0.0008221871103160083
Validation loss = 0.0007782470784150064
Validation loss = 0.0014917554799467325
Validation loss = 0.0007858865428715944
Validation loss = 0.0009780835825949907
Validation loss = 0.0008660114253871143
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0009456918342038989
Validation loss = 0.0006522092735394835
Validation loss = 0.0019002660410478711
Validation loss = 0.0006768869934603572
Validation loss = 0.0008421217207796872
Validation loss = 0.0007671073544770479
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0010617281077429652
Validation loss = 0.0006593874422833323
Validation loss = 0.0006617965409532189
Validation loss = 0.0006366169545799494
Validation loss = 0.0010273793013766408
Validation loss = 0.000860557600390166
Validation loss = 0.0007431659614667296
Validation loss = 0.00084015226457268
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0012163053033873439
Validation loss = 0.0007282077567651868
Validation loss = 0.0009273776086047292
Validation loss = 0.0010818953160196543
Validation loss = 0.0007036901661194861
Validation loss = 0.0007824348285794258
Validation loss = 0.0007572126341983676
Validation loss = 0.0007932332227937877
Validation loss = 0.0007893778965808451
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0010015987791121006
Validation loss = 0.001024149009026587
Validation loss = 0.0009396575624123216
Validation loss = 0.0007869529072195292
Validation loss = 0.0007849791436456144
Validation loss = 0.0008637133287265897
Validation loss = 0.00112621346488595
Validation loss = 0.00109061726834625
Validation loss = 0.000833059661090374
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 164      |
| Iteration     | 8        |
| MaximumReturn | 199      |
| MinimumReturn | 116      |
| TotalSamples  | 33330    |
----------------------------
itr #9 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006616143509745598
Validation loss = 0.0020279695745557547
Validation loss = 0.0006879607681185007
Validation loss = 0.0007520425133407116
Validation loss = 0.0010168571025133133
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.001130995457060635
Validation loss = 0.0008533332729712129
Validation loss = 0.0013766285264864564
Validation loss = 0.0008595516555942595
Validation loss = 0.0008150679641403258
Validation loss = 0.0009805852314457297
Validation loss = 0.0007109242724254727
Validation loss = 0.0008815012406557798
Validation loss = 0.0008543030708096921
Validation loss = 0.0006919755251146853
Validation loss = 0.0007562637329101562
Validation loss = 0.0006734748021699488
Validation loss = 0.0012937316205352545
Validation loss = 0.0006599047919735312
Validation loss = 0.0012203848455101252
Validation loss = 0.0010954185854643583
Validation loss = 0.0006321888649836183
Validation loss = 0.0011611746158450842
Validation loss = 0.000723604520317167
Validation loss = 0.0007083301898092031
Validation loss = 0.0007224710425361991
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.001136497245170176
Validation loss = 0.0007391212857328355
Validation loss = 0.0008579312707297504
Validation loss = 0.0013641241239383817
Validation loss = 0.00075417326297611
Validation loss = 0.0006629577837884426
Validation loss = 0.0006909925141371787
Validation loss = 0.0008341129287146032
Validation loss = 0.0007397545268759131
Validation loss = 0.0009223684901371598
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0010759386932477355
Validation loss = 0.0007792761898599565
Validation loss = 0.000751457002479583
Validation loss = 0.0007582645048387349
Validation loss = 0.0006727500585839152
Validation loss = 0.0009113270789384842
Validation loss = 0.0010679501574486494
Validation loss = 0.0012664139503613114
Validation loss = 0.0006392642972059548
Validation loss = 0.000986787024885416
Validation loss = 0.0006620383355766535
Validation loss = 0.0014278856106102467
Validation loss = 0.0006384066073223948
Validation loss = 0.001054249471053481
Validation loss = 0.0007226042216643691
Validation loss = 0.0007860042969696224
Validation loss = 0.0007093930034898221
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0007347084465436637
Validation loss = 0.001857310300692916
Validation loss = 0.0006658025085926056
Validation loss = 0.0008443059050478041
Validation loss = 0.000771595980040729
Validation loss = 0.0006705497507937253
Validation loss = 0.0011218045838177204
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 159      |
| Iteration     | 9        |
| MaximumReturn | 198      |
| MinimumReturn | 118      |
| TotalSamples  | 36663    |
----------------------------
itr #10 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.000958544434979558
Validation loss = 0.0017301380867138505
Validation loss = 0.0006815759697929025
Validation loss = 0.0007081495714373887
Validation loss = 0.0008254668791778386
Validation loss = 0.0007476767641492188
Validation loss = 0.0015544937923550606
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.000851402641274035
Validation loss = 0.0010560130467638373
Validation loss = 0.0006617885082960129
Validation loss = 0.00073474389500916
Validation loss = 0.0007144498522393405
Validation loss = 0.0009647855185903609
Validation loss = 0.0006649307324551046
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0007447710377164185
Validation loss = 0.0006953377742320299
Validation loss = 0.0006476410781033337
Validation loss = 0.0006775945075787604
Validation loss = 0.000902606057934463
Validation loss = 0.0018042823066934943
Validation loss = 0.0005939911352470517
Validation loss = 0.001016503432765603
Validation loss = 0.0006845034658908844
Validation loss = 0.0009570929105393589
Validation loss = 0.0014017093926668167
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0009455443941988051
Validation loss = 0.0006455287802964449
Validation loss = 0.0006841113790869713
Validation loss = 0.0006756713846698403
Validation loss = 0.0010523120872676373
Validation loss = 0.0009712839964777231
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0006759851239621639
Validation loss = 0.0009097058791667223
Validation loss = 0.000739149865694344
Validation loss = 0.000801831716671586
Validation loss = 0.0009008513297885656
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 10       |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 39996    |
----------------------------
itr #11 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0020822344813495874
Validation loss = 0.0008203050238080323
Validation loss = 0.0006443498423323035
Validation loss = 0.001097761676646769
Validation loss = 0.0008275398286059499
Validation loss = 0.0013889430556446314
Validation loss = 0.0007263334118761122
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0008453543414361775
Validation loss = 0.0007415090221911669
Validation loss = 0.0007426817319355905
Validation loss = 0.0008022888796404004
Validation loss = 0.00066043168772012
Validation loss = 0.0006769412430003285
Validation loss = 0.0006872350932098925
Validation loss = 0.0007487244438380003
Validation loss = 0.0006943893968127668
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0008863065158948302
Validation loss = 0.0008602348389104009
Validation loss = 0.000663086655549705
Validation loss = 0.0006788354367017746
Validation loss = 0.0006451097433455288
Validation loss = 0.0006382360588759184
Validation loss = 0.000668019347358495
Validation loss = 0.0005979989073239267
Validation loss = 0.0007046444807201624
Validation loss = 0.0009597673779353499
Validation loss = 0.0008052882039919496
Validation loss = 0.0006365039153024554
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0011629600776359439
Validation loss = 0.0009620373020879924
Validation loss = 0.0007248834008350968
Validation loss = 0.001752057345584035
Validation loss = 0.0007530698785558343
Validation loss = 0.0007202939596027136
Validation loss = 0.0013197304215282202
Validation loss = 0.0007290249923244119
Validation loss = 0.000847665301989764
Validation loss = 0.0006673928000964224
Validation loss = 0.000746257952414453
Validation loss = 0.0007577413925901055
Validation loss = 0.0008172864909283817
Validation loss = 0.0007361081661656499
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0007820434984751046
Validation loss = 0.0007464280351996422
Validation loss = 0.0007428242242895067
Validation loss = 0.0007803404005244374
Validation loss = 0.0008103857981041074
Validation loss = 0.0006523816846311092
Validation loss = 0.0013939677737653255
Validation loss = 0.0006256980122998357
Validation loss = 0.0006607376271858811
Validation loss = 0.0007001938647590578
Validation loss = 0.0009031885419972241
Validation loss = 0.0005791484145447612
Validation loss = 0.0006754690548405051
Validation loss = 0.0007581759127788246
Validation loss = 0.0014590582577511668
Validation loss = 0.0007458961918018758
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 11       |
| MaximumReturn | 199      |
| MinimumReturn | 126      |
| TotalSamples  | 43329    |
----------------------------
itr #12 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006746588624082506
Validation loss = 0.0007904982776381075
Validation loss = 0.0007138976361602545
Validation loss = 0.0007013178546912968
Validation loss = 0.0006520712049677968
Validation loss = 0.0008136961841955781
Validation loss = 0.0010124781401827931
Validation loss = 0.0007788701332174242
Validation loss = 0.0006874309037812054
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0008181874873116612
Validation loss = 0.0006714954506605864
Validation loss = 0.0006607683608308434
Validation loss = 0.0007134677725844085
Validation loss = 0.0006518342415802181
Validation loss = 0.000623474596068263
Validation loss = 0.001067039673216641
Validation loss = 0.0006513106636703014
Validation loss = 0.0010289070196449757
Validation loss = 0.0007559465593658388
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0006902314489707351
Validation loss = 0.0006212989683263004
Validation loss = 0.0007607408915646374
Validation loss = 0.0006155315786600113
Validation loss = 0.0007145365816541016
Validation loss = 0.0007071598083712161
Validation loss = 0.0007381907780654728
Validation loss = 0.0010019975015893579
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0006729551823809743
Validation loss = 0.0006500457529909909
Validation loss = 0.0006212760345079005
Validation loss = 0.0008751860004849732
Validation loss = 0.000620463106315583
Validation loss = 0.0006344259018078446
Validation loss = 0.0007497415645048022
Validation loss = 0.0006730285240337253
Validation loss = 0.0007126607233658433
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0007592456531710923
Validation loss = 0.0006601709756068885
Validation loss = 0.0006432372611016035
Validation loss = 0.0008351600845344365
Validation loss = 0.0007363880868069828
Validation loss = 0.0006474999827332795
Validation loss = 0.0008020129753276706
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 180      |
| Iteration     | 12       |
| MaximumReturn | 199      |
| MinimumReturn | 137      |
| TotalSamples  | 46662    |
----------------------------
itr #13 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0008397921919822693
Validation loss = 0.000737131864298135
Validation loss = 0.0006740136304870248
Validation loss = 0.0009490870288573205
Validation loss = 0.0008706995286047459
Validation loss = 0.00090376305161044
Validation loss = 0.0006841120775789022
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0008316675666719675
Validation loss = 0.0006891104276292026
Validation loss = 0.0007229759357869625
Validation loss = 0.0009235673933289945
Validation loss = 0.0007249783957377076
Validation loss = 0.0006844517192803323
Validation loss = 0.000661209225654602
Validation loss = 0.0008294928702525795
Validation loss = 0.0007152081234380603
Validation loss = 0.0006995578296482563
Validation loss = 0.0010173608316108584
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0008941993000917137
Validation loss = 0.0006707320571877062
Validation loss = 0.0008524834411218762
Validation loss = 0.0006804516306146979
Validation loss = 0.0006716441712342203
Validation loss = 0.0006309569580480456
Validation loss = 0.000747289159335196
Validation loss = 0.0007558696670457721
Validation loss = 0.0006592121790163219
Validation loss = 0.0006873715901747346
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007135246996767819
Validation loss = 0.0007071472937241197
Validation loss = 0.000818869739305228
Validation loss = 0.000894733879249543
Validation loss = 0.0009521537576802075
Validation loss = 0.0007019379991106689
Validation loss = 0.0009429444326087832
Validation loss = 0.0013389344094321132
Validation loss = 0.00066632020752877
Validation loss = 0.0014835615875199437
Validation loss = 0.0007101078517735004
Validation loss = 0.000647529901470989
Validation loss = 0.0007449100958183408
Validation loss = 0.0007507556583732367
Validation loss = 0.000661152764223516
Validation loss = 0.0008240159368142486
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0008743269136175513
Validation loss = 0.0007620269898325205
Validation loss = 0.0008590969955548644
Validation loss = 0.0007708721095696092
Validation loss = 0.0008057455415837467
Validation loss = 0.000833974510896951
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 13       |
| MaximumReturn | 199      |
| MinimumReturn | 126      |
| TotalSamples  | 49995    |
----------------------------
itr #14 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0008092838688753545
Validation loss = 0.0007832522969692945
Validation loss = 0.0008444514242000878
Validation loss = 0.0007956461631692946
Validation loss = 0.0008051908225752413
Validation loss = 0.0009295783820562065
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0008015494677238166
Validation loss = 0.0009044514736160636
Validation loss = 0.0007359102601185441
Validation loss = 0.0007598429801873863
Validation loss = 0.001079521025530994
Validation loss = 0.0008561518043279648
Validation loss = 0.0007569784647785127
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0015236580511555076
Validation loss = 0.0009304577833972871
Validation loss = 0.0007174413185566664
Validation loss = 0.0008763668593019247
Validation loss = 0.0007155633065849543
Validation loss = 0.000815981940831989
Validation loss = 0.000817568157799542
Validation loss = 0.0010957567719742656
Validation loss = 0.0007975117769092321
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0009042397723533213
Validation loss = 0.0007433615392073989
Validation loss = 0.0007940083160065114
Validation loss = 0.0007478274637833238
Validation loss = 0.0007597421063110232
Validation loss = 0.0007168633746914566
Validation loss = 0.0007082967204041779
Validation loss = 0.0010166896972805262
Validation loss = 0.0010125205153599381
Validation loss = 0.0008891430334188044
Validation loss = 0.0008808770217001438
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00093338533770293
Validation loss = 0.00079676992027089
Validation loss = 0.0007970098522491753
Validation loss = 0.00103876949287951
Validation loss = 0.0007441659108735621
Validation loss = 0.0007431666599586606
Validation loss = 0.000791013240814209
Validation loss = 0.0025731322821229696
Validation loss = 0.000677005504257977
Validation loss = 0.000683142221532762
Validation loss = 0.0007808423251844943
Validation loss = 0.0007691041100770235
Validation loss = 0.0009218729101121426
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 14       |
| MaximumReturn | 199      |
| MinimumReturn | 128      |
| TotalSamples  | 53328    |
----------------------------
itr #15 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0008102108840830624
Validation loss = 0.0009012125083245337
Validation loss = 0.0008938225801102817
Validation loss = 0.000809434219263494
Validation loss = 0.0007919631898403168
Validation loss = 0.0007686110911890864
Validation loss = 0.0007877771276980639
Validation loss = 0.0009023342281579971
Validation loss = 0.0011000807862728834
Validation loss = 0.000910811242647469
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0009830526541918516
Validation loss = 0.0007372420514002442
Validation loss = 0.0007401263574138284
Validation loss = 0.0008373807067982852
Validation loss = 0.0009997256565839052
Validation loss = 0.0008523121941834688
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0007788122165948153
Validation loss = 0.0007365300552919507
Validation loss = 0.0010006483644247055
Validation loss = 0.0008957963436841965
Validation loss = 0.0007233190117403865
Validation loss = 0.0008176619885489345
Validation loss = 0.0007448854157701135
Validation loss = 0.0008412369643338025
Validation loss = 0.0007361804600805044
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0009144636569544673
Validation loss = 0.0008841946255415678
Validation loss = 0.0007133663166314363
Validation loss = 0.00074196100467816
Validation loss = 0.0007829018286429346
Validation loss = 0.0007436381420120597
Validation loss = 0.0008830286096781492
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.000926332431845367
Validation loss = 0.0008261868497356772
Validation loss = 0.0008613638929091394
Validation loss = 0.0007448196411132812
Validation loss = 0.0008578045526519418
Validation loss = 0.000889839546289295
Validation loss = 0.000785584154073149
Validation loss = 0.0007356171263381839
Validation loss = 0.0008224925259128213
Validation loss = 0.0008244002237915993
Validation loss = 0.0007445342489518225
Validation loss = 0.0008146923501044512
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 178      |
| Iteration     | 15       |
| MaximumReturn | 199      |
| MinimumReturn | 123      |
| TotalSamples  | 56661    |
----------------------------
itr #16 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0010676023084670305
Validation loss = 0.0008608124335296452
Validation loss = 0.0007514014723710716
Validation loss = 0.0009003180894069374
Validation loss = 0.001061108079738915
Validation loss = 0.0007580452947877347
Validation loss = 0.0007950427825562656
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.001233194605447352
Validation loss = 0.0007871332345530391
Validation loss = 0.0008041138644330204
Validation loss = 0.0011041774414479733
Validation loss = 0.0011068625608459115
Validation loss = 0.0007527228444814682
Validation loss = 0.0008569256169721484
Validation loss = 0.0008808412821963429
Validation loss = 0.0007878486649133265
Validation loss = 0.0010504730744287372
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0008556836401112378
Validation loss = 0.0008321422501467168
Validation loss = 0.0007959571667015553
Validation loss = 0.0008559784037061036
Validation loss = 0.0007464511436410248
Validation loss = 0.0015883812448009849
Validation loss = 0.0007603332633152604
Validation loss = 0.0008283362840302289
Validation loss = 0.000738505448680371
Validation loss = 0.000787415832746774
Validation loss = 0.0009015167015604675
Validation loss = 0.0007374436827376485
Validation loss = 0.0007271144422702491
Validation loss = 0.0008910242468118668
Validation loss = 0.0007846440421417356
Validation loss = 0.0007557463250122964
Validation loss = 0.0007945631514303386
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007985252887010574
Validation loss = 0.0010043204529210925
Validation loss = 0.0007357032736763358
Validation loss = 0.0007922874647192657
Validation loss = 0.0007188213057816029
Validation loss = 0.0009587244130671024
Validation loss = 0.0008552842191420496
Validation loss = 0.0007645533187314868
Validation loss = 0.0007539392681792378
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0009808506583794951
Validation loss = 0.0008362751686945558
Validation loss = 0.0008347860421054065
Validation loss = 0.0008273065323010087
Validation loss = 0.0007915051537565887
Validation loss = 0.0008195138652808964
Validation loss = 0.0007320553995668888
Validation loss = 0.0007669127662666142
Validation loss = 0.0007481179782189429
Validation loss = 0.0007480643107555807
Validation loss = 0.0007627637241967022
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 178      |
| Iteration     | 16       |
| MaximumReturn | 199      |
| MinimumReturn | 128      |
| TotalSamples  | 59994    |
----------------------------
itr #17 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.001068021054379642
Validation loss = 0.0009243358508683741
Validation loss = 0.0011313741561025381
Validation loss = 0.0009327988373115659
Validation loss = 0.001129767159000039
Validation loss = 0.0009277689969167113
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0009148165117949247
Validation loss = 0.0011650108499452472
Validation loss = 0.000936308701056987
Validation loss = 0.0008954252698458731
Validation loss = 0.0010336381383240223
Validation loss = 0.0010096380719915032
Validation loss = 0.0009042657911777496
Validation loss = 0.0009020966826938093
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0008968451875261962
Validation loss = 0.0009095975547097623
Validation loss = 0.0011861585080623627
Validation loss = 0.000879965431522578
Validation loss = 0.001145685208030045
Validation loss = 0.0008256309083662927
Validation loss = 0.0009285159176215529
Validation loss = 0.0008678702870383859
Validation loss = 0.0008855984196998179
Validation loss = 0.0008354479796253145
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0010049429256469011
Validation loss = 0.0009137150482274592
Validation loss = 0.0009291928727179766
Validation loss = 0.0009361411212012172
Validation loss = 0.0010721657890826464
Validation loss = 0.000918887322768569
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.000864512228872627
Validation loss = 0.0009320173412561417
Validation loss = 0.00082395103527233
Validation loss = 0.0009439727873541415
Validation loss = 0.0009587231324985623
Validation loss = 0.0008269425597973168
Validation loss = 0.0009352730703540146
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 17       |
| MaximumReturn | 199      |
| MinimumReturn | 126      |
| TotalSamples  | 63327    |
----------------------------
itr #18 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0010372912511229515
Validation loss = 0.000967611966188997
Validation loss = 0.0010032112477347255
Validation loss = 0.0009666698169894516
Validation loss = 0.0009904290782287717
Validation loss = 0.0009546155342832208
Validation loss = 0.0009665648103691638
Validation loss = 0.0010178453521803021
Validation loss = 0.0010220573749393225
Validation loss = 0.0009125177748501301
Validation loss = 0.0010442949132993817
Validation loss = 0.0009560571634210646
Validation loss = 0.0010029131080955267
Validation loss = 0.0009343390702269971
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0010397242149338126
Validation loss = 0.0011285984655842185
Validation loss = 0.0010302163427695632
Validation loss = 0.000925427011679858
Validation loss = 0.0010135640623047948
Validation loss = 0.0009361201082356274
Validation loss = 0.000947433989495039
Validation loss = 0.0009732283069752157
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0010286971228197217
Validation loss = 0.001039707800373435
Validation loss = 0.0009866374311968684
Validation loss = 0.0013108249986544251
Validation loss = 0.0009657251066528261
Validation loss = 0.0009546360815875232
Validation loss = 0.000925131724216044
Validation loss = 0.0009504167246632278
Validation loss = 0.0009470778750255704
Validation loss = 0.001066791475750506
Validation loss = 0.0009669741848483682
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0009546782239340246
Validation loss = 0.0009691390441730618
Validation loss = 0.0009677538182586432
Validation loss = 0.0009410235797986388
Validation loss = 0.0010042370995506644
Validation loss = 0.0011045146966353059
Validation loss = 0.0009500339510850608
Validation loss = 0.0009682032978162169
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.000963678874541074
Validation loss = 0.0010530150029808283
Validation loss = 0.0010706469183787704
Validation loss = 0.0010092961601912975
Validation loss = 0.0010465917875990272
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 179      |
| Iteration     | 18       |
| MaximumReturn | 199      |
| MinimumReturn | 126      |
| TotalSamples  | 66660    |
----------------------------
itr #19 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0009703882387839258
Validation loss = 0.0010282800067216158
Validation loss = 0.0009897961281239986
Validation loss = 0.000995382433757186
Validation loss = 0.0010784863261505961
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.000978567055426538
Validation loss = 0.000988911953754723
Validation loss = 0.001058025867678225
Validation loss = 0.0009817256359383464
Validation loss = 0.000990736880339682
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.000959640892688185
Validation loss = 0.0009937640279531479
Validation loss = 0.0010398447047919035
Validation loss = 0.0010253597283735871
Validation loss = 0.0009805737063288689
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0010141093516722322
Validation loss = 0.0010701663559302688
Validation loss = 0.0010220190742984414
Validation loss = 0.0009799354011192918
Validation loss = 0.0010159255471080542
Validation loss = 0.000986482365988195
Validation loss = 0.0010054906597360969
Validation loss = 0.0011100287083536386
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0010334642138332129
Validation loss = 0.0009973847772926092
Validation loss = 0.0010177184594795108
Validation loss = 0.0009396913228556514
Validation loss = 0.0011279298923909664
Validation loss = 0.0009730564197525382
Validation loss = 0.0010299445129930973
Validation loss = 0.001130505814217031
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 19       |
| MaximumReturn | 198      |
| MinimumReturn | 132      |
| TotalSamples  | 69993    |
----------------------------
itr #20 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0010090796276926994
Validation loss = 0.0010873153805732727
Validation loss = 0.0011617429554462433
Validation loss = 0.0014144362648949027
Validation loss = 0.001002770964987576
Validation loss = 0.001053555402904749
Validation loss = 0.0010204740101471543
Validation loss = 0.001088806544430554
Validation loss = 0.0011144056916236877
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0011197071289643645
Validation loss = 0.0011802298249676824
Validation loss = 0.0011248179944232106
Validation loss = 0.0010564232943579555
Validation loss = 0.0010732809314504266
Validation loss = 0.001026209443807602
Validation loss = 0.0010733861709013581
Validation loss = 0.0015981984324753284
Validation loss = 0.001036026980727911
Validation loss = 0.001024478580802679
Validation loss = 0.0010960722574964166
Validation loss = 0.001008870778605342
Validation loss = 0.0010235266527161002
Validation loss = 0.0010605896823108196
Validation loss = 0.000997001538053155
Validation loss = 0.000993774039670825
Validation loss = 0.0010057850740849972
Validation loss = 0.0010479213669896126
Validation loss = 0.0010105491383001208
Validation loss = 0.0010266994358971715
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0010261129355058074
Validation loss = 0.0010439635952934623
Validation loss = 0.001013127388432622
Validation loss = 0.00103272357955575
Validation loss = 0.001102520152926445
Validation loss = 0.0009883231250569224
Validation loss = 0.001071825623512268
Validation loss = 0.0009882701560854912
Validation loss = 0.0017202890012413263
Validation loss = 0.001051369239576161
Validation loss = 0.0010461403289809823
Validation loss = 0.0010253998916596174
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.001001996686682105
Validation loss = 0.0011492240009829402
Validation loss = 0.0011972712818533182
Validation loss = 0.0010380912572145462
Validation loss = 0.0009951626416295767
Validation loss = 0.0010108704445883632
Validation loss = 0.0010389380622655153
Validation loss = 0.001130079966969788
Validation loss = 0.0011453930055722594
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0010743588209152222
Validation loss = 0.0012069004587829113
Validation loss = 0.0010734222596511245
Validation loss = 0.001060962094925344
Validation loss = 0.0010352896060794592
Validation loss = 0.0010708806803449988
Validation loss = 0.001109240809455514
Validation loss = 0.0012152863200753927
Validation loss = 0.0010328928474336863
Validation loss = 0.001067174132913351
Validation loss = 0.0010153657058253884
Validation loss = 0.001005869242362678
Validation loss = 0.0010283930459991097
Validation loss = 0.001050586928613484
Validation loss = 0.0010195854119956493
Validation loss = 0.0010328312637284398
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 20       |
| MaximumReturn | 199      |
| MinimumReturn | 132      |
| TotalSamples  | 73326    |
----------------------------
itr #21 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00111227051820606
Validation loss = 0.001118244370445609
Validation loss = 0.001104185008443892
Validation loss = 0.0012124632485210896
Validation loss = 0.001188538153655827
Validation loss = 0.001125906826928258
Validation loss = 0.0011864625848829746
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0011739518959075212
Validation loss = 0.0010921149514615536
Validation loss = 0.0010848450474441051
Validation loss = 0.0011495159706100821
Validation loss = 0.0011112458305433393
Validation loss = 0.0011096304515376687
Validation loss = 0.00121108943130821
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0010859481990337372
Validation loss = 0.001096036983653903
Validation loss = 0.0013207043521106243
Validation loss = 0.001053975080139935
Validation loss = 0.0011319293407723308
Validation loss = 0.0011824036482721567
Validation loss = 0.0010924270609393716
Validation loss = 0.001154315541498363
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0013116775080561638
Validation loss = 0.0011362065561115742
Validation loss = 0.001108284224756062
Validation loss = 0.0011283662170171738
Validation loss = 0.001139136147685349
Validation loss = 0.0010933146113529801
Validation loss = 0.0011058985255658627
Validation loss = 0.0012816963717341423
Validation loss = 0.0011905666906386614
Validation loss = 0.0011908465530723333
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0012901616282761097
Validation loss = 0.0011265412904322147
Validation loss = 0.0010899568442255259
Validation loss = 0.0011312079150229692
Validation loss = 0.001154463505372405
Validation loss = 0.0010800707386806607
Validation loss = 0.0011605932377278805
Validation loss = 0.0011050390312448144
Validation loss = 0.00117371731903404
Validation loss = 0.0011189791839569807
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 21       |
| MaximumReturn | 199      |
| MinimumReturn | 128      |
| TotalSamples  | 76659    |
----------------------------
itr #22 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.001351983635686338
Validation loss = 0.001354929176159203
Validation loss = 0.0013331194641068578
Validation loss = 0.0013462691567838192
Validation loss = 0.0013655315851792693
Validation loss = 0.0013103835517540574
Validation loss = 0.0012856608955189586
Validation loss = 0.001294420100748539
Validation loss = 0.0013278998667374253
Validation loss = 0.0014969557523727417
Validation loss = 0.001321436488069594
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0013683520955964923
Validation loss = 0.0013117976486682892
Validation loss = 0.0013049491681158543
Validation loss = 0.001275682938285172
Validation loss = 0.0013323378516361117
Validation loss = 0.001288071507588029
Validation loss = 0.0013276279205456376
Validation loss = 0.0013451356207951903
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0012890425277873874
Validation loss = 0.0013305731117725372
Validation loss = 0.0013747094199061394
Validation loss = 0.001467783353291452
Validation loss = 0.001321468036621809
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.001343785785138607
Validation loss = 0.001460351631976664
Validation loss = 0.0013802764005959034
Validation loss = 0.0013256024103611708
Validation loss = 0.0012508661020547152
Validation loss = 0.001309187151491642
Validation loss = 0.0013742998708039522
Validation loss = 0.00131599185988307
Validation loss = 0.0012572385603561997
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.001349514932371676
Validation loss = 0.001305291778407991
Validation loss = 0.0012390570482239127
Validation loss = 0.001255378476344049
Validation loss = 0.0013342753518372774
Validation loss = 0.0013077069306746125
Validation loss = 0.0012989871902391315
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 22       |
| MaximumReturn | 199      |
| MinimumReturn | 113      |
| TotalSamples  | 79992    |
----------------------------
itr #23 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0014436582569032907
Validation loss = 0.0014107811730355024
Validation loss = 0.0015031399670988321
Validation loss = 0.0014304417418316007
Validation loss = 0.001429969910532236
Validation loss = 0.0015161228366196156
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0014234205009415746
Validation loss = 0.0018230568384751678
Validation loss = 0.0014703718479722738
Validation loss = 0.0014808131381869316
Validation loss = 0.0013768773060292006
Validation loss = 0.0013912464492022991
Validation loss = 0.0013817576691508293
Validation loss = 0.0013997178757563233
Validation loss = 0.001394593040458858
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0014141569845378399
Validation loss = 0.0015652745496481657
Validation loss = 0.00137759605422616
Validation loss = 0.0014451735187321901
Validation loss = 0.001405663206242025
Validation loss = 0.0014450266025960445
Validation loss = 0.001631419756449759
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0014839263167232275
Validation loss = 0.001495884032920003
Validation loss = 0.0014651421224698424
Validation loss = 0.0014867066638544202
Validation loss = 0.0014302863273769617
Validation loss = 0.0014555032830685377
Validation loss = 0.0014170773793011904
Validation loss = 0.0014270131941884756
Validation loss = 0.001568813226185739
Validation loss = 0.0014297841116786003
Validation loss = 0.0014595739776268601
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0015584557550027966
Validation loss = 0.0014032820472493768
Validation loss = 0.0015557421138510108
Validation loss = 0.0014619693392887712
Validation loss = 0.0013777029234915972
Validation loss = 0.0014014167245477438
Validation loss = 0.0014732189010828733
Validation loss = 0.0014081676490604877
Validation loss = 0.0015058281132951379
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 180      |
| Iteration     | 23       |
| MaximumReturn | 199      |
| MinimumReturn | 119      |
| TotalSamples  | 83325    |
----------------------------
itr #24 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0015044563915580511
Validation loss = 0.0014860673109069467
Validation loss = 0.001463665859773755
Validation loss = 0.0015118559822440147
Validation loss = 0.001518260221928358
Validation loss = 0.0014440638478845358
Validation loss = 0.0014364594826474786
Validation loss = 0.0014580058632418513
Validation loss = 0.0014096490340307355
Validation loss = 0.0014741523191332817
Validation loss = 0.0014735185541212559
Validation loss = 0.0014050639001652598
Validation loss = 0.0014819646021351218
Validation loss = 0.0014759718906134367
Validation loss = 0.0014262162148952484
Validation loss = 0.0015696634072810411
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0014427504502236843
Validation loss = 0.0014683357439935207
Validation loss = 0.0014408366987481713
Validation loss = 0.001439064508304
Validation loss = 0.0017207680502906442
Validation loss = 0.0014141651336103678
Validation loss = 0.0014335827436298132
Validation loss = 0.0015780406538397074
Validation loss = 0.0015097352443262935
Validation loss = 0.0014272553380578756
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.001461324281990528
Validation loss = 0.0014132825890555978
Validation loss = 0.0016268626786768436
Validation loss = 0.001506952103227377
Validation loss = 0.001467765192501247
Validation loss = 0.001419938518665731
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0014973583165556192
Validation loss = 0.001428422867320478
Validation loss = 0.001451827702112496
Validation loss = 0.0014515500515699387
Validation loss = 0.0015172346029430628
Validation loss = 0.0016071947757154703
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0014461964601650834
Validation loss = 0.001632743515074253
Validation loss = 0.0016614622436463833
Validation loss = 0.0014493158087134361
Validation loss = 0.0015246988041326404
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 184      |
| Iteration     | 24       |
| MaximumReturn | 199      |
| MinimumReturn | 136      |
| TotalSamples  | 86658    |
----------------------------
itr #25 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0018423530273139477
Validation loss = 0.001688081305474043
Validation loss = 0.0018394908402115107
Validation loss = 0.0018171760020777583
Validation loss = 0.0017558584222570062
Validation loss = 0.00187881279271096
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0017220632871612906
Validation loss = 0.0016946224495768547
Validation loss = 0.0016756369732320309
Validation loss = 0.0016821591416373849
Validation loss = 0.0017159120179712772
Validation loss = 0.001672401325777173
Validation loss = 0.0016530577559024096
Validation loss = 0.0016860759351402521
Validation loss = 0.0016611535102128983
Validation loss = 0.0017631046939641237
Validation loss = 0.0016835976857692003
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0017419822979718447
Validation loss = 0.0018282241653651
Validation loss = 0.0016950180288404226
Validation loss = 0.0017165567260235548
Validation loss = 0.0016911717830225825
Validation loss = 0.0016826451756060123
Validation loss = 0.0016535265604034066
Validation loss = 0.001715031685307622
Validation loss = 0.0016954413149505854
Validation loss = 0.0017723157070577145
Validation loss = 0.0016910923877730966
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00168852717615664
Validation loss = 0.001662580412812531
Validation loss = 0.0016828086227178574
Validation loss = 0.0017666262574493885
Validation loss = 0.0016781791346147656
Validation loss = 0.0016574497567489743
Validation loss = 0.001702452776953578
Validation loss = 0.0016467634122818708
Validation loss = 0.0016736785182729363
Validation loss = 0.001732162432745099
Validation loss = 0.0016486133681610227
Validation loss = 0.0017519958782941103
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.001711259363219142
Validation loss = 0.0017640725709497929
Validation loss = 0.0017068784218281507
Validation loss = 0.0016851882683113217
Validation loss = 0.0016937680775299668
Validation loss = 0.0016874156426638365
Validation loss = 0.0016341160517185926
Validation loss = 0.0016932161524891853
Validation loss = 0.0016917589819058776
Validation loss = 0.0016780169680714607
Validation loss = 0.0017042093677446246
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 25       |
| MaximumReturn | 199      |
| MinimumReturn | 124      |
| TotalSamples  | 89991    |
----------------------------
itr #26 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.002172380918636918
Validation loss = 0.0019055260345339775
Validation loss = 0.001832813723012805
Validation loss = 0.0018395158695057034
Validation loss = 0.0017634715186432004
Validation loss = 0.0018167735543102026
Validation loss = 0.0017838665517047048
Validation loss = 0.0017648189095780253
Validation loss = 0.0017570563359186053
Validation loss = 0.0017607341287657619
Validation loss = 0.001970783807337284
Validation loss = 0.001773068681359291
Validation loss = 0.0017885417910292745
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0018668409902602434
Validation loss = 0.0018266875995323062
Validation loss = 0.0017471526516601443
Validation loss = 0.001775238080881536
Validation loss = 0.001752552343532443
Validation loss = 0.001755705801770091
Validation loss = 0.0017743275966495275
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.001835433766245842
Validation loss = 0.0018158585298806429
Validation loss = 0.0018996906001120806
Validation loss = 0.0018061116570606828
Validation loss = 0.001757937017828226
Validation loss = 0.001747427275404334
Validation loss = 0.0017852660967037082
Validation loss = 0.0017534906510263681
Validation loss = 0.0017301321495324373
Validation loss = 0.0017599031561985612
Validation loss = 0.0018071410013362765
Validation loss = 0.0018932815873995423
Validation loss = 0.001819646218791604
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0018987326184287667
Validation loss = 0.001747617614455521
Validation loss = 0.0018067999044433236
Validation loss = 0.0017802895745262504
Validation loss = 0.0018409275216981769
Validation loss = 0.0017485172720625997
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0018015659879893064
Validation loss = 0.001837620628066361
Validation loss = 0.0018161879852414131
Validation loss = 0.0018202927894890308
Validation loss = 0.001856790273450315
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 26       |
| MaximumReturn | 199      |
| MinimumReturn | 130      |
| TotalSamples  | 93324    |
----------------------------
itr #27 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0021772936452180147
Validation loss = 0.0019195559434592724
Validation loss = 0.0019404867198318243
Validation loss = 0.0019505834206938744
Validation loss = 0.001999860629439354
Validation loss = 0.0019515060121193528
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.001977072563022375
Validation loss = 0.00193004438187927
Validation loss = 0.0019559659995138645
Validation loss = 0.0020896124187856913
Validation loss = 0.0020895646885037422
Validation loss = 0.0019281014101579785
Validation loss = 0.0019323520828038454
Validation loss = 0.002010688418522477
Validation loss = 0.0024112830869853497
Validation loss = 0.001983210677281022
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.001974220620468259
Validation loss = 0.002014393452554941
Validation loss = 0.001935053733177483
Validation loss = 0.002119197743013501
Validation loss = 0.0019358847057446837
Validation loss = 0.0019541862420737743
Validation loss = 0.0019483171636238694
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0019910449627786875
Validation loss = 0.002007509348914027
Validation loss = 0.001964195165783167
Validation loss = 0.0019962165970355272
Validation loss = 0.001953525934368372
Validation loss = 0.0019351851660758257
Validation loss = 0.002007645322009921
Validation loss = 0.0019940051715821028
Validation loss = 0.0020652590319514275
Validation loss = 0.0019953588489443064
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.001947383163496852
Validation loss = 0.001961193745955825
Validation loss = 0.0019921408966183662
Validation loss = 0.0019258549436926842
Validation loss = 0.0019539061468094587
Validation loss = 0.0019306458998471498
Validation loss = 0.0019433352863416076
Validation loss = 0.001935282489284873
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 27       |
| MaximumReturn | 199      |
| MinimumReturn | 122      |
| TotalSamples  | 96657    |
----------------------------
itr #28 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0021459488198161125
Validation loss = 0.002094752388074994
Validation loss = 0.0021374307107180357
Validation loss = 0.0021406058222055435
Validation loss = 0.002095770090818405
Validation loss = 0.0020757827442139387
Validation loss = 0.0020493995398283005
Validation loss = 0.002064615720883012
Validation loss = 0.0021383368875831366
Validation loss = 0.002094293013215065
Validation loss = 0.002038805978372693
Validation loss = 0.002087655244395137
Validation loss = 0.0020891393069177866
Validation loss = 0.00229874555952847
Validation loss = 0.0020445932168513536
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.002093252958729863
Validation loss = 0.002058940241113305
Validation loss = 0.002055732300505042
Validation loss = 0.0020615344401448965
Validation loss = 0.0020598520059138536
Validation loss = 0.002064175670966506
Validation loss = 0.0023034277837723494
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.002115318551659584
Validation loss = 0.002172560663893819
Validation loss = 0.002142758807167411
Validation loss = 0.0020550584886223078
Validation loss = 0.0020608913619071245
Validation loss = 0.0021731408778578043
Validation loss = 0.0020421352237462997
Validation loss = 0.002062023151665926
Validation loss = 0.002094596391543746
Validation loss = 0.002065237844362855
Validation loss = 0.002094097202643752
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.002094386611133814
Validation loss = 0.0021050346549600363
Validation loss = 0.002254452323541045
Validation loss = 0.0020597390830516815
Validation loss = 0.002058624289929867
Validation loss = 0.002035531448200345
Validation loss = 0.0020645433105528355
Validation loss = 0.00210119248367846
Validation loss = 0.002051395131275058
Validation loss = 0.0020552221685647964
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.002124209888279438
Validation loss = 0.0020627372432500124
Validation loss = 0.0020562682766467333
Validation loss = 0.0020186288747936487
Validation loss = 0.0020558771211653948
Validation loss = 0.002050296403467655
Validation loss = 0.0020846251863986254
Validation loss = 0.0021848336327821016
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 163      |
| Iteration     | 28       |
| MaximumReturn | 198      |
| MinimumReturn | 114      |
| TotalSamples  | 99990    |
----------------------------
itr #29 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0025971694849431515
Validation loss = 0.0028381552547216415
Validation loss = 0.0025156610645353794
Validation loss = 0.002582874381914735
Validation loss = 0.0025525533128529787
Validation loss = 0.0025984125677496195
Validation loss = 0.0025348730850964785
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.002583771012723446
Validation loss = 0.0025731318164616823
Validation loss = 0.002760855248197913
Validation loss = 0.002567087300121784
Validation loss = 0.0025576918851584196
Validation loss = 0.002519149100407958
Validation loss = 0.0025237672962248325
Validation loss = 0.0025404617190361023
Validation loss = 0.0025949853006750345
Validation loss = 0.002524022478610277
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0025206839200109243
Validation loss = 0.0025625359266996384
Validation loss = 0.0025728156324476004
Validation loss = 0.0025087224785238504
Validation loss = 0.0027752292808145285
Validation loss = 0.002537646796554327
Validation loss = 0.002518066205084324
Validation loss = 0.0025549272540956736
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0025541752111166716
Validation loss = 0.002554473467171192
Validation loss = 0.0025581202935427427
Validation loss = 0.0027346028946340084
Validation loss = 0.0026105325669050217
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.002596834674477577
Validation loss = 0.002600348088890314
Validation loss = 0.0025412412360310555
Validation loss = 0.0026032549794763327
Validation loss = 0.0025636469945311546
Validation loss = 0.002556044841185212
Validation loss = 0.0025888746604323387
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 29       |
| MaximumReturn | 199      |
| MinimumReturn | 124      |
| TotalSamples  | 103323   |
----------------------------
itr #30 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.002679361030459404
Validation loss = 0.002777836285531521
Validation loss = 0.0026828895788639784
Validation loss = 0.0026951590552926064
Validation loss = 0.0026983232237398624
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0027309292927384377
Validation loss = 0.0027180202305316925
Validation loss = 0.002818807726725936
Validation loss = 0.002671355614438653
Validation loss = 0.002679949626326561
Validation loss = 0.002669221954420209
Validation loss = 0.0027873695362359285
Validation loss = 0.0027048566844314337
Validation loss = 0.002677400130778551
Validation loss = 0.0026993800420314074
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0027177182491868734
Validation loss = 0.0027452316135168076
Validation loss = 0.002754488028585911
Validation loss = 0.0027154164854437113
Validation loss = 0.0026902954559773207
Validation loss = 0.002718019997701049
Validation loss = 0.002730769570916891
Validation loss = 0.00274206162430346
Validation loss = 0.002654740586876869
Validation loss = 0.002719799755141139
Validation loss = 0.0027335803024470806
Validation loss = 0.0026983791030943394
Validation loss = 0.0026823554653674364
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0026888810098171234
Validation loss = 0.0026989292819052935
Validation loss = 0.0026768422685563564
Validation loss = 0.002717947354540229
Validation loss = 0.0027490451466292143
Validation loss = 0.0026687064673751593
Validation loss = 0.002678830176591873
Validation loss = 0.0027144893538206816
Validation loss = 0.002771503757685423
Validation loss = 0.0026921676471829414
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.002768824575468898
Validation loss = 0.0028369349893182516
Validation loss = 0.0027609688695520163
Validation loss = 0.002973343478515744
Validation loss = 0.002695515751838684
Validation loss = 0.002764218021184206
Validation loss = 0.0027093696407973766
Validation loss = 0.002659293357282877
Validation loss = 0.0027394641656428576
Validation loss = 0.0027848631143569946
Validation loss = 0.0027151438407599926
Validation loss = 0.0027007758617401123
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 178      |
| Iteration     | 30       |
| MaximumReturn | 199      |
| MinimumReturn | 129      |
| TotalSamples  | 106656   |
----------------------------
itr #31 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00323971058242023
Validation loss = 0.0033533235546201468
Validation loss = 0.003211453789845109
Validation loss = 0.0032720386516302824
Validation loss = 0.003231199225410819
Validation loss = 0.0032958961091935635
Validation loss = 0.0033834476489573717
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0032472561579197645
Validation loss = 0.0033006048761308193
Validation loss = 0.0033418331295251846
Validation loss = 0.003203449072316289
Validation loss = 0.0032533486373722553
Validation loss = 0.003243344370275736
Validation loss = 0.0032889852300286293
Validation loss = 0.003241435857489705
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.003422437235713005
Validation loss = 0.003195290919393301
Validation loss = 0.0032721220050007105
Validation loss = 0.0032497551292181015
Validation loss = 0.0032340208999812603
Validation loss = 0.0031949495896697044
Validation loss = 0.003381696529686451
Validation loss = 0.0032494186889380217
Validation loss = 0.003276614937931299
Validation loss = 0.0031973186414688826
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0036025187000632286
Validation loss = 0.00322781503200531
Validation loss = 0.003238982753828168
Validation loss = 0.003246838226914406
Validation loss = 0.0032340053003281355
Validation loss = 0.003325850935652852
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.003385144053027034
Validation loss = 0.0032298497390002012
Validation loss = 0.0032685529440641403
Validation loss = 0.0032780002802610397
Validation loss = 0.0032361927442252636
Validation loss = 0.0033288621343672276
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 31       |
| MaximumReturn | 198      |
| MinimumReturn | 127      |
| TotalSamples  | 109989   |
----------------------------
itr #32 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.003776698838919401
Validation loss = 0.0035726497881114483
Validation loss = 0.0035923360846936703
Validation loss = 0.0036736533511430025
Validation loss = 0.0037198481149971485
Validation loss = 0.003608420491218567
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0035735818091779947
Validation loss = 0.0038406357634812593
Validation loss = 0.003553748829290271
Validation loss = 0.003594721667468548
Validation loss = 0.0036208212841302156
Validation loss = 0.003641698742285371
Validation loss = 0.0037162243388593197
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0037467211950570345
Validation loss = 0.0036015515215694904
Validation loss = 0.003569667460396886
Validation loss = 0.003583165816962719
Validation loss = 0.0036690887063741684
Validation loss = 0.003622837597504258
Validation loss = 0.003639325499534607
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0035446714609861374
Validation loss = 0.003639907343313098
Validation loss = 0.0035616024397313595
Validation loss = 0.0035736956633627415
Validation loss = 0.0035988539457321167
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.003510345472022891
Validation loss = 0.003551655216142535
Validation loss = 0.003788974601775408
Validation loss = 0.003631094004958868
Validation loss = 0.003673034720122814
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 32       |
| MaximumReturn | 199      |
| MinimumReturn | 124      |
| TotalSamples  | 113322   |
----------------------------
itr #33 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.003752582473680377
Validation loss = 0.0035481315571814775
Validation loss = 0.003606813494116068
Validation loss = 0.0036258138716220856
Validation loss = 0.00368920317851007
Validation loss = 0.003572429995983839
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.00353785022161901
Validation loss = 0.003710668534040451
Validation loss = 0.0037329636979848146
Validation loss = 0.00358704780228436
Validation loss = 0.003596595022827387
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0036761166993528605
Validation loss = 0.0035606021992862225
Validation loss = 0.0036532136145979166
Validation loss = 0.0036950423382222652
Validation loss = 0.003808183129876852
Validation loss = 0.003607373684644699
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0036653648130595684
Validation loss = 0.0036694600712507963
Validation loss = 0.003560935612767935
Validation loss = 0.0036795842461287975
Validation loss = 0.003629400860518217
Validation loss = 0.003573050955310464
Validation loss = 0.0035551153123378754
Validation loss = 0.0035647547338157892
Validation loss = 0.0036178193986415863
Validation loss = 0.0036461572162806988
Validation loss = 0.0035541073884814978
Validation loss = 0.003634764114394784
Validation loss = 0.003573363646864891
Validation loss = 0.0035727901849895716
Validation loss = 0.0035588128957897425
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.003787904977798462
Validation loss = 0.00359305739402771
Validation loss = 0.003531843889504671
Validation loss = 0.0035718027502298355
Validation loss = 0.0035881411749869585
Validation loss = 0.003603570628911257
Validation loss = 0.003766942536458373
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 33       |
| MaximumReturn | 199      |
| MinimumReturn | 124      |
| TotalSamples  | 116655   |
----------------------------
itr #34 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.003971438389271498
Validation loss = 0.0038832679856568575
Validation loss = 0.003938907291740179
Validation loss = 0.003904476761817932
Validation loss = 0.004074803553521633
Validation loss = 0.00397602329030633
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.004034799057990313
Validation loss = 0.00396158080548048
Validation loss = 0.004080366808921099
Validation loss = 0.0038490118458867073
Validation loss = 0.003971326630562544
Validation loss = 0.00388053129427135
Validation loss = 0.003836131887510419
Validation loss = 0.003983331378549337
Validation loss = 0.004007738083600998
Validation loss = 0.0038730278611183167
Validation loss = 0.003904088633134961
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0038908172864466906
Validation loss = 0.00386082474142313
Validation loss = 0.003910825587809086
Validation loss = 0.0040087043307721615
Validation loss = 0.003867862746119499
Validation loss = 0.0038825704250484705
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0039884177967906
Validation loss = 0.003925490193068981
Validation loss = 0.003954289946705103
Validation loss = 0.003920448012650013
Validation loss = 0.003940411377698183
Validation loss = 0.003884795820340514
Validation loss = 0.003874642075970769
Validation loss = 0.0039017749950289726
Validation loss = 0.003806067630648613
Validation loss = 0.0038586044684052467
Validation loss = 0.003949353471398354
Validation loss = 0.00388107867911458
Validation loss = 0.0038859446067363024
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0038458157796412706
Validation loss = 0.003924317192286253
Validation loss = 0.003937411587685347
Validation loss = 0.003963487222790718
Validation loss = 0.003858300158753991
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 34       |
| MaximumReturn | 199      |
| MinimumReturn | 117      |
| TotalSamples  | 119988   |
----------------------------
itr #35 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00411230931058526
Validation loss = 0.004060181323438883
Validation loss = 0.004131615627557039
Validation loss = 0.004085796419531107
Validation loss = 0.004098826088011265
Validation loss = 0.00410477677360177
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.004148201085627079
Validation loss = 0.004153454210609198
Validation loss = 0.004108130000531673
Validation loss = 0.004107861313968897
Validation loss = 0.0041529214940965176
Validation loss = 0.00415163254365325
Validation loss = 0.004098181147128344
Validation loss = 0.00402586255222559
Validation loss = 0.00420775730162859
Validation loss = 0.004132857546210289
Validation loss = 0.004023547284305096
Validation loss = 0.004158076364547014
Validation loss = 0.004094670061022043
Validation loss = 0.004102486651390791
Validation loss = 0.004076899960637093
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.004210682585835457
Validation loss = 0.004068868700414896
Validation loss = 0.004191450774669647
Validation loss = 0.004050423391163349
Validation loss = 0.004226141609251499
Validation loss = 0.004065387882292271
Validation loss = 0.0042114416137337685
Validation loss = 0.0041450816206634045
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.004178015515208244
Validation loss = 0.004122091457247734
Validation loss = 0.004108617082238197
Validation loss = 0.004136876203119755
Validation loss = 0.0041459910571575165
Validation loss = 0.0040865191258490086
Validation loss = 0.004061050247400999
Validation loss = 0.004149549640715122
Validation loss = 0.0041101244278252125
Validation loss = 0.004088584799319506
Validation loss = 0.004121645353734493
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.004089732654392719
Validation loss = 0.004070580471307039
Validation loss = 0.004278070293366909
Validation loss = 0.004125200677663088
Validation loss = 0.004297216422855854
Validation loss = 0.0040947445668280125
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 35       |
| MaximumReturn | 199      |
| MinimumReturn | 117      |
| TotalSamples  | 123321   |
----------------------------
itr #36 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.004595327656716108
Validation loss = 0.004539286717772484
Validation loss = 0.004509877413511276
Validation loss = 0.004545192234218121
Validation loss = 0.004519336856901646
Validation loss = 0.004521411843597889
Validation loss = 0.004480218514800072
Validation loss = 0.0045425803400576115
Validation loss = 0.004547552205622196
Validation loss = 0.0044995928183197975
Validation loss = 0.004498820286244154
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.004522022791206837
Validation loss = 0.004467142280191183
Validation loss = 0.004494203720241785
Validation loss = 0.004570165183395147
Validation loss = 0.0046291532926261425
Validation loss = 0.004496284760534763
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00449941772967577
Validation loss = 0.0044733029790222645
Validation loss = 0.004471671301871538
Validation loss = 0.004514956381171942
Validation loss = 0.004515192471444607
Validation loss = 0.004473064094781876
Validation loss = 0.0045196181163191795
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.004595544654875994
Validation loss = 0.004526051227003336
Validation loss = 0.0045282575301826
Validation loss = 0.004479246214032173
Validation loss = 0.004512188956141472
Validation loss = 0.004482703283429146
Validation loss = 0.004598922561854124
Validation loss = 0.004545506089925766
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.004550334066152573
Validation loss = 0.0044546653516590595
Validation loss = 0.004611158277839422
Validation loss = 0.004513684660196304
Validation loss = 0.004483736120164394
Validation loss = 0.004585138522088528
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 36       |
| MaximumReturn | 199      |
| MinimumReturn | 117      |
| TotalSamples  | 126654   |
----------------------------
itr #37 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0049078152514994144
Validation loss = 0.004943962674587965
Validation loss = 0.0050202361308038235
Validation loss = 0.004925963934510946
Validation loss = 0.005111409816890955
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.005133804865181446
Validation loss = 0.005104275420308113
Validation loss = 0.004902595188468695
Validation loss = 0.004863589536398649
Validation loss = 0.0049442085437476635
Validation loss = 0.005009328946471214
Validation loss = 0.004966768901795149
Validation loss = 0.0049673281610012054
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.005190487485378981
Validation loss = 0.00488842511549592
Validation loss = 0.00499258516356349
Validation loss = 0.004952846560627222
Validation loss = 0.004967629909515381
Validation loss = 0.004853407386690378
Validation loss = 0.00493819871917367
Validation loss = 0.00486507685855031
Validation loss = 0.0048444620333611965
Validation loss = 0.005159503780305386
Validation loss = 0.005035583395510912
Validation loss = 0.004922609310597181
Validation loss = 0.004865806549787521
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0049313330091536045
Validation loss = 0.004874830599874258
Validation loss = 0.004871218465268612
Validation loss = 0.004953081253916025
Validation loss = 0.0049324436113238335
Validation loss = 0.004973924718797207
Validation loss = 0.004869604483246803
Validation loss = 0.004918374121189117
Validation loss = 0.005041070282459259
Validation loss = 0.004888816270977259
Validation loss = 0.004933232441544533
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.004951277747750282
Validation loss = 0.004958438687026501
Validation loss = 0.004834059625864029
Validation loss = 0.0049116769805550575
Validation loss = 0.004925701301544905
Validation loss = 0.004926626104861498
Validation loss = 0.004907533992081881
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 177      |
| Iteration     | 37       |
| MaximumReturn | 199      |
| MinimumReturn | 147      |
| TotalSamples  | 129987   |
----------------------------
itr #38 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.005136369727551937
Validation loss = 0.005215038545429707
Validation loss = 0.005185713991522789
Validation loss = 0.0052462066523730755
Validation loss = 0.0051455385982990265
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.005178222432732582
Validation loss = 0.005116855259984732
Validation loss = 0.00512128509581089
Validation loss = 0.005239732097834349
Validation loss = 0.005074166227132082
Validation loss = 0.005103358533233404
Validation loss = 0.005083655472844839
Validation loss = 0.0051451330073177814
Validation loss = 0.005112358368933201
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.005333780776709318
Validation loss = 0.005164435598999262
Validation loss = 0.005075372289866209
Validation loss = 0.0051674610003829
Validation loss = 0.005057172849774361
Validation loss = 0.005178512539714575
Validation loss = 0.005075447261333466
Validation loss = 0.00515395263209939
Validation loss = 0.005218275357037783
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00519762234762311
Validation loss = 0.005220000632107258
Validation loss = 0.005108145531266928
Validation loss = 0.005155190825462341
Validation loss = 0.0050616757944226265
Validation loss = 0.005104932468384504
Validation loss = 0.005184672772884369
Validation loss = 0.005062233190983534
Validation loss = 0.005156534258276224
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.005054394248872995
Validation loss = 0.005097678862512112
Validation loss = 0.0051699490286409855
Validation loss = 0.005183047149330378
Validation loss = 0.005085535813122988
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 38       |
| MaximumReturn | 198      |
| MinimumReturn | 134      |
| TotalSamples  | 133320   |
----------------------------
itr #39 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.005396428983658552
Validation loss = 0.005379769019782543
Validation loss = 0.0052762930281460285
Validation loss = 0.0053228409960865974
Validation loss = 0.005336947739124298
Validation loss = 0.00531837297603488
Validation loss = 0.005364670883864164
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.005264252424240112
Validation loss = 0.005198337137699127
Validation loss = 0.005321652628481388
Validation loss = 0.005365282297134399
Validation loss = 0.005174735095351934
Validation loss = 0.00524174002930522
Validation loss = 0.005327196791768074
Validation loss = 0.005162721965461969
Validation loss = 0.005266754887998104
Validation loss = 0.00528659438714385
Validation loss = 0.00527953403070569
Validation loss = 0.0055074384436011314
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0052723814733326435
Validation loss = 0.0052759344689548016
Validation loss = 0.005234572570770979
Validation loss = 0.005300556775182486
Validation loss = 0.005350547842681408
Validation loss = 0.00543174659833312
Validation loss = 0.005256285425275564
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.005207161419093609
Validation loss = 0.005379359237849712
Validation loss = 0.005198043771088123
Validation loss = 0.005272203125059605
Validation loss = 0.005347363650798798
Validation loss = 0.005316901486366987
Validation loss = 0.005356380715966225
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0052375090308487415
Validation loss = 0.005315080750733614
Validation loss = 0.005288844928145409
Validation loss = 0.005423300433903933
Validation loss = 0.005248981062322855
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 39       |
| MaximumReturn | 199      |
| MinimumReturn | 121      |
| TotalSamples  | 136653   |
----------------------------
