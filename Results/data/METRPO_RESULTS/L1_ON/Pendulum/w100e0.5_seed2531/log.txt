Logging to experiments/pendulum/Pendulum/w100e0.5_seed2531
Print configuration .....
{'env_name': 'pendulum', 'random_seeds': [3214, 2431, 2531, 2231], 'save_variables': False, 'model_save_dir': '/tmp/pendulum_models/', 'restore_variables': False, 'start_onpol_iter': 0, 'onpol_iters': 40, 'num_path_random': 25, 'num_path_onpol': 25, 'env_horizon': 200, 'max_train_data': 200000, 'max_val_data': 100000, 'discard_ratio': 0.0, 'dynamics': {'pre_training': {'mode': 'intrinsic_reward', 'itr': 0, 'policy_itr': 20}, 'model': 'nn', 'ensemble': True, 'ensemble_model_count': 5, 'enable_particle_ensemble': True, 'particles': 5, 'obs_var': 1.0, 'intrinsic_reward_coeff': 1.0, 'ita': 1.0, 'mode': 'random', 'val': True, 'n_layers': 4, 'hidden_size': 1000, 'activation': 'relu', 'batch_size': 1000, 'learning_rate': 0.001, 'reg_coeff': 0.0, 'epochs': 200, 'kfac_params': {'learning_rate': 0.1, 'damping': 0.001, 'momentum': 0.9, 'kl_clip': 0.0001, 'cov_ema_decay': 0.99}}, 'policy': {'network_shape': [64, 64], 'init_logstd': 0.0, 'activation': 'tanh', 'reinitialize_every_itr': False}, 'trpo': {'horizon': 200, 'gamma': 0.99, 'step_size': 0.01, 'iterations': 20, 'batch_size': 50000, 'gae': 0.95, 'visualization': False, 'visualize_iterations': [0]}, 'algo': 'trpo'}
Generating random rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating random rollouts.
Creating normalization for training data.
Done creating normalization for training data.
Particle ensemble enabled? True
An ensemble of 5 dynamics model <class 'model.dynamics.NNDynamicsModel'> initialized
Train dynamics model with intrinsic reward only? False
Pre-training enabled. Using only intrinsic reward.
Pre-training dynamics model for 0 iterations...
Done pre-training dynamics model.
Using external reward only.
itr #0 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.27411285042762756
Validation loss = 0.02363481931388378
Validation loss = 0.008760250173509121
Validation loss = 0.0032446887344121933
Validation loss = 0.0015179362380877137
Validation loss = 0.0009325392311438918
Validation loss = 0.0007737255655229092
Validation loss = 0.0007250285125337541
Validation loss = 0.0006883838796056807
Validation loss = 0.0006603256915695965
Validation loss = 0.0008779796771705151
Validation loss = 0.0009568381356075406
Validation loss = 0.0014162337174639106
Validation loss = 0.0008438449003733695
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.2234400361776352
Validation loss = 0.0305407103151083
Validation loss = 0.011497347615659237
Validation loss = 0.0035552941262722015
Validation loss = 0.001569557236507535
Validation loss = 0.0009885699255391955
Validation loss = 0.0007906573009677231
Validation loss = 0.0007297865813598037
Validation loss = 0.0006942424806766212
Validation loss = 0.0006411627982743084
Validation loss = 0.0006609488045796752
Validation loss = 0.0006415401003323495
Validation loss = 0.0007492285803891718
Validation loss = 0.0011764791561290622
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.2642552852630615
Validation loss = 0.0321519635617733
Validation loss = 0.008295935578644276
Validation loss = 0.0033125453628599644
Validation loss = 0.001706902519799769
Validation loss = 0.0010121376253664494
Validation loss = 0.0008108010515570641
Validation loss = 0.0007551888120360672
Validation loss = 0.0006878342828713357
Validation loss = 0.0006778376991860569
Validation loss = 0.0006458722637034953
Validation loss = 0.0007037609466351569
Validation loss = 0.0010724276071414351
Validation loss = 0.0006963813211768866
Validation loss = 0.0008637800347059965
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.2599864900112152
Validation loss = 0.0296500064432621
Validation loss = 0.009109488688409328
Validation loss = 0.0027252603322267532
Validation loss = 0.0012687441194429994
Validation loss = 0.0009192080469802022
Validation loss = 0.0007693780353292823
Validation loss = 0.0006977316807024181
Validation loss = 0.0008845574338920414
Validation loss = 0.0007348974468186498
Validation loss = 0.0007994762854650617
Validation loss = 0.0008539584232494235
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.2588752210140228
Validation loss = 0.03081252984702587
Validation loss = 0.009184672497212887
Validation loss = 0.0032342420890927315
Validation loss = 0.001377312932163477
Validation loss = 0.0008781040669418871
Validation loss = 0.0007273724186234176
Validation loss = 0.0007016041199676692
Validation loss = 0.0006545964861288667
Validation loss = 0.0006238377536647022
Validation loss = 0.0006029465002939105
Validation loss = 0.0006566491210833192
Validation loss = 0.0016931543359532952
Validation loss = 0.000677544972859323
Validation loss = 0.0007920894422568381
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 0        |
| MaximumReturn | 198      |
| MinimumReturn | 121      |
| TotalSamples  | 6666     |
----------------------------
itr #1 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.022386332973837852
Validation loss = 0.0017149584600701928
Validation loss = 0.0007461619097739458
Validation loss = 0.000710890453774482
Validation loss = 0.0015369878383353353
Validation loss = 0.0004900582716800272
Validation loss = 0.0005060775438323617
Validation loss = 0.00037520358455367386
Validation loss = 0.0022722925059497356
Validation loss = 0.0005634239059872925
Validation loss = 0.005281646270304918
Validation loss = 0.0009396731038577855
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.014473807998001575
Validation loss = 0.0020726558286696672
Validation loss = 0.0005923602730035782
Validation loss = 0.001121494802646339
Validation loss = 0.001779071637429297
Validation loss = 0.0006151258130557835
Validation loss = 0.0026205286849290133
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0178265031427145
Validation loss = 0.00207382976077497
Validation loss = 0.0007646782323718071
Validation loss = 0.0007120927330106497
Validation loss = 0.0006366684683598578
Validation loss = 0.0004788639780599624
Validation loss = 0.0006467003258876503
Validation loss = 0.0008644104818813503
Validation loss = 0.0004176965157967061
Validation loss = 0.0008105391170829535
Validation loss = 0.00043945666402578354
Validation loss = 0.0010691473726183176
Validation loss = 0.003448129864409566
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.01737079583108425
Validation loss = 0.0022417656145989895
Validation loss = 0.0008875083294697106
Validation loss = 0.0006107002845965326
Validation loss = 0.0009645858663134277
Validation loss = 0.00048576915287412703
Validation loss = 0.000493252940941602
Validation loss = 0.0006299444357864559
Validation loss = 0.001164404908195138
Validation loss = 0.0004467971157282591
Validation loss = 0.0003792011411860585
Validation loss = 0.0014493606286123395
Validation loss = 0.0005209669470787048
Validation loss = 0.0005064072320237756
Validation loss = 0.0009221992804668844
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.016278263181447983
Validation loss = 0.0028110267594456673
Validation loss = 0.0007867021486163139
Validation loss = 0.00047325249761343
Validation loss = 0.0010634878417477012
Validation loss = 0.001292707398533821
Validation loss = 0.0006946222856640816
Validation loss = 0.0012144552310928702
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 1        |
| MaximumReturn | 199      |
| MinimumReturn | 120      |
| TotalSamples  | 9999     |
----------------------------
itr #2 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0035054583568125963
Validation loss = 0.00040154444286599755
Validation loss = 0.0009648589184507728
Validation loss = 0.0005044008721597493
Validation loss = 0.0007733582169748843
Validation loss = 0.005880664102733135
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0030545892659574747
Validation loss = 0.000783841940574348
Validation loss = 0.0020338520407676697
Validation loss = 0.000712149019818753
Validation loss = 0.0005890743341296911
Validation loss = 0.0005537801189348102
Validation loss = 0.0012852080399170518
Validation loss = 0.0004886565147899091
Validation loss = 0.0008776029571890831
Validation loss = 0.0009203789522871375
Validation loss = 0.00042346277041360736
Validation loss = 0.0008995079551823437
Validation loss = 0.0006521257455460727
Validation loss = 0.000868957198690623
Validation loss = 0.0014750654809176922
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.006209149956703186
Validation loss = 0.0010899566113948822
Validation loss = 0.00040066661313176155
Validation loss = 0.00041434323065914214
Validation loss = 0.00042423204286023974
Validation loss = 0.0007085270481184125
Validation loss = 0.00040581487701274455
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0024973598774522543
Validation loss = 0.0005361591465771198
Validation loss = 0.0008933719363994896
Validation loss = 0.0011506422888487577
Validation loss = 0.001078435918316245
Validation loss = 0.0018763428088277578
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00447920523583889
Validation loss = 0.0007146380376070738
Validation loss = 0.0008362079970538616
Validation loss = 0.0005646586650982499
Validation loss = 0.0008059822721406817
Validation loss = 0.0009988776873797178
Validation loss = 0.00044005303061567247
Validation loss = 0.0030555143021047115
Validation loss = 0.00037578685441985726
Validation loss = 0.0005800207727588713
Validation loss = 0.0005956317763775587
Validation loss = 0.0016813648398965597
Validation loss = 0.000403182755690068
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 2        |
| MaximumReturn | 199      |
| MinimumReturn | 107      |
| TotalSamples  | 13332    |
----------------------------
itr #3 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00276424759067595
Validation loss = 0.0006499748560599983
Validation loss = 0.0015194850275292993
Validation loss = 0.0009027125779539347
Validation loss = 0.0005914122448302805
Validation loss = 0.004115855786949396
Validation loss = 0.0010334268445149064
Validation loss = 0.002341405488550663
Validation loss = 0.0008927413146011531
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0015004632296040654
Validation loss = 0.002091428264975548
Validation loss = 0.0007532428135164082
Validation loss = 0.0011378050548955798
Validation loss = 0.0011400793446227908
Validation loss = 0.0038713181857019663
Validation loss = 0.0014580601127818227
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0016691923374310136
Validation loss = 0.0005993981030769646
Validation loss = 0.001541169942356646
Validation loss = 0.0010294938692823052
Validation loss = 0.00044842215720564127
Validation loss = 0.0011765867238864303
Validation loss = 0.004626312758773565
Validation loss = 0.0004918328486382961
Validation loss = 0.000959483499173075
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0020280806347727776
Validation loss = 0.001453609555028379
Validation loss = 0.0019400367746129632
Validation loss = 0.0009800399420782924
Validation loss = 0.0010768742067739367
Validation loss = 0.001304065459407866
Validation loss = 0.0004632111231330782
Validation loss = 0.0004466751415748149
Validation loss = 0.0006168652907945216
Validation loss = 0.0010101421503350139
Validation loss = 0.0007408246747218072
Validation loss = 0.0012458933051675558
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0013745295582339168
Validation loss = 0.0004207927267998457
Validation loss = 0.001292139757424593
Validation loss = 0.0008779875352047384
Validation loss = 0.001349390484392643
Validation loss = 0.0006977925077080727
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 3        |
| MaximumReturn | 199      |
| MinimumReturn | 122      |
| TotalSamples  | 16665    |
----------------------------
itr #4 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0035435608588159084
Validation loss = 0.0007421824848279357
Validation loss = 0.000905301480088383
Validation loss = 0.0005643293261528015
Validation loss = 0.00111562036909163
Validation loss = 0.0024533956311643124
Validation loss = 0.000935795484110713
Validation loss = 0.0005889738677069545
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0026259138248860836
Validation loss = 0.00095586315728724
Validation loss = 0.0007223438005894423
Validation loss = 0.0008183429017663002
Validation loss = 0.0013270532945170999
Validation loss = 0.0010571733582764864
Validation loss = 0.0010506372200325131
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0020569218322634697
Validation loss = 0.0004267592739779502
Validation loss = 0.0007761012529954314
Validation loss = 0.000905363354831934
Validation loss = 0.0007362981559708714
Validation loss = 0.000800806621555239
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0043621668592095375
Validation loss = 0.000617496611084789
Validation loss = 0.00045173068065196276
Validation loss = 0.0008165292092598975
Validation loss = 0.001443891553208232
Validation loss = 0.0008828003192320466
Validation loss = 0.0011402004165574908
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0013420470058918
Validation loss = 0.001201162813231349
Validation loss = 0.0004874324076808989
Validation loss = 0.0011209737276658416
Validation loss = 0.00044748984510079026
Validation loss = 0.0005751888384111226
Validation loss = 0.0004635233199223876
Validation loss = 0.0013503958471119404
Validation loss = 0.0018544604536145926
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 4        |
| MaximumReturn | 199      |
| MinimumReturn | 120      |
| TotalSamples  | 19998    |
----------------------------
itr #5 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0015509820077568293
Validation loss = 0.0006668392452411354
Validation loss = 0.0017439803341403604
Validation loss = 0.000779760186560452
Validation loss = 0.0005449978052638471
Validation loss = 0.0005110804922878742
Validation loss = 0.0005338319460861385
Validation loss = 0.0014102518325671554
Validation loss = 0.0005618781433440745
Validation loss = 0.000410982349421829
Validation loss = 0.0007526801200583577
Validation loss = 0.0003121763584204018
Validation loss = 0.002079188823699951
Validation loss = 0.0005741155473515391
Validation loss = 0.0004276650142855942
Validation loss = 0.0004055711906403303
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.002850118326023221
Validation loss = 0.0014172804076224566
Validation loss = 0.0010715004755184054
Validation loss = 0.0005470978212542832
Validation loss = 0.0009322349214926362
Validation loss = 0.0006926164496690035
Validation loss = 0.0020230920054018497
Validation loss = 0.00040495331631973386
Validation loss = 0.0003104315255768597
Validation loss = 0.001013902248814702
Validation loss = 0.0025606106501072645
Validation loss = 0.00032468754216097295
Validation loss = 0.0014631400117650628
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0010356891434639692
Validation loss = 0.0005027211736887693
Validation loss = 0.0005833235336467624
Validation loss = 0.0006123996572569013
Validation loss = 0.0010338148567825556
Validation loss = 0.0014574589440599084
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0015157891903072596
Validation loss = 0.0005948169273324311
Validation loss = 0.00048391884774900973
Validation loss = 0.0008755213348194957
Validation loss = 0.00046174274757504463
Validation loss = 0.0014365933602675796
Validation loss = 0.0008625659975223243
Validation loss = 0.000473694788524881
Validation loss = 0.0005565530736930668
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0009791194461286068
Validation loss = 0.0009046653285622597
Validation loss = 0.001994494115933776
Validation loss = 0.000513814389705658
Validation loss = 0.0008686506189405918
Validation loss = 0.0005765010719187558
Validation loss = 0.0026290188543498516
Validation loss = 0.0004511214210651815
Validation loss = 0.000356438773451373
Validation loss = 0.0008126226020976901
Validation loss = 0.0005601005395874381
Validation loss = 0.0010393348056823015
Validation loss = 0.0019296087557449937
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 5        |
| MaximumReturn | 198      |
| MinimumReturn | 129      |
| TotalSamples  | 23331    |
----------------------------
itr #6 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00117321009747684
Validation loss = 0.0034136264584958553
Validation loss = 0.0006371755735017359
Validation loss = 0.0006517923902720213
Validation loss = 0.0004976787022314966
Validation loss = 0.0009462872403673828
Validation loss = 0.0006240319344215095
Validation loss = 0.0003326056757941842
Validation loss = 0.0004553517501335591
Validation loss = 0.0011334026930853724
Validation loss = 0.0004410519322846085
Validation loss = 0.0008253366104327142
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0009182654321193695
Validation loss = 0.0005932627827860415
Validation loss = 0.0009773331694304943
Validation loss = 0.0012145510409027338
Validation loss = 0.0008160186698660254
Validation loss = 0.001794870593585074
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0009566210210323334
Validation loss = 0.0005147093324922025
Validation loss = 0.0011565370950847864
Validation loss = 0.001802153536118567
Validation loss = 0.0010052983416244388
Validation loss = 0.0004884822992607951
Validation loss = 0.0003257320786360651
Validation loss = 0.004864383023232222
Validation loss = 0.0006214742315933108
Validation loss = 0.0007057311595417559
Validation loss = 0.0010905205272138119
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0017153255175799131
Validation loss = 0.0003808099718298763
Validation loss = 0.0012473230017349124
Validation loss = 0.0010553892934694886
Validation loss = 0.0007573676994070411
Validation loss = 0.0005252831033430994
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0010510311694815755
Validation loss = 0.0004032352881040424
Validation loss = 0.0002649388334248215
Validation loss = 0.0003920731833204627
Validation loss = 0.0011037614895030856
Validation loss = 0.00034883408807218075
Validation loss = 0.0019266256131231785
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 181      |
| Iteration     | 6        |
| MaximumReturn | 199      |
| MinimumReturn | 130      |
| TotalSamples  | 26664    |
----------------------------
itr #7 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0004917713813483715
Validation loss = 0.0006155997980386019
Validation loss = 0.0008273409912362695
Validation loss = 0.0002567537303548306
Validation loss = 0.0005498923128470778
Validation loss = 0.00030516652623191476
Validation loss = 0.0009111131075769663
Validation loss = 0.0003364875738043338
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0008265596115961671
Validation loss = 0.00034560917993076146
Validation loss = 0.0009335200884379447
Validation loss = 0.0004229815676808357
Validation loss = 0.00033557333517819643
Validation loss = 0.00040829565841704607
Validation loss = 0.0008352749282494187
Validation loss = 0.00029123626882210374
Validation loss = 0.0005585271865129471
Validation loss = 0.0005020796088501811
Validation loss = 0.0013815253041684628
Validation loss = 0.0002233758568763733
Validation loss = 0.0002641762257553637
Validation loss = 0.00036351248854771256
Validation loss = 0.00031541401403956115
Validation loss = 0.0004087910638190806
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0009924460900947452
Validation loss = 0.0012205775128677487
Validation loss = 0.0003332661872263998
Validation loss = 0.0003883026947733015
Validation loss = 0.0005433414480648935
Validation loss = 0.0008483680430799723
Validation loss = 0.0004157638468313962
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0029646754264831543
Validation loss = 0.0008791852160356939
Validation loss = 0.0005193522665649652
Validation loss = 0.0011112882057204843
Validation loss = 0.00038881454383954406
Validation loss = 0.0003254104231018573
Validation loss = 0.0009901102166622877
Validation loss = 0.0009470410295762122
Validation loss = 0.0003831178473774344
Validation loss = 0.00035647410550154746
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0012761281104758382
Validation loss = 0.0007437439053319395
Validation loss = 0.0006133096758276224
Validation loss = 0.00037104872171767056
Validation loss = 0.0010719317942857742
Validation loss = 0.0005820929654873908
Validation loss = 0.00033215852454304695
Validation loss = 0.0006986407097429037
Validation loss = 0.0007381967734545469
Validation loss = 0.00093524728436023
Validation loss = 0.0003848164342343807
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 7        |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 29997    |
----------------------------
itr #8 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00060260109603405
Validation loss = 0.0005648747901432216
Validation loss = 0.00039811336318962276
Validation loss = 0.0005811218288727105
Validation loss = 0.0003142065543215722
Validation loss = 0.0024084190372377634
Validation loss = 0.0002870616444852203
Validation loss = 0.00029935926431789994
Validation loss = 0.0005782583029940724
Validation loss = 0.00048557313857600093
Validation loss = 0.0004791301325894892
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006871374207548797
Validation loss = 0.00038214997039176524
Validation loss = 0.000443017459474504
Validation loss = 0.000533454236574471
Validation loss = 0.000987080973573029
Validation loss = 0.00033715699100866914
Validation loss = 0.0006154616712592542
Validation loss = 0.00038038374623283744
Validation loss = 0.00039863018901087344
Validation loss = 0.00031440527527593076
Validation loss = 0.0003085053467657417
Validation loss = 0.0004943046369589865
Validation loss = 0.00035924353869631886
Validation loss = 0.0004888578550890088
Validation loss = 0.00028262147679924965
Validation loss = 0.00036993197863921523
Validation loss = 0.0007554223411716521
Validation loss = 0.00035943364491686225
Validation loss = 0.0005964005249552429
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0005511168274097145
Validation loss = 0.00034096732269972563
Validation loss = 0.0006450471701100469
Validation loss = 0.0008517057867720723
Validation loss = 0.0012929781805723906
Validation loss = 0.0003399235720280558
Validation loss = 0.0016577321803197265
Validation loss = 0.0007401894545182586
Validation loss = 0.0005948942271061242
Validation loss = 0.00035097976797260344
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0013510043499991298
Validation loss = 0.00027857517125084996
Validation loss = 0.0007259857375174761
Validation loss = 0.00045031795161776245
Validation loss = 0.00031868330552242696
Validation loss = 0.0013529997086152434
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00042779810610227287
Validation loss = 0.0004962800885550678
Validation loss = 0.0011823864188045263
Validation loss = 0.0005520410486496985
Validation loss = 0.0004558475047815591
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 8        |
| MaximumReturn | 199      |
| MinimumReturn | 115      |
| TotalSamples  | 33330    |
----------------------------
itr #9 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.000583377608563751
Validation loss = 0.0004470812273211777
Validation loss = 0.000399767275666818
Validation loss = 0.00034343492006883025
Validation loss = 0.0004804875934496522
Validation loss = 0.00046259528608061373
Validation loss = 0.0010526576079428196
Validation loss = 0.000527774216607213
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.001275247079320252
Validation loss = 0.00029286782955750823
Validation loss = 0.00029510754393413663
Validation loss = 0.0003376615350134671
Validation loss = 0.0014285764191299677
Validation loss = 0.0003116309817414731
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00031878057052381337
Validation loss = 0.0005821454687975347
Validation loss = 0.0004812648403458297
Validation loss = 0.0004987884312868118
Validation loss = 0.001244796672835946
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.001703906455077231
Validation loss = 0.0004030928248539567
Validation loss = 0.0006357382517307997
Validation loss = 0.0007599159143865108
Validation loss = 0.00037520111072808504
Validation loss = 0.0007947650738060474
Validation loss = 0.0004945544060319662
Validation loss = 0.0005851108580827713
Validation loss = 0.0005260189063847065
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0029622837901115417
Validation loss = 0.0005390841397456825
Validation loss = 0.00041354083805345
Validation loss = 0.0005100186099298298
Validation loss = 0.00035595812369138
Validation loss = 0.0005438273656181991
Validation loss = 0.0008564511081203818
Validation loss = 0.0003684297844301909
Validation loss = 0.0005502870189957321
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 9        |
| MaximumReturn | 199      |
| MinimumReturn | 130      |
| TotalSamples  | 36663    |
----------------------------
itr #10 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00046998090692795813
Validation loss = 0.0003638429334387183
Validation loss = 0.0006623765802942216
Validation loss = 0.0014206101186573505
Validation loss = 0.0006191307329572737
Validation loss = 0.0006124401115812361
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.00033762038219720125
Validation loss = 0.000543474976439029
Validation loss = 0.0006240892107598484
Validation loss = 0.0005798054044134915
Validation loss = 0.00042328497511334717
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0008546182070858777
Validation loss = 0.00034293916542083025
Validation loss = 0.000360342935891822
Validation loss = 0.0004348515358287841
Validation loss = 0.000493793108034879
Validation loss = 0.0005029817693866789
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0003591557906474918
Validation loss = 0.0007322769961319864
Validation loss = 0.0002843543188646436
Validation loss = 0.0009849222842603922
Validation loss = 0.0006036257254891098
Validation loss = 0.00065184774575755
Validation loss = 0.0003492938703857362
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005381418741308153
Validation loss = 0.00035825479426421225
Validation loss = 0.0004957849159836769
Validation loss = 0.00036625497159548104
Validation loss = 0.0003460000443737954
Validation loss = 0.00041624647565186024
Validation loss = 0.0006185073871165514
Validation loss = 0.0005125204916112125
Validation loss = 0.0006310391472652555
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 10       |
| MaximumReturn | 199      |
| MinimumReturn | 129      |
| TotalSamples  | 39996    |
----------------------------
itr #11 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0008691214025020599
Validation loss = 0.000779772992245853
Validation loss = 0.0005002993275411427
Validation loss = 0.0005042165867052972
Validation loss = 0.0005379604990594089
Validation loss = 0.0004945816472172737
Validation loss = 0.0006346510490402579
Validation loss = 0.0007171564502641559
Validation loss = 0.0004239384434185922
Validation loss = 0.000573835801333189
Validation loss = 0.0004945094115100801
Validation loss = 0.00039978729910217226
Validation loss = 0.0011950017651543021
Validation loss = 0.0006250079604797065
Validation loss = 0.0005515137454494834
Validation loss = 0.0005909815081395209
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.000353159848600626
Validation loss = 0.0006302563706412911
Validation loss = 0.0007367492071352899
Validation loss = 0.0004309341893531382
Validation loss = 0.00035950762685388327
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0004201114643365145
Validation loss = 0.000453022716101259
Validation loss = 0.000366353866411373
Validation loss = 0.0004715455579571426
Validation loss = 0.0009561149636283517
Validation loss = 0.0008807696285657585
Validation loss = 0.0004870678822044283
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0018898319685831666
Validation loss = 0.00038023266824893653
Validation loss = 0.0003782083513215184
Validation loss = 0.0006993329734541476
Validation loss = 0.00035155171644873917
Validation loss = 0.00043741436093114316
Validation loss = 0.0006416476098820567
Validation loss = 0.0006535968859679997
Validation loss = 0.0008560877176932991
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005595175316557288
Validation loss = 0.0008396001649089158
Validation loss = 0.0003847674815915525
Validation loss = 0.0004276417603250593
Validation loss = 0.0005214806878939271
Validation loss = 0.0005283498903736472
Validation loss = 0.00037814374081790447
Validation loss = 0.0003669586149044335
Validation loss = 0.000827539712190628
Validation loss = 0.0003467911446932703
Validation loss = 0.00040707914740778506
Validation loss = 0.0004003798239864409
Validation loss = 0.0005410356679931283
Validation loss = 0.0003570853150449693
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 177      |
| Iteration     | 11       |
| MaximumReturn | 199      |
| MinimumReturn | 134      |
| TotalSamples  | 43329    |
----------------------------
itr #12 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0004192782216705382
Validation loss = 0.0003931801184080541
Validation loss = 0.0005960240960121155
Validation loss = 0.00044003644143231213
Validation loss = 0.0006662197993136942
Validation loss = 0.0006854250677861273
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006087663350626826
Validation loss = 0.0005619100411422551
Validation loss = 0.00037280545802786946
Validation loss = 0.00044396554585546255
Validation loss = 0.00042021102854050696
Validation loss = 0.00037761483690701425
Validation loss = 0.0005196043057367206
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00039708224358037114
Validation loss = 0.0005134814418852329
Validation loss = 0.00041144719580188394
Validation loss = 0.0004052408039569855
Validation loss = 0.0006225341348908842
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.00062116130720824
Validation loss = 0.0005752050201408565
Validation loss = 0.0006261856760829687
Validation loss = 0.00042673718417063355
Validation loss = 0.0005019370000809431
Validation loss = 0.001941430731676519
Validation loss = 0.00039907271275296807
Validation loss = 0.00033818199881352484
Validation loss = 0.00047849403927102685
Validation loss = 0.00041653242078609765
Validation loss = 0.0006293200422078371
Validation loss = 0.00040940442704595625
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00040136007010005414
Validation loss = 0.0011354387970641255
Validation loss = 0.00033212112612091005
Validation loss = 0.0006325574358925223
Validation loss = 0.0004606583679560572
Validation loss = 0.0004613808705471456
Validation loss = 0.0005159528227522969
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 12       |
| MaximumReturn | 199      |
| MinimumReturn | 123      |
| TotalSamples  | 46662    |
----------------------------
itr #13 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0013503845548257232
Validation loss = 0.0004862169153057039
Validation loss = 0.0005263404455035925
Validation loss = 0.0006728555308654904
Validation loss = 0.0006470474763773382
Validation loss = 0.0006206049583852291
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0007279869168996811
Validation loss = 0.001004837453365326
Validation loss = 0.00046431730152107775
Validation loss = 0.0007117971545085311
Validation loss = 0.0006760929245501757
Validation loss = 0.0005715388688258827
Validation loss = 0.00043342020944692194
Validation loss = 0.0007041904027573764
Validation loss = 0.0005118503468111157
Validation loss = 0.00040553187136538327
Validation loss = 0.0008700932958163321
Validation loss = 0.0005733667640015483
Validation loss = 0.0008156869444064796
Validation loss = 0.0005512120551429689
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0005112573271617293
Validation loss = 0.0009440711583010852
Validation loss = 0.0006288143340498209
Validation loss = 0.0007048153784126043
Validation loss = 0.0004921024083159864
Validation loss = 0.0005272855050861835
Validation loss = 0.000387027976103127
Validation loss = 0.0007030023843981326
Validation loss = 0.0007671022904105484
Validation loss = 0.0004662245628423989
Validation loss = 0.0007661230629310012
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0004978147917427123
Validation loss = 0.0005778525955975056
Validation loss = 0.0005701316404156387
Validation loss = 0.00044487419654615223
Validation loss = 0.00043375903624109924
Validation loss = 0.0007599347736686468
Validation loss = 0.0006886376650072634
Validation loss = 0.0007960554212331772
Validation loss = 0.0006857428816147149
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005473580094985664
Validation loss = 0.0011180337751284242
Validation loss = 0.0005455126520246267
Validation loss = 0.0005778084159828722
Validation loss = 0.00044698009151034057
Validation loss = 0.0006565215298905969
Validation loss = 0.0006027001654729247
Validation loss = 0.0005337430629879236
Validation loss = 0.00044460862409323454
Validation loss = 0.0004684208834078163
Validation loss = 0.0005375471082516015
Validation loss = 0.000450973428087309
Validation loss = 0.00043623492820188403
Validation loss = 0.0004685028688982129
Validation loss = 0.0007474195444956422
Validation loss = 0.0005609330837614834
Validation loss = 0.0006194607121869922
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 13       |
| MaximumReturn | 199      |
| MinimumReturn | 120      |
| TotalSamples  | 49995    |
----------------------------
itr #14 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0012903637252748013
Validation loss = 0.00047458946937695146
Validation loss = 0.0005776285543106496
Validation loss = 0.0007548244320787489
Validation loss = 0.0005346714751794934
Validation loss = 0.0006366870366036892
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0005575531395152211
Validation loss = 0.000547533156350255
Validation loss = 0.0005299780750647187
Validation loss = 0.0005857069627381861
Validation loss = 0.0005588719504885375
Validation loss = 0.0005069211474619806
Validation loss = 0.0008476727525703609
Validation loss = 0.0009938149014487863
Validation loss = 0.0005403811228461564
Validation loss = 0.000551777717191726
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0005698547465726733
Validation loss = 0.0005591379012912512
Validation loss = 0.0006231965380720794
Validation loss = 0.0006592498975805938
Validation loss = 0.0006280291127040982
Validation loss = 0.0006179609335958958
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005540213896892965
Validation loss = 0.000492274179123342
Validation loss = 0.0008321729837916791
Validation loss = 0.0010566433193162084
Validation loss = 0.0007040811469778419
Validation loss = 0.000484178657643497
Validation loss = 0.0006235074251890182
Validation loss = 0.000520030502229929
Validation loss = 0.0008311194833368063
Validation loss = 0.000535033002961427
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0005698840832337737
Validation loss = 0.0005143770831637084
Validation loss = 0.000565925205592066
Validation loss = 0.0010890546254813671
Validation loss = 0.0004943441017530859
Validation loss = 0.0005464189453050494
Validation loss = 0.0007002782658673823
Validation loss = 0.0005554825183935463
Validation loss = 0.0005248139495961368
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 14       |
| MaximumReturn | 199      |
| MinimumReturn | 126      |
| TotalSamples  | 53328    |
----------------------------
itr #15 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0005109885823912919
Validation loss = 0.0005945405573584139
Validation loss = 0.0005263940547592938
Validation loss = 0.0005336446920409799
Validation loss = 0.0006532172556035221
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.000493051134981215
Validation loss = 0.0005094695370644331
Validation loss = 0.0006327741430141032
Validation loss = 0.000522350543178618
Validation loss = 0.0007154509075917304
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0007361184107139707
Validation loss = 0.0007828945526853204
Validation loss = 0.0007180563407018781
Validation loss = 0.0005421662935987115
Validation loss = 0.0008506355807185173
Validation loss = 0.0006022824672982097
Validation loss = 0.0004988620057702065
Validation loss = 0.0005982673028483987
Validation loss = 0.0007760401349514723
Validation loss = 0.0006180070340633392
Validation loss = 0.0005550447385758162
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0005138574633747339
Validation loss = 0.0005112059880048037
Validation loss = 0.000468867045128718
Validation loss = 0.0006718554650433362
Validation loss = 0.0006139076431281865
Validation loss = 0.0005207965150475502
Validation loss = 0.0006308881565928459
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.000656197196803987
Validation loss = 0.0006564214127138257
Validation loss = 0.0006682938546873629
Validation loss = 0.0005199883598834276
Validation loss = 0.0006146309897303581
Validation loss = 0.0010654297657310963
Validation loss = 0.00047752258251421154
Validation loss = 0.0005231526447460055
Validation loss = 0.0005929082981310785
Validation loss = 0.0007691531791351736
Validation loss = 0.0005419261287897825
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 163      |
| Iteration     | 15       |
| MaximumReturn | 199      |
| MinimumReturn | 101      |
| TotalSamples  | 56661    |
----------------------------
itr #16 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0007902702200226486
Validation loss = 0.00064464082242921
Validation loss = 0.0006424019811674953
Validation loss = 0.0007507888949476182
Validation loss = 0.0006906182388775051
Validation loss = 0.0006247180281206965
Validation loss = 0.0007838833262212574
Validation loss = 0.0007034741574898362
Validation loss = 0.0007006750092841685
Validation loss = 0.0005569772911258042
Validation loss = 0.0007021334022283554
Validation loss = 0.0006384497391991317
Validation loss = 0.000581584288738668
Validation loss = 0.0008820694056339562
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0006613624864257872
Validation loss = 0.0007070950814522803
Validation loss = 0.0005840481608174741
Validation loss = 0.0006945605273358524
Validation loss = 0.0008609213982708752
Validation loss = 0.0008470857865177095
Validation loss = 0.000636839191429317
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00069832504959777
Validation loss = 0.0006850339123047888
Validation loss = 0.0006004354800097644
Validation loss = 0.000627651868853718
Validation loss = 0.0006385206943377852
Validation loss = 0.0009168678661808372
Validation loss = 0.0007647525635547936
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0006777649978175759
Validation loss = 0.0007120147347450256
Validation loss = 0.000650004658382386
Validation loss = 0.0007227518362924457
Validation loss = 0.0006234875763766468
Validation loss = 0.0006240126676857471
Validation loss = 0.0005414251936599612
Validation loss = 0.0008130864007398486
Validation loss = 0.0006713808397762477
Validation loss = 0.0005840991507284343
Validation loss = 0.0006096140132285655
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0006697266944684088
Validation loss = 0.0007463525398634374
Validation loss = 0.0006644235108979046
Validation loss = 0.0008668957161717117
Validation loss = 0.0006797093083150685
Validation loss = 0.0006661851075477898
Validation loss = 0.0006114191492088139
Validation loss = 0.0008200629963539541
Validation loss = 0.0006498511065728962
Validation loss = 0.000706321734469384
Validation loss = 0.0006040152511559427
Validation loss = 0.0007367297657765448
Validation loss = 0.0006872760131955147
Validation loss = 0.000830446369946003
Validation loss = 0.0005983380251564085
Validation loss = 0.000637792341876775
Validation loss = 0.0005977586843073368
Validation loss = 0.0006128819077275693
Validation loss = 0.0006804093718528748
Validation loss = 0.0005805743276141584
Validation loss = 0.0006617172039113939
Validation loss = 0.0008265213691629469
Validation loss = 0.000951309863012284
Validation loss = 0.0006005706964060664
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 16       |
| MaximumReturn | 199      |
| MinimumReturn | 115      |
| TotalSamples  | 59994    |
----------------------------
itr #17 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0006886981427669525
Validation loss = 0.0007202344131655991
Validation loss = 0.0007413460989482701
Validation loss = 0.0007069966522976756
Validation loss = 0.0007246746099554002
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0008605375187471509
Validation loss = 0.0007940519717521966
Validation loss = 0.0008332438301295042
Validation loss = 0.0009947216603904963
Validation loss = 0.0008610052173025906
Validation loss = 0.000711191794835031
Validation loss = 0.0007159076048992574
Validation loss = 0.0006984638166613877
Validation loss = 0.0007472755969502032
Validation loss = 0.0008424082188867033
Validation loss = 0.0018659651977941394
Validation loss = 0.0006930024828761816
Validation loss = 0.0006735079223290086
Validation loss = 0.0007509112474508584
Validation loss = 0.0007088329875841737
Validation loss = 0.0007694291998632252
Validation loss = 0.0007607311708852649
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0009379915427416563
Validation loss = 0.0006852233782410622
Validation loss = 0.000690287328325212
Validation loss = 0.0008155940449796617
Validation loss = 0.0007868279935792089
Validation loss = 0.000982140307314694
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.001050617080181837
Validation loss = 0.0006934310076758265
Validation loss = 0.0007456733728758991
Validation loss = 0.0007270464557223022
Validation loss = 0.0008682428160682321
Validation loss = 0.000730761734303087
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.00069978809915483
Validation loss = 0.000669199856929481
Validation loss = 0.0007224332657642663
Validation loss = 0.0007345582125708461
Validation loss = 0.0006984596257098019
Validation loss = 0.0008253481355495751
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 17       |
| MaximumReturn | 199      |
| MinimumReturn | 126      |
| TotalSamples  | 63327    |
----------------------------
itr #18 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0007272462826222181
Validation loss = 0.0007008796674199402
Validation loss = 0.0007375163258984685
Validation loss = 0.0007383194752037525
Validation loss = 0.0007398301386274397
Validation loss = 0.0007698316476307809
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0007196560618467629
Validation loss = 0.0007552785682491958
Validation loss = 0.0007605136488564312
Validation loss = 0.0009170927805826068
Validation loss = 0.0007227592868730426
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0007842638879083097
Validation loss = 0.000715318659786135
Validation loss = 0.0007441134075634181
Validation loss = 0.000754769251216203
Validation loss = 0.000782821502070874
Validation loss = 0.0007297013653442264
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0007089885766617954
Validation loss = 0.0007595376810058951
Validation loss = 0.0016655626241117716
Validation loss = 0.0008026731084100902
Validation loss = 0.0008751191198825836
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0008463725098408759
Validation loss = 0.0007298760465346277
Validation loss = 0.000982455676421523
Validation loss = 0.0006989621324464679
Validation loss = 0.0008043304551392794
Validation loss = 0.0007055813912302256
Validation loss = 0.0007030272972770035
Validation loss = 0.0007013698341324925
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 18       |
| MaximumReturn | 199      |
| MinimumReturn | 106      |
| TotalSamples  | 66660    |
----------------------------
itr #19 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0011370126157999039
Validation loss = 0.0011210619704797864
Validation loss = 0.001064576325006783
Validation loss = 0.0009619406191632152
Validation loss = 0.0009813806973397732
Validation loss = 0.0009715001797303557
Validation loss = 0.0009625146631151438
Validation loss = 0.0010056522442027926
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.001275300164707005
Validation loss = 0.0009346570004709065
Validation loss = 0.0009816928068175912
Validation loss = 0.0009552391711622477
Validation loss = 0.0010823042830452323
Validation loss = 0.0009631690918467939
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0009866859763860703
Validation loss = 0.0010456611635163426
Validation loss = 0.0011354534653946757
Validation loss = 0.0009767767041921616
Validation loss = 0.001007207203656435
Validation loss = 0.0012710479786619544
Validation loss = 0.0009631974971853197
Validation loss = 0.0011292387498542666
Validation loss = 0.0009283536346629262
Validation loss = 0.0011730339610949159
Validation loss = 0.0009919373551383615
Validation loss = 0.000985720893368125
Validation loss = 0.0010173816699534655
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0011104075238108635
Validation loss = 0.001086765667423606
Validation loss = 0.0010376224527135491
Validation loss = 0.0009981588227674365
Validation loss = 0.00093045923858881
Validation loss = 0.0009493613615632057
Validation loss = 0.001049006823450327
Validation loss = 0.0010527875274419785
Validation loss = 0.0010539881186559796
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0010186266154050827
Validation loss = 0.0010182112455368042
Validation loss = 0.0009044524049386382
Validation loss = 0.0009059985168278217
Validation loss = 0.0012869711499661207
Validation loss = 0.0010586853604763746
Validation loss = 0.0009997810702770948
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 19       |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 69993    |
----------------------------
itr #20 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.001175750163383782
Validation loss = 0.0011793740559369326
Validation loss = 0.0012724170228466392
Validation loss = 0.0010946632828563452
Validation loss = 0.0010637306841090322
Validation loss = 0.0010910416021943092
Validation loss = 0.0011528190225362778
Validation loss = 0.0011960753472521901
Validation loss = 0.0010786980856209993
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0011328118853271008
Validation loss = 0.0010662581771612167
Validation loss = 0.0012327727163210511
Validation loss = 0.0010995238553732634
Validation loss = 0.0011330057168379426
Validation loss = 0.001242728903889656
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0010818472364917397
Validation loss = 0.0010488709667697549
Validation loss = 0.001320319133810699
Validation loss = 0.0015345541760325432
Validation loss = 0.0010926302056759596
Validation loss = 0.0011546837631613016
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0011656638234853745
Validation loss = 0.0011781558860093355
Validation loss = 0.0011413998436182737
Validation loss = 0.0011654860572889447
Validation loss = 0.0019554062746465206
Validation loss = 0.001044019008986652
Validation loss = 0.0010836429428309202
Validation loss = 0.0011244871420785785
Validation loss = 0.001103027374483645
Validation loss = 0.0010649616597220302
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0010631992481648922
Validation loss = 0.001411034376360476
Validation loss = 0.0010605094721540809
Validation loss = 0.0011136376997455955
Validation loss = 0.001117053208872676
Validation loss = 0.0010624012211337686
Validation loss = 0.0014679459854960442
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 20       |
| MaximumReturn | 199      |
| MinimumReturn | 128      |
| TotalSamples  | 73326    |
----------------------------
itr #21 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0014456049539148808
Validation loss = 0.0012416881509125233
Validation loss = 0.0012794414069503546
Validation loss = 0.0012205145321786404
Validation loss = 0.001268114079721272
Validation loss = 0.0013428378151729703
Validation loss = 0.0011701880721375346
Validation loss = 0.0012180306948721409
Validation loss = 0.001167679438367486
Validation loss = 0.0012282204115763307
Validation loss = 0.0011812330922111869
Validation loss = 0.0012321834219619632
Validation loss = 0.0012259799987077713
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0013321125879883766
Validation loss = 0.0012435485841706395
Validation loss = 0.0012040402507409453
Validation loss = 0.0013982419623062015
Validation loss = 0.0013166354037821293
Validation loss = 0.0011726960074156523
Validation loss = 0.0012564091011881828
Validation loss = 0.0012302591931074858
Validation loss = 0.0011831863084807992
Validation loss = 0.0011840122751891613
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0012478348799049854
Validation loss = 0.0012127218069508672
Validation loss = 0.0013800737215206027
Validation loss = 0.0011908975429832935
Validation loss = 0.0012764380080625415
Validation loss = 0.00120387296192348
Validation loss = 0.001195227145217359
Validation loss = 0.0012010944774374366
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0012235152535140514
Validation loss = 0.0013506654649972916
Validation loss = 0.0012157335877418518
Validation loss = 0.0012598285684362054
Validation loss = 0.001230144640430808
Validation loss = 0.0014477560762315989
Validation loss = 0.0012552747502923012
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.001370670273900032
Validation loss = 0.0012143994681537151
Validation loss = 0.0012893930543214083
Validation loss = 0.0012762196129187942
Validation loss = 0.0012222998775541782
Validation loss = 0.001208922709338367
Validation loss = 0.0011349278502166271
Validation loss = 0.0011611967347562313
Validation loss = 0.0012766314903274179
Validation loss = 0.0011906077852472663
Validation loss = 0.0012424506712704897
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 21       |
| MaximumReturn | 199      |
| MinimumReturn | 123      |
| TotalSamples  | 76659    |
----------------------------
itr #22 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0013467574026435614
Validation loss = 0.0012815841473639011
Validation loss = 0.001362475915811956
Validation loss = 0.0013048186665400863
Validation loss = 0.0013936897739768028
Validation loss = 0.0015889392234385014
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.001366024254821241
Validation loss = 0.00140512571670115
Validation loss = 0.0014202436432242393
Validation loss = 0.0013609100133180618
Validation loss = 0.0014161551371216774
Validation loss = 0.0013228508178144693
Validation loss = 0.0017675120616331697
Validation loss = 0.0013164402917027473
Validation loss = 0.0014124158769845963
Validation loss = 0.0013508888659998775
Validation loss = 0.0014644325710833073
Validation loss = 0.0013621997786685824
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.001280055963434279
Validation loss = 0.001415172591805458
Validation loss = 0.0013451770646497607
Validation loss = 0.0013512977166101336
Validation loss = 0.00131166260689497
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0015798155218362808
Validation loss = 0.0013936000177636743
Validation loss = 0.0013640911784023046
Validation loss = 0.0013403402408584952
Validation loss = 0.0014208439970389009
Validation loss = 0.001341894268989563
Validation loss = 0.0012822533026337624
Validation loss = 0.0013692283537238836
Validation loss = 0.0014190658694133162
Validation loss = 0.0013580300146713853
Validation loss = 0.0013566762208938599
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0013362769968807697
Validation loss = 0.0013843404594808817
Validation loss = 0.0013768820790573955
Validation loss = 0.0014150713104754686
Validation loss = 0.0013321732403710485
Validation loss = 0.0013766626361757517
Validation loss = 0.001345330267213285
Validation loss = 0.001329011283814907
Validation loss = 0.0015861289575695992
Validation loss = 0.0014703356428071856
Validation loss = 0.0013217132072895765
Validation loss = 0.0012999596074223518
Validation loss = 0.0013305449392646551
Validation loss = 0.0013470909325405955
Validation loss = 0.0014997160760685802
Validation loss = 0.0013459671754390001
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 22       |
| MaximumReturn | 199      |
| MinimumReturn | 119      |
| TotalSamples  | 79992    |
----------------------------
itr #23 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0017012804746627808
Validation loss = 0.0018330890452489257
Validation loss = 0.0017994486261159182
Validation loss = 0.0017642632592469454
Validation loss = 0.0018770161550492048
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0019684156868606806
Validation loss = 0.0016752645606175065
Validation loss = 0.0017209528014063835
Validation loss = 0.0017150519415736198
Validation loss = 0.0017048564041033387
Validation loss = 0.0018986152717843652
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0017358132172375917
Validation loss = 0.0019595276098698378
Validation loss = 0.0016773365205153823
Validation loss = 0.0017266061622649431
Validation loss = 0.0017350586131215096
Validation loss = 0.001677989261224866
Validation loss = 0.0017893038457259536
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0018569559324532747
Validation loss = 0.0017444889526814222
Validation loss = 0.0017318923491984606
Validation loss = 0.0017110969638451934
Validation loss = 0.001806004554964602
Validation loss = 0.001716824248433113
Validation loss = 0.0017402598168700933
Validation loss = 0.0017205530311912298
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0017582746222615242
Validation loss = 0.0016778644639998674
Validation loss = 0.0017539222026243806
Validation loss = 0.0019050665432587266
Validation loss = 0.0017221085727214813
Validation loss = 0.001698796870186925
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 23       |
| MaximumReturn | 199      |
| MinimumReturn | 124      |
| TotalSamples  | 83325    |
----------------------------
itr #24 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0017909219022840261
Validation loss = 0.0018254832830280066
Validation loss = 0.0018124525668099523
Validation loss = 0.001861907308921218
Validation loss = 0.0018855793168768287
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0018369756871834397
Validation loss = 0.0019080678466707468
Validation loss = 0.0018993241246789694
Validation loss = 0.0018443410517647862
Validation loss = 0.0018543830374255776
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0019069906556978822
Validation loss = 0.0018840325064957142
Validation loss = 0.0020439347717911005
Validation loss = 0.0018405676819384098
Validation loss = 0.001859211130067706
Validation loss = 0.001864428399130702
Validation loss = 0.0019418661249801517
Validation loss = 0.001860253163613379
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0019241201225668192
Validation loss = 0.0017861647065728903
Validation loss = 0.0017908072331920266
Validation loss = 0.0018706379923969507
Validation loss = 0.0021160817705094814
Validation loss = 0.0018752508331090212
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0018301260424777865
Validation loss = 0.0018928874051198363
Validation loss = 0.0018966651987284422
Validation loss = 0.0018965783528983593
Validation loss = 0.0020112115889787674
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 24       |
| MaximumReturn | 199      |
| MinimumReturn | 124      |
| TotalSamples  | 86658    |
----------------------------
itr #25 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0024073035456240177
Validation loss = 0.0022032023407518864
Validation loss = 0.002211576560512185
Validation loss = 0.002190480474382639
Validation loss = 0.002505148993805051
Validation loss = 0.002321509411558509
Validation loss = 0.002255062572658062
Validation loss = 0.0022829652298241854
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0024018818512558937
Validation loss = 0.0022110361605882645
Validation loss = 0.0023663833271712065
Validation loss = 0.002258952474221587
Validation loss = 0.0022185028064996004
Validation loss = 0.0021907195914536715
Validation loss = 0.0022204522974789143
Validation loss = 0.0022059879265725613
Validation loss = 0.0022602188400924206
Validation loss = 0.0022637678775936365
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0023117894306778908
Validation loss = 0.0022662265691906214
Validation loss = 0.0023750767577439547
Validation loss = 0.0022053804714232683
Validation loss = 0.0022044547367841005
Validation loss = 0.0023049800656735897
Validation loss = 0.0022742419969290495
Validation loss = 0.0022406266070902348
Validation loss = 0.0022184026893228292
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0022666500881314278
Validation loss = 0.0023199133574962616
Validation loss = 0.0022356114350259304
Validation loss = 0.002309296280145645
Validation loss = 0.0023375828750431538
Validation loss = 0.0022121816873550415
Validation loss = 0.0022965918760746717
Validation loss = 0.002198219997808337
Validation loss = 0.002194669796153903
Validation loss = 0.0023021770175546408
Validation loss = 0.0022064356599003077
Validation loss = 0.0022676833905279636
Validation loss = 0.002294151345267892
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0023173652589321136
Validation loss = 0.0022254360374063253
Validation loss = 0.0023017525672912598
Validation loss = 0.0022074198350310326
Validation loss = 0.0022565675899386406
Validation loss = 0.0022265473380684853
Validation loss = 0.002289414871484041
Validation loss = 0.002283320529386401
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 25       |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 89991    |
----------------------------
itr #26 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.002668389119207859
Validation loss = 0.0026597536634653807
Validation loss = 0.0027356878854334354
Validation loss = 0.002871610689908266
Validation loss = 0.002692381152883172
Validation loss = 0.002735220128670335
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.00290615763515234
Validation loss = 0.002667532069608569
Validation loss = 0.002774602733552456
Validation loss = 0.00266255927272141
Validation loss = 0.0027125494088977575
Validation loss = 0.002694888971745968
Validation loss = 0.0026121416594833136
Validation loss = 0.002583271125331521
Validation loss = 0.0025877049192786217
Validation loss = 0.002636542310938239
Validation loss = 0.0026393320877104998
Validation loss = 0.002604444744065404
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.002656975295394659
Validation loss = 0.0026907173451036215
Validation loss = 0.002658913377672434
Validation loss = 0.0026735500432550907
Validation loss = 0.002610517432913184
Validation loss = 0.0026835163589566946
Validation loss = 0.0026356661692261696
Validation loss = 0.0026161319110542536
Validation loss = 0.0027004911098629236
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.002627696143463254
Validation loss = 0.0028894527349621058
Validation loss = 0.0027271227445453405
Validation loss = 0.002732550259679556
Validation loss = 0.002619813196361065
Validation loss = 0.002811875892803073
Validation loss = 0.002600093837827444
Validation loss = 0.0026523738633841276
Validation loss = 0.0026914046611636877
Validation loss = 0.0026349129620939493
Validation loss = 0.002611891133710742
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0027881006244570017
Validation loss = 0.002642800100147724
Validation loss = 0.0028375189285725355
Validation loss = 0.002682947553694248
Validation loss = 0.0027306033298373222
Validation loss = 0.0027501892764121294
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 26       |
| MaximumReturn | 199      |
| MinimumReturn | 117      |
| TotalSamples  | 93324    |
----------------------------
itr #27 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.002796248300001025
Validation loss = 0.0027877655811607838
Validation loss = 0.00307377683930099
Validation loss = 0.002862226916477084
Validation loss = 0.002785363933071494
Validation loss = 0.0030387737788259983
Validation loss = 0.002777701709419489
Validation loss = 0.002907354151830077
Validation loss = 0.0029269366059452295
Validation loss = 0.002845373470336199
Validation loss = 0.0027996681164950132
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0028356523253023624
Validation loss = 0.0027909260243177414
Validation loss = 0.00279905810020864
Validation loss = 0.0029500960372388363
Validation loss = 0.0028096968308091164
Validation loss = 0.0028575502801686525
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.002853104844689369
Validation loss = 0.003007532563060522
Validation loss = 0.0028993890155106783
Validation loss = 0.0028583446983247995
Validation loss = 0.00295203342102468
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0028366982005536556
Validation loss = 0.0030014794319868088
Validation loss = 0.002822811948135495
Validation loss = 0.0028828533831983805
Validation loss = 0.002836843952536583
Validation loss = 0.0028310767374932766
Validation loss = 0.002835697028785944
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0030561587773263454
Validation loss = 0.0029618311673402786
Validation loss = 0.002797851338982582
Validation loss = 0.0028234594501554966
Validation loss = 0.0028505593072623014
Validation loss = 0.0032420028001070023
Validation loss = 0.0031734739895910025
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 27       |
| MaximumReturn | 199      |
| MinimumReturn | 126      |
| TotalSamples  | 96657    |
----------------------------
itr #28 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0031707168091088533
Validation loss = 0.0031575746834278107
Validation loss = 0.003085847245529294
Validation loss = 0.003086690790951252
Validation loss = 0.0030730124562978745
Validation loss = 0.003102291375398636
Validation loss = 0.003170629730448127
Validation loss = 0.003080880269408226
Validation loss = 0.003116898238658905
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.003069828497245908
Validation loss = 0.0031804535537958145
Validation loss = 0.0032317976001650095
Validation loss = 0.0031641386449337006
Validation loss = 0.0031522565986961126
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0032252862583845854
Validation loss = 0.0032756428699940443
Validation loss = 0.003291439963504672
Validation loss = 0.0031020052265375853
Validation loss = 0.003121700370684266
Validation loss = 0.0033437598031014204
Validation loss = 0.003369726240634918
Validation loss = 0.003136219223961234
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0031324613373726606
Validation loss = 0.003069493221119046
Validation loss = 0.0032903875689953566
Validation loss = 0.0031011130195111036
Validation loss = 0.00321228988468647
Validation loss = 0.003200744278728962
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.003164983121678233
Validation loss = 0.003109534038230777
Validation loss = 0.0031443715561181307
Validation loss = 0.003035208908841014
Validation loss = 0.003192843170836568
Validation loss = 0.0030769326258450747
Validation loss = 0.003222733037546277
Validation loss = 0.003054096130654216
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 28       |
| MaximumReturn | 199      |
| MinimumReturn | 117      |
| TotalSamples  | 99990    |
----------------------------
itr #29 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.003869790118187666
Validation loss = 0.00396799435839057
Validation loss = 0.003964191768318415
Validation loss = 0.00397948594763875
Validation loss = 0.0038970550522208214
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.004173429682850838
Validation loss = 0.003939291927963495
Validation loss = 0.003931240178644657
Validation loss = 0.003906618803739548
Validation loss = 0.004017015919089317
Validation loss = 0.003933578263968229
Validation loss = 0.0038914300967007875
Validation loss = 0.003928580321371555
Validation loss = 0.0038766353391110897
Validation loss = 0.004058863967657089
Validation loss = 0.003882463788613677
Validation loss = 0.0040103537030518055
Validation loss = 0.003931842278689146
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.003971128724515438
Validation loss = 0.00385762145742774
Validation loss = 0.00390959344804287
Validation loss = 0.004055222496390343
Validation loss = 0.0040422119200229645
Validation loss = 0.003964119125157595
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.004086194094270468
Validation loss = 0.0038867322728037834
Validation loss = 0.0038705351762473583
Validation loss = 0.003875286318361759
Validation loss = 0.0038308121729642153
Validation loss = 0.003910448867827654
Validation loss = 0.0039612287655472755
Validation loss = 0.003934962209314108
Validation loss = 0.0038892868906259537
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.003914399538189173
Validation loss = 0.0038667405024170876
Validation loss = 0.003918083384633064
Validation loss = 0.003952833358198404
Validation loss = 0.0040540508925914764
Validation loss = 0.003953277599066496
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 29       |
| MaximumReturn | 199      |
| MinimumReturn | 119      |
| TotalSamples  | 103323   |
----------------------------
itr #30 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.004115869291126728
Validation loss = 0.0040709758177399635
Validation loss = 0.00393429072573781
Validation loss = 0.003962032031267881
Validation loss = 0.003969793673604727
Validation loss = 0.003955089952796698
Validation loss = 0.004106918349862099
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.004084128420799971
Validation loss = 0.004026958253234625
Validation loss = 0.004103051498532295
Validation loss = 0.004013736732304096
Validation loss = 0.004038416780531406
Validation loss = 0.004101875238120556
Validation loss = 0.004046283662319183
Validation loss = 0.004064745269715786
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.00402420898899436
Validation loss = 0.004067022353410721
Validation loss = 0.0040208990685641766
Validation loss = 0.003981426823884249
Validation loss = 0.004086542874574661
Validation loss = 0.00411846861243248
Validation loss = 0.00396527536213398
Validation loss = 0.004092101473361254
Validation loss = 0.0040088314563035965
Validation loss = 0.004048572853207588
Validation loss = 0.00403927406296134
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0040209474973380566
Validation loss = 0.003942360635846853
Validation loss = 0.004021778702735901
Validation loss = 0.004008316434919834
Validation loss = 0.0039563262835145
Validation loss = 0.004025239963084459
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.0038978392258286476
Validation loss = 0.00396425137296319
Validation loss = 0.004020040389150381
Validation loss = 0.003977852873504162
Validation loss = 0.004027957562357187
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 30       |
| MaximumReturn | 199      |
| MinimumReturn | 134      |
| TotalSamples  | 106656   |
----------------------------
itr #31 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.0043673766776919365
Validation loss = 0.004359148442745209
Validation loss = 0.004321918822824955
Validation loss = 0.004229917656630278
Validation loss = 0.004286335781216621
Validation loss = 0.004198330454528332
Validation loss = 0.0042580654844641685
Validation loss = 0.004394948948174715
Validation loss = 0.0041906218975782394
Validation loss = 0.004357228521257639
Validation loss = 0.004269167315214872
Validation loss = 0.004320237785577774
Validation loss = 0.004328571259975433
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.004331457894295454
Validation loss = 0.004354684613645077
Validation loss = 0.004288820084184408
Validation loss = 0.004311166703701019
Validation loss = 0.004290973301976919
Validation loss = 0.004330003168433905
Validation loss = 0.004242683295160532
Validation loss = 0.004290027543902397
Validation loss = 0.004315636120736599
Validation loss = 0.004638874437659979
Validation loss = 0.004402142949402332
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.004289065487682819
Validation loss = 0.004312966950237751
Validation loss = 0.004290638491511345
Validation loss = 0.0043031638488173485
Validation loss = 0.004265511874109507
Validation loss = 0.004446872975677252
Validation loss = 0.004295755177736282
Validation loss = 0.004285003058612347
Validation loss = 0.004346031229943037
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.004486849997192621
Validation loss = 0.004276105668395758
Validation loss = 0.004290771670639515
Validation loss = 0.004576029721647501
Validation loss = 0.004284268245100975
Validation loss = 0.004490033723413944
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.004278114531189203
Validation loss = 0.00426388019695878
Validation loss = 0.004636354744434357
Validation loss = 0.004307780414819717
Validation loss = 0.004371007438749075
Validation loss = 0.004294322803616524
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 31       |
| MaximumReturn | 199      |
| MinimumReturn | 126      |
| TotalSamples  | 109989   |
----------------------------
itr #32 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.004762514494359493
Validation loss = 0.0048955585807561874
Validation loss = 0.004745547194033861
Validation loss = 0.004684416111558676
Validation loss = 0.00464464258402586
Validation loss = 0.004906810820102692
Validation loss = 0.004753808490931988
Validation loss = 0.004671825096011162
Validation loss = 0.005017368588596582
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.00466336403042078
Validation loss = 0.004652812145650387
Validation loss = 0.004711126908659935
Validation loss = 0.0048060729168355465
Validation loss = 0.004632025491446257
Validation loss = 0.004712696187198162
Validation loss = 0.004767271690070629
Validation loss = 0.00469793239608407
Validation loss = 0.004721705801784992
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.004703408107161522
Validation loss = 0.0046677496284246445
Validation loss = 0.004720054566860199
Validation loss = 0.004745830781757832
Validation loss = 0.004668445326387882
Validation loss = 0.004627123940736055
Validation loss = 0.004661940038204193
Validation loss = 0.0047960709780454636
Validation loss = 0.004648017231374979
Validation loss = 0.004974092822521925
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.004641616716980934
Validation loss = 0.00470902631059289
Validation loss = 0.004686153028160334
Validation loss = 0.0048987953923642635
Validation loss = 0.004688048269599676
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.004747302271425724
Validation loss = 0.004673341289162636
Validation loss = 0.004742247983813286
Validation loss = 0.004620485007762909
Validation loss = 0.004647579975426197
Validation loss = 0.004677757155150175
Validation loss = 0.004871489945799112
Validation loss = 0.004794795997440815
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 32       |
| MaximumReturn | 199      |
| MinimumReturn | 128      |
| TotalSamples  | 113322   |
----------------------------
itr #33 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.004748095292598009
Validation loss = 0.00475243479013443
Validation loss = 0.004767097532749176
Validation loss = 0.004802255891263485
Validation loss = 0.00485574034973979
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.00483045494183898
Validation loss = 0.004933730699121952
Validation loss = 0.005050832871347666
Validation loss = 0.004904806613922119
Validation loss = 0.004790525883436203
Validation loss = 0.0047533041797578335
Validation loss = 0.004766891710460186
Validation loss = 0.004729722160845995
Validation loss = 0.00476450938731432
Validation loss = 0.004752923734486103
Validation loss = 0.00476393848657608
Validation loss = 0.004730726592242718
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.004813870880752802
Validation loss = 0.004832591395825148
Validation loss = 0.004669386427849531
Validation loss = 0.004789416212588549
Validation loss = 0.004859300795942545
Validation loss = 0.00473835738375783
Validation loss = 0.004775679670274258
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.004742200952023268
Validation loss = 0.004858371336013079
Validation loss = 0.004979095421731472
Validation loss = 0.004784929100424051
Validation loss = 0.004766587633639574
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.004865097347646952
Validation loss = 0.004885306116193533
Validation loss = 0.005021342076361179
Validation loss = 0.004814320243895054
Validation loss = 0.004932178650051355
Validation loss = 0.0048127928748726845
Validation loss = 0.0047849309630692005
Validation loss = 0.004770087543874979
Validation loss = 0.004781039897352457
Validation loss = 0.004852617625147104
Validation loss = 0.004764760844409466
Validation loss = 0.0048408363945782185
Validation loss = 0.004744015634059906
Validation loss = 0.004765487276017666
Validation loss = 0.004743568133562803
Validation loss = 0.004794000647962093
Validation loss = 0.004689303692430258
Validation loss = 0.004695569630712271
Validation loss = 0.004737786948680878
Validation loss = 0.00471727317199111
Validation loss = 0.0046824193559587
Validation loss = 0.004898628685623407
Validation loss = 0.0048925080336630344
Validation loss = 0.004772820509970188
Validation loss = 0.004859917797148228
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 179      |
| Iteration     | 33       |
| MaximumReturn | 199      |
| MinimumReturn | 132      |
| TotalSamples  | 116655   |
----------------------------
itr #34 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.004963499028235674
Validation loss = 0.004929255228489637
Validation loss = 0.004975658375769854
Validation loss = 0.004950626287609339
Validation loss = 0.004872913006693125
Validation loss = 0.0050452882423996925
Validation loss = 0.004936227109283209
Validation loss = 0.004914288874715567
Validation loss = 0.005078325513750315
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.0049555543810129166
Validation loss = 0.004889774601906538
Validation loss = 0.005049999337643385
Validation loss = 0.004966130945831537
Validation loss = 0.0049280873499810696
Validation loss = 0.004994652699679136
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.005085256416350603
Validation loss = 0.0049277422949671745
Validation loss = 0.004941920749843121
Validation loss = 0.004880994558334351
Validation loss = 0.004971646703779697
Validation loss = 0.004914548713713884
Validation loss = 0.005030822940170765
Validation loss = 0.00497482530772686
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.005120584275573492
Validation loss = 0.005037552211433649
Validation loss = 0.005037028342485428
Validation loss = 0.004949379712343216
Validation loss = 0.0049632852897048
Validation loss = 0.004993888549506664
Validation loss = 0.004976311698555946
Validation loss = 0.0049834419041872025
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.005014910828322172
Validation loss = 0.005114481784403324
Validation loss = 0.005003972444683313
Validation loss = 0.004885147325694561
Validation loss = 0.004901775624603033
Validation loss = 0.0049196770414710045
Validation loss = 0.004894494079053402
Validation loss = 0.005042751785367727
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 160      |
| Iteration     | 34       |
| MaximumReturn | 197      |
| MinimumReturn | 126      |
| TotalSamples  | 119988   |
----------------------------
itr #35 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.005020119249820709
Validation loss = 0.005214270669966936
Validation loss = 0.005097826011478901
Validation loss = 0.004977526143193245
Validation loss = 0.005091345403343439
Validation loss = 0.005033900495618582
Validation loss = 0.004985241685062647
Validation loss = 0.005081734154373407
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.005021556280553341
Validation loss = 0.005015779752284288
Validation loss = 0.005199932958930731
Validation loss = 0.0051638600416481495
Validation loss = 0.0051436410285532475
Validation loss = 0.005063921213150024
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.005085701122879982
Validation loss = 0.005036042537540197
Validation loss = 0.0052117034792900085
Validation loss = 0.005183415021747351
Validation loss = 0.005065079312771559
Validation loss = 0.00503324531018734
Validation loss = 0.00513693829998374
Validation loss = 0.005078006070107222
Validation loss = 0.005077824927866459
Validation loss = 0.005136467516422272
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0052445679903030396
Validation loss = 0.004967470653355122
Validation loss = 0.005359035916626453
Validation loss = 0.004999652970582247
Validation loss = 0.005057728383690119
Validation loss = 0.004987499210983515
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.005000974517315626
Validation loss = 0.005044838413596153
Validation loss = 0.005029140040278435
Validation loss = 0.005114589352160692
Validation loss = 0.005182584282010794
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 35       |
| MaximumReturn | 199      |
| MinimumReturn | 124      |
| TotalSamples  | 123321   |
----------------------------
itr #36 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.005839806515723467
Validation loss = 0.005769104231148958
Validation loss = 0.005779413040727377
Validation loss = 0.005915068089962006
Validation loss = 0.005910187494009733
Validation loss = 0.0058641694486141205
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.006157702766358852
Validation loss = 0.00583925424143672
Validation loss = 0.00574857834726572
Validation loss = 0.005811517126858234
Validation loss = 0.005785471294075251
Validation loss = 0.005845591891556978
Validation loss = 0.005742467939853668
Validation loss = 0.0058320024982094765
Validation loss = 0.005749461241066456
Validation loss = 0.0058027696795761585
Validation loss = 0.00581073435023427
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.005811003036797047
Validation loss = 0.005803071428090334
Validation loss = 0.005774926394224167
Validation loss = 0.005913814529776573
Validation loss = 0.005875970236957073
Validation loss = 0.005728056654334068
Validation loss = 0.005958957131952047
Validation loss = 0.005859795957803726
Validation loss = 0.005820897873491049
Validation loss = 0.005751579534262419
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.005800426471978426
Validation loss = 0.006005007307976484
Validation loss = 0.005811362527310848
Validation loss = 0.005867623724043369
Validation loss = 0.005911677610129118
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.005933353211730719
Validation loss = 0.005855702329427004
Validation loss = 0.0057607851922512054
Validation loss = 0.00592774897813797
Validation loss = 0.005794350057840347
Validation loss = 0.005750654265284538
Validation loss = 0.005796154960989952
Validation loss = 0.00572006031870842
Validation loss = 0.005789326038211584
Validation loss = 0.005869749467819929
Validation loss = 0.005874903406947851
Validation loss = 0.00584917888045311
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 178      |
| Iteration     | 36       |
| MaximumReturn | 199      |
| MinimumReturn | 128      |
| TotalSamples  | 126654   |
----------------------------
itr #37 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.005901630036532879
Validation loss = 0.006053203251212835
Validation loss = 0.005933486390858889
Validation loss = 0.0059341369196772575
Validation loss = 0.005974331870675087
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.006090730428695679
Validation loss = 0.005971839651465416
Validation loss = 0.0058943466283380985
Validation loss = 0.00602500606328249
Validation loss = 0.0061100805178284645
Validation loss = 0.0059450906701385975
Validation loss = 0.005924103315919638
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.0060490695759654045
Validation loss = 0.00593622587621212
Validation loss = 0.006022740621119738
Validation loss = 0.005916638299822807
Validation loss = 0.005847222171723843
Validation loss = 0.0059350961819291115
Validation loss = 0.005971434060484171
Validation loss = 0.005850944202393293
Validation loss = 0.006160310935229063
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.005934377666562796
Validation loss = 0.0059194848872721195
Validation loss = 0.005920924246311188
Validation loss = 0.005889478139579296
Validation loss = 0.005928985308855772
Validation loss = 0.005975802894681692
Validation loss = 0.005917668808251619
Validation loss = 0.005911651998758316
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.005908286664634943
Validation loss = 0.006075672339648008
Validation loss = 0.005886066239327192
Validation loss = 0.006018680054694414
Validation loss = 0.00623800465837121
Validation loss = 0.005988600663840771
Validation loss = 0.005948435515165329
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 159      |
| Iteration     | 37       |
| MaximumReturn | 198      |
| MinimumReturn | 119      |
| TotalSamples  | 129987   |
----------------------------
itr #38 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.00623914273455739
Validation loss = 0.006399028934538364
Validation loss = 0.00620651850476861
Validation loss = 0.0061799935065209866
Validation loss = 0.006252184510231018
Validation loss = 0.006359181832522154
Validation loss = 0.0061776116490364075
Validation loss = 0.006143084727227688
Validation loss = 0.006200320087373257
Validation loss = 0.006130194757133722
Validation loss = 0.006401821970939636
Validation loss = 0.006336647551506758
Validation loss = 0.0061039491556584835
Validation loss = 0.006246186327189207
Validation loss = 0.006299789063632488
Validation loss = 0.0063280644826591015
Validation loss = 0.0064929695799946785
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.006200205534696579
Validation loss = 0.0061371829360723495
Validation loss = 0.006267359014600515
Validation loss = 0.006181239150464535
Validation loss = 0.006240758113563061
Validation loss = 0.006171318236738443
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.006197080947458744
Validation loss = 0.006213572807610035
Validation loss = 0.006343722343444824
Validation loss = 0.006310923025012016
Validation loss = 0.00621369294822216
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.0062868837267160416
Validation loss = 0.006568798329681158
Validation loss = 0.006233771797269583
Validation loss = 0.00616054143756628
Validation loss = 0.006176258437335491
Validation loss = 0.0062145451083779335
Validation loss = 0.006251179147511721
Validation loss = 0.006211169995367527
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.006237697321921587
Validation loss = 0.006111677270382643
Validation loss = 0.006292407400906086
Validation loss = 0.006207203026860952
Validation loss = 0.00633236113935709
Validation loss = 0.0065887803211808205
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 38       |
| MaximumReturn | 199      |
| MinimumReturn | 118      |
| TotalSamples  | 133320   |
----------------------------
itr #39 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.006684452760964632
Validation loss = 0.006809823215007782
Validation loss = 0.006738577038049698
Validation loss = 0.006738375406712294
Validation loss = 0.006846920121461153
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.007272740360349417
Validation loss = 0.00686968257650733
Validation loss = 0.006732056383043528
Validation loss = 0.006631460506469011
Validation loss = 0.00663762167096138
Validation loss = 0.006750958040356636
Validation loss = 0.0068528130650520325
Validation loss = 0.00676873791962862
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.006771116983145475
Validation loss = 0.0067848279140889645
Validation loss = 0.006718014366924763
Validation loss = 0.006687049753963947
Validation loss = 0.006835846230387688
Validation loss = 0.00669601745903492
Validation loss = 0.006806846708059311
Validation loss = 0.006718301679939032
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.006720178760588169
Validation loss = 0.0067484816536307335
Validation loss = 0.006695942487567663
Validation loss = 0.006918122060596943
Validation loss = 0.006688210181891918
Validation loss = 0.006738380994647741
Validation loss = 0.006766107864677906
Validation loss = 0.006940152030438185
Validation loss = 0.006777680478990078
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.006812578067183495
Validation loss = 0.006694522220641375
Validation loss = 0.006626488175243139
Validation loss = 0.006830346770584583
Validation loss = 0.006641291547566652
Validation loss = 0.006751888431608677
Validation loss = 0.006688703317195177
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 39       |
| MaximumReturn | 199      |
| MinimumReturn | 133      |
| TotalSamples  | 136653   |
----------------------------
