Logging to experiments/gym_pendulumO01/Pendulum_Observation_Noise/w100e0.5_seed3214
Print configuration .....
{'env_name': 'gym_pendulumO01', 'random_seeds': [3214, 2431, 2531, 2231], 'save_variables': False, 'model_save_dir': '/tmp/pendulumO01_models/', 'restore_variables': False, 'start_onpol_iter': 0, 'onpol_iters': 40, 'num_path_random': 25, 'num_path_onpol': 25, 'env_horizon': 200, 'max_train_data': 200000, 'max_val_data': 100000, 'discard_ratio': 0.0, 'dynamics': {'pre_training': {'mode': 'intrinsic_reward', 'itr': 0, 'policy_itr': 20}, 'model': 'nn', 'ensemble': True, 'ensemble_model_count': 5, 'enable_particle_ensemble': True, 'particles': 5, 'obs_var': 1.0, 'intrinsic_reward_coeff': 1.0, 'ita': 1.0, 'mode': 'random', 'val': True, 'n_layers': 4, 'hidden_size': 1000, 'activation': 'relu', 'batch_size': 1000, 'learning_rate': 0.001, 'reg_coeff': 0.0, 'epochs': 200, 'kfac_params': {'learning_rate': 0.1, 'damping': 0.001, 'momentum': 0.9, 'kl_clip': 0.0001, 'cov_ema_decay': 0.99}}, 'policy': {'network_shape': [64, 64], 'init_logstd': 0.0, 'activation': 'tanh', 'reinitialize_every_itr': False}, 'trpo': {'horizon': 200, 'gamma': 0.99, 'step_size': 0.01, 'iterations': 20, 'batch_size': 50000, 'gae': 0.95, 'visualization': False, 'visualize_iterations': [0]}, 'algo': 'trpo'}
Generating random rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating random rollouts.
Creating normalization for training data.
Done creating normalization for training data.
Particle ensemble enabled? True
An ensemble of 5 dynamics model <class 'model.dynamics.NNDynamicsModel'> initialized
Train dynamics model with intrinsic reward only? False
Pre-training enabled. Using only intrinsic reward.
Pre-training dynamics model for 0 iterations...
Done pre-training dynamics model.
Using external reward only.
itr #0 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.363094687461853
Validation loss = 0.24621720612049103
Validation loss = 0.23119278252124786
Validation loss = 0.22428207099437714
Validation loss = 0.2203017771244049
Validation loss = 0.21736456453800201
Validation loss = 0.21881014108657837
Validation loss = 0.20992588996887207
Validation loss = 0.2070671021938324
Validation loss = 0.21409811079502106
Validation loss = 0.20813986659049988
Validation loss = 0.19804492592811584
Validation loss = 0.19605638086795807
Validation loss = 0.19838133454322815
Validation loss = 0.1933547556400299
Validation loss = 0.19233393669128418
Validation loss = 0.19574348628520966
Validation loss = 0.19162850081920624
Validation loss = 0.19531245529651642
Validation loss = 0.19342033565044403
Validation loss = 0.18870633840560913
Validation loss = 0.18751855194568634
Validation loss = 0.19877882301807404
Validation loss = 0.1942048966884613
Validation loss = 0.18979300558567047
Validation loss = 0.1896841824054718
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4157271683216095
Validation loss = 0.24030616879463196
Validation loss = 0.22966428101062775
Validation loss = 0.22499169409275055
Validation loss = 0.22181563079357147
Validation loss = 0.21894344687461853
Validation loss = 0.21785074472427368
Validation loss = 0.21369315683841705
Validation loss = 0.2233942598104477
Validation loss = 0.20271480083465576
Validation loss = 0.2017427533864975
Validation loss = 0.19876359403133392
Validation loss = 0.202327698469162
Validation loss = 0.20505814254283905
Validation loss = 0.19543327391147614
Validation loss = 0.19152966141700745
Validation loss = 0.19373781979084015
Validation loss = 0.19494207203388214
Validation loss = 0.20148706436157227
Validation loss = 0.19685925543308258
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.38831910490989685
Validation loss = 0.23904402554035187
Validation loss = 0.2294304370880127
Validation loss = 0.22631306946277618
Validation loss = 0.22137263417243958
Validation loss = 0.2217555195093155
Validation loss = 0.21245284378528595
Validation loss = 0.21927013993263245
Validation loss = 0.20193682610988617
Validation loss = 0.20832344889640808
Validation loss = 0.19685052335262299
Validation loss = 0.1954619288444519
Validation loss = 0.19548596441745758
Validation loss = 0.20216023921966553
Validation loss = 0.19365821778774261
Validation loss = 0.19637615978717804
Validation loss = 0.1921864151954651
Validation loss = 0.189199298620224
Validation loss = 0.19073408842086792
Validation loss = 0.1926572471857071
Validation loss = 0.19315388798713684
Validation loss = 0.19409336149692535
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.381539523601532
Validation loss = 0.23983925580978394
Validation loss = 0.22792765498161316
Validation loss = 0.2245914489030838
Validation loss = 0.22116762399673462
Validation loss = 0.216333270072937
Validation loss = 0.21054990589618683
Validation loss = 0.21456047892570496
Validation loss = 0.20993223786354065
Validation loss = 0.20124687254428864
Validation loss = 0.20086699724197388
Validation loss = 0.19749228656291962
Validation loss = 0.19772137701511383
Validation loss = 0.1966925412416458
Validation loss = 0.19729910790920258
Validation loss = 0.19255058467388153
Validation loss = 0.19393527507781982
Validation loss = 0.2058151513338089
Validation loss = 0.19840380549430847
Validation loss = 0.18986117839813232
Validation loss = 0.19478018581867218
Validation loss = 0.18918420374393463
Validation loss = 0.19530408084392548
Validation loss = 0.18917453289031982
Validation loss = 0.18636398017406464
Validation loss = 0.19201712310314178
Validation loss = 0.1891542226076126
Validation loss = 0.18818245828151703
Validation loss = 0.18799923360347748
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.41714921593666077
Validation loss = 0.23555298149585724
Validation loss = 0.22757090628147125
Validation loss = 0.22462457418441772
Validation loss = 0.2270139902830124
Validation loss = 0.21749626100063324
Validation loss = 0.21768532693386078
Validation loss = 0.22002460062503815
Validation loss = 0.22455407679080963
Validation loss = 0.2026749849319458
Validation loss = 0.20708419382572174
Validation loss = 0.20265984535217285
Validation loss = 0.19907931983470917
Validation loss = 0.19600342214107513
Validation loss = 0.2055615782737732
Validation loss = 0.19234618544578552
Validation loss = 0.19640421867370605
Validation loss = 0.19165171682834625
Validation loss = 0.19368654489517212
Validation loss = 0.19682379066944122
Validation loss = 0.19402697682380676
Validation loss = 0.19387030601501465
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 0        |
| MaximumReturn | 198      |
| MinimumReturn | 118      |
| TotalSamples  | 6666     |
----------------------------
itr #1 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3752039968967438
Validation loss = 0.2606901526451111
Validation loss = 0.24664174020290375
Validation loss = 0.24461138248443604
Validation loss = 0.24208283424377441
Validation loss = 0.24710841476917267
Validation loss = 0.24062716960906982
Validation loss = 0.24296647310256958
Validation loss = 0.24709367752075195
Validation loss = 0.2490038424730301
Validation loss = 0.2421402782201767
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.35641980171203613
Validation loss = 0.255938857793808
Validation loss = 0.25519588589668274
Validation loss = 0.24918103218078613
Validation loss = 0.2428964376449585
Validation loss = 0.24713508784770966
Validation loss = 0.24284107983112335
Validation loss = 0.24348799884319305
Validation loss = 0.24561886489391327
Validation loss = 0.2395433634519577
Validation loss = 0.2459559291601181
Validation loss = 0.24367408454418182
Validation loss = 0.24296395480632782
Validation loss = 0.24401627480983734
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.37006470561027527
Validation loss = 0.25912466645240784
Validation loss = 0.2518985867500305
Validation loss = 0.2515626847743988
Validation loss = 0.24516618251800537
Validation loss = 0.24737997353076935
Validation loss = 0.2403663992881775
Validation loss = 0.24888066947460175
Validation loss = 0.2452041357755661
Validation loss = 0.2458985596895218
Validation loss = 0.24736805260181427
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.38720810413360596
Validation loss = 0.2507748305797577
Validation loss = 0.24474890530109406
Validation loss = 0.24464978277683258
Validation loss = 0.24072767794132233
Validation loss = 0.243295356631279
Validation loss = 0.2429603934288025
Validation loss = 0.24402861297130585
Validation loss = 0.24572642147541046
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3783378601074219
Validation loss = 0.26004698872566223
Validation loss = 0.2494567185640335
Validation loss = 0.24939537048339844
Validation loss = 0.24381668865680695
Validation loss = 0.24481098353862762
Validation loss = 0.2479218989610672
Validation loss = 0.24534817039966583
Validation loss = 0.25270602107048035
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 1        |
| MaximumReturn | 198      |
| MinimumReturn | 116      |
| TotalSamples  | 9999     |
----------------------------
itr #2 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3004363179206848
Validation loss = 0.2867130637168884
Validation loss = 0.2767685055732727
Validation loss = 0.27608782052993774
Validation loss = 0.28030893206596375
Validation loss = 0.2796953618526459
Validation loss = 0.27839094400405884
Validation loss = 0.28273242712020874
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.29203325510025024
Validation loss = 0.2766529619693756
Validation loss = 0.2812078595161438
Validation loss = 0.27749893069267273
Validation loss = 0.2811204791069031
Validation loss = 0.2757270634174347
Validation loss = 0.27773672342300415
Validation loss = 0.2783694863319397
Validation loss = 0.27923837304115295
Validation loss = 0.2816507816314697
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.2998318374156952
Validation loss = 0.2763342261314392
Validation loss = 0.28181108832359314
Validation loss = 0.278311163187027
Validation loss = 0.2855277955532074
Validation loss = 0.28399229049682617
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.29119858145713806
Validation loss = 0.28288862109184265
Validation loss = 0.27897825837135315
Validation loss = 0.2794170081615448
Validation loss = 0.2766056954860687
Validation loss = 0.2770705819129944
Validation loss = 0.27717286348342896
Validation loss = 0.280388742685318
Validation loss = 0.2812786102294922
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.29381120204925537
Validation loss = 0.283303439617157
Validation loss = 0.27864694595336914
Validation loss = 0.28146982192993164
Validation loss = 0.27614349126815796
Validation loss = 0.28227099776268005
Validation loss = 0.2776964008808136
Validation loss = 0.2778370678424835
Validation loss = 0.2850586473941803
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 2        |
| MaximumReturn | 197      |
| MinimumReturn | 99.7     |
| TotalSamples  | 13332    |
----------------------------
itr #3 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.31301912665367126
Validation loss = 0.3082614243030548
Validation loss = 0.3087180554866791
Validation loss = 0.30897003412246704
Validation loss = 0.3079436719417572
Validation loss = 0.30655786395072937
Validation loss = 0.3033393323421478
Validation loss = 0.3106226623058319
Validation loss = 0.3069647550582886
Validation loss = 0.3089583218097687
Validation loss = 0.3141286373138428
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3117038309574127
Validation loss = 0.31310662627220154
Validation loss = 0.3037508726119995
Validation loss = 0.3068375289440155
Validation loss = 0.3072350323200226
Validation loss = 0.3071214258670807
Validation loss = 0.3140173852443695
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.31178349256515503
Validation loss = 0.3065207600593567
Validation loss = 0.3072855770587921
Validation loss = 0.30740875005722046
Validation loss = 0.30919012427330017
Validation loss = 0.31117019057273865
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3147414028644562
Validation loss = 0.30937767028808594
Validation loss = 0.30864718556404114
Validation loss = 0.3067449629306793
Validation loss = 0.3034451901912689
Validation loss = 0.310467928647995
Validation loss = 0.3067050874233246
Validation loss = 0.3104316294193268
Validation loss = 0.3101394474506378
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.31485939025878906
Validation loss = 0.31116780638694763
Validation loss = 0.31416836380958557
Validation loss = 0.3065003454685211
Validation loss = 0.30915746092796326
Validation loss = 0.3065185546875
Validation loss = 0.30575457215309143
Validation loss = 0.3071487843990326
Validation loss = 0.30787086486816406
Validation loss = 0.3097195625305176
Validation loss = 0.30492621660232544
Validation loss = 0.3057086765766144
Validation loss = 0.3132685124874115
Validation loss = 0.3089791238307953
Validation loss = 0.3098812401294708
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.008403361344537815
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.008333333333333333
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.008264462809917356
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.00819672131147541
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.016260162601626018
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.016129032258064516
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.016
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 3        |
| MaximumReturn | 196      |
| MinimumReturn | 101      |
| TotalSamples  | 16665    |
----------------------------
itr #4 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3400808572769165
Validation loss = 0.3324001729488373
Validation loss = 0.3349878787994385
Validation loss = 0.33505308628082275
Validation loss = 0.33296269178390503
Validation loss = 0.33320584893226624
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.33275163173675537
Validation loss = 0.332561731338501
Validation loss = 0.334654837846756
Validation loss = 0.3343654274940491
Validation loss = 0.3330884873867035
Validation loss = 0.3383793532848358
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3330014944076538
Validation loss = 0.3339253067970276
Validation loss = 0.33802929520606995
Validation loss = 0.3338104486465454
Validation loss = 0.3345716595649719
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.33600321412086487
Validation loss = 0.33078381419181824
Validation loss = 0.3374275267124176
Validation loss = 0.33310478925704956
Validation loss = 0.33412379026412964
Validation loss = 0.3317078649997711
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3352392315864563
Validation loss = 0.3309949040412903
Validation loss = 0.33127081394195557
Validation loss = 0.3358888030052185
Validation loss = 0.3365430533885956
Validation loss = 0.33465278148651123
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.015873015873015872
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.015748031496062992
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.015625
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.015503875968992248
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.015384615384615385
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.015267175572519083
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.015151515151515152
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.015037593984962405
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.014925373134328358
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.014814814814814815
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.014705882352941176
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.014598540145985401
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.014492753623188406
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.014388489208633094
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.014285714285714285
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.014184397163120567
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.014084507042253521
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.013986013986013986
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.013888888888888888
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.013793103448275862
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0136986301369863
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.013605442176870748
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.013513513513513514
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.013422818791946308
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.013333333333333334
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 4        |
| MaximumReturn | 197      |
| MinimumReturn | 99.6     |
| TotalSamples  | 19998    |
----------------------------
itr #5 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.35752665996551514
Validation loss = 0.3606104254722595
Validation loss = 0.3591372072696686
Validation loss = 0.35824960470199585
Validation loss = 0.3610026240348816
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.35732942819595337
Validation loss = 0.35776570439338684
Validation loss = 0.36657896637916565
Validation loss = 0.36235731840133667
Validation loss = 0.35739096999168396
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.35896891355514526
Validation loss = 0.35755982995033264
Validation loss = 0.36053305864334106
Validation loss = 0.363007128238678
Validation loss = 0.35706827044487
Validation loss = 0.35723230242729187
Validation loss = 0.36052125692367554
Validation loss = 0.360594242811203
Validation loss = 0.3585086464881897
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3582859933376312
Validation loss = 0.3603196144104004
Validation loss = 0.3625040352344513
Validation loss = 0.35702094435691833
Validation loss = 0.35774701833724976
Validation loss = 0.3627553880214691
Validation loss = 0.35917872190475464
Validation loss = 0.35765698552131653
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3628920912742615
Validation loss = 0.3574729561805725
Validation loss = 0.3569972813129425
Validation loss = 0.3613449037075043
Validation loss = 0.35900944471359253
Validation loss = 0.3624342083930969
Validation loss = 0.36113789677619934
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.013245033112582781
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.013157894736842105
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.013071895424836602
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.012987012987012988
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.012903225806451613
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.01282051282051282
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.012738853503184714
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.012658227848101266
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.012578616352201259
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0125
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.012422360248447204
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.012345679012345678
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.012269938650306749
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.012195121951219513
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.012121212121212121
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.012048192771084338
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.011976047904191617
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.011904761904761904
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.011834319526627219
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.011764705882352941
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.011695906432748537
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.011627906976744186
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.011560693641618497
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.011494252873563218
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.011428571428571429
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 5        |
| MaximumReturn | 198      |
| MinimumReturn | 103      |
| TotalSamples  | 23331    |
----------------------------
itr #6 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.37143638730049133
Validation loss = 0.37084418535232544
Validation loss = 0.37125301361083984
Validation loss = 0.3724428117275238
Validation loss = 0.37173986434936523
Validation loss = 0.3717498779296875
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.36907893419265747
Validation loss = 0.36879703402519226
Validation loss = 0.3720649480819702
Validation loss = 0.37141770124435425
Validation loss = 0.3745371103286743
Validation loss = 0.37104663252830505
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.37187090516090393
Validation loss = 0.3731786012649536
Validation loss = 0.36969345808029175
Validation loss = 0.3704085350036621
Validation loss = 0.37095415592193604
Validation loss = 0.3700268864631653
Validation loss = 0.36860933899879456
Validation loss = 0.3697105646133423
Validation loss = 0.3688596785068512
Validation loss = 0.3706229329109192
Validation loss = 0.37487971782684326
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3729242980480194
Validation loss = 0.36893022060394287
Validation loss = 0.37354424595832825
Validation loss = 0.36924466490745544
Validation loss = 0.3713579475879669
Validation loss = 0.3728780448436737
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3716025650501251
Validation loss = 0.369606614112854
Validation loss = 0.37250664830207825
Validation loss = 0.37193727493286133
Validation loss = 0.3741421401500702
Validation loss = 0.3734634220600128
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.017045454545454544
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.022598870056497175
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.02247191011235955
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.027932960893854747
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.03333333333333333
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.03314917127071823
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.038461538461538464
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.03825136612021858
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.03804347826086957
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.03783783783783784
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.03763440860215054
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0374331550802139
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.03723404255319149
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.042328042328042326
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.042105263157894736
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.041884816753926704
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.046875
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.046632124352331605
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.04639175257731959
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.046153846153846156
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.04591836734693878
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.050761421319796954
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.050505050505050504
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.05527638190954774
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.055
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 155      |
| Iteration     | 6        |
| MaximumReturn | 195      |
| MinimumReturn | 92.2     |
| TotalSamples  | 26664    |
----------------------------
itr #7 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3836849629878998
Validation loss = 0.3848031163215637
Validation loss = 0.3852458596229553
Validation loss = 0.38574349880218506
Validation loss = 0.3863217532634735
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3825073838233948
Validation loss = 0.38704660534858704
Validation loss = 0.38337084650993347
Validation loss = 0.38328102231025696
Validation loss = 0.38327792286872864
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3849620223045349
Validation loss = 0.3847563862800598
Validation loss = 0.38307058811187744
Validation loss = 0.3849812150001526
Validation loss = 0.38595783710479736
Validation loss = 0.3847799003124237
Validation loss = 0.38462960720062256
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3848592936992645
Validation loss = 0.38721764087677
Validation loss = 0.38361164927482605
Validation loss = 0.385435551404953
Validation loss = 0.38426658511161804
Validation loss = 0.3837529420852661
Validation loss = 0.38701730966567993
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3844662010669708
Validation loss = 0.38650083541870117
Validation loss = 0.3832734525203705
Validation loss = 0.38686034083366394
Validation loss = 0.3835044503211975
Validation loss = 0.3856571912765503
Validation loss = 0.38619211316108704
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05472636815920398
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.054455445544554455
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.059113300492610835
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.06372549019607843
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.06341463414634146
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.06310679611650485
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.06280193236714976
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0625
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.06698564593301436
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.06666666666666667
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.06635071090047394
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0660377358490566
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.06572769953051644
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.06542056074766354
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.06976744186046512
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.06944444444444445
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.06912442396313365
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.07339449541284404
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0730593607305936
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07272727272727272
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.07692307692307693
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07657657657657657
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07623318385650224
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07589285714285714
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07555555555555556
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 7        |
| MaximumReturn | 199      |
| MinimumReturn | 117      |
| TotalSamples  | 29997    |
----------------------------
itr #8 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3947864770889282
Validation loss = 0.396542489528656
Validation loss = 0.39707881212234497
Validation loss = 0.3955284655094147
Validation loss = 0.3971671462059021
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3958571255207062
Validation loss = 0.394157350063324
Validation loss = 0.3958234190940857
Validation loss = 0.39515405893325806
Validation loss = 0.3926725387573242
Validation loss = 0.393320232629776
Validation loss = 0.3971687853336334
Validation loss = 0.3953666687011719
Validation loss = 0.3960415720939636
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.39479050040245056
Validation loss = 0.3969734013080597
Validation loss = 0.39484041929244995
Validation loss = 0.3999332785606384
Validation loss = 0.3951738178730011
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3957003653049469
Validation loss = 0.3961685597896576
Validation loss = 0.3944406509399414
Validation loss = 0.39826613664627075
Validation loss = 0.3969358205795288
Validation loss = 0.39701196551322937
Validation loss = 0.39899083971977234
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3953719139099121
Validation loss = 0.3950149714946747
Validation loss = 0.3984513580799103
Validation loss = 0.39655402302742004
Validation loss = 0.3984469771385193
Validation loss = 0.394551157951355
Validation loss = 0.3964138329029083
Validation loss = 0.3997214734554291
Validation loss = 0.3981592357158661
Validation loss = 0.4013867974281311
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0752212389380531
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.07929515418502203
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07894736842105263
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.08296943231441048
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08260869565217391
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08225108225108226
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.08620689655172414
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08583690987124463
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08547008547008547
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0851063829787234
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0847457627118644
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08438818565400844
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08403361344537816
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08368200836820083
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08333333333333333
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08298755186721991
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08264462809917356
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0823045267489712
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08196721311475409
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08163265306122448
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08130081300813008
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08097165991902834
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08064516129032258
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08032128514056225
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 8        |
| MaximumReturn | 198      |
| MinimumReturn | 96.7     |
| TotalSamples  | 33330    |
----------------------------
itr #9 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4010249972343445
Validation loss = 0.4031112492084503
Validation loss = 0.40267035365104675
Validation loss = 0.4035831391811371
Validation loss = 0.4021443724632263
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.40168923139572144
Validation loss = 0.3996140658855438
Validation loss = 0.4012070298194885
Validation loss = 0.4014037251472473
Validation loss = 0.40306204557418823
Validation loss = 0.4012286961078644
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.40269774198532104
Validation loss = 0.4015674591064453
Validation loss = 0.404004842042923
Validation loss = 0.4013981223106384
Validation loss = 0.4006468653678894
Validation loss = 0.401264488697052
Validation loss = 0.4044854938983917
Validation loss = 0.40494227409362793
Validation loss = 0.40921324491500854
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.40163177251815796
Validation loss = 0.40320220589637756
Validation loss = 0.40201252698898315
Validation loss = 0.404353529214859
Validation loss = 0.40290969610214233
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.40215206146240234
Validation loss = 0.40324923396110535
Validation loss = 0.4064989387989044
Validation loss = 0.4035648703575134
Validation loss = 0.4036875367164612
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0796812749003984
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.08333333333333333
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08300395256916997
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08267716535433071
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08235294117647059
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08203125
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08171206225680934
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08139534883720931
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08108108108108109
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08076923076923077
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08045977011494253
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08015267175572519
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07984790874524715
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07954545454545454
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07924528301886792
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07894736842105263
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07865168539325842
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07835820895522388
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07806691449814127
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07777777777777778
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07749077490774908
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.08088235294117647
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08058608058608059
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.08394160583941605
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08363636363636363
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 9        |
| MaximumReturn | 199      |
| MinimumReturn | 111      |
| TotalSamples  | 36663    |
----------------------------
itr #10 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4092564284801483
Validation loss = 0.40992471575737
Validation loss = 0.406072199344635
Validation loss = 0.4073241055011749
Validation loss = 0.40719276666641235
Validation loss = 0.4128570258617401
Validation loss = 0.4072730839252472
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4074651896953583
Validation loss = 0.40656816959381104
Validation loss = 0.4096025228500366
Validation loss = 0.40991050004959106
Validation loss = 0.408073753118515
Validation loss = 0.4103519022464752
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.40811699628829956
Validation loss = 0.40877243876457214
Validation loss = 0.4095780849456787
Validation loss = 0.41206204891204834
Validation loss = 0.4120548963546753
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.40922224521636963
Validation loss = 0.4121234118938446
Validation loss = 0.40663647651672363
Validation loss = 0.4087599813938141
Validation loss = 0.4092148542404175
Validation loss = 0.4099472463130951
Validation loss = 0.40879642963409424
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4097343385219574
Validation loss = 0.40955954790115356
Validation loss = 0.4103221297264099
Validation loss = 0.4090937376022339
Validation loss = 0.41288474202156067
Validation loss = 0.4088004529476166
Validation loss = 0.41392213106155396
Validation loss = 0.4104415774345398
Validation loss = 0.41312530636787415
Validation loss = 0.41430872678756714
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.08695652173913043
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08664259927797834
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08633093525179857
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08602150537634409
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08571428571428572
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08540925266903915
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0851063829787234
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08480565371024736
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08450704225352113
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08421052631578947
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08391608391608392
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08362369337979095
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08333333333333333
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08304498269896193
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08275862068965517
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08247422680412371
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0821917808219178
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08191126279863481
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.08503401360544217
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0847457627118644
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.08783783783783784
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08754208754208755
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.09060402684563758
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0903010033444816
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.09
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 163      |
| Iteration     | 10       |
| MaximumReturn | 199      |
| MinimumReturn | 103      |
| TotalSamples  | 39996    |
----------------------------
itr #11 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.41312623023986816
Validation loss = 0.4135967791080475
Validation loss = 0.41493654251098633
Validation loss = 0.4154372811317444
Validation loss = 0.4143349528312683
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4155299663543701
Validation loss = 0.4158552289009094
Validation loss = 0.4143758714199066
Validation loss = 0.41489291191101074
Validation loss = 0.4160228371620178
Validation loss = 0.41505590081214905
Validation loss = 0.4149167537689209
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.41648659110069275
Validation loss = 0.4160583019256592
Validation loss = 0.41783761978149414
Validation loss = 0.41607171297073364
Validation loss = 0.41479259729385376
Validation loss = 0.4166923463344574
Validation loss = 0.41855478286743164
Validation loss = 0.4166882038116455
Validation loss = 0.419342577457428
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.41510844230651855
Validation loss = 0.41499024629592896
Validation loss = 0.41604819893836975
Validation loss = 0.4143284857273102
Validation loss = 0.4165270924568176
Validation loss = 0.41518983244895935
Validation loss = 0.4162953794002533
Validation loss = 0.4157598614692688
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4188617765903473
Validation loss = 0.41744518280029297
Validation loss = 0.41696697473526
Validation loss = 0.4178870618343353
Validation loss = 0.4150214195251465
Validation loss = 0.42066988348960876
Validation loss = 0.4166392683982849
Validation loss = 0.4212198257446289
Validation loss = 0.41914790868759155
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.09634551495016612
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.09933774834437085
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.09900990099009901
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.10197368421052631
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.10163934426229508
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.10457516339869281
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.10423452768729642
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.1038961038961039
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.10679611650485436
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.1064516129032258
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.10932475884244373
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.11217948717948718
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.11182108626198083
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.11146496815286625
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.1111111111111111
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.11708860759493671
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.1167192429022082
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.11635220125786164
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.11912225705329153
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.11875
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.11838006230529595
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.11801242236024845
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.12074303405572756
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.12037037037037036
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.12
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 11       |
| MaximumReturn | 198      |
| MinimumReturn | 122      |
| TotalSamples  | 43329    |
----------------------------
itr #12 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4170488715171814
Validation loss = 0.4181951880455017
Validation loss = 0.4151412844657898
Validation loss = 0.4159914553165436
Validation loss = 0.415879487991333
Validation loss = 0.4157485365867615
Validation loss = 0.4174034595489502
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.42103880643844604
Validation loss = 0.41921788454055786
Validation loss = 0.416149765253067
Validation loss = 0.41974571347236633
Validation loss = 0.4189889430999756
Validation loss = 0.4182629883289337
Validation loss = 0.42144376039505005
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4193016290664673
Validation loss = 0.42034217715263367
Validation loss = 0.4181528091430664
Validation loss = 0.4186595678329468
Validation loss = 0.4176960587501526
Validation loss = 0.419913649559021
Validation loss = 0.41852977871894836
Validation loss = 0.41854754090309143
Validation loss = 0.42267781496047974
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4213463068008423
Validation loss = 0.4191221594810486
Validation loss = 0.4180007576942444
Validation loss = 0.4184040427207947
Validation loss = 0.4191831052303314
Validation loss = 0.41901665925979614
Validation loss = 0.4171617925167084
Validation loss = 0.41834399104118347
Validation loss = 0.4201272428035736
Validation loss = 0.4185362756252289
Validation loss = 0.419875830411911
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.41993242502212524
Validation loss = 0.4186467230319977
Validation loss = 0.4221448004245758
Validation loss = 0.4196769595146179
Validation loss = 0.4223473370075226
Validation loss = 0.42518025636672974
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.12269938650306748
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.12232415902140673
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.12195121951219512
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.12462006079027356
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.12727272727272726
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.1299093655589124
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.13253012048192772
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.13213213213213212
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.1377245508982036
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.1373134328358209
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.13690476190476192
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.13649851632047477
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.1390532544378698
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.1415929203539823
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.14411764705882352
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.1436950146627566
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.14327485380116958
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.14285714285714285
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.14244186046511628
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.14492753623188406
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.15028901734104047
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.15561959654178675
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.15804597701149425
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.16045845272206305
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.16
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 12       |
| MaximumReturn | 198      |
| MinimumReturn | 129      |
| TotalSamples  | 46662    |
----------------------------
itr #13 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.41629016399383545
Validation loss = 0.4144285023212433
Validation loss = 0.41691863536834717
Validation loss = 0.4198651611804962
Validation loss = 0.41802966594696045
Validation loss = 0.41570621728897095
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.418408066034317
Validation loss = 0.41709405183792114
Validation loss = 0.421208918094635
Validation loss = 0.41967231035232544
Validation loss = 0.4172317683696747
Validation loss = 0.4202447831630707
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4196406304836273
Validation loss = 0.4192962646484375
Validation loss = 0.42089369893074036
Validation loss = 0.42049214243888855
Validation loss = 0.41970568895339966
Validation loss = 0.42295894026756287
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4180886745452881
Validation loss = 0.4188116192817688
Validation loss = 0.41913384199142456
Validation loss = 0.4193201959133148
Validation loss = 0.4210359454154968
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.42093244194984436
Validation loss = 0.4213884472846985
Validation loss = 0.423653781414032
Validation loss = 0.4223523437976837
Validation loss = 0.4240529537200928
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.15954415954415954
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.1590909090909091
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.16147308781869688
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.16101694915254236
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.16056338028169015
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.1601123595505618
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.15966386554621848
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.15921787709497207
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.1615598885793872
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.16111111111111112
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.16066481994459833
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.16022099447513813
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.15977961432506887
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.15934065934065933
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.1589041095890411
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.16393442622950818
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.16893732970027248
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.16847826086956522
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.17073170731707318
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.17567567567567569
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.1752021563342318
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.1774193548387097
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.1769436997319035
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.17647058823529413
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.176
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 179      |
| Iteration     | 13       |
| MaximumReturn | 199      |
| MinimumReturn | 122      |
| TotalSamples  | 49995    |
----------------------------
itr #14 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4218023419380188
Validation loss = 0.4238507151603699
Validation loss = 0.4212055504322052
Validation loss = 0.4238003194332123
Validation loss = 0.4234257936477661
Validation loss = 0.4225625693798065
Validation loss = 0.42083144187927246
Validation loss = 0.4228062331676483
Validation loss = 0.4214613437652588
Validation loss = 0.423376202583313
Validation loss = 0.4228667914867401
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.42332878708839417
Validation loss = 0.42335593700408936
Validation loss = 0.42226850986480713
Validation loss = 0.42430174350738525
Validation loss = 0.4242175221443176
Validation loss = 0.4241340756416321
Validation loss = 0.42495015263557434
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.42725634574890137
Validation loss = 0.42619022727012634
Validation loss = 0.4254339635372162
Validation loss = 0.42826369404792786
Validation loss = 0.42486608028411865
Validation loss = 0.4280858337879181
Validation loss = 0.42796143889427185
Validation loss = 0.42777904868125916
Validation loss = 0.42859604954719543
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.42421653866767883
Validation loss = 0.4232059121131897
Validation loss = 0.4238505959510803
Validation loss = 0.42272135615348816
Validation loss = 0.4235988259315491
Validation loss = 0.4256289601325989
Validation loss = 0.42667800188064575
Validation loss = 0.4269461929798126
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4280143082141876
Validation loss = 0.42523738741874695
Validation loss = 0.42557671666145325
Validation loss = 0.42813655734062195
Validation loss = 0.42880791425704956
Validation loss = 0.43061408400535583
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.17553191489361702
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.17771883289124668
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.17989417989417988
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.17941952506596306
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.17894736842105263
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.18110236220472442
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.1806282722513089
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.18276762402088773
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.18489583333333334
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.18441558441558442
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.18393782383419688
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.18604651162790697
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.19072164948453607
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.19023136246786632
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.18974358974358974
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.19437340153452684
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.19642857142857142
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.19592875318066158
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.19543147208121828
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.1949367088607595
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.19696969696969696
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.20151133501259447
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.20100502512562815
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.20551378446115287
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.205
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 14       |
| MaximumReturn | 199      |
| MinimumReturn | 126      |
| TotalSamples  | 53328    |
----------------------------
itr #15 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4270866811275482
Validation loss = 0.42723405361175537
Validation loss = 0.42786481976509094
Validation loss = 0.42782941460609436
Validation loss = 0.4266613721847534
Validation loss = 0.4291553497314453
Validation loss = 0.4290276765823364
Validation loss = 0.4301600754261017
Validation loss = 0.4298146963119507
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4261375963687897
Validation loss = 0.4291205108165741
Validation loss = 0.42794355750083923
Validation loss = 0.42935436964035034
Validation loss = 0.42919132113456726
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4323601722717285
Validation loss = 0.43030500411987305
Validation loss = 0.4327036738395691
Validation loss = 0.43650272488594055
Validation loss = 0.43545401096343994
Validation loss = 0.43374887108802795
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.42814159393310547
Validation loss = 0.4321193993091583
Validation loss = 0.43003082275390625
Validation loss = 0.4319097697734833
Validation loss = 0.4311113953590393
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4307076036930084
Validation loss = 0.4302099943161011
Validation loss = 0.4303315281867981
Validation loss = 0.43100181221961975
Validation loss = 0.43310996890068054
Validation loss = 0.43320149183273315
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.20698254364089774
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.21393034825870647
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.21339950372208435
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.2202970297029703
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.22469135802469137
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.22413793103448276
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.22358722358722358
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.22549019607843138
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.22493887530562348
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.22926829268292684
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.22871046228710462
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.23058252427184467
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.23728813559322035
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.2391304347826087
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.2385542168674699
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.24519230769230768
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.24940047961630696
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.25598086124401914
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.26014319809069214
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.26666666666666666
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.27315914489311166
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.27488151658767773
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.27423167848699764
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.2783018867924528
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.28
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 15       |
| MaximumReturn | 198      |
| MinimumReturn | 122      |
| TotalSamples  | 56661    |
----------------------------
itr #16 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4316464066505432
Validation loss = 0.43271976709365845
Validation loss = 0.4336504638195038
Validation loss = 0.4347913861274719
Validation loss = 0.43420252203941345
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.432955801486969
Validation loss = 0.430765300989151
Validation loss = 0.43279528617858887
Validation loss = 0.431706041097641
Validation loss = 0.4342481791973114
Validation loss = 0.43158504366874695
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4372231662273407
Validation loss = 0.43612509965896606
Validation loss = 0.438131719827652
Validation loss = 0.43715330958366394
Validation loss = 0.4392683506011963
Validation loss = 0.4371406137943268
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.43206697702407837
Validation loss = 0.43262478709220886
Validation loss = 0.432891309261322
Validation loss = 0.432778537273407
Validation loss = 0.434550940990448
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.43703415989875793
Validation loss = 0.4349000155925751
Validation loss = 0.4341058135032654
Validation loss = 0.43608975410461426
Validation loss = 0.43536338210105896
Validation loss = 0.4354831576347351
Validation loss = 0.436308890581131
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.284037558685446
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.28805620608899296
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.29205607476635514
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.3006993006993007
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.3023255813953488
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.3062645011600928
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.3055555555555556
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.30484988452655887
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.31336405529953915
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.3195402298850575
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.31880733944954126
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.3180778032036613
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.3219178082191781
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.32801822323462415
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.32954545454545453
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.3356009070294785
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.3416289592760181
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.3431151241534989
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.3490990990990991
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.34831460674157305
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.35201793721973096
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.3534675615212528
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.3549107142857143
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.3585746102449889
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.36444444444444446
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 16       |
| MaximumReturn | 196      |
| MinimumReturn | 115      |
| TotalSamples  | 59994    |
----------------------------
itr #17 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.43690773844718933
Validation loss = 0.4369949400424957
Validation loss = 0.4354153275489807
Validation loss = 0.43484872579574585
Validation loss = 0.43795356154441833
Validation loss = 0.43904435634613037
Validation loss = 0.4368232488632202
Validation loss = 0.43996232748031616
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.43420878052711487
Validation loss = 0.4374365210533142
Validation loss = 0.436252236366272
Validation loss = 0.4363088011741638
Validation loss = 0.4349153935909271
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.440251886844635
Validation loss = 0.44156068563461304
Validation loss = 0.43952903151512146
Validation loss = 0.44218316674232483
Validation loss = 0.4427247941493988
Validation loss = 0.4411129057407379
Validation loss = 0.4448522627353668
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.43574297428131104
Validation loss = 0.43605712056159973
Validation loss = 0.43730226159095764
Validation loss = 0.4368537962436676
Validation loss = 0.43588969111442566
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4384952485561371
Validation loss = 0.4407070577144623
Validation loss = 0.4392833411693573
Validation loss = 0.439540296792984
Validation loss = 0.4426020383834839
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.37250554323725055
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.3827433628318584
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.39072847682119205
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.40308370044052866
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 0.4197802197802198
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.4276315789473684
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.43326039387308535
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.4410480349344978
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.45098039215686275
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.45
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.4490238611713666
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.45454545454545453
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.46220302375809935
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.47198275862068967
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.4817204301075269
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.48497854077253216
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.4860813704496788
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.49786324786324787
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.5031982942430704
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.5148936170212766
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.5201698513800425
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.5233050847457628
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.5306553911205074
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.5358649789029536
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.5389473684210526
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 152      |
| Iteration     | 17       |
| MaximumReturn | 195      |
| MinimumReturn | 111      |
| TotalSamples  | 63327    |
----------------------------
itr #18 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.44066768884658813
Validation loss = 0.43787217140197754
Validation loss = 0.43898847699165344
Validation loss = 0.4416390657424927
Validation loss = 0.44001540541648865
Validation loss = 0.44093722105026245
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4355679452419281
Validation loss = 0.4403183162212372
Validation loss = 0.43584707379341125
Validation loss = 0.4379116892814636
Validation loss = 0.43877577781677246
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.44253361225128174
Validation loss = 0.44132015109062195
Validation loss = 0.4425337314605713
Validation loss = 0.44315776228904724
Validation loss = 0.44342586398124695
Validation loss = 0.4447428286075592
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4433228671550751
Validation loss = 0.43933743238449097
Validation loss = 0.4374159276485443
Validation loss = 0.43731942772865295
Validation loss = 0.43938538432121277
Validation loss = 0.4403381049633026
Validation loss = 0.4394768476486206
Validation loss = 0.44182273745536804
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.44025135040283203
Validation loss = 0.44001004099845886
Validation loss = 0.4423345625400543
Validation loss = 0.4437527060508728
Validation loss = 0.4435081481933594
Validation loss = 0.44294464588165283
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.5462184873949579
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.5576519916142557
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 0.5711297071129707
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.5720250521920668
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.58125
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.5904365904365905
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.5892116182572614
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.5900621118012422
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.5991735537190083
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.6041237113402061
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 0.6193415637860082
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.62217659137577
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.6331967213114754
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.6339468302658486
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.6346938775510204
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.6334012219959266
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.6382113821138211
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.6450304259634888
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.6538461538461539
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.6585858585858586
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.6693548387096774
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.6720321931589537
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.6827309236947792
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.685370741482966
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.688
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 18       |
| MaximumReturn | 198      |
| MinimumReturn | 121      |
| TotalSamples  | 66660    |
----------------------------
itr #19 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.44395190477371216
Validation loss = 0.4431016445159912
Validation loss = 0.445935994386673
Validation loss = 0.4438589811325073
Validation loss = 0.44413289427757263
Validation loss = 0.4433102011680603
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.44076624512672424
Validation loss = 0.4404122233390808
Validation loss = 0.4397459030151367
Validation loss = 0.4426017999649048
Validation loss = 0.44062814116477966
Validation loss = 0.44181859493255615
Validation loss = 0.4433170258998871
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.44505763053894043
Validation loss = 0.44644615054130554
Validation loss = 0.4485519826412201
Validation loss = 0.44787275791168213
Validation loss = 0.4490281343460083
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.44213613867759705
Validation loss = 0.44113487005233765
Validation loss = 0.4444958567619324
Validation loss = 0.4435904324054718
Validation loss = 0.44538965821266174
Validation loss = 0.4431948661804199
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4437508285045624
Validation loss = 0.4452679753303528
Validation loss = 0.4451739192008972
Validation loss = 0.44415661692619324
Validation loss = 0.4460965096950531
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.6966067864271457
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.7071713147410359
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 0.7216699801192843
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 0.7361111111111112
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.7405940594059406
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.741106719367589
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.7514792899408284
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.7598425196850394
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 0.7760314341846758
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.7823529411764706
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 0.7945205479452054
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 0.8125
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 0.8245614035087719
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.8326848249027238
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.8349514563106796
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.8391472868217055
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.8375241779497099
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.8416988416988417
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.8497109826589595
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.8596153846153847
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.8637236084452975
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.8735632183908046
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 0.8910133843212237
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 0.9103053435114504
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.92
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 19       |
| MaximumReturn | 198      |
| MinimumReturn | 114      |
| TotalSamples  | 69993    |
----------------------------
itr #20 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4463852047920227
Validation loss = 0.4494776427745819
Validation loss = 0.447783499956131
Validation loss = 0.44794103503227234
Validation loss = 0.44964170455932617
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.44389137625694275
Validation loss = 0.4449155330657959
Validation loss = 0.4454100728034973
Validation loss = 0.4445335566997528
Validation loss = 0.4460715353488922
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.44994470477104187
Validation loss = 0.44994059205055237
Validation loss = 0.4521087408065796
Validation loss = 0.4511857032775879
Validation loss = 0.4522133767604828
Validation loss = 0.4536955952644348
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.44668900966644287
Validation loss = 0.4451482892036438
Validation loss = 0.44729456305503845
Validation loss = 0.44605138897895813
Validation loss = 0.44752126932144165
Validation loss = 0.4519878923892975
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4461994469165802
Validation loss = 0.4493391215801239
Validation loss = 0.44947004318237305
Validation loss = 0.4507695734500885
Validation loss = 0.44975098967552185
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.9296577946768061
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.9354838709677419
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.9450757575757576
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.943289224952741
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.9528301886792453
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.9623352165725048
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 0.981203007518797
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.9906191369606003
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.9925093632958801
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 1.0205223880597014
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 1.0242085661080074
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 1.0446096654275092
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 1.0630797773654916
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.0703703703703704
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.0813308687615526
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.0922509225092252
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.0994475138121547
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 1.0974264705882353
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.110091743119266
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.120879120879121
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 1.1243144424131628
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.1332116788321167
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.1420765027322404
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.1527272727272728
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 20       |
| MaximumReturn | 198      |
| MinimumReturn | 105      |
| TotalSamples  | 73326    |
----------------------------
itr #21 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4486026167869568
Validation loss = 0.44864070415496826
Validation loss = 0.45026180148124695
Validation loss = 0.45173823833465576
Validation loss = 0.45075422525405884
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.446856290102005
Validation loss = 0.4456554651260376
Validation loss = 0.44534412026405334
Validation loss = 0.44710594415664673
Validation loss = 0.4461715519428253
Validation loss = 0.4456549286842346
Validation loss = 0.44918230175971985
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4544195532798767
Validation loss = 0.4524054527282715
Validation loss = 0.45269477367401123
Validation loss = 0.45393338799476624
Validation loss = 0.45588746666908264
Validation loss = 0.4561224579811096
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.44916555285453796
Validation loss = 0.44915348291397095
Validation loss = 0.449741005897522
Validation loss = 0.44878149032592773
Validation loss = 0.4496799111366272
Validation loss = 0.4490550756454468
Validation loss = 0.4519729018211365
Validation loss = 0.45136961340904236
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.44886937737464905
Validation loss = 0.44995519518852234
Validation loss = 0.4491707384586334
Validation loss = 0.45015567541122437
Validation loss = 0.4527410864830017
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 1.1705989110707804
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.181159420289855
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 1.1790235081374323
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.1913357400722022
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 1.190990990990991
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 1.2050359712230216
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 1.2262118491921006
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.2365591397849462
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 1.2683363148479427
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 1.2821428571428573
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 1.2834224598930482
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.290035587188612
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 1.3037300177619893
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.3156028368794326
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 1.3292035398230089
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.3409893992932862
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 1.3562610229276897
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.3609154929577465
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 1.360281195079086
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.3666666666666667
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.3730297723292468
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 1.3723776223776223
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.3804537521815008
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.386759581881533
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 1.4069565217391304
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 159      |
| Iteration     | 21       |
| MaximumReturn | 198      |
| MinimumReturn | 119      |
| TotalSamples  | 76659    |
----------------------------
itr #22 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4491840600967407
Validation loss = 0.45069605112075806
Validation loss = 0.45112618803977966
Validation loss = 0.4502720534801483
Validation loss = 0.452923059463501
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.44861435890197754
Validation loss = 0.4499566853046417
Validation loss = 0.44836896657943726
Validation loss = 0.4501366913318634
Validation loss = 0.44891878962516785
Validation loss = 0.4522712230682373
Validation loss = 0.4511091709136963
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.45716485381126404
Validation loss = 0.4530647099018097
Validation loss = 0.4579624831676483
Validation loss = 0.45660385489463806
Validation loss = 0.45694777369499207
Validation loss = 0.45782461762428284
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4510253965854645
Validation loss = 0.4511297047138214
Validation loss = 0.45095863938331604
Validation loss = 0.4533926248550415
Validation loss = 0.4529436528682709
Validation loss = 0.45304393768310547
Validation loss = 0.45376381278038025
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4509895145893097
Validation loss = 0.45061948895454407
Validation loss = 0.45531392097473145
Validation loss = 0.45394888520240784
Validation loss = 0.45383429527282715
Validation loss = 0.4527837336063385
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 1.4236111111111112
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.4315424610051992
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.4429065743944636
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 1.4663212435233162
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 1.4879310344827585
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 1.5077452667814113
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.5171821305841924
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.523156089193825
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.5342465753424657
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 1.5572649572649573
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 1.581911262798635
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 1.5996592844974447
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 1.6275510204081634
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 1.6502546689303905
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 1.6694915254237288
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 1.6666666666666667
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.677364864864865
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 1.691399662731872
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.7003367003367003
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 1.7176470588235293
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 1.7147651006711409
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 1.7152428810720268
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.725752508361204
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 1.7412353923205341
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 1.76
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 22       |
| MaximumReturn | 198      |
| MinimumReturn | 123      |
| TotalSamples  | 79992    |
----------------------------
itr #23 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.45186692476272583
Validation loss = 0.45256537199020386
Validation loss = 0.45240306854248047
Validation loss = 0.4527455270290375
Validation loss = 0.4526873230934143
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4502771496772766
Validation loss = 0.45278993248939514
Validation loss = 0.4509601593017578
Validation loss = 0.4517236649990082
Validation loss = 0.4523530900478363
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4603787958621979
Validation loss = 0.46147385239601135
Validation loss = 0.4576706886291504
Validation loss = 0.45823612809181213
Validation loss = 0.4622071385383606
Validation loss = 0.4602091312408447
Validation loss = 0.46093541383743286
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.45369893312454224
Validation loss = 0.45253461599349976
Validation loss = 0.45518335700035095
Validation loss = 0.4551887512207031
Validation loss = 0.4555519223213196
Validation loss = 0.45882105827331543
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.45298853516578674
Validation loss = 0.4533640742301941
Validation loss = 0.455753892660141
Validation loss = 0.45358410477638245
Validation loss = 0.45530176162719727
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 1.7920133111480865
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 1.8172757475083057
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 1.8391376451077943
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 1.8675496688741722
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 1.884297520661157
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 1.9092409240924093
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 1.9242174629324547
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 1.949013157894737
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 1.9671592775041051
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 1.981967213114754
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 1.9934533551554827
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 2.0163398692810457
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 2.0146818923327894
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 2.027687296416938
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 2.0439024390243903
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 2.0584415584415585
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 2.0745542949756888
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 2.0906148867313914
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 2.109854604200323
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 2.129032258064516
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 2.1481481481481484
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 2.152733118971061
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 2.173354735152488
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 2.1939102564102564
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 2.208
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 159      |
| Iteration     | 23       |
| MaximumReturn | 199      |
| MinimumReturn | 110      |
| TotalSamples  | 83325    |
----------------------------
itr #24 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4516885280609131
Validation loss = 0.45117005705833435
Validation loss = 0.4542069733142853
Validation loss = 0.4544627070426941
Validation loss = 0.4523337185382843
Validation loss = 0.4524608254432678
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4526059329509735
Validation loss = 0.45201441645622253
Validation loss = 0.45199453830718994
Validation loss = 0.45269668102264404
Validation loss = 0.4526434540748596
Validation loss = 0.4525822103023529
Validation loss = 0.45445260405540466
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4597999155521393
Validation loss = 0.4580451250076294
Validation loss = 0.45850706100463867
Validation loss = 0.4608432352542877
Validation loss = 0.4614790380001068
Validation loss = 0.46273255348205566
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.45643559098243713
Validation loss = 0.45439115166664124
Validation loss = 0.4541502296924591
Validation loss = 0.4544418752193451
Validation loss = 0.4563305974006653
Validation loss = 0.4593619704246521
Validation loss = 0.4567033350467682
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.45114225149154663
Validation loss = 0.45279285311698914
Validation loss = 0.45240575075149536
Validation loss = 0.45270994305610657
Validation loss = 0.45328646898269653
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 2.236421725239617
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 2.2472089314194577
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 2.251592356687898
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 2.259141494435612
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 2.2825396825396824
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 2.301109350237718
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 2.3132911392405062
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 2.322274881516588
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 2.329652996845426
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 2.3338582677165354
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 2.356918238993711
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 2.3594976452119307
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 2.3683385579937304
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 2.383411580594679
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 2.3984375
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 2.418096723868955
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 2.445482866043614
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 2.457231726283048
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 2.472049689440994
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 2.4852713178294574
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 2.5046439628482973
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 2.516228748068006
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 2.5231481481481484
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 2.5254237288135593
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 2.5384615384615383
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 24       |
| MaximumReturn | 199      |
| MinimumReturn | 112      |
| TotalSamples  | 86658    |
----------------------------
itr #25 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4505293667316437
Validation loss = 0.4513314366340637
Validation loss = 0.4528261125087738
Validation loss = 0.4511564373970032
Validation loss = 0.4562290906906128
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.45406776666641235
Validation loss = 0.4510970115661621
Validation loss = 0.4523773193359375
Validation loss = 0.45420509576797485
Validation loss = 0.45418328046798706
Validation loss = 0.4575464427471161
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.46117693185806274
Validation loss = 0.46043747663497925
Validation loss = 0.461070716381073
Validation loss = 0.46054357290267944
Validation loss = 0.46241769194602966
Validation loss = 0.4622664451599121
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.45451948046684265
Validation loss = 0.45440036058425903
Validation loss = 0.4536452293395996
Validation loss = 0.4553377330303192
Validation loss = 0.4581255316734314
Validation loss = 0.4572697579860687
Validation loss = 0.4564639627933502
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.45289963483810425
Validation loss = 0.45365777611732483
Validation loss = 0.45185399055480957
Validation loss = 0.4547168016433716
Validation loss = 0.4546433687210083
Validation loss = 0.45863306522369385
Validation loss = 0.4541812837123871
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 2.554531490015361
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 2.5644171779141103
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 2.57427258805513
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 2.585626911314985
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 2.593893129770992
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 2.6128048780487805
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 2.6316590563165905
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 2.648936170212766
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 2.6631259484066767
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 2.6893939393939394
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 2.708018154311649
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 2.716012084592145
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 2.717948717948718
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 2.7454819277108435
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 2.7639097744360903
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 2.7732732732732734
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 2.7946026986506745
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 2.808383233532934
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 2.8251121076233185
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 2.8402985074626868
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 2.849478390461997
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 2.8675595238095237
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 2.8915304606240713
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 2.913946587537092
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 2.9229629629629628
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 25       |
| MaximumReturn | 199      |
| MinimumReturn | 119      |
| TotalSamples  | 89991    |
----------------------------
itr #26 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4523257911205292
Validation loss = 0.4546548128128052
Validation loss = 0.453265517950058
Validation loss = 0.45400989055633545
Validation loss = 0.4531837999820709
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4534284770488739
Validation loss = 0.45422273874282837
Validation loss = 0.45469778776168823
Validation loss = 0.45449960231781006
Validation loss = 0.4558252990245819
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.46092817187309265
Validation loss = 0.4606052339076996
Validation loss = 0.4593961536884308
Validation loss = 0.46384096145629883
Validation loss = 0.46361368894577026
Validation loss = 0.4637550413608551
Validation loss = 0.46226081252098083
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.45773768424987793
Validation loss = 0.45721909403800964
Validation loss = 0.4568260610103607
Validation loss = 0.45841044187545776
Validation loss = 0.45778796076774597
Validation loss = 0.4592570960521698
Validation loss = 0.4592216908931732
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.45408350229263306
Validation loss = 0.4563518166542053
Validation loss = 0.45466148853302
Validation loss = 0.45431679487228394
Validation loss = 0.4568527638912201
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 2.940828402366864
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 2.9630723781388477
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 2.9867256637168142
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 3.0103092783505154
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 3.0323529411764705
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 3.0469897209985315
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 3.0821114369501466
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 3.111273792093704
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 3.1374269005847952
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 3.1576642335766425
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 3.1793002915451893
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 3.2037845705967976
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 3.2238372093023258
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 22
average number of affinization = 3.251088534107402
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 3.272463768115942
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 3.289435600578871
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 26
average number of affinization = 3.3222543352601157
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 24
average number of affinization = 3.352092352092352
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 3.367435158501441
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 3.366906474820144
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 3.3879310344827585
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 3.411764705882353
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 3.4426934097421205
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 3.463519313304721
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 3.4785714285714286
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 160      |
| Iteration     | 26       |
| MaximumReturn | 199      |
| MinimumReturn | 112      |
| TotalSamples  | 93324    |
----------------------------
itr #27 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.45415976643562317
Validation loss = 0.45204564929008484
Validation loss = 0.45270615816116333
Validation loss = 0.45301640033721924
Validation loss = 0.4536360800266266
Validation loss = 0.45559054613113403
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.45564574003219604
Validation loss = 0.4535530209541321
Validation loss = 0.4549601972103119
Validation loss = 0.4546446204185486
Validation loss = 0.4578699469566345
Validation loss = 0.45643413066864014
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.46274757385253906
Validation loss = 0.46041297912597656
Validation loss = 0.46182918548583984
Validation loss = 0.46323737502098083
Validation loss = 0.46396708488464355
Validation loss = 0.46277615427970886
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4593958258628845
Validation loss = 0.4572109282016754
Validation loss = 0.45898470282554626
Validation loss = 0.46048206090927124
Validation loss = 0.4600614607334137
Validation loss = 0.4607662856578827
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4557320177555084
Validation loss = 0.45409953594207764
Validation loss = 0.4551035463809967
Validation loss = 0.45644643902778625
Validation loss = 0.45725589990615845
Validation loss = 0.458771675825119
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 3.5021398002853066
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 3.517094017094017
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 3.536273115220484
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 3.544034090909091
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 29
average number of affinization = 3.580141843971631
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 24
average number of affinization = 3.6090651558073654
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 3.6308345120226306
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 3.6483050847457625
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 3.669957686882934
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 3.6943661971830988
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 3.721518987341772
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 24
average number of affinization = 3.75
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 3.7545582047685833
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 3.7815126050420167
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 3.8111888111888113
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 3.8240223463687153
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 28
average number of affinization = 3.8577405857740588
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 3.870473537604457
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 3.888734353268428
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 3.904166666666667
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 3.9361997226074896
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 22
average number of affinization = 3.961218836565097
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 22
average number of affinization = 3.9861687413554634
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 30
average number of affinization = 4.0220994475138125
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 24
average number of affinization = 4.049655172413793
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 27       |
| MaximumReturn | 199      |
| MinimumReturn | 114      |
| TotalSamples  | 96657    |
----------------------------
itr #28 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.45267224311828613
Validation loss = 0.4539353847503662
Validation loss = 0.45344462990760803
Validation loss = 0.45348310470581055
Validation loss = 0.4541458785533905
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.454100638628006
Validation loss = 0.4525684416294098
Validation loss = 0.453366756439209
Validation loss = 0.4545409679412842
Validation loss = 0.45523667335510254
Validation loss = 0.4543808400630951
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4639308452606201
Validation loss = 0.4620455801486969
Validation loss = 0.46507367491722107
Validation loss = 0.46407225728034973
Validation loss = 0.4663503170013428
Validation loss = 0.46423423290252686
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4563249349594116
Validation loss = 0.45808520913124084
Validation loss = 0.45756372809410095
Validation loss = 0.4589911997318268
Validation loss = 0.4604872763156891
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.45294806361198425
Validation loss = 0.45486029982566833
Validation loss = 0.4541613757610321
Validation loss = 0.45642054080963135
Validation loss = 0.4566849172115326
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 4.059228650137741
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 4.0880330123796425
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 24
average number of affinization = 4.115384615384615
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 4.141289437585734
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 24
average number of affinization = 4.168493150684932
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 24
average number of affinization = 4.19562243502052
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 4.224043715846994
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 4.245566166439291
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 4.258855585831062
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 4.284353741496599
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 31
average number of affinization = 4.320652173913044
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 4.335142469470828
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 4.356368563685637
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 4.377537212449256
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 4.391891891891892
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 22
average number of affinization = 4.415654520917679
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 22
average number of affinization = 4.439353099730458
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 4.457604306864065
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 26
average number of affinization = 4.486559139784946
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 4.5087248322147655
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 4.533512064343164
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 26
average number of affinization = 4.562248995983936
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 24
average number of affinization = 4.588235294117647
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 4.608811748998665
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 4.624
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 28       |
| MaximumReturn | 199      |
| MinimumReturn | 107      |
| TotalSamples  | 99990    |
----------------------------
itr #29 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.45391494035720825
Validation loss = 0.4515528082847595
Validation loss = 0.4538755714893341
Validation loss = 0.4537089467048645
Validation loss = 0.4540542662143707
Validation loss = 0.4545254111289978
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.45188361406326294
Validation loss = 0.4543898105621338
Validation loss = 0.452353298664093
Validation loss = 0.4550643861293793
Validation loss = 0.4537461996078491
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.46181827783584595
Validation loss = 0.46210017800331116
Validation loss = 0.461576908826828
Validation loss = 0.46193772554397583
Validation loss = 0.46634146571159363
Validation loss = 0.46578383445739746
Validation loss = 0.4650294780731201
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4563572406768799
Validation loss = 0.45824775099754333
Validation loss = 0.4551389217376709
Validation loss = 0.4563906490802765
Validation loss = 0.45889246463775635
Validation loss = 0.4573974907398224
Validation loss = 0.45675554871559143
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4541647434234619
Validation loss = 0.4536148011684418
Validation loss = 0.45474132895469666
Validation loss = 0.4556216895580292
Validation loss = 0.45505648851394653
Validation loss = 0.4532949924468994
Validation loss = 0.45652279257774353
Validation loss = 0.45575061440467834
Validation loss = 0.4574830234050751
Validation loss = 0.4577454626560211
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 4.637816245006658
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 34
average number of affinization = 4.67686170212766
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 4.706507304116866
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 4.718832891246684
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 22
average number of affinization = 4.741721854304636
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 37
average number of affinization = 4.784391534391534
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 4.805812417437252
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 4.83509234828496
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 29
average number of affinization = 4.866930171277997
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 4.8881578947368425
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 4.909329829172142
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 4.925196850393701
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 4.946264744429882
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 4.975130890052356
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 4.998692810457516
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 22
average number of affinization = 5.02088772845953
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 5.044328552803129
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 5.061197916666667
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 28
average number of affinization = 5.091027308192458
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 5.106493506493506
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 5.124513618677042
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 5.139896373056994
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 5.155239327296249
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 5.183462532299742
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 5.19483870967742
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 29       |
| MaximumReturn | 199      |
| MinimumReturn | 115      |
| TotalSamples  | 103323   |
----------------------------
itr #30 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.45319363474845886
Validation loss = 0.45376870036125183
Validation loss = 0.4548806846141815
Validation loss = 0.4544210135936737
Validation loss = 0.45423978567123413
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.45437124371528625
Validation loss = 0.45349815487861633
Validation loss = 0.4540814459323883
Validation loss = 0.4557279348373413
Validation loss = 0.455353707075119
Validation loss = 0.4561154544353485
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.46308383345603943
Validation loss = 0.4636019468307495
Validation loss = 0.46382996439933777
Validation loss = 0.4652981162071228
Validation loss = 0.46583566069602966
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4571108818054199
Validation loss = 0.4566667675971985
Validation loss = 0.45715582370758057
Validation loss = 0.4579940736293793
Validation loss = 0.459462434053421
Validation loss = 0.46032723784446716
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4601243734359741
Validation loss = 0.4563347399234772
Validation loss = 0.4586855173110962
Validation loss = 0.4581599831581116
Validation loss = 0.4576103389263153
Validation loss = 0.457258939743042
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 26
average number of affinization = 5.221649484536083
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 5.24967824967825
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 5.272493573264781
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 5.291399229781772
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 24
average number of affinization = 5.315384615384615
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 5.330345710627401
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 5.3554987212276215
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 29
average number of affinization = 5.385696040868455
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 36
average number of affinization = 5.424744897959184
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 26
average number of affinization = 5.450955414012739
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 5.484732824427481
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 28
average number of affinization = 5.513341804320203
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 5.522842639593908
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 31
average number of affinization = 5.555133079847908
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 5.582278481012659
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 34
average number of affinization = 5.618204804045512
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 24
average number of affinization = 5.641414141414141
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 5.648171500630517
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 22
average number of affinization = 5.668765743073048
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 5.693081761006289
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 24
average number of affinization = 5.71608040201005
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 31
average number of affinization = 5.74780426599749
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 5.771929824561403
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 5.787234042553192
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 5.80125
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 30       |
| MaximumReturn | 198      |
| MinimumReturn | 122      |
| TotalSamples  | 106656   |
----------------------------
itr #31 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.45162272453308105
Validation loss = 0.4520230293273926
Validation loss = 0.4552912414073944
Validation loss = 0.45271822810173035
Validation loss = 0.45331814885139465
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4514862596988678
Validation loss = 0.4555990695953369
Validation loss = 0.45329222083091736
Validation loss = 0.45579272508621216
Validation loss = 0.4539019465446472
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4619760811328888
Validation loss = 0.4625934362411499
Validation loss = 0.4625403583049774
Validation loss = 0.4617403447628021
Validation loss = 0.4655577540397644
Validation loss = 0.4643842577934265
Validation loss = 0.46422436833381653
Validation loss = 0.4665217995643616
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4591381251811981
Validation loss = 0.45824217796325684
Validation loss = 0.45760244131088257
Validation loss = 0.45836836099624634
Validation loss = 0.4598211944103241
Validation loss = 0.45943471789360046
Validation loss = 0.45779353380203247
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.45636963844299316
Validation loss = 0.4566567540168762
Validation loss = 0.4563262164592743
Validation loss = 0.4567905068397522
Validation loss = 0.458125501871109
Validation loss = 0.46121808886528015
Validation loss = 0.4574046730995178
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 5.82521847690387
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 5.85785536159601
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 5.8816936488169365
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 29
average number of affinization = 5.91044776119403
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 5.93664596273292
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 5.9689826302729525
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 31
average number of affinization = 6.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 6.033415841584159
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 6.065512978986403
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 6.091358024691358
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 6.103575832305795
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 29
average number of affinization = 6.1317733990147785
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 6.138991389913899
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 36
average number of affinization = 6.175675675675675
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 35
average number of affinization = 6.211042944785276
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 22
average number of affinization = 6.230392156862745
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 24
average number of affinization = 6.252141982864137
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 35
average number of affinization = 6.287286063569682
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 36
average number of affinization = 6.323565323565323
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 24
average number of affinization = 6.345121951219512
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 24
average number of affinization = 6.366626065773447
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 6.397810218978102
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 31
average number of affinization = 6.427703523693803
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 26
average number of affinization = 6.451456310679611
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 34
average number of affinization = 6.484848484848484
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 31       |
| MaximumReturn | 199      |
| MinimumReturn | 119      |
| TotalSamples  | 109989   |
----------------------------
itr #32 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.45321956276893616
Validation loss = 0.45326825976371765
Validation loss = 0.45441752672195435
Validation loss = 0.4537295401096344
Validation loss = 0.45407846570014954
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4542723596096039
Validation loss = 0.45489752292633057
Validation loss = 0.4547586143016815
Validation loss = 0.4554996192455292
Validation loss = 0.45373913645744324
Validation loss = 0.4547361135482788
Validation loss = 0.4563603699207306
Validation loss = 0.45705655217170715
Validation loss = 0.45700475573539734
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.46167197823524475
Validation loss = 0.4641653895378113
Validation loss = 0.4643993377685547
Validation loss = 0.46203234791755676
Validation loss = 0.4642866551876068
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.45836570858955383
Validation loss = 0.45962241291999817
Validation loss = 0.4597304165363312
Validation loss = 0.4589550495147705
Validation loss = 0.4597555100917816
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4577631652355194
Validation loss = 0.45611703395843506
Validation loss = 0.45801904797554016
Validation loss = 0.4588432013988495
Validation loss = 0.4562564492225647
Validation loss = 0.4593502879142761
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 30
average number of affinization = 6.513317191283293
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 36
average number of affinization = 6.548972188633615
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 22
average number of affinization = 6.567632850241546
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 34
average number of affinization = 6.600723763570567
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 6.62289156626506
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 6.66546329723225
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 6.6766826923076925
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 6.70828331332533
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 29
average number of affinization = 6.735011990407674
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 26
average number of affinization = 6.75808383233533
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 6.782296650717703
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 28
average number of affinization = 6.807646356033453
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 28
average number of affinization = 6.832935560859188
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 6.8545887961859355
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 6.878571428571429
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 36
average number of affinization = 6.91319857312723
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 35
average number of affinization = 6.946555819477434
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 36
average number of affinization = 6.981020166073547
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 7.010663507109005
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 28
average number of affinization = 7.035502958579881
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 7.07210401891253
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 34
average number of affinization = 7.103896103896104
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 35
average number of affinization = 7.136792452830188
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 26
average number of affinization = 7.159010600706714
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 40
average number of affinization = 7.197647058823529
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 32       |
| MaximumReturn | 199      |
| MinimumReturn | 120      |
| TotalSamples  | 113322   |
----------------------------
itr #33 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4539739191532135
Validation loss = 0.45260050892829895
Validation loss = 0.454486608505249
Validation loss = 0.4566313326358795
Validation loss = 0.456158846616745
Validation loss = 0.45533615350723267
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.45544618368148804
Validation loss = 0.4544714391231537
Validation loss = 0.45499366521835327
Validation loss = 0.45738014578819275
Validation loss = 0.45806601643562317
Validation loss = 0.4568885266780853
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4646165668964386
Validation loss = 0.4657599627971649
Validation loss = 0.4630432426929474
Validation loss = 0.4652930200099945
Validation loss = 0.46551188826560974
Validation loss = 0.4662017226219177
Validation loss = 0.4667963981628418
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.46110042929649353
Validation loss = 0.46023163199424744
Validation loss = 0.4610593318939209
Validation loss = 0.45935362577438354
Validation loss = 0.4612472951412201
Validation loss = 0.4601915180683136
Validation loss = 0.45965856313705444
Validation loss = 0.4626733362674713
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4586893618106842
Validation loss = 0.4589063227176666
Validation loss = 0.4586874842643738
Validation loss = 0.460602343082428
Validation loss = 0.45856308937072754
Validation loss = 0.45886754989624023
Validation loss = 0.4616207480430603
Validation loss = 0.46152207255363464
Validation loss = 0.45967477560043335
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 45
average number of affinization = 7.2420681551116335
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 34
average number of affinization = 7.273474178403756
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 45
average number of affinization = 7.317702227432591
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 7.340749414519906
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 34
average number of affinization = 7.371929824561404
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 7.4077102803738315
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 7.444574095682614
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 7.480186480186481
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 7.516880093131548
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 7.534883720930233
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 37
average number of affinization = 7.569105691056911
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 40
average number of affinization = 7.606728538283063
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 22
average number of affinization = 7.623406720741599
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 52
average number of affinization = 7.674768518518518
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 31
average number of affinization = 7.701734104046243
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 7.736720554272518
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 34
average number of affinization = 7.767012687427912
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 7.802995391705069
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 37
average number of affinization = 7.836593785960875
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 44
average number of affinization = 7.87816091954023
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 49
average number of affinization = 7.925373134328358
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 41
average number of affinization = 7.963302752293578
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 8.002290950744559
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 40
average number of affinization = 8.038901601830664
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 43
average number of affinization = 8.078857142857142
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 33       |
| MaximumReturn | 199      |
| MinimumReturn | 119      |
| TotalSamples  | 116655   |
----------------------------
itr #34 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.454190194606781
Validation loss = 0.4520533084869385
Validation loss = 0.4542672634124756
Validation loss = 0.4546223282814026
Validation loss = 0.45720574259757996
Validation loss = 0.45413684844970703
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4556392729282379
Validation loss = 0.4534073770046234
Validation loss = 0.4556867480278015
Validation loss = 0.4575468599796295
Validation loss = 0.4570634067058563
Validation loss = 0.4581097662448883
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4639323949813843
Validation loss = 0.4659141004085541
Validation loss = 0.4636153280735016
Validation loss = 0.4640071392059326
Validation loss = 0.46543022990226746
Validation loss = 0.4665570855140686
Validation loss = 0.46590670943260193
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.46094924211502075
Validation loss = 0.45797592401504517
Validation loss = 0.461234450340271
Validation loss = 0.4611186981201172
Validation loss = 0.45974138379096985
Validation loss = 0.4618294835090637
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4605783522129059
Validation loss = 0.46310269832611084
Validation loss = 0.45860472321510315
Validation loss = 0.4603007435798645
Validation loss = 0.4620557129383087
Validation loss = 0.4614920914173126
Validation loss = 0.46142107248306274
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 8.114155251141552
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 52
average number of affinization = 8.164196123147093
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 26
average number of affinization = 8.184510250569476
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 44
average number of affinization = 8.225255972696246
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 44
average number of affinization = 8.26590909090909
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 8.280363223609534
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 8.307256235827664
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 8.342015855039637
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 24
average number of affinization = 8.35972850678733
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 8.39774011299435
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 40
average number of affinization = 8.433408577878104
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 8.467869222096956
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 35
average number of affinization = 8.497747747747749
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 45
average number of affinization = 8.53880764904387
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 8.565168539325843
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 40
average number of affinization = 8.600448933782268
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 36
average number of affinization = 8.63116591928251
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 35
average number of affinization = 8.660694288913774
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 41
average number of affinization = 8.696868008948545
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 47
average number of affinization = 8.739664804469275
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 34
average number of affinization = 8.767857142857142
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 30
average number of affinization = 8.791527313266444
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 8.824053452115812
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 44
average number of affinization = 8.863181312569521
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 8.88888888888889
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 163      |
| Iteration     | 34       |
| MaximumReturn | 199      |
| MinimumReturn | 119      |
| TotalSamples  | 119988   |
----------------------------
itr #35 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4533540606498718
Validation loss = 0.4527215361595154
Validation loss = 0.4528012275695801
Validation loss = 0.4560340940952301
Validation loss = 0.4539693593978882
Validation loss = 0.45479100942611694
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4540993869304657
Validation loss = 0.4549958407878876
Validation loss = 0.4546709358692169
Validation loss = 0.4534471333026886
Validation loss = 0.455917626619339
Validation loss = 0.45431873202323914
Validation loss = 0.45759299397468567
Validation loss = 0.4560430347919464
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.46319445967674255
Validation loss = 0.462772011756897
Validation loss = 0.46492835879325867
Validation loss = 0.46372538805007935
Validation loss = 0.46600767970085144
Validation loss = 0.46420514583587646
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.45933258533477783
Validation loss = 0.4594389796257019
Validation loss = 0.4595174491405487
Validation loss = 0.45893481373786926
Validation loss = 0.45927509665489197
Validation loss = 0.4610041379928589
Validation loss = 0.46059563755989075
Validation loss = 0.45807206630706787
Validation loss = 0.46163126826286316
Validation loss = 0.4620192348957062
Validation loss = 0.46500352025032043
Validation loss = 0.4615201950073242
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4575510323047638
Validation loss = 0.4596252739429474
Validation loss = 0.45812731981277466
Validation loss = 0.4592992961406708
Validation loss = 0.4598744213581085
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 40
average number of affinization = 8.923418423973363
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 43
average number of affinization = 8.96119733924612
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 31
average number of affinization = 8.985603543743078
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 56
average number of affinization = 9.037610619469026
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 9.053038674033148
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 9.084988962472407
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 35
average number of affinization = 9.113561190738698
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 9.139867841409691
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 48
average number of affinization = 9.182618261826182
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 9.215384615384615
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 35
average number of affinization = 9.243688254665203
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 9.279605263157896
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 35
average number of affinization = 9.307776560788609
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 9.339168490153172
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 31
average number of affinization = 9.362841530054645
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 9.388646288209607
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 28
average number of affinization = 9.408942202835332
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 40
average number of affinization = 9.442265795206971
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 41
average number of affinization = 9.476605005440696
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 40
average number of affinization = 9.509782608695652
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 9.53528773072747
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 40
average number of affinization = 9.568329718004339
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 45
average number of affinization = 9.606717226435537
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 37
average number of affinization = 9.636363636363637
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 43
average number of affinization = 9.672432432432432
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 35       |
| MaximumReturn | 198      |
| MinimumReturn | 125      |
| TotalSamples  | 123321   |
----------------------------
itr #36 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4529039263725281
Validation loss = 0.45483535528182983
Validation loss = 0.45622244477272034
Validation loss = 0.4530572295188904
Validation loss = 0.4546986222267151
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4564272165298462
Validation loss = 0.4563409686088562
Validation loss = 0.45583266019821167
Validation loss = 0.4564841389656067
Validation loss = 0.4554094970226288
Validation loss = 0.45655256509780884
Validation loss = 0.45720192790031433
Validation loss = 0.45535826683044434
Validation loss = 0.4584367275238037
Validation loss = 0.45871254801750183
Validation loss = 0.4593016505241394
Validation loss = 0.4594179689884186
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.46272268891334534
Validation loss = 0.4651731550693512
Validation loss = 0.46336930990219116
Validation loss = 0.4659605324268341
Validation loss = 0.4642444849014282
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.46038782596588135
Validation loss = 0.4597334861755371
Validation loss = 0.4604683518409729
Validation loss = 0.46223339438438416
Validation loss = 0.4631006121635437
Validation loss = 0.4616033434867859
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.458746075630188
Validation loss = 0.46020638942718506
Validation loss = 0.45780685544013977
Validation loss = 0.459539532661438
Validation loss = 0.45945146679878235
Validation loss = 0.45894932746887207
Validation loss = 0.4587346315383911
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 40
average number of affinization = 9.705183585313176
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 45
average number of affinization = 9.743257820927724
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 37
average number of affinization = 9.772629310344827
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 9.803013993541443
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 49
average number of affinization = 9.845161290322581
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 48
average number of affinization = 9.886143931256713
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 37
average number of affinization = 9.915236051502147
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 49
average number of affinization = 9.957127545551982
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 9.991434689507495
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 49
average number of affinization = 10.033155080213904
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 55
average number of affinization = 10.081196581196581
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 52
average number of affinization = 10.125933831376734
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 10.159914712153519
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 48
average number of affinization = 10.20021299254526
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 10.22340425531915
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 49
average number of affinization = 10.26461211477152
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 40
average number of affinization = 10.296178343949045
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 45
average number of affinization = 10.332979851537646
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 52
average number of affinization = 10.377118644067796
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 40
average number of affinization = 10.408465608465608
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 10.431289640591967
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 10.461457233368533
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 47
average number of affinization = 10.5
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 10.533192834562698
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 46
average number of affinization = 10.570526315789474
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 162      |
| Iteration     | 36       |
| MaximumReturn | 198      |
| MinimumReturn | 117      |
| TotalSamples  | 126654   |
----------------------------
itr #37 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4513203799724579
Validation loss = 0.4505927562713623
Validation loss = 0.45297443866729736
Validation loss = 0.45281943678855896
Validation loss = 0.45334869623184204
Validation loss = 0.45339956879615784
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4579212963581085
Validation loss = 0.45565345883369446
Validation loss = 0.4565391540527344
Validation loss = 0.46017998456954956
Validation loss = 0.4568701386451721
Validation loss = 0.45831298828125
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4623745381832123
Validation loss = 0.4628717005252838
Validation loss = 0.46258607506752014
Validation loss = 0.4626208543777466
Validation loss = 0.4615303575992584
Validation loss = 0.464973121881485
Validation loss = 0.4640418291091919
Validation loss = 0.4635794460773468
Validation loss = 0.4641976058483124
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.45985502004623413
Validation loss = 0.4594084918498993
Validation loss = 0.46230050921440125
Validation loss = 0.46026548743247986
Validation loss = 0.4617212414741516
Validation loss = 0.46019574999809265
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.45847246050834656
Validation loss = 0.45858001708984375
Validation loss = 0.4590153992176056
Validation loss = 0.45845669507980347
Validation loss = 0.4578617215156555
Validation loss = 0.45942893624305725
Validation loss = 0.46113234758377075
Validation loss = 0.45886027812957764
Validation loss = 0.4596804976463318
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 54
average number of affinization = 10.616193480546793
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 48
average number of affinization = 10.655462184873949
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 54
average number of affinization = 10.700944386149002
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 48
average number of affinization = 10.740041928721174
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 56
average number of affinization = 10.787434554973823
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 57
average number of affinization = 10.835774058577407
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 50
average number of affinization = 10.87669801462905
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 45
average number of affinization = 10.91231732776618
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 54
average number of affinization = 10.957247132429615
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 45
average number of affinization = 10.992708333333333
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 11.021852237252862
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 55
average number of affinization = 11.067567567567568
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 54
average number of affinization = 11.11214953271028
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 50
average number of affinization = 11.152489626556017
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 52
average number of affinization = 11.19481865284974
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 67
average number of affinization = 11.252587991718427
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 57
average number of affinization = 11.29989658738366
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 45
average number of affinization = 11.334710743801653
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 46
average number of affinization = 11.37048503611971
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 51
average number of affinization = 11.411340206185567
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 44
average number of affinization = 11.444902162718847
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 41
average number of affinization = 11.475308641975309
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 51
average number of affinization = 11.515930113052415
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 50
average number of affinization = 11.555441478439425
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 48
average number of affinization = 11.592820512820513
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 37       |
| MaximumReturn | 199      |
| MinimumReturn | 113      |
| TotalSamples  | 129987   |
----------------------------
itr #38 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4518496096134186
Validation loss = 0.4523216187953949
Validation loss = 0.45091408491134644
Validation loss = 0.4550754725933075
Validation loss = 0.4518221616744995
Validation loss = 0.45248380303382874
Validation loss = 0.45437848567962646
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4549892544746399
Validation loss = 0.4568713307380676
Validation loss = 0.45542097091674805
Validation loss = 0.45665836334228516
Validation loss = 0.4581485390663147
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4617994427680969
Validation loss = 0.4626102149486542
Validation loss = 0.4616905450820923
Validation loss = 0.4631368815898895
Validation loss = 0.4647672474384308
Validation loss = 0.46529796719551086
Validation loss = 0.46378761529922485
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.45829126238822937
Validation loss = 0.4591445028781891
Validation loss = 0.45865169167518616
Validation loss = 0.4597470164299011
Validation loss = 0.45903125405311584
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.45977702736854553
Validation loss = 0.4580649733543396
Validation loss = 0.456787109375
Validation loss = 0.45977309346199036
Validation loss = 0.4593455493450165
Validation loss = 0.4615594148635864
Validation loss = 0.4606616795063019
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 46
average number of affinization = 11.628073770491802
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 59
average number of affinization = 11.67656090071648
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 37
average number of affinization = 11.702453987730062
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 52
average number of affinization = 11.74361593462717
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 11.774489795918367
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 51
average number of affinization = 11.8144750254842
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 40
average number of affinization = 11.843177189409369
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 50
average number of affinization = 11.881993896236013
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 40
average number of affinization = 11.910569105691057
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 53
average number of affinization = 11.95228426395939
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 58
average number of affinization = 11.998985801217039
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 45
average number of affinization = 12.03242147922999
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 71
average number of affinization = 12.092105263157896
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 12.112234580384227
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 62
average number of affinization = 12.162626262626263
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 56
average number of affinization = 12.20686175580222
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 40
average number of affinization = 12.234879032258064
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 48
average number of affinization = 12.270896273917423
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 52
average number of affinization = 12.310865191146881
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 37
average number of affinization = 12.3356783919598
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 47
average number of affinization = 12.370481927710843
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 52
average number of affinization = 12.41023069207623
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 54
average number of affinization = 12.451903807615231
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 49
average number of affinization = 12.488488488488489
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 45
average number of affinization = 12.521
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 38       |
| MaximumReturn | 198      |
| MinimumReturn | 110      |
| TotalSamples  | 133320   |
----------------------------
itr #39 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.45656195282936096
Validation loss = 0.45298174023628235
Validation loss = 0.4537808895111084
Validation loss = 0.4523046910762787
Validation loss = 0.45283448696136475
Validation loss = 0.45477089285850525
Validation loss = 0.4542780816555023
Validation loss = 0.452862411737442
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.45546823740005493
Validation loss = 0.4557006359100342
Validation loss = 0.45490044355392456
Validation loss = 0.45492786169052124
Validation loss = 0.455351859331131
Validation loss = 0.45766955614089966
Validation loss = 0.4560546576976776
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.46304845809936523
Validation loss = 0.46405908465385437
Validation loss = 0.4638875424861908
Validation loss = 0.4620886743068695
Validation loss = 0.4634755551815033
Validation loss = 0.46364977955818176
Validation loss = 0.46300041675567627
Validation loss = 0.462588906288147
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4577602446079254
Validation loss = 0.4555789828300476
Validation loss = 0.458799809217453
Validation loss = 0.46077996492385864
Validation loss = 0.4585111737251282
Validation loss = 0.45912230014801025
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4585852026939392
Validation loss = 0.4616530239582062
Validation loss = 0.45703086256980896
Validation loss = 0.4593939483165741
Validation loss = 0.4600186049938202
Validation loss = 0.45957666635513306
Validation loss = 0.46225112676620483
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 41
average number of affinization = 12.54945054945055
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 49
average number of affinization = 12.585828343313374
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 48
average number of affinization = 12.621136590229312
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 12.650398406374501
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 47
average number of affinization = 12.68457711442786
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 43
average number of affinization = 12.714711729622266
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 56
average number of affinization = 12.75769612711023
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 59
average number of affinization = 12.803571428571429
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 52
average number of affinization = 12.842418235877107
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 49
average number of affinization = 12.878217821782178
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 56
average number of affinization = 12.920870425321464
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 52
average number of affinization = 12.959486166007904
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 43
average number of affinization = 12.98914116485686
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 55
average number of affinization = 13.030571992110454
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 50
average number of affinization = 13.066995073891626
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 51
average number of affinization = 13.104330708661417
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 64
average number of affinization = 13.154375614552606
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 56
average number of affinization = 13.196463654223969
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 44
average number of affinization = 13.226692836113838
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 56
average number of affinization = 13.268627450980393
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 44
average number of affinization = 13.298726738491675
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 54
average number of affinization = 13.338551859099804
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 52
average number of affinization = 13.376344086021506
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 68
average number of affinization = 13.4296875
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 48
average number of affinization = 13.463414634146341
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 39       |
| MaximumReturn | 198      |
| MinimumReturn | 119      |
| TotalSamples  | 136653   |
----------------------------
