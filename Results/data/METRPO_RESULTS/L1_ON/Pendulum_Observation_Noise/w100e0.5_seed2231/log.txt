Logging to experiments/gym_pendulumO01/Pendulum_Observation_Noise/w100e0.5_seed2231
Print configuration .....
{'env_name': 'gym_pendulumO01', 'random_seeds': [3214, 2431, 2531, 2231], 'save_variables': False, 'model_save_dir': '/tmp/pendulumO01_models/', 'restore_variables': False, 'start_onpol_iter': 0, 'onpol_iters': 40, 'num_path_random': 25, 'num_path_onpol': 25, 'env_horizon': 200, 'max_train_data': 200000, 'max_val_data': 100000, 'discard_ratio': 0.0, 'dynamics': {'pre_training': {'mode': 'intrinsic_reward', 'itr': 0, 'policy_itr': 20}, 'model': 'nn', 'ensemble': True, 'ensemble_model_count': 5, 'enable_particle_ensemble': True, 'particles': 5, 'obs_var': 1.0, 'intrinsic_reward_coeff': 1.0, 'ita': 1.0, 'mode': 'random', 'val': True, 'n_layers': 4, 'hidden_size': 1000, 'activation': 'relu', 'batch_size': 1000, 'learning_rate': 0.001, 'reg_coeff': 0.0, 'epochs': 200, 'kfac_params': {'learning_rate': 0.1, 'damping': 0.001, 'momentum': 0.9, 'kl_clip': 0.0001, 'cov_ema_decay': 0.99}}, 'policy': {'network_shape': [64, 64], 'init_logstd': 0.0, 'activation': 'tanh', 'reinitialize_every_itr': False}, 'trpo': {'horizon': 200, 'gamma': 0.99, 'step_size': 0.01, 'iterations': 20, 'batch_size': 50000, 'gae': 0.95, 'visualization': False, 'visualize_iterations': [0]}, 'algo': 'trpo'}
Generating random rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating random rollouts.
Creating normalization for training data.
Done creating normalization for training data.
Particle ensemble enabled? True
An ensemble of 5 dynamics model <class 'model.dynamics.NNDynamicsModel'> initialized
Train dynamics model with intrinsic reward only? False
Pre-training enabled. Using only intrinsic reward.
Pre-training dynamics model for 0 iterations...
Done pre-training dynamics model.
Using external reward only.
itr #0 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4546523988246918
Validation loss = 0.22840213775634766
Validation loss = 0.21686503291130066
Validation loss = 0.21287454664707184
Validation loss = 0.20846915245056152
Validation loss = 0.20813411474227905
Validation loss = 0.20622123777866364
Validation loss = 0.2026856690645218
Validation loss = 0.20564909279346466
Validation loss = 0.19782963395118713
Validation loss = 0.19736291468143463
Validation loss = 0.1983906477689743
Validation loss = 0.1960369050502777
Validation loss = 0.19021524488925934
Validation loss = 0.19944588840007782
Validation loss = 0.19114801287651062
Validation loss = 0.1905084252357483
Validation loss = 0.19090929627418518
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.48895972967147827
Validation loss = 0.2292117327451706
Validation loss = 0.2133675068616867
Validation loss = 0.21106554567813873
Validation loss = 0.20951659977436066
Validation loss = 0.21516068279743195
Validation loss = 0.20634891092777252
Validation loss = 0.2080102115869522
Validation loss = 0.2041933387517929
Validation loss = 0.20024019479751587
Validation loss = 0.1946680098772049
Validation loss = 0.20521707832813263
Validation loss = 0.20151610672473907
Validation loss = 0.19124169647693634
Validation loss = 0.19297493994235992
Validation loss = 0.189113587141037
Validation loss = 0.19626519083976746
Validation loss = 0.19439135491847992
Validation loss = 0.18744616210460663
Validation loss = 0.1914781630039215
Validation loss = 0.19319193065166473
Validation loss = 0.1855480819940567
Validation loss = 0.18682684004306793
Validation loss = 0.188791424036026
Validation loss = 0.1879209727048874
Validation loss = 0.20551735162734985
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.44883993268013
Validation loss = 0.2306772917509079
Validation loss = 0.21267175674438477
Validation loss = 0.20967760682106018
Validation loss = 0.20790594816207886
Validation loss = 0.2078481912612915
Validation loss = 0.20596164464950562
Validation loss = 0.2094382792711258
Validation loss = 0.19750209152698517
Validation loss = 0.1949944645166397
Validation loss = 0.2057996243238449
Validation loss = 0.19261936843395233
Validation loss = 0.192696675658226
Validation loss = 0.19380804896354675
Validation loss = 0.1908206343650818
Validation loss = 0.19082710146903992
Validation loss = 0.20128774642944336
Validation loss = 0.19548240303993225
Validation loss = 0.1898115575313568
Validation loss = 0.19114026427268982
Validation loss = 0.18885478377342224
Validation loss = 0.19203396141529083
Validation loss = 0.1863488107919693
Validation loss = 0.18881727755069733
Validation loss = 0.18478183448314667
Validation loss = 0.19455640017986298
Validation loss = 0.1908368468284607
Validation loss = 0.19604036211967468
Validation loss = 0.1888473480939865
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.46249014139175415
Validation loss = 0.22570914030075073
Validation loss = 0.21376065909862518
Validation loss = 0.20844027400016785
Validation loss = 0.20914007723331451
Validation loss = 0.20483499765396118
Validation loss = 0.23579172790050507
Validation loss = 0.2022223174571991
Validation loss = 0.2000652700662613
Validation loss = 0.19484126567840576
Validation loss = 0.20201216638088226
Validation loss = 0.19422100484371185
Validation loss = 0.19794417917728424
Validation loss = 0.19669486582279205
Validation loss = 0.19371728599071503
Validation loss = 0.19916567206382751
Validation loss = 0.18953292071819305
Validation loss = 0.19268858432769775
Validation loss = 0.19099833071231842
Validation loss = 0.18658103048801422
Validation loss = 0.1891525834798813
Validation loss = 0.18609625101089478
Validation loss = 0.1842012256383896
Validation loss = 0.19080883264541626
Validation loss = 0.1894744485616684
Validation loss = 0.18458890914916992
Validation loss = 0.19531717896461487
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.47135111689567566
Validation loss = 0.23884625732898712
Validation loss = 0.21474529802799225
Validation loss = 0.20977695286273956
Validation loss = 0.21213988959789276
Validation loss = 0.2080518752336502
Validation loss = 0.20564550161361694
Validation loss = 0.20572304725646973
Validation loss = 0.20245249569416046
Validation loss = 0.20512090623378754
Validation loss = 0.19413986802101135
Validation loss = 0.19589827954769135
Validation loss = 0.19494695961475372
Validation loss = 0.1927284300327301
Validation loss = 0.1911412924528122
Validation loss = 0.19269150495529175
Validation loss = 0.195766881108284
Validation loss = 0.18949070572853088
Validation loss = 0.19363559782505035
Validation loss = 0.19423902034759521
Validation loss = 0.1863555908203125
Validation loss = 0.19405293464660645
Validation loss = 0.18943998217582703
Validation loss = 0.19849121570587158
Validation loss = 0.18760181963443756
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 0        |
| MaximumReturn | 199      |
| MinimumReturn | 95.7     |
| TotalSamples  | 6666     |
----------------------------
itr #1 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3544101417064667
Validation loss = 0.2618428170681
Validation loss = 0.2544444799423218
Validation loss = 0.2538672983646393
Validation loss = 0.2559347152709961
Validation loss = 0.24814872443675995
Validation loss = 0.24986134469509125
Validation loss = 0.255146861076355
Validation loss = 0.2536222040653229
Validation loss = 0.2428596466779709
Validation loss = 0.2440575808286667
Validation loss = 0.25370174646377563
Validation loss = 0.24639515578746796
Validation loss = 0.24785161018371582
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.34488406777381897
Validation loss = 0.2576395273208618
Validation loss = 0.2512975037097931
Validation loss = 0.24888992309570312
Validation loss = 0.25074782967567444
Validation loss = 0.24415560066699982
Validation loss = 0.25136998295783997
Validation loss = 0.24892573058605194
Validation loss = 0.2521212697029114
Validation loss = 0.24399489164352417
Validation loss = 0.24326641857624054
Validation loss = 0.2435830980539322
Validation loss = 0.24442440271377563
Validation loss = 0.24047225713729858
Validation loss = 0.23975814878940582
Validation loss = 0.24410851299762726
Validation loss = 0.24158339202404022
Validation loss = 0.24012112617492676
Validation loss = 0.2415553331375122
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3608005940914154
Validation loss = 0.25940051674842834
Validation loss = 0.2475510984659195
Validation loss = 0.25553080439567566
Validation loss = 0.24973970651626587
Validation loss = 0.25410839915275574
Validation loss = 0.24112696945667267
Validation loss = 0.244834303855896
Validation loss = 0.2413540631532669
Validation loss = 0.24375461041927338
Validation loss = 0.24383544921875
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.36468908190727234
Validation loss = 0.2574966251850128
Validation loss = 0.25588563084602356
Validation loss = 0.24964581429958344
Validation loss = 0.2525588572025299
Validation loss = 0.24737995862960815
Validation loss = 0.24719642102718353
Validation loss = 0.24385304749011993
Validation loss = 0.24443332850933075
Validation loss = 0.24861454963684082
Validation loss = 0.24339373409748077
Validation loss = 0.243072047829628
Validation loss = 0.24179799854755402
Validation loss = 0.24024896323680878
Validation loss = 0.24404007196426392
Validation loss = 0.23624521493911743
Validation loss = 0.24098138511180878
Validation loss = 0.24493683874607086
Validation loss = 0.2394280582666397
Validation loss = 0.2458309531211853
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.34336861968040466
Validation loss = 0.2584006190299988
Validation loss = 0.25150537490844727
Validation loss = 0.24909229576587677
Validation loss = 0.2485366016626358
Validation loss = 0.24540282785892487
Validation loss = 0.2551802396774292
Validation loss = 0.2525946795940399
Validation loss = 0.24436552822589874
Validation loss = 0.2476176619529724
Validation loss = 0.25275254249572754
Validation loss = 0.24318964779376984
Validation loss = 0.25728940963745117
Validation loss = 0.2425006628036499
Validation loss = 0.24038289487361908
Validation loss = 0.2483251541852951
Validation loss = 0.2387305498123169
Validation loss = 0.23630374670028687
Validation loss = 0.23720641434192657
Validation loss = 0.23947246372699738
Validation loss = 0.23899036645889282
Validation loss = 0.24065597355365753
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 1        |
| MaximumReturn | 199      |
| MinimumReturn | 94.6     |
| TotalSamples  | 9999     |
----------------------------
itr #2 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.2850675880908966
Validation loss = 0.2719472646713257
Validation loss = 0.2703501880168915
Validation loss = 0.27341732382774353
Validation loss = 0.2707831859588623
Validation loss = 0.26872140169143677
Validation loss = 0.2659688889980316
Validation loss = 0.2665015459060669
Validation loss = 0.2686045169830322
Validation loss = 0.269137978553772
Validation loss = 0.26638007164001465
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.2865738868713379
Validation loss = 0.2691117823123932
Validation loss = 0.26612788438796997
Validation loss = 0.26789605617523193
Validation loss = 0.26958879828453064
Validation loss = 0.26969218254089355
Validation loss = 0.26621225476264954
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.2994895577430725
Validation loss = 0.2689763605594635
Validation loss = 0.27328234910964966
Validation loss = 0.27069345116615295
Validation loss = 0.27267393469810486
Validation loss = 0.268617182970047
Validation loss = 0.27036482095718384
Validation loss = 0.26757192611694336
Validation loss = 0.2694680094718933
Validation loss = 0.2680862545967102
Validation loss = 0.265437513589859
Validation loss = 0.26501744985580444
Validation loss = 0.27124398946762085
Validation loss = 0.26685357093811035
Validation loss = 0.2644500434398651
Validation loss = 0.26577386260032654
Validation loss = 0.2672518789768219
Validation loss = 0.26895958185195923
Validation loss = 0.2738580107688904
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.29867708683013916
Validation loss = 0.2680441737174988
Validation loss = 0.269853800535202
Validation loss = 0.26544755697250366
Validation loss = 0.2639932930469513
Validation loss = 0.2695184648036957
Validation loss = 0.2626548409461975
Validation loss = 0.2665138840675354
Validation loss = 0.2658481001853943
Validation loss = 0.2629874348640442
Validation loss = 0.2645793557167053
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.2874423563480377
Validation loss = 0.2680300772190094
Validation loss = 0.2682690918445587
Validation loss = 0.2672015428543091
Validation loss = 0.27003201842308044
Validation loss = 0.26577848196029663
Validation loss = 0.26583534479141235
Validation loss = 0.2672368288040161
Validation loss = 0.2649124264717102
Validation loss = 0.26716986298561096
Validation loss = 0.26311054825782776
Validation loss = 0.26338306069374084
Validation loss = 0.26755887269973755
Validation loss = 0.26697683334350586
Validation loss = 0.2661159634590149
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 157      |
| Iteration     | 2        |
| MaximumReturn | 199      |
| MinimumReturn | 111      |
| TotalSamples  | 13332    |
----------------------------
itr #3 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.30271676182746887
Validation loss = 0.30101242661476135
Validation loss = 0.30667170882225037
Validation loss = 0.2987036406993866
Validation loss = 0.29845598340034485
Validation loss = 0.29651159048080444
Validation loss = 0.30306366086006165
Validation loss = 0.30819639563560486
Validation loss = 0.3032316267490387
Validation loss = 0.29783758521080017
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.30590739846229553
Validation loss = 0.2978428900241852
Validation loss = 0.30442652106285095
Validation loss = 0.3004873991012573
Validation loss = 0.302317351102829
Validation loss = 0.29708388447761536
Validation loss = 0.3026812970638275
Validation loss = 0.29981914162635803
Validation loss = 0.3007696866989136
Validation loss = 0.2971254289150238
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3071388900279999
Validation loss = 0.29779449105262756
Validation loss = 0.29820334911346436
Validation loss = 0.3065742552280426
Validation loss = 0.29747000336647034
Validation loss = 0.29907411336898804
Validation loss = 0.3032808005809784
Validation loss = 0.29705944657325745
Validation loss = 0.29791614413261414
Validation loss = 0.306378573179245
Validation loss = 0.29722118377685547
Validation loss = 0.30159446597099304
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3066633641719818
Validation loss = 0.3001243472099304
Validation loss = 0.3017462193965912
Validation loss = 0.3000338673591614
Validation loss = 0.3021925389766693
Validation loss = 0.2990129590034485
Validation loss = 0.2972587049007416
Validation loss = 0.297770619392395
Validation loss = 0.2977815866470337
Validation loss = 0.29861176013946533
Validation loss = 0.30109480023384094
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.302998811006546
Validation loss = 0.2985996901988983
Validation loss = 0.2968023419380188
Validation loss = 0.2970258891582489
Validation loss = 0.29818102717399597
Validation loss = 0.299727201461792
Validation loss = 0.2995585501194
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 163      |
| Iteration     | 3        |
| MaximumReturn | 195      |
| MinimumReturn | 112      |
| TotalSamples  | 16665    |
----------------------------
itr #4 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.33775556087493896
Validation loss = 0.3309556245803833
Validation loss = 0.3304208517074585
Validation loss = 0.3295590877532959
Validation loss = 0.33337274193763733
Validation loss = 0.3260546624660492
Validation loss = 0.32778269052505493
Validation loss = 0.3300800323486328
Validation loss = 0.33118271827697754
Validation loss = 0.33005598187446594
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3418169319629669
Validation loss = 0.3321743309497833
Validation loss = 0.33095866441726685
Validation loss = 0.3297860622406006
Validation loss = 0.33169370889663696
Validation loss = 0.331292986869812
Validation loss = 0.3292880058288574
Validation loss = 0.32773643732070923
Validation loss = 0.32699304819107056
Validation loss = 0.32739442586898804
Validation loss = 0.3279169797897339
Validation loss = 0.32909590005874634
Validation loss = 0.3350966274738312
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.336529016494751
Validation loss = 0.3311402499675751
Validation loss = 0.3275545835494995
Validation loss = 0.3297651410102844
Validation loss = 0.331787645816803
Validation loss = 0.3306519389152527
Validation loss = 0.3279843330383301
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.33165600895881653
Validation loss = 0.3317543864250183
Validation loss = 0.3294401466846466
Validation loss = 0.3310166001319885
Validation loss = 0.3305608928203583
Validation loss = 0.3338576853275299
Validation loss = 0.3377093970775604
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3402661681175232
Validation loss = 0.3308366537094116
Validation loss = 0.33417046070098877
Validation loss = 0.3290996551513672
Validation loss = 0.32987362146377563
Validation loss = 0.3304937183856964
Validation loss = 0.3332672119140625
Validation loss = 0.3299565613269806
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 157      |
| Iteration     | 4        |
| MaximumReturn | 197      |
| MinimumReturn | 110      |
| TotalSamples  | 19998    |
----------------------------
itr #5 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.36088234186172485
Validation loss = 0.357416033744812
Validation loss = 0.35310450196266174
Validation loss = 0.35328957438468933
Validation loss = 0.3524538278579712
Validation loss = 0.35175806283950806
Validation loss = 0.3509700894355774
Validation loss = 0.3528256118297577
Validation loss = 0.3492443561553955
Validation loss = 0.35312265157699585
Validation loss = 0.3550626337528229
Validation loss = 0.3539026975631714
Validation loss = 0.35172930359840393
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3551742434501648
Validation loss = 0.3550977408885956
Validation loss = 0.35335949063301086
Validation loss = 0.35792696475982666
Validation loss = 0.35069483518600464
Validation loss = 0.3501148223876953
Validation loss = 0.3507657051086426
Validation loss = 0.35201871395111084
Validation loss = 0.35641974210739136
Validation loss = 0.35410648584365845
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.35392874479293823
Validation loss = 0.35454124212265015
Validation loss = 0.3570309579372406
Validation loss = 0.35252007842063904
Validation loss = 0.3550068438053131
Validation loss = 0.35646137595176697
Validation loss = 0.35196226835250854
Validation loss = 0.3522927165031433
Validation loss = 0.35551950335502625
Validation loss = 0.3589443564414978
Validation loss = 0.35457393527030945
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.35483282804489136
Validation loss = 0.35273078083992004
Validation loss = 0.35265278816223145
Validation loss = 0.35504037141799927
Validation loss = 0.35166677832603455
Validation loss = 0.35276609659194946
Validation loss = 0.3520665764808655
Validation loss = 0.35409167408943176
Validation loss = 0.3564170002937317
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3581690788269043
Validation loss = 0.3520275354385376
Validation loss = 0.3521391749382019
Validation loss = 0.3535688519477844
Validation loss = 0.35476890206336975
Validation loss = 0.35510745644569397
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 163      |
| Iteration     | 5        |
| MaximumReturn | 198      |
| MinimumReturn | 110      |
| TotalSamples  | 23331    |
----------------------------
itr #6 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.36665648221969604
Validation loss = 0.37142160534858704
Validation loss = 0.3685264587402344
Validation loss = 0.3709729015827179
Validation loss = 0.3676258325576782
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.36715736985206604
Validation loss = 0.3674238920211792
Validation loss = 0.36610639095306396
Validation loss = 0.3713602125644684
Validation loss = 0.3672589957714081
Validation loss = 0.3674849271774292
Validation loss = 0.36560726165771484
Validation loss = 0.3715840280056
Validation loss = 0.36751335859298706
Validation loss = 0.36916598677635193
Validation loss = 0.37041425704956055
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.36842650175094604
Validation loss = 0.3659364581108093
Validation loss = 0.36635467410087585
Validation loss = 0.3681950867176056
Validation loss = 0.37373340129852295
Validation loss = 0.36920058727264404
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.36923739314079285
Validation loss = 0.36754006147384644
Validation loss = 0.36709412932395935
Validation loss = 0.36698010563850403
Validation loss = 0.36845025420188904
Validation loss = 0.3700941801071167
Validation loss = 0.36753857135772705
Validation loss = 0.37005650997161865
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3704272508621216
Validation loss = 0.36509475111961365
Validation loss = 0.3678143620491028
Validation loss = 0.36822426319122314
Validation loss = 0.36892062425613403
Validation loss = 0.3712671399116516
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.00558659217877095
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.011111111111111112
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.011049723756906077
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.01098901098901099
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.01092896174863388
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.010869565217391304
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.010810810810810811
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.010752688172043012
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.016042780748663103
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.02127659574468085
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.021164021164021163
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.021052631578947368
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.020942408376963352
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.026041666666666668
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.025906735751295335
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.02577319587628866
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.03076923076923077
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.030612244897959183
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.030456852791878174
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.030303030303030304
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.03015075376884422
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.03
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 6        |
| MaximumReturn | 198      |
| MinimumReturn | 127      |
| TotalSamples  | 26664    |
----------------------------
itr #7 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3767806887626648
Validation loss = 0.377115398645401
Validation loss = 0.3730134665966034
Validation loss = 0.3749104142189026
Validation loss = 0.3750799000263214
Validation loss = 0.3793661892414093
Validation loss = 0.37621527910232544
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3797941505908966
Validation loss = 0.3755040764808655
Validation loss = 0.3770602345466614
Validation loss = 0.37507134675979614
Validation loss = 0.3818119168281555
Validation loss = 0.3772251009941101
Validation loss = 0.3783092200756073
Validation loss = 0.37798357009887695
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3780893087387085
Validation loss = 0.3780401349067688
Validation loss = 0.37694182991981506
Validation loss = 0.37711694836616516
Validation loss = 0.3778529763221741
Validation loss = 0.37481576204299927
Validation loss = 0.37985628843307495
Validation loss = 0.3809729814529419
Validation loss = 0.3766877055168152
Validation loss = 0.3787922263145447
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3776470422744751
Validation loss = 0.37665244936943054
Validation loss = 0.3739471733570099
Validation loss = 0.37704482674598694
Validation loss = 0.375947505235672
Validation loss = 0.3763982951641083
Validation loss = 0.37608802318573
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.37727171182632446
Validation loss = 0.3746676445007324
Validation loss = 0.3810052275657654
Validation loss = 0.37856924533843994
Validation loss = 0.3779429793357849
Validation loss = 0.37678247690200806
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.03482587064676617
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.039603960396039604
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.03940886699507389
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0392156862745098
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.03902439024390244
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.038834951456310676
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.03864734299516908
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.038461538461538464
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.03827751196172249
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0380952380952381
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.037914691943127965
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.03773584905660377
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.04225352112676056
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.04205607476635514
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.04186046511627907
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.041666666666666664
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.04608294930875576
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.05045871559633028
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0502283105022831
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.049773755656108594
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.04954954954954955
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.04932735426008968
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.049107142857142856
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.04888888888888889
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 164      |
| Iteration     | 7        |
| MaximumReturn | 197      |
| MinimumReturn | 116      |
| TotalSamples  | 29997    |
----------------------------
itr #8 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.38155657052993774
Validation loss = 0.38046613335609436
Validation loss = 0.3787592649459839
Validation loss = 0.3794301450252533
Validation loss = 0.3825286328792572
Validation loss = 0.3794242739677429
Validation loss = 0.38486534357070923
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.38261422514915466
Validation loss = 0.3812099099159241
Validation loss = 0.38131004571914673
Validation loss = 0.38587459921836853
Validation loss = 0.38261017203330994
Validation loss = 0.38375338912010193
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.38308247923851013
Validation loss = 0.38106030225753784
Validation loss = 0.38203486800193787
Validation loss = 0.381854385137558
Validation loss = 0.38386502861976624
Validation loss = 0.3818754255771637
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.38252490758895874
Validation loss = 0.3813665211200714
Validation loss = 0.38328734040260315
Validation loss = 0.3809159994125366
Validation loss = 0.3808959126472473
Validation loss = 0.3835676312446594
Validation loss = 0.38015657663345337
Validation loss = 0.3824956715106964
Validation loss = 0.383163183927536
Validation loss = 0.38399872183799744
Validation loss = 0.38290590047836304
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3799794018268585
Validation loss = 0.38267672061920166
Validation loss = 0.3796674907207489
Validation loss = 0.3793056607246399
Validation loss = 0.38021835684776306
Validation loss = 0.3796844184398651
Validation loss = 0.3828516900539398
Validation loss = 0.3809733986854553
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.05309734513274336
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05286343612334802
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05263157894736842
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05240174672489083
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05217391304347826
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05194805194805195
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05172413793103448
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05150214592274678
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05128205128205128
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05106382978723404
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05084745762711865
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05063291139240506
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05042016806722689
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.05439330543933055
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05416666666666667
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05394190871369295
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05371900826446281
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.05761316872427984
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05737704918032787
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.061224489795918366
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.06097560975609756
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.06477732793522267
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.06451612903225806
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.06827309236947791
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.072
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 160      |
| Iteration     | 8        |
| MaximumReturn | 199      |
| MinimumReturn | 116      |
| TotalSamples  | 33330    |
----------------------------
itr #9 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3885704278945923
Validation loss = 0.3866782784461975
Validation loss = 0.3880399465560913
Validation loss = 0.38681459426879883
Validation loss = 0.3891875743865967
Validation loss = 0.39072561264038086
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.38980212807655334
Validation loss = 0.3888548016548157
Validation loss = 0.39071622490882874
Validation loss = 0.3923438787460327
Validation loss = 0.39441901445388794
Validation loss = 0.3906378448009491
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3891160488128662
Validation loss = 0.3885955214500427
Validation loss = 0.3901333808898926
Validation loss = 0.3871923089027405
Validation loss = 0.39121997356414795
Validation loss = 0.39304471015930176
Validation loss = 0.39249852299690247
Validation loss = 0.3923226296901703
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3907846510410309
Validation loss = 0.3912833034992218
Validation loss = 0.3879489302635193
Validation loss = 0.3906415104866028
Validation loss = 0.3931998014450073
Validation loss = 0.3903026878833771
Validation loss = 0.39208999276161194
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.38770192861557007
Validation loss = 0.3873699903488159
Validation loss = 0.3911939263343811
Validation loss = 0.3880878686904907
Validation loss = 0.388252317905426
Validation loss = 0.3883112370967865
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07171314741035857
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07142857142857142
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07114624505928854
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07086614173228346
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.07450980392156863
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07421875
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07392996108949416
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07364341085271318
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07335907335907337
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07307692307692308
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07279693486590039
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07251908396946564
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07224334600760456
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07196969696969698
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07169811320754717
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07142857142857142
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07116104868913857
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.07462686567164178
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.07806691449814127
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07777777777777778
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07749077490774908
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07720588235294118
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07692307692307693
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07664233576642336
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07636363636363637
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 9        |
| MaximumReturn | 199      |
| MinimumReturn | 130      |
| TotalSamples  | 36663    |
----------------------------
itr #10 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.39172422885894775
Validation loss = 0.39360061287879944
Validation loss = 0.3955012559890747
Validation loss = 0.3910108208656311
Validation loss = 0.3932613134384155
Validation loss = 0.3948976993560791
Validation loss = 0.39337390661239624
Validation loss = 0.39361003041267395
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3977786600589752
Validation loss = 0.393537312746048
Validation loss = 0.3968208134174347
Validation loss = 0.39626583456993103
Validation loss = 0.39438360929489136
Validation loss = 0.39866068959236145
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.39555034041404724
Validation loss = 0.3955859839916229
Validation loss = 0.3939388692378998
Validation loss = 0.3948296308517456
Validation loss = 0.39785119891166687
Validation loss = 0.39563411474227905
Validation loss = 0.3946622610092163
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3963576555252075
Validation loss = 0.39360687136650085
Validation loss = 0.39682674407958984
Validation loss = 0.39288222789764404
Validation loss = 0.3946892023086548
Validation loss = 0.39526259899139404
Validation loss = 0.395625501871109
Validation loss = 0.39504802227020264
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.39255428314208984
Validation loss = 0.39351850748062134
Validation loss = 0.39359185099601746
Validation loss = 0.3927735686302185
Validation loss = 0.3913686275482178
Validation loss = 0.39140617847442627
Validation loss = 0.39707934856414795
Validation loss = 0.3940275013446808
Validation loss = 0.39448925852775574
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07608695652173914
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07581227436823104
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.07913669064748201
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.08602150537634409
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08571428571428572
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.09252669039145907
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.09219858156028368
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.09540636042402827
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.10211267605633803
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.10175438596491228
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.1048951048951049
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.10801393728222997
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.1111111111111111
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.11418685121107267
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.11724137931034483
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.12027491408934708
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.11986301369863013
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.12286689419795221
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.12244897959183673
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.12203389830508475
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.12162162162162163
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.12457912457912458
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.12416107382550336
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.12374581939799331
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.12333333333333334
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 10       |
| MaximumReturn | 198      |
| MinimumReturn | 117      |
| TotalSamples  | 39996    |
----------------------------
itr #11 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4014512896537781
Validation loss = 0.40055593848228455
Validation loss = 0.39973166584968567
Validation loss = 0.4000277519226074
Validation loss = 0.4001882076263428
Validation loss = 0.40200075507164
Validation loss = 0.4019327163696289
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4022170901298523
Validation loss = 0.40271615982055664
Validation loss = 0.4044589102268219
Validation loss = 0.4020608961582184
Validation loss = 0.40329495072364807
Validation loss = 0.4031197428703308
Validation loss = 0.40235990285873413
Validation loss = 0.4046046733856201
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4000501036643982
Validation loss = 0.40000900626182556
Validation loss = 0.40113624930381775
Validation loss = 0.40158629417419434
Validation loss = 0.4019540250301361
Validation loss = 0.4034663140773773
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4042142331600189
Validation loss = 0.4030846953392029
Validation loss = 0.4041362404823303
Validation loss = 0.4032984673976898
Validation loss = 0.40530699491500854
Validation loss = 0.4049989581108093
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.40051379799842834
Validation loss = 0.3996550440788269
Validation loss = 0.3991805613040924
Validation loss = 0.4002395570278168
Validation loss = 0.3996259570121765
Validation loss = 0.39966529607772827
Validation loss = 0.40244364738464355
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.12292358803986711
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.12251655629139073
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.1254125412541254
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.12828947368421054
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.13114754098360656
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.13071895424836602
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.13680781758957655
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.1396103896103896
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.13915857605177995
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.14193548387096774
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.1414790996784566
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.14423076923076922
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.14376996805111822
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.1464968152866242
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.14603174603174604
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.14556962025316456
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.15457413249211358
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.1540880503144654
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.16300940438871472
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.171875
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.17757009345794392
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.18322981366459629
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.18575851393188855
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.18518518518518517
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.19076923076923077
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 179      |
| Iteration     | 11       |
| MaximumReturn | 198      |
| MinimumReturn | 134      |
| TotalSamples  | 43329    |
----------------------------
itr #12 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4083539545536041
Validation loss = 0.40884605050086975
Validation loss = 0.4092191159725189
Validation loss = 0.4087701439857483
Validation loss = 0.4090837240219116
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.411896288394928
Validation loss = 0.4121350347995758
Validation loss = 0.41036534309387207
Validation loss = 0.4113334119319916
Validation loss = 0.41332322359085083
Validation loss = 0.41320088505744934
Validation loss = 0.41448602080345154
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4097966253757477
Validation loss = 0.4114859998226166
Validation loss = 0.41242384910583496
Validation loss = 0.4096571207046509
Validation loss = 0.4101545810699463
Validation loss = 0.410705029964447
Validation loss = 0.4105108678340912
Validation loss = 0.41190528869628906
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.41498464345932007
Validation loss = 0.41026929020881653
Validation loss = 0.4100465774536133
Validation loss = 0.4097402095794678
Validation loss = 0.41159698367118835
Validation loss = 0.41095101833343506
Validation loss = 0.4109290838241577
Validation loss = 0.41274991631507874
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.41092219948768616
Validation loss = 0.41009584069252014
Validation loss = 0.40873032808303833
Validation loss = 0.40932708978652954
Validation loss = 0.40974557399749756
Validation loss = 0.4086853265762329
Validation loss = 0.412521630525589
Validation loss = 0.4094201922416687
Validation loss = 0.4103740155696869
Validation loss = 0.41163018345832825
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.1901840490797546
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.19571865443425077
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.20121951219512196
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.2006079027355623
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.20606060606060606
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.2084592145015106
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.21686746987951808
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.23123123123123124
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.23952095808383234
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.24776119402985075
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.25297619047619047
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.258160237388724
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.257396449704142
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.25663716814159293
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.2676470588235294
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.2668621700879765
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.2719298245614035
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.27405247813411077
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.27906976744186046
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.28695652173913044
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.2861271676300578
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.2881844380403458
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.28735632183908044
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.2979942693409742
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.30857142857142855
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 154      |
| Iteration     | 12       |
| MaximumReturn | 198      |
| MinimumReturn | 109      |
| TotalSamples  | 46662    |
----------------------------
itr #13 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.41465651988983154
Validation loss = 0.4179142117500305
Validation loss = 0.4141915440559387
Validation loss = 0.41987907886505127
Validation loss = 0.4167998731136322
Validation loss = 0.41538006067276
Validation loss = 0.4141378700733185
Validation loss = 0.41812825202941895
Validation loss = 0.41780802607536316
Validation loss = 0.4186088442802429
Validation loss = 0.4187168478965759
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.41997063159942627
Validation loss = 0.4191528856754303
Validation loss = 0.4201979339122772
Validation loss = 0.42048701643943787
Validation loss = 0.42053014039993286
Validation loss = 0.4204607307910919
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4177089035511017
Validation loss = 0.4158262312412262
Validation loss = 0.41690462827682495
Validation loss = 0.4179758131504059
Validation loss = 0.4178048074245453
Validation loss = 0.41924867033958435
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4203074872493744
Validation loss = 0.41880276799201965
Validation loss = 0.41682347655296326
Validation loss = 0.41995686292648315
Validation loss = 0.4178451895713806
Validation loss = 0.4214141368865967
Validation loss = 0.42013218998908997
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.41765713691711426
Validation loss = 0.415922075510025
Validation loss = 0.41688841581344604
Validation loss = 0.41693049669265747
Validation loss = 0.4174977242946625
Validation loss = 0.41726237535476685
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.3076923076923077
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.3125
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.32011331444759206
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.327683615819209
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.3267605633802817
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.3258426966292135
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.3333333333333333
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.33798882681564246
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.3370473537604457
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.3388888888888889
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.3407202216066482
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.34806629834254144
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.3526170798898072
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.35714285714285715
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.3643835616438356
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.3633879781420765
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.37057220708446864
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.3804347826086957
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.38482384823848237
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.3918918918918919
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.3908355795148248
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.3897849462365591
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.3914209115281501
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.39572192513368987
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.4026666666666667
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 13       |
| MaximumReturn | 198      |
| MinimumReturn | 120      |
| TotalSamples  | 49995    |
----------------------------
itr #14 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.42404189705848694
Validation loss = 0.424763560295105
Validation loss = 0.4233754873275757
Validation loss = 0.42450863122940063
Validation loss = 0.42714980244636536
Validation loss = 0.4242333173751831
Validation loss = 0.4260194003582001
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.42673587799072266
Validation loss = 0.4249953329563141
Validation loss = 0.4252780079841614
Validation loss = 0.42624205350875854
Validation loss = 0.4266328811645508
Validation loss = 0.42648905515670776
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.42456790804862976
Validation loss = 0.42304545640945435
Validation loss = 0.42362961173057556
Validation loss = 0.4233347773551941
Validation loss = 0.4241727590560913
Validation loss = 0.4244745373725891
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4224935472011566
Validation loss = 0.42504075169563293
Validation loss = 0.42548543214797974
Validation loss = 0.425392746925354
Validation loss = 0.4275738000869751
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.422324538230896
Validation loss = 0.42291542887687683
Validation loss = 0.4247629642486572
Validation loss = 0.42355620861053467
Validation loss = 0.42316505312919617
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.4148936170212766
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.41379310344827586
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.42857142857142855
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.43535620052770446
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.4342105263157895
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.4330708661417323
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.4319371727748691
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.4412532637075718
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.4505208333333333
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.45194805194805193
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.45077720207253885
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.45478036175710596
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.46134020618556704
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.46786632390745503
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.47692307692307695
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.48081841432225064
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.4846938775510204
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.49363867684478374
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 0.5152284263959391
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.5189873417721519
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.5303030303030303
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.5390428211586902
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.5376884422110553
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.5413533834586466
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.5475
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 163      |
| Iteration     | 14       |
| MaximumReturn | 198      |
| MinimumReturn | 113      |
| TotalSamples  | 53328    |
----------------------------
itr #15 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.42886126041412354
Validation loss = 0.4288751184940338
Validation loss = 0.4299425780773163
Validation loss = 0.42971330881118774
Validation loss = 0.428944855928421
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4325529634952545
Validation loss = 0.42844200134277344
Validation loss = 0.4304649233818054
Validation loss = 0.429708331823349
Validation loss = 0.4306469261646271
Validation loss = 0.43133556842803955
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.42648860812187195
Validation loss = 0.42726653814315796
Validation loss = 0.42870938777923584
Validation loss = 0.4279390871524811
Validation loss = 0.4282586872577667
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4286954700946808
Validation loss = 0.42802396416664124
Validation loss = 0.43002086877822876
Validation loss = 0.4313398003578186
Validation loss = 0.4300740957260132
Validation loss = 0.43076542019844055
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4267309606075287
Validation loss = 0.4277864098548889
Validation loss = 0.4267236590385437
Validation loss = 0.4289563000202179
Validation loss = 0.4274979829788208
Validation loss = 0.4254618287086487
Validation loss = 0.4290764033794403
Validation loss = 0.43117693066596985
Validation loss = 0.42936691641807556
Validation loss = 0.4302552342414856
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.5461346633416458
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.554726368159204
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 0.575682382133995
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.5891089108910891
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 0.6098765432098765
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 0.625615763546798
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.6240786240786241
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.6225490196078431
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.6210268948655256
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.6268292682926829
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.6374695863746959
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.6407766990291263
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.639225181598063
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.6376811594202898
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.6457831325301204
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.6490384615384616
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.657074340527578
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.6602870813397129
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.6634844868735084
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.6666666666666666
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.6769596199524941
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.6753554502369669
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.6879432624113475
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.6933962264150944
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.7035294117647058
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 15       |
| MaximumReturn | 198      |
| MinimumReturn | 114      |
| TotalSamples  | 56661    |
----------------------------
itr #16 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.43208131194114685
Validation loss = 0.4328368604183197
Validation loss = 0.43288496136665344
Validation loss = 0.43237295746803284
Validation loss = 0.4355026185512543
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4355381429195404
Validation loss = 0.435978502035141
Validation loss = 0.43266719579696655
Validation loss = 0.4339807331562042
Validation loss = 0.43628939986228943
Validation loss = 0.43661579489707947
Validation loss = 0.4356321394443512
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4309559762477875
Validation loss = 0.430118590593338
Validation loss = 0.43198567628860474
Validation loss = 0.43161144852638245
Validation loss = 0.43129095435142517
Validation loss = 0.4337391257286072
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4317982792854309
Validation loss = 0.43337634205818176
Validation loss = 0.4323866367340088
Validation loss = 0.4342806041240692
Validation loss = 0.4335765242576599
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.43088603019714355
Validation loss = 0.43149274587631226
Validation loss = 0.4340452253818512
Validation loss = 0.43402567505836487
Validation loss = 0.4349345564842224
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.704225352112676
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.7142857142857143
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 0.7313084112149533
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.7365967365967366
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.7488372093023256
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.7587006960556845
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.7708333333333334
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.7736720554272517
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.7811059907834101
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 0.7977011494252874
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.8073394495412844
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.816933638443936
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 0.8333333333333334
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.835990888382688
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.8363636363636363
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.8412698412698413
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.8529411764705882
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.8555304740406321
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.8671171171171171
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 0.8808988764044944
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.8878923766816144
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 0.9038031319910514
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.9084821428571429
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.9153674832962138
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 0.9333333333333333
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 164      |
| Iteration     | 16       |
| MaximumReturn | 197      |
| MinimumReturn | 124      |
| TotalSamples  | 59994    |
----------------------------
itr #17 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4357143044471741
Validation loss = 0.43814200162887573
Validation loss = 0.43763935565948486
Validation loss = 0.43678590655326843
Validation loss = 0.4374478757381439
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4390655755996704
Validation loss = 0.4391945004463196
Validation loss = 0.4372360110282898
Validation loss = 0.4418303072452545
Validation loss = 0.4386378824710846
Validation loss = 0.4394698143005371
Validation loss = 0.4407500624656677
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4360293745994568
Validation loss = 0.4343090355396271
Validation loss = 0.4344688653945923
Validation loss = 0.436458557844162
Validation loss = 0.4356622099876404
Validation loss = 0.4365575909614563
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4355333149433136
Validation loss = 0.4370932877063751
Validation loss = 0.43623313307762146
Validation loss = 0.43728429079055786
Validation loss = 0.4384295642375946
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4348655045032501
Validation loss = 0.4349480867385864
Validation loss = 0.4373745322227478
Validation loss = 0.4395082890987396
Validation loss = 0.4358152449131012
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.9401330376940134
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.9446902654867256
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.9558498896247241
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.9647577092511013
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.9626373626373627
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.9605263157894737
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.962800875273523
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.9606986899563319
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.9694989106753813
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.9782608695652174
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.9848156182212582
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 1.0021645021645023
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 1.0043196544276458
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.019396551724138
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.027956989247312
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 1.0278969957081545
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.0364025695931478
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 1.0555555555555556
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.070362473347548
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 1.0680851063829788
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.0743099787685775
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 1.0911016949152543
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 1.0887949260042282
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 1.090717299578059
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.1031578947368421
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 164      |
| Iteration     | 17       |
| MaximumReturn | 198      |
| MinimumReturn | 112      |
| TotalSamples  | 63327    |
----------------------------
itr #18 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.43624383211135864
Validation loss = 0.43616095185279846
Validation loss = 0.4364846646785736
Validation loss = 0.43795740604400635
Validation loss = 0.4375894069671631
Validation loss = 0.43757539987564087
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.44135966897010803
Validation loss = 0.443109929561615
Validation loss = 0.4406753182411194
Validation loss = 0.4412114918231964
Validation loss = 0.441938579082489
Validation loss = 0.4430180788040161
Validation loss = 0.44429734349250793
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4360553026199341
Validation loss = 0.4359356462955475
Validation loss = 0.4360707402229309
Validation loss = 0.4371282756328583
Validation loss = 0.4383498430252075
Validation loss = 0.439046710729599
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.43750837445259094
Validation loss = 0.43750664591789246
Validation loss = 0.43923646211624146
Validation loss = 0.43879225850105286
Validation loss = 0.44015368819236755
Validation loss = 0.4389289319515228
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4395146667957306
Validation loss = 0.43801844120025635
Validation loss = 0.4374789297580719
Validation loss = 0.4370318651199341
Validation loss = 0.43764203786849976
Validation loss = 0.44319021701812744
Validation loss = 0.4403636157512665
Validation loss = 0.43913018703460693
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.1134453781512605
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 1.1425576519916143
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.1569037656903767
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 1.1607515657620042
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.16875
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.180873180873181
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 1.1825726141078838
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 1.186335403726708
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.2004132231404958
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 1.1979381443298969
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.2098765432098766
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 1.211498973305955
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.221311475409836
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.231083844580777
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 1.2530612244897958
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 1.2708757637474541
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.2784552845528456
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.2860040567951319
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 1.3076923076923077
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.3191919191919192
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.3326612903225807
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.3460764587525151
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.357429718875502
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 1.3587174348697395
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 1.374
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 161      |
| Iteration     | 18       |
| MaximumReturn | 198      |
| MinimumReturn | 119      |
| TotalSamples  | 66660    |
----------------------------
itr #19 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.43851548433303833
Validation loss = 0.437689870595932
Validation loss = 0.43960240483283997
Validation loss = 0.43779850006103516
Validation loss = 0.4393998086452484
Validation loss = 0.4395471513271332
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4406861364841461
Validation loss = 0.4410160183906555
Validation loss = 0.4412830173969269
Validation loss = 0.4431164264678955
Validation loss = 0.44649893045425415
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4380882680416107
Validation loss = 0.4374610185623169
Validation loss = 0.4371252655982971
Validation loss = 0.43736889958381653
Validation loss = 0.44117408990859985
Validation loss = 0.43836644291877747
Validation loss = 0.44074133038520813
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.44062238931655884
Validation loss = 0.43983545899391174
Validation loss = 0.43993911147117615
Validation loss = 0.4399007558822632
Validation loss = 0.43839073181152344
Validation loss = 0.44233429431915283
Validation loss = 0.44138410687446594
Validation loss = 0.44135230779647827
Validation loss = 0.4408104419708252
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4384400248527527
Validation loss = 0.44314634799957275
Validation loss = 0.43883711099624634
Validation loss = 0.4402046203613281
Validation loss = 0.43967363238334656
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 1.3732534930139721
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 1.3764940239043826
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 1.393638170974155
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.3988095238095237
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.4099009900990098
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.4229249011857708
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.435897435897436
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.4468503937007875
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.455795677799607
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 1.4705882352941178
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.4794520547945205
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.4921875
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 1.510721247563353
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 1.5116731517509727
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 1.5126213592233009
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.5193798449612403
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.5241779497098646
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 1.5231660231660231
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.533718689788054
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 1.5326923076923078
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 1.5355086372360844
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 1.549808429118774
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 1.5717017208413002
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.5763358778625953
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.582857142857143
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 19       |
| MaximumReturn | 199      |
| MinimumReturn | 109      |
| TotalSamples  | 69993    |
----------------------------
itr #20 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4372122883796692
Validation loss = 0.43932998180389404
Validation loss = 0.43757492303848267
Validation loss = 0.4390357732772827
Validation loss = 0.44016599655151367
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.44278138875961304
Validation loss = 0.44208279252052307
Validation loss = 0.4457223415374756
Validation loss = 0.4438976049423218
Validation loss = 0.441622257232666
Validation loss = 0.44325608015060425
Validation loss = 0.4428360164165497
Validation loss = 0.4447438418865204
Validation loss = 0.44445937871932983
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4379977881908417
Validation loss = 0.43705514073371887
Validation loss = 0.44006553292274475
Validation loss = 0.4381166696548462
Validation loss = 0.43956488370895386
Validation loss = 0.4415926933288574
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4400176405906677
Validation loss = 0.44325852394104004
Validation loss = 0.4422863721847534
Validation loss = 0.44219639897346497
Validation loss = 0.4444328844547272
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.43823981285095215
Validation loss = 0.4376643896102905
Validation loss = 0.4389544725418091
Validation loss = 0.43940088152885437
Validation loss = 0.4395360052585602
Validation loss = 0.44033390283584595
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.5931558935361216
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.5977229601518026
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 1.6117424242424243
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.616257088846881
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 1.639622641509434
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 1.64030131826742
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 1.6409774436090225
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 1.6416510318949344
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 1.6573033707865168
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 1.6579439252336448
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 1.6828358208955223
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 1.685288640595903
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 1.7063197026022305
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.7161410018552876
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.7222222222222223
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 1.7208872458410351
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.7287822878228782
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.732965009208103
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 1.7334558823529411
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 1.7486238532110092
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 1.749084249084249
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 1.7659963436928703
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 1.7664233576642336
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 1.7686703096539163
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 1.7818181818181817
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 20       |
| MaximumReturn | 199      |
| MinimumReturn | 122      |
| TotalSamples  | 73326    |
----------------------------
itr #21 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4362623393535614
Validation loss = 0.4391922950744629
Validation loss = 0.43722474575042725
Validation loss = 0.43685653805732727
Validation loss = 0.4382103383541107
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.44184258580207825
Validation loss = 0.4433439075946808
Validation loss = 0.44406870007514954
Validation loss = 0.4462644159793854
Validation loss = 0.4446408450603485
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.43938958644866943
Validation loss = 0.43914949893951416
Validation loss = 0.43710407614707947
Validation loss = 0.4413612484931946
Validation loss = 0.4392252564430237
Validation loss = 0.4406963884830475
Validation loss = 0.4406655728816986
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.43976208567619324
Validation loss = 0.4420931339263916
Validation loss = 0.4423495829105377
Validation loss = 0.442840576171875
Validation loss = 0.4418191909790039
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.43813765048980713
Validation loss = 0.4392240643501282
Validation loss = 0.43892762064933777
Validation loss = 0.4410674571990967
Validation loss = 0.44076985120773315
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 1.8039927404718694
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 1.8242753623188406
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 1.8517179023508137
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 1.8537906137184115
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 1.8558558558558558
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.8633093525179856
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.867145421903052
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.8781362007168458
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.885509838998211
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 1.8839285714285714
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 1.9001782531194296
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.9092526690391458
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 1.9236234458259325
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 1.9379432624113475
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.9451327433628318
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.9505300353356891
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 1.966490299823633
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.9700704225352113
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 1.9859402460456943
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.9964912280701754
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 24
average number of affinization = 2.0350262697022767
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 2.050699300699301
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 2.0593368237347294
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 2.0609756097560976
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 2.0678260869565217
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 21       |
| MaximumReturn | 198      |
| MinimumReturn | 120      |
| TotalSamples  | 76659    |
----------------------------
itr #22 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.43728867173194885
Validation loss = 0.4372391402721405
Validation loss = 0.43718695640563965
Validation loss = 0.43642762303352356
Validation loss = 0.4372808635234833
Validation loss = 0.4392387568950653
Validation loss = 0.43869611620903015
Validation loss = 0.43729791045188904
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4407157897949219
Validation loss = 0.4411189556121826
Validation loss = 0.4429047703742981
Validation loss = 0.44284960627555847
Validation loss = 0.4416229724884033
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.43675827980041504
Validation loss = 0.43774479627609253
Validation loss = 0.4382152557373047
Validation loss = 0.4392884373664856
Validation loss = 0.44134145975112915
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4379468262195587
Validation loss = 0.4390698969364166
Validation loss = 0.4402277171611786
Validation loss = 0.441059410572052
Validation loss = 0.4393361806869507
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.43545231223106384
Validation loss = 0.43813565373420715
Validation loss = 0.43660062551498413
Validation loss = 0.4359323978424072
Validation loss = 0.43779730796813965
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 2.0868055555555554
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 2.0831889081455808
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 2.096885813148789
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 2.1157167530224523
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 2.127586206896552
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 2.1376936316695354
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 2.147766323024055
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 2.1663807890222984
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 2.1746575342465753
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 2.176068376068376
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 2.1860068259385668
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 2.1908006814310053
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 2.207482993197279
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 2.2224108658743633
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 2.227118644067797
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 2.233502538071066
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 2.238175675675676
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 2.2445193929173692
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 2.255892255892256
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 2.272268907563025
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 2.2751677852348995
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 2.288107202680067
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 2.297658862876254
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 2.300500834724541
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 2.305
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 159      |
| Iteration     | 22       |
| MaximumReturn | 198      |
| MinimumReturn | 113      |
| TotalSamples  | 79992    |
----------------------------
itr #23 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4366990923881531
Validation loss = 0.4360000491142273
Validation loss = 0.4379194378852844
Validation loss = 0.4372660517692566
Validation loss = 0.4392845034599304
Validation loss = 0.4387669563293457
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.44118794798851013
Validation loss = 0.44278043508529663
Validation loss = 0.4402742385864258
Validation loss = 0.4447481036186218
Validation loss = 0.44229307770729065
Validation loss = 0.4422098994255066
Validation loss = 0.44303464889526367
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4379686415195465
Validation loss = 0.43568873405456543
Validation loss = 0.43737488985061646
Validation loss = 0.43752890825271606
Validation loss = 0.44034481048583984
Validation loss = 0.4420742392539978
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4375379681587219
Validation loss = 0.43862977623939514
Validation loss = 0.4381534457206726
Validation loss = 0.4385438859462738
Validation loss = 0.439995676279068
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.43723350763320923
Validation loss = 0.4366721212863922
Validation loss = 0.4398620128631592
Validation loss = 0.43766283988952637
Validation loss = 0.43821221590042114
Validation loss = 0.4384555220603943
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 2.307820299500832
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 2.320598006644518
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 2.3333333333333335
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 2.346026490066225
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 2.353719008264463
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 2.372937293729373
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 2.3887973640856672
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 2.411184210526316
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 2.4203612479474548
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 2.4327868852459016
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 2.454991816693944
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 2.4607843137254903
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 2.463295269168026
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 2.465798045602606
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 2.473170731707317
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 2.4756493506493507
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 2.487844408427877
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 2.5080906148867315
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 2.510500807754443
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 2.529032258064516
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 2.5458937198067635
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 2.557877813504823
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 2.5601926163723916
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 24
average number of affinization = 2.594551282051282
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 2.6112
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 162      |
| Iteration     | 23       |
| MaximumReturn | 199      |
| MinimumReturn | 101      |
| TotalSamples  | 83325    |
----------------------------
itr #24 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.43743714690208435
Validation loss = 0.4392188787460327
Validation loss = 0.4386516213417053
Validation loss = 0.44086748361587524
Validation loss = 0.4405604600906372
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4409931004047394
Validation loss = 0.4415777623653412
Validation loss = 0.4451359510421753
Validation loss = 0.4439365565776825
Validation loss = 0.44412288069725037
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.43899860978126526
Validation loss = 0.43891027569770813
Validation loss = 0.43945685029029846
Validation loss = 0.4385972321033478
Validation loss = 0.4393424391746521
Validation loss = 0.4402995705604553
Validation loss = 0.4409327208995819
Validation loss = 0.44136252999305725
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.44002747535705566
Validation loss = 0.4397663474082947
Validation loss = 0.43940261006355286
Validation loss = 0.4409395158290863
Validation loss = 0.44012826681137085
Validation loss = 0.4407658874988556
Validation loss = 0.4406296908855438
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4386976361274719
Validation loss = 0.4370400011539459
Validation loss = 0.4377329647541046
Validation loss = 0.44013357162475586
Validation loss = 0.4398767948150635
Validation loss = 0.4408581852912903
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 2.6134185303514377
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 2.6251993620414673
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 2.627388535031847
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 2.627980922098569
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 2.638095238095238
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 2.652931854199683
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 2.6582278481012658
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 2.674565560821485
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 2.695583596214511
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 2.7133858267716535
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 2.7248427672955975
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 2.720565149136578
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 2.739811912225705
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 2.7652582159624415
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 2.7796875
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 2.7893915756630263
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 2.8021806853582554
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 2.8102643856920686
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 2.812111801242236
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 2.827906976744186
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 2.8343653250773992
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 2.8377125193199384
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 2.859567901234568
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 2.87211093990755
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 2.8846153846153846
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 24       |
| MaximumReturn | 199      |
| MinimumReturn | 112      |
| TotalSamples  | 86658    |
----------------------------
itr #25 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4379187822341919
Validation loss = 0.4405444860458374
Validation loss = 0.44220560789108276
Validation loss = 0.441869854927063
Validation loss = 0.44225940108299255
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4437577426433563
Validation loss = 0.44391995668411255
Validation loss = 0.445421427488327
Validation loss = 0.4448215961456299
Validation loss = 0.4452732503414154
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4419994354248047
Validation loss = 0.44129690527915955
Validation loss = 0.4411936104297638
Validation loss = 0.44324713945388794
Validation loss = 0.4431743323802948
Validation loss = 0.44426679611206055
Validation loss = 0.44248896837234497
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.44455817341804504
Validation loss = 0.4413234293460846
Validation loss = 0.4437958002090454
Validation loss = 0.4434731602668762
Validation loss = 0.44234421849250793
Validation loss = 0.4436335563659668
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4380403161048889
Validation loss = 0.4399168789386749
Validation loss = 0.440891832113266
Validation loss = 0.4411914646625519
Validation loss = 0.44098928570747375
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 2.9047619047619047
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 2.9263803680981595
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 2.9326186830015315
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 2.937308868501529
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 2.954198473282443
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 22
average number of affinization = 2.9832317073170733
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 2.990867579908676
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 2.992401215805471
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 3.0060698027314112
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 3.021212121212121
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 3.031770045385779
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 3.0513595166163143
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 3.0542986425339365
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 3.0647590361445785
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 3.0736842105263156
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 3.0870870870870872
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 3.1049475262368817
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 3.1152694610778444
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 3.118086696562033
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 3.126865671641791
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 3.135618479880775
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 3.1547619047619047
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 3.1589895988112926
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 3.1602373887240356
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 3.1792592592592595
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 164      |
| Iteration     | 25       |
| MaximumReturn | 199      |
| MinimumReturn | 121      |
| TotalSamples  | 89991    |
----------------------------
itr #26 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.44035160541534424
Validation loss = 0.4397762417793274
Validation loss = 0.440093070268631
Validation loss = 0.44037920236587524
Validation loss = 0.44012510776519775
Validation loss = 0.4400591552257538
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.44232791662216187
Validation loss = 0.442030131816864
Validation loss = 0.44215089082717896
Validation loss = 0.4431719183921814
Validation loss = 0.4453546404838562
Validation loss = 0.44308751821517944
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.44089218974113464
Validation loss = 0.44143494963645935
Validation loss = 0.44105178117752075
Validation loss = 0.4428213834762573
Validation loss = 0.44184672832489014
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4418054223060608
Validation loss = 0.44225382804870605
Validation loss = 0.4405958354473114
Validation loss = 0.4431701600551605
Validation loss = 0.4418284595012665
Validation loss = 0.4424733519554138
Validation loss = 0.4418846666812897
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4394121468067169
Validation loss = 0.43945786356925964
Validation loss = 0.4402623474597931
Validation loss = 0.43732860684394836
Validation loss = 0.4397876262664795
Validation loss = 0.43986380100250244
Validation loss = 0.439833402633667
Validation loss = 0.4437962770462036
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 3.198224852071006
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 3.209748892171344
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 3.218289085545723
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 3.228276877761414
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 3.2294117647058824
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 3.2422907488986783
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 3.2609970674486806
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 3.279648609077599
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 3.2953216374269005
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 3.3124087591240876
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 22
average number of affinization = 3.3396501457725947
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 3.3522561863173217
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 3.363372093023256
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 3.3759071117561685
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 3.389855072463768
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 3.418234442836469
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 3.4407514450867054
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 3.4574314574314573
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 3.478386167146974
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 3.4920863309352517
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 3.5028735632183907
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 3.5007173601147774
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 3.511461318051576
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 3.51931330472103
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 3.537142857142857
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 26       |
| MaximumReturn | 199      |
| MinimumReturn | 105      |
| TotalSamples  | 93324    |
----------------------------
itr #27 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.44082289934158325
Validation loss = 0.44137224555015564
Validation loss = 0.4403667151927948
Validation loss = 0.4401537775993347
Validation loss = 0.4420401453971863
Validation loss = 0.44227010011672974
Validation loss = 0.44214171171188354
Validation loss = 0.44476932287216187
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.44388484954833984
Validation loss = 0.44522976875305176
Validation loss = 0.4465460181236267
Validation loss = 0.44382718205451965
Validation loss = 0.4456382691860199
Validation loss = 0.4455797076225281
Validation loss = 0.44693049788475037
Validation loss = 0.4461528956890106
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4418582618236542
Validation loss = 0.44099828600883484
Validation loss = 0.4436299204826355
Validation loss = 0.4439297914505005
Validation loss = 0.4467415511608124
Validation loss = 0.4441169798374176
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4445312023162842
Validation loss = 0.44172999262809753
Validation loss = 0.443970263004303
Validation loss = 0.44564977288246155
Validation loss = 0.4459761679172516
Validation loss = 0.4460802376270294
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4412815272808075
Validation loss = 0.44239306449890137
Validation loss = 0.4416229724884033
Validation loss = 0.4423835873603821
Validation loss = 0.44247427582740784
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 3.549215406562054
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 3.5626780626780628
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 3.5761024182076815
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 3.580965909090909
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 3.5971631205673757
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 3.6274787535410766
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 3.6478076379066477
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 3.6652542372881354
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 3.684062059238364
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 3.6971830985915495
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 3.711673699015471
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 3.7148876404494384
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 24
average number of affinization = 3.7433380084151473
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 3.7633053221288515
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 26
average number of affinization = 3.7944055944055943
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 3.810055865921788
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 3.8284518828451883
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 3.84958217270195
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 3.8720445062586926
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 3.8930555555555557
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 3.909847434119279
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 3.904432132963989
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 3.921161825726141
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 3.9392265193370166
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 3.9682758620689653
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 27       |
| MaximumReturn | 199      |
| MinimumReturn | 119      |
| TotalSamples  | 96657    |
----------------------------
itr #28 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.44182059168815613
Validation loss = 0.4392296373844147
Validation loss = 0.44192036986351013
Validation loss = 0.4440651834011078
Validation loss = 0.44228804111480713
Validation loss = 0.4419879615306854
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4446404278278351
Validation loss = 0.4452333152294159
Validation loss = 0.4425938129425049
Validation loss = 0.4439043700695038
Validation loss = 0.44774165749549866
Validation loss = 0.4467032849788666
Validation loss = 0.45016059279441833
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4411417543888092
Validation loss = 0.43934884667396545
Validation loss = 0.44157537817955017
Validation loss = 0.4412784278392792
Validation loss = 0.4428759813308716
Validation loss = 0.4451814889907837
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4429844617843628
Validation loss = 0.44342994689941406
Validation loss = 0.44254299998283386
Validation loss = 0.4425235688686371
Validation loss = 0.4444478452205658
Validation loss = 0.4444042146205902
Validation loss = 0.44489753246307373
Validation loss = 0.4471067488193512
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4400577247142792
Validation loss = 0.4401823580265045
Validation loss = 0.44132912158966064
Validation loss = 0.44156041741371155
Validation loss = 0.4419825077056885
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 3.975206611570248
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 3.9889958734525446
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 3.9972527472527473
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 4.012345679012346
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 4.021917808219178
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 4.047879616963065
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 4.065573770491803
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 4.094133697135061
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 4.1035422343324255
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 4.122448979591836
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 4.145380434782608
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 4.16010854816825
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 22
average number of affinization = 4.184281842818428
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 4.194857916102841
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 4.197297297297298
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 4.222672064777328
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 4.218328840970351
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 26
average number of affinization = 4.24764468371467
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 4.258064516129032
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 4.263087248322147
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 4.2882037533512065
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 4.306559571619813
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 4.324866310160428
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 4.333778371161548
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 4.3533333333333335
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 177      |
| Iteration     | 28       |
| MaximumReturn | 199      |
| MinimumReturn | 127      |
| TotalSamples  | 99990    |
----------------------------
itr #29 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.44116246700286865
Validation loss = 0.4412752091884613
Validation loss = 0.4414743483066559
Validation loss = 0.4414953589439392
Validation loss = 0.4445365071296692
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4457564055919647
Validation loss = 0.4440472722053528
Validation loss = 0.4449431896209717
Validation loss = 0.44607383012771606
Validation loss = 0.45089614391326904
Validation loss = 0.44669586420059204
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.44115638732910156
Validation loss = 0.4416710138320923
Validation loss = 0.44234204292297363
Validation loss = 0.44222405552864075
Validation loss = 0.4407249391078949
Validation loss = 0.4414490759372711
Validation loss = 0.44422003626823425
Validation loss = 0.4414673149585724
Validation loss = 0.4430376887321472
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4435092806816101
Validation loss = 0.4431515634059906
Validation loss = 0.4442891776561737
Validation loss = 0.44472214579582214
Validation loss = 0.4455645680427551
Validation loss = 0.4451213479042053
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4410818815231323
Validation loss = 0.44237643480300903
Validation loss = 0.44120123982429504
Validation loss = 0.4401385486125946
Validation loss = 0.4398513436317444
Validation loss = 0.44099003076553345
Validation loss = 0.4410940408706665
Validation loss = 0.44145941734313965
Validation loss = 0.44288498163223267
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 4.383488681757656
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 4.402925531914893
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 4.422310756972111
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 4.440318302387268
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 4.470198675496689
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 4.482804232804233
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 36
average number of affinization = 4.52443857331572
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 28
average number of affinization = 4.555408970976253
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 24
average number of affinization = 4.5810276679841895
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 4.602631578947369
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 4.617608409986859
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 4.6299212598425195
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 4.651376146788991
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 4.6662303664921465
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 4.686274509803922
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 4.6945169712793735
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 4.713168187744459
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 4.7421875
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 4.756827048114435
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 4.78051948051948
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 4.7989623865110245
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 4.805699481865285
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 24
average number of affinization = 4.830530401034929
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 4.82687338501292
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 4.84
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 29       |
| MaximumReturn | 199      |
| MinimumReturn | 116      |
| TotalSamples  | 103323   |
----------------------------
itr #30 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4444330334663391
Validation loss = 0.44287359714508057
Validation loss = 0.4420962929725647
Validation loss = 0.44168925285339355
Validation loss = 0.4432384967803955
Validation loss = 0.44434332847595215
Validation loss = 0.44738829135894775
Validation loss = 0.4440973997116089
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4484616816043854
Validation loss = 0.44699835777282715
Validation loss = 0.44707244634628296
Validation loss = 0.44680869579315186
Validation loss = 0.44712772965431213
Validation loss = 0.4481782615184784
Validation loss = 0.44794631004333496
Validation loss = 0.4496651887893677
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4478881061077118
Validation loss = 0.4457339942455292
Validation loss = 0.44377806782722473
Validation loss = 0.44513383507728577
Validation loss = 0.44526979327201843
Validation loss = 0.44759896397590637
Validation loss = 0.44601863622665405
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.44660595059394836
Validation loss = 0.44598522782325745
Validation loss = 0.4454089105129242
Validation loss = 0.4475407600402832
Validation loss = 0.4472132921218872
Validation loss = 0.4473326802253723
Validation loss = 0.44854089617729187
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.44284874200820923
Validation loss = 0.4442659318447113
Validation loss = 0.4433166980743408
Validation loss = 0.44298624992370605
Validation loss = 0.4444638192653656
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 28
average number of affinization = 4.869845360824742
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 4.88030888030888
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 4.889460154241645
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 4.912708600770218
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 26
average number of affinization = 4.93974358974359
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 4.960307298335468
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 4.961636828644501
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 4.979565772669221
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 5.002551020408164
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 28
average number of affinization = 5.031847133757962
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 5.048346055979644
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 5.058449809402795
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 5.0761421319796955
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 5.092522179974652
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 5.10253164556962
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 5.127686472819216
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 5.150252525252525
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 5.16015132408575
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 35
average number of affinization = 5.197732997481109
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 5.231446540880503
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 5.256281407035176
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 5.281053952321204
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 5.296992481203008
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 5.314142678347935
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 5.3325
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 30       |
| MaximumReturn | 199      |
| MinimumReturn | 103      |
| TotalSamples  | 106656   |
----------------------------
itr #31 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.44292810559272766
Validation loss = 0.44243234395980835
Validation loss = 0.4430423974990845
Validation loss = 0.44303271174430847
Validation loss = 0.44414734840393066
Validation loss = 0.4454698860645294
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.44597429037094116
Validation loss = 0.44744473695755005
Validation loss = 0.4470561444759369
Validation loss = 0.44664159417152405
Validation loss = 0.4477216899394989
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4436829090118408
Validation loss = 0.4433307349681854
Validation loss = 0.44400107860565186
Validation loss = 0.44450220465660095
Validation loss = 0.44621536135673523
Validation loss = 0.44308269023895264
Validation loss = 0.4456751346588135
Validation loss = 0.4447134733200073
Validation loss = 0.4467420279979706
Validation loss = 0.44792455434799194
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4457796812057495
Validation loss = 0.4464734196662903
Validation loss = 0.44534599781036377
Validation loss = 0.44410064816474915
Validation loss = 0.44533562660217285
Validation loss = 0.4473797678947449
Validation loss = 0.4500276446342468
Validation loss = 0.4489557445049286
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4431055784225464
Validation loss = 0.44205376505851746
Validation loss = 0.4418494701385498
Validation loss = 0.44385847449302673
Validation loss = 0.4409814774990082
Validation loss = 0.44293442368507385
Validation loss = 0.44281819462776184
Validation loss = 0.44569551944732666
Validation loss = 0.44592782855033875
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 5.352059925093633
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 5.386533665835412
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 5.428393524283935
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 35
average number of affinization = 5.465174129353234
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 28
average number of affinization = 5.493167701863354
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 5.5334987593052105
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 5.567534076827757
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 30
average number of affinization = 5.5977722772277225
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 5.621755253399258
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 28
average number of affinization = 5.649382716049383
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 5.668310727496918
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 5.713054187192118
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 5.746617466174662
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 29
average number of affinization = 5.775184275184275
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 5.7828220858895705
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 5.806372549019608
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 5.817625458996328
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 5.858190709046455
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 5.87057387057387
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 28
average number of affinization = 5.897560975609756
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 5.903775883069428
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 5.927007299270073
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 5.9501822600243015
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 22
average number of affinization = 5.969660194174757
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 35
average number of affinization = 6.004848484848485
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 31       |
| MaximumReturn | 198      |
| MinimumReturn | 120      |
| TotalSamples  | 109989   |
----------------------------
itr #32 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4427403211593628
Validation loss = 0.44196027517318726
Validation loss = 0.44217729568481445
Validation loss = 0.4426797032356262
Validation loss = 0.44334059953689575
Validation loss = 0.4438266456127167
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4463306665420532
Validation loss = 0.4462137520313263
Validation loss = 0.44608333706855774
Validation loss = 0.44671565294265747
Validation loss = 0.44594863057136536
Validation loss = 0.44691988825798035
Validation loss = 0.4488476514816284
Validation loss = 0.44774508476257324
Validation loss = 0.4492802917957306
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4455578923225403
Validation loss = 0.44350242614746094
Validation loss = 0.4452793598175049
Validation loss = 0.44549793004989624
Validation loss = 0.4475400149822235
Validation loss = 0.4450090229511261
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.44479817152023315
Validation loss = 0.4459869861602783
Validation loss = 0.44529199600219727
Validation loss = 0.4458927810192108
Validation loss = 0.4497135281562805
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.44443169236183167
Validation loss = 0.44161033630371094
Validation loss = 0.4423448145389557
Validation loss = 0.44432637095451355
Validation loss = 0.4460974633693695
Validation loss = 0.4452155828475952
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 6.030266343825666
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 6.055622732769045
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 6.073671497584541
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 6.094089264173704
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 6.102409638554217
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 6.134777376654633
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 6.159855769230769
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 30
average number of affinization = 6.1884753901560625
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 22
average number of affinization = 6.207434052757794
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 6.225149700598802
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 37
average number of affinization = 6.261961722488039
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 6.273596176821983
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 41
average number of affinization = 6.315035799522673
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 43
average number of affinization = 6.358760429082241
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 45
average number of affinization = 6.404761904761905
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 30
average number of affinization = 6.4328180737217595
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 37
average number of affinization = 6.4691211401425175
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 6.500593119810202
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 6.539099526066351
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 6.545562130177514
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 40
average number of affinization = 6.585106382978723
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 6.623376623376624
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 6.647405660377358
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 43
average number of affinization = 6.6902237926972905
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 28
average number of affinization = 6.715294117647058
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 32       |
| MaximumReturn | 199      |
| MinimumReturn | 115      |
| TotalSamples  | 113322   |
----------------------------
itr #33 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4438459873199463
Validation loss = 0.44265004992485046
Validation loss = 0.44441840052604675
Validation loss = 0.4423203468322754
Validation loss = 0.4442419409751892
Validation loss = 0.4445132613182068
Validation loss = 0.44487452507019043
Validation loss = 0.44287893176078796
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4472850263118744
Validation loss = 0.44888836145401
Validation loss = 0.4490261375904083
Validation loss = 0.44788557291030884
Validation loss = 0.4482426345348358
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.44302162528038025
Validation loss = 0.44263404607772827
Validation loss = 0.44452929496765137
Validation loss = 0.44819217920303345
Validation loss = 0.4475874900817871
Validation loss = 0.44870635867118835
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4448529779911041
Validation loss = 0.44649335741996765
Validation loss = 0.4458582103252411
Validation loss = 0.4463529586791992
Validation loss = 0.4461338222026825
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.44130733609199524
Validation loss = 0.44386181235313416
Validation loss = 0.44103431701660156
Validation loss = 0.44400152564048767
Validation loss = 0.4449549615383148
Validation loss = 0.4436366856098175
Validation loss = 0.4451453387737274
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 45
average number of affinization = 6.760282021151586
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 6.789906103286385
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 31
average number of affinization = 6.8182883939038685
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 6.847775175644028
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 6.87719298245614
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 28
average number of affinization = 6.901869158878505
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 6.9311551925320884
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 6.961538461538462
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 29
average number of affinization = 6.987194412107101
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 36
average number of affinization = 7.02093023255814
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 30
average number of affinization = 7.0476190476190474
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 43
average number of affinization = 7.089327146171693
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 7.107763615295481
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 7.144675925925926
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 7.174566473988439
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 36
average number of affinization = 7.2078521939953815
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 37
average number of affinization = 7.242214532871972
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 37
average number of affinization = 7.276497695852535
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 47
average number of affinization = 7.322209436133487
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 7.3517241379310345
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 7.39150401836969
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 7.397935779816514
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 7.420389461626575
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 7.449656750572083
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 36
average number of affinization = 7.482285714285714
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 33       |
| MaximumReturn | 198      |
| MinimumReturn | 113      |
| TotalSamples  | 116655   |
----------------------------
itr #34 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4423283636569977
Validation loss = 0.44412505626678467
Validation loss = 0.4450683295726776
Validation loss = 0.4457157254219055
Validation loss = 0.4445585310459137
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.44845008850097656
Validation loss = 0.44562122225761414
Validation loss = 0.44810232520103455
Validation loss = 0.44704756140708923
Validation loss = 0.4496149718761444
Validation loss = 0.44817066192626953
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.443104088306427
Validation loss = 0.44424596428871155
Validation loss = 0.4447178542613983
Validation loss = 0.4436127841472626
Validation loss = 0.4451102018356323
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4458787143230438
Validation loss = 0.4459410011768341
Validation loss = 0.4441779553890228
Validation loss = 0.4449194073677063
Validation loss = 0.4464479684829712
Validation loss = 0.44839736819267273
Validation loss = 0.44865697622299194
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.44258394837379456
Validation loss = 0.4431498646736145
Validation loss = 0.44212353229522705
Validation loss = 0.4430510699748993
Validation loss = 0.44429442286491394
Validation loss = 0.44579973816871643
Validation loss = 0.4421916604042053
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 7.494292237442922
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 7.523375142531357
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 31
average number of affinization = 7.550113895216401
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 22
average number of affinization = 7.566552901023891
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 30
average number of affinization = 7.592045454545454
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 7.619750283768445
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 7.63718820861678
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 44
average number of affinization = 7.67836919592299
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 7.705882352941177
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 22
average number of affinization = 7.722033898305085
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 31
average number of affinization = 7.748306997742664
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 7.782412626832018
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 7.809684684684685
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 29
average number of affinization = 7.833520809898762
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 7.861797752808989
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 7.888888888888889
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 7.891255605381166
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 45
average number of affinization = 7.932810750279955
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 58
average number of affinization = 7.988814317673378
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 37
average number of affinization = 8.021229050279329
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 37
average number of affinization = 8.053571428571429
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 34
average number of affinization = 8.082497212931996
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 8.101336302895323
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 8.129032258064516
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 34
average number of affinization = 8.157777777777778
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 34       |
| MaximumReturn | 199      |
| MinimumReturn | 119      |
| TotalSamples  | 119988   |
----------------------------
itr #35 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.44127514958381653
Validation loss = 0.43969622254371643
Validation loss = 0.4418048560619354
Validation loss = 0.44128912687301636
Validation loss = 0.442228764295578
Validation loss = 0.4425297677516937
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.44366130232810974
Validation loss = 0.4454896152019501
Validation loss = 0.44417741894721985
Validation loss = 0.44449561834335327
Validation loss = 0.44445762038230896
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.44075414538383484
Validation loss = 0.44156593084335327
Validation loss = 0.4415918290615082
Validation loss = 0.4409056305885315
Validation loss = 0.4429156184196472
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4439105987548828
Validation loss = 0.44532454013824463
Validation loss = 0.4439220726490021
Validation loss = 0.44355645775794983
Validation loss = 0.4440790116786957
Validation loss = 0.4450019299983978
Validation loss = 0.44526705145835876
Validation loss = 0.44568929076194763
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.44197189807891846
Validation loss = 0.441240131855011
Validation loss = 0.44279077649116516
Validation loss = 0.44089701771736145
Validation loss = 0.4429076910018921
Validation loss = 0.4432249069213867
Validation loss = 0.4429689347743988
Validation loss = 0.4450460374355316
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 35
average number of affinization = 8.187569367369589
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 31
average number of affinization = 8.212860310421286
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 41
average number of affinization = 8.249169435215947
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 8.275442477876107
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 8.30828729281768
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 8.328918322295806
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 8.362734288864388
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 8.381057268722467
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 8.401540154015402
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 29
average number of affinization = 8.424175824175824
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 43
average number of affinization = 8.462129527991218
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 28
average number of affinization = 8.483552631578947
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 49
average number of affinization = 8.527929901423878
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 8.560175054704596
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 30
average number of affinization = 8.583606557377049
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 24
average number of affinization = 8.600436681222707
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 26
average number of affinization = 8.619411123227916
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 8.632897603485839
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 41
average number of affinization = 8.668117519042438
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 30
average number of affinization = 8.691304347826087
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 29
average number of affinization = 8.713355048859935
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 8.739696312364424
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 8.755146262188516
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 36
average number of affinization = 8.784632034632034
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 8.80972972972973
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 35       |
| MaximumReturn | 199      |
| MinimumReturn | 124      |
| TotalSamples  | 123321   |
----------------------------
itr #36 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4410962760448456
Validation loss = 0.44286322593688965
Validation loss = 0.4415883719921112
Validation loss = 0.4422086775302887
Validation loss = 0.440905898809433
Validation loss = 0.44248947501182556
Validation loss = 0.443288654088974
Validation loss = 0.44267532229423523
Validation loss = 0.445496141910553
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4441816210746765
Validation loss = 0.44456663727760315
Validation loss = 0.445756196975708
Validation loss = 0.4454335868358612
Validation loss = 0.44521865248680115
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.44162237644195557
Validation loss = 0.44076135754585266
Validation loss = 0.43986424803733826
Validation loss = 0.4411618113517761
Validation loss = 0.4425623118877411
Validation loss = 0.4419233500957489
Validation loss = 0.44343307614326477
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4446520507335663
Validation loss = 0.4440385401248932
Validation loss = 0.4431672692298889
Validation loss = 0.4477374255657196
Validation loss = 0.44536593556404114
Validation loss = 0.44519636034965515
Validation loss = 0.4455680847167969
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.44253435730934143
Validation loss = 0.4428221881389618
Validation loss = 0.44245603680610657
Validation loss = 0.4434967637062073
Validation loss = 0.44231176376342773
Validation loss = 0.443161278963089
Validation loss = 0.44465237855911255
Validation loss = 0.4454011619091034
Validation loss = 0.44526612758636475
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 37
average number of affinization = 8.840172786177106
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 46
average number of affinization = 8.880258899676376
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 49
average number of affinization = 8.923491379310345
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 44
average number of affinization = 8.96124865446717
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 45
average number of affinization = 9.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 9.032223415682063
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 9.05793991416309
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 9.082529474812434
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 9.107066381156317
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 9.132620320855615
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 9.151709401709402
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 9.186766275346852
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 9.205756929637527
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 41
average number of affinization = 9.23961661341853
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 34
average number of affinization = 9.26595744680851
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 9.2826780021254
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 29
average number of affinization = 9.303609341825902
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 9.328738069989395
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 41
average number of affinization = 9.36228813559322
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 9.371428571428572
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 35
average number of affinization = 9.398520084566597
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 46
average number of affinization = 9.437170010559662
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 9.467299578059071
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 9.501580611169652
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 29
average number of affinization = 9.522105263157895
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 164      |
| Iteration     | 36       |
| MaximumReturn | 199      |
| MinimumReturn | 111      |
| TotalSamples  | 126654   |
----------------------------
itr #37 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4433005750179291
Validation loss = 0.44269877672195435
Validation loss = 0.4435727596282959
Validation loss = 0.44280561804771423
Validation loss = 0.4424937665462494
Validation loss = 0.44462689757347107
Validation loss = 0.44258350133895874
Validation loss = 0.4426112473011017
Validation loss = 0.4452846646308899
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4447208046913147
Validation loss = 0.4441857933998108
Validation loss = 0.4420681297779083
Validation loss = 0.445752888917923
Validation loss = 0.44394204020500183
Validation loss = 0.44663479924201965
Validation loss = 0.4466424584388733
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.44110241532325745
Validation loss = 0.442571759223938
Validation loss = 0.441976398229599
Validation loss = 0.4426511824131012
Validation loss = 0.44195517897605896
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.44630736112594604
Validation loss = 0.4448087513446808
Validation loss = 0.4438543915748596
Validation loss = 0.4471488296985626
Validation loss = 0.4474234879016876
Validation loss = 0.44689586758613586
Validation loss = 0.4459078907966614
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4449228048324585
Validation loss = 0.4436098635196686
Validation loss = 0.444577157497406
Validation loss = 0.44304195046424866
Validation loss = 0.44569313526153564
Validation loss = 0.44312813878059387
Validation loss = 0.4448704123497009
Validation loss = 0.44464433193206787
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 36
average number of affinization = 9.549947423764458
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 34
average number of affinization = 9.57563025210084
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 9.609653725078699
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 43
average number of affinization = 9.644654088050315
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 9.66910994764398
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 45
average number of affinization = 9.706066945606695
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 9.735632183908047
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 29
average number of affinization = 9.755741127348642
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 28
average number of affinization = 9.774765380604796
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 45
average number of affinization = 9.811458333333333
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 35
average number of affinization = 9.837669094693029
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 9.867983367983369
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 41
average number of affinization = 9.90031152647975
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 46
average number of affinization = 9.937759336099585
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 36
average number of affinization = 9.964766839378239
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 9.98033126293996
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 44
average number of affinization = 10.01551189245088
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 48
average number of affinization = 10.054752066115702
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 10.08359133126935
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 47
average number of affinization = 10.121649484536082
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 10.13491246138002
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 50
average number of affinization = 10.175925925925926
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 10.205549845837616
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 69
average number of affinization = 10.265913757700206
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 10.283076923076923
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 181      |
| Iteration     | 37       |
| MaximumReturn | 199      |
| MinimumReturn | 124      |
| TotalSamples  | 129987   |
----------------------------
itr #38 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4427696466445923
Validation loss = 0.4427323043346405
Validation loss = 0.4414617717266083
Validation loss = 0.44273465871810913
Validation loss = 0.44579964876174927
Validation loss = 0.44336503744125366
Validation loss = 0.44464126229286194
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4444478452205658
Validation loss = 0.44416120648384094
Validation loss = 0.44422242045402527
Validation loss = 0.44497618079185486
Validation loss = 0.4458806812763214
Validation loss = 0.4494052827358246
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4444163739681244
Validation loss = 0.44304388761520386
Validation loss = 0.4434484839439392
Validation loss = 0.44423049688339233
Validation loss = 0.4435916543006897
Validation loss = 0.4426436722278595
Validation loss = 0.44453638792037964
Validation loss = 0.44367024302482605
Validation loss = 0.4440932273864746
Validation loss = 0.4429960548877716
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.445811003446579
Validation loss = 0.44434526562690735
Validation loss = 0.4453391432762146
Validation loss = 0.4452369511127472
Validation loss = 0.44603559374809265
Validation loss = 0.44513770937919617
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4442472457885742
Validation loss = 0.44493573904037476
Validation loss = 0.4436746835708618
Validation loss = 0.44444677233695984
Validation loss = 0.4451597332954407
Validation loss = 0.44485369324684143
Validation loss = 0.4458092451095581
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 41
average number of affinization = 10.31454918032787
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 29
average number of affinization = 10.33367451381781
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 10.348670756646216
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 30
average number of affinization = 10.368743615934628
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 10.379591836734694
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 52
average number of affinization = 10.422018348623853
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 47
average number of affinization = 10.459266802443992
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 28
average number of affinization = 10.477110885045779
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 41
average number of affinization = 10.508130081300813
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 10.52994923857868
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 36
average number of affinization = 10.55578093306288
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 10.570415400202634
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 43
average number of affinization = 10.603238866396762
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 50
average number of affinization = 10.643073811931243
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 49
average number of affinization = 10.681818181818182
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 43
average number of affinization = 10.714429868819375
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 35
average number of affinization = 10.73891129032258
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 34
average number of affinization = 10.76233635448137
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 47
average number of affinization = 10.798792756539235
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 10.815075376884423
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 31
average number of affinization = 10.835341365461847
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 52
average number of affinization = 10.876629889669006
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 43
average number of affinization = 10.908817635270541
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 46
average number of affinization = 10.943943943943944
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 28
average number of affinization = 10.961
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 38       |
| MaximumReturn | 198      |
| MinimumReturn | 126      |
| TotalSamples  | 133320   |
----------------------------
itr #39 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.44662782549858093
Validation loss = 0.4429129958152771
Validation loss = 0.44374802708625793
Validation loss = 0.4436967968940735
Validation loss = 0.44469499588012695
Validation loss = 0.443209707736969
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.44438695907592773
Validation loss = 0.44437164068222046
Validation loss = 0.44906944036483765
Validation loss = 0.4463738799095154
Validation loss = 0.44571444392204285
Validation loss = 0.4457515478134155
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4426456391811371
Validation loss = 0.44242429733276367
Validation loss = 0.4434989094734192
Validation loss = 0.44371548295021057
Validation loss = 0.4433213472366333
Validation loss = 0.4439215362071991
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4451000690460205
Validation loss = 0.44426822662353516
Validation loss = 0.4448518455028534
Validation loss = 0.44603872299194336
Validation loss = 0.4478657841682434
Validation loss = 0.44575339555740356
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.44389447569847107
Validation loss = 0.44493913650512695
Validation loss = 0.44513821601867676
Validation loss = 0.44561296701431274
Validation loss = 0.4459898769855499
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 10.992007992007991
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 11.022954091816366
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 35
average number of affinization = 11.046859421734796
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 29
average number of affinization = 11.064741035856574
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 11.091542288557214
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 49
average number of affinization = 11.129224652087474
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 28
average number of affinization = 11.145978152929494
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 47
average number of affinization = 11.181547619047619
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 36
average number of affinization = 11.206144697720516
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 54
average number of affinization = 11.24851485148515
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 28
average number of affinization = 11.265084075173096
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 30
average number of affinization = 11.283596837944664
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 44
average number of affinization = 11.31589338598223
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 31
average number of affinization = 11.335305719921104
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 11.361576354679803
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 44
average number of affinization = 11.393700787401574
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 37
average number of affinization = 11.418879056047198
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 52
average number of affinization = 11.458742632612967
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 11.462217860647694
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 29
average number of affinization = 11.479411764705882
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 53
average number of affinization = 11.520078354554359
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 22
average number of affinization = 11.530332681017612
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 11.541544477028348
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 34
average number of affinization = 11.5634765625
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 36
average number of affinization = 11.587317073170732
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 39       |
| MaximumReturn | 199      |
| MinimumReturn | 121      |
| TotalSamples  | 136653   |
----------------------------
