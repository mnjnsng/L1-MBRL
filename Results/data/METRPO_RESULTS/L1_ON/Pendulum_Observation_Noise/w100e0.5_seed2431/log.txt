Logging to experiments/gym_pendulumO01/Pendulum_Observation_Noise/w100e0.5_seed2431
Print configuration .....
{'env_name': 'gym_pendulumO01', 'random_seeds': [3214, 2431, 2531, 2231], 'save_variables': False, 'model_save_dir': '/tmp/pendulumO01_models/', 'restore_variables': False, 'start_onpol_iter': 0, 'onpol_iters': 40, 'num_path_random': 25, 'num_path_onpol': 25, 'env_horizon': 200, 'max_train_data': 200000, 'max_val_data': 100000, 'discard_ratio': 0.0, 'dynamics': {'pre_training': {'mode': 'intrinsic_reward', 'itr': 0, 'policy_itr': 20}, 'model': 'nn', 'ensemble': True, 'ensemble_model_count': 5, 'enable_particle_ensemble': True, 'particles': 5, 'obs_var': 1.0, 'intrinsic_reward_coeff': 1.0, 'ita': 1.0, 'mode': 'random', 'val': True, 'n_layers': 4, 'hidden_size': 1000, 'activation': 'relu', 'batch_size': 1000, 'learning_rate': 0.001, 'reg_coeff': 0.0, 'epochs': 200, 'kfac_params': {'learning_rate': 0.1, 'damping': 0.001, 'momentum': 0.9, 'kl_clip': 0.0001, 'cov_ema_decay': 0.99}}, 'policy': {'network_shape': [64, 64], 'init_logstd': 0.0, 'activation': 'tanh', 'reinitialize_every_itr': False}, 'trpo': {'horizon': 200, 'gamma': 0.99, 'step_size': 0.01, 'iterations': 20, 'batch_size': 50000, 'gae': 0.95, 'visualization': False, 'visualize_iterations': [0]}, 'algo': 'trpo'}
Generating random rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating random rollouts.
Creating normalization for training data.
Done creating normalization for training data.
Particle ensemble enabled? True
An ensemble of 5 dynamics model <class 'model.dynamics.NNDynamicsModel'> initialized
Train dynamics model with intrinsic reward only? False
Pre-training enabled. Using only intrinsic reward.
Pre-training dynamics model for 0 iterations...
Done pre-training dynamics model.
Using external reward only.
itr #0 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.32153165340423584
Validation loss = 0.19768080115318298
Validation loss = 0.1870446354150772
Validation loss = 0.18469277024269104
Validation loss = 0.18497778475284576
Validation loss = 0.18281938135623932
Validation loss = 0.18642787635326385
Validation loss = 0.18289139866828918
Validation loss = 0.1808215081691742
Validation loss = 0.18266500532627106
Validation loss = 0.17725402116775513
Validation loss = 0.18128599226474762
Validation loss = 0.1741858869791031
Validation loss = 0.17090444266796112
Validation loss = 0.1681767702102661
Validation loss = 0.16729845106601715
Validation loss = 0.1617564857006073
Validation loss = 0.16968035697937012
Validation loss = 0.1597837507724762
Validation loss = 0.16198039054870605
Validation loss = 0.1512724757194519
Validation loss = 0.1531529277563095
Validation loss = 0.16527995467185974
Validation loss = 0.15648499131202698
Validation loss = 0.1496208757162094
Validation loss = 0.147050142288208
Validation loss = 0.15198731422424316
Validation loss = 0.15033739805221558
Validation loss = 0.1440587341785431
Validation loss = 0.14908234775066376
Validation loss = 0.14341449737548828
Validation loss = 0.14168328046798706
Validation loss = 0.14318867027759552
Validation loss = 0.15443050861358643
Validation loss = 0.14489299058914185
Validation loss = 0.14820347726345062
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3521522581577301
Validation loss = 0.19987386465072632
Validation loss = 0.18738757073879242
Validation loss = 0.18418990075588226
Validation loss = 0.1856793612241745
Validation loss = 0.18288935720920563
Validation loss = 0.18282060325145721
Validation loss = 0.18149776756763458
Validation loss = 0.18239697813987732
Validation loss = 0.17945100367069244
Validation loss = 0.18370524048805237
Validation loss = 0.18382084369659424
Validation loss = 0.17445628345012665
Validation loss = 0.16900719702243805
Validation loss = 0.1731553077697754
Validation loss = 0.16710548102855682
Validation loss = 0.16180504858493805
Validation loss = 0.16884812712669373
Validation loss = 0.15539807081222534
Validation loss = 0.1532885581254959
Validation loss = 0.15597404539585114
Validation loss = 0.1516309678554535
Validation loss = 0.1522809863090515
Validation loss = 0.14684759080410004
Validation loss = 0.14407843351364136
Validation loss = 0.14934168756008148
Validation loss = 0.1446235179901123
Validation loss = 0.14658698439598083
Validation loss = 0.15025916695594788
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.32201921939849854
Validation loss = 0.20177827775478363
Validation loss = 0.1891213208436966
Validation loss = 0.1852390170097351
Validation loss = 0.18493953347206116
Validation loss = 0.1835031360387802
Validation loss = 0.18296051025390625
Validation loss = 0.18335740268230438
Validation loss = 0.18085888028144836
Validation loss = 0.17920652031898499
Validation loss = 0.17823490500450134
Validation loss = 0.17900894582271576
Validation loss = 0.17697754502296448
Validation loss = 0.17973226308822632
Validation loss = 0.17307817935943604
Validation loss = 0.16758254170417786
Validation loss = 0.17166517674922943
Validation loss = 0.16131508350372314
Validation loss = 0.15971863269805908
Validation loss = 0.15526579320430756
Validation loss = 0.1563919633626938
Validation loss = 0.15055502951145172
Validation loss = 0.14819884300231934
Validation loss = 0.14689400792121887
Validation loss = 0.14760729670524597
Validation loss = 0.14367815852165222
Validation loss = 0.14868535101413727
Validation loss = 0.15456414222717285
Validation loss = 0.1483132541179657
Validation loss = 0.149229496717453
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3230249285697937
Validation loss = 0.2097291648387909
Validation loss = 0.187504842877388
Validation loss = 0.1852102428674698
Validation loss = 0.182939350605011
Validation loss = 0.18293710052967072
Validation loss = 0.18578562140464783
Validation loss = 0.1818561851978302
Validation loss = 0.17970816791057587
Validation loss = 0.18090341985225677
Validation loss = 0.17774473130702972
Validation loss = 0.18888282775878906
Validation loss = 0.17564770579338074
Validation loss = 0.1714290976524353
Validation loss = 0.1764124482870102
Validation loss = 0.17658697068691254
Validation loss = 0.16642680764198303
Validation loss = 0.1624128669500351
Validation loss = 0.159809410572052
Validation loss = 0.15874555706977844
Validation loss = 0.15585491061210632
Validation loss = 0.15286463499069214
Validation loss = 0.14947383105754852
Validation loss = 0.14918215572834015
Validation loss = 0.15811537206172943
Validation loss = 0.1509777009487152
Validation loss = 0.14671409130096436
Validation loss = 0.16422079503536224
Validation loss = 0.1461746245622635
Validation loss = 0.1466977745294571
Validation loss = 0.14641132950782776
Validation loss = 0.15501558780670166
Validation loss = 0.14575597643852234
Validation loss = 0.14699828624725342
Validation loss = 0.1429893970489502
Validation loss = 0.14089110493659973
Validation loss = 0.14370355010032654
Validation loss = 0.14259803295135498
Validation loss = 0.1422881931066513
Validation loss = 0.14306123554706573
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.35957539081573486
Validation loss = 0.19777652621269226
Validation loss = 0.18909883499145508
Validation loss = 0.18466082215309143
Validation loss = 0.18448060750961304
Validation loss = 0.1825728714466095
Validation loss = 0.18269899487495422
Validation loss = 0.18454797565937042
Validation loss = 0.18204058706760406
Validation loss = 0.1787823736667633
Validation loss = 0.17977526783943176
Validation loss = 0.17549914121627808
Validation loss = 0.17416764795780182
Validation loss = 0.16886155307292938
Validation loss = 0.16711819171905518
Validation loss = 0.16322852671146393
Validation loss = 0.16970659792423248
Validation loss = 0.1627412885427475
Validation loss = 0.15982051193714142
Validation loss = 0.153819277882576
Validation loss = 0.15475741028785706
Validation loss = 0.16159792244434357
Validation loss = 0.16557356715202332
Validation loss = 0.15451708436012268
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 0        |
| MaximumReturn | 199      |
| MinimumReturn | 91.5     |
| TotalSamples  | 6666     |
----------------------------
itr #1 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.32621410489082336
Validation loss = 0.2226395606994629
Validation loss = 0.2137092798948288
Validation loss = 0.20682044327259064
Validation loss = 0.20904035866260529
Validation loss = 0.20957475900650024
Validation loss = 0.20678354799747467
Validation loss = 0.20816655457019806
Validation loss = 0.21000517904758453
Validation loss = 0.20568300783634186
Validation loss = 0.2060638666152954
Validation loss = 0.20345282554626465
Validation loss = 0.20363973081111908
Validation loss = 0.2046690732240677
Validation loss = 0.20488417148590088
Validation loss = 0.2055438905954361
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3315616846084595
Validation loss = 0.2262144684791565
Validation loss = 0.21977607905864716
Validation loss = 0.2132396548986435
Validation loss = 0.2115994244813919
Validation loss = 0.2094498872756958
Validation loss = 0.21305608749389648
Validation loss = 0.20797841250896454
Validation loss = 0.20958466827869415
Validation loss = 0.20431102812290192
Validation loss = 0.20796310901641846
Validation loss = 0.20680548250675201
Validation loss = 0.20348741114139557
Validation loss = 0.20785415172576904
Validation loss = 0.20441269874572754
Validation loss = 0.20683090388774872
Validation loss = 0.20376558601856232
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3365134000778198
Validation loss = 0.2231692522764206
Validation loss = 0.2156059592962265
Validation loss = 0.21584880352020264
Validation loss = 0.21069736778736115
Validation loss = 0.20988743007183075
Validation loss = 0.205108642578125
Validation loss = 0.2064426690340042
Validation loss = 0.20689606666564941
Validation loss = 0.20541958510875702
Validation loss = 0.2051544189453125
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.32750675082206726
Validation loss = 0.22618965804576874
Validation loss = 0.21253740787506104
Validation loss = 0.21348434686660767
Validation loss = 0.2075950652360916
Validation loss = 0.20633654296398163
Validation loss = 0.20575283467769623
Validation loss = 0.2070615291595459
Validation loss = 0.20505934953689575
Validation loss = 0.20337975025177002
Validation loss = 0.20141613483428955
Validation loss = 0.20306168496608734
Validation loss = 0.2071293592453003
Validation loss = 0.2030690759420395
Validation loss = 0.2058316320180893
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3286072313785553
Validation loss = 0.22138623893260956
Validation loss = 0.21811111271381378
Validation loss = 0.2128525823354721
Validation loss = 0.21097777783870697
Validation loss = 0.21343643963336945
Validation loss = 0.2054900974035263
Validation loss = 0.20733696222305298
Validation loss = 0.20711983740329742
Validation loss = 0.2056799679994583
Validation loss = 0.20442545413970947
Validation loss = 0.2091844081878662
Validation loss = 0.20553743839263916
Validation loss = 0.20352019369602203
Validation loss = 0.20643450319766998
Validation loss = 0.20223186910152435
Validation loss = 0.2082860916852951
Validation loss = 0.2050933837890625
Validation loss = 0.20657168328762054
Validation loss = 0.20511294901371002
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 160      |
| Iteration     | 1        |
| MaximumReturn | 198      |
| MinimumReturn | 99.2     |
| TotalSamples  | 9999     |
----------------------------
itr #2 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.27181392908096313
Validation loss = 0.238493874669075
Validation loss = 0.2346346080303192
Validation loss = 0.2341780662536621
Validation loss = 0.23580162227153778
Validation loss = 0.2342691868543625
Validation loss = 0.241850808262825
Validation loss = 0.24089452624320984
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.2635171413421631
Validation loss = 0.23685172200202942
Validation loss = 0.23370322585105896
Validation loss = 0.24082991480827332
Validation loss = 0.2347007691860199
Validation loss = 0.23749299347400665
Validation loss = 0.234986811876297
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.26743578910827637
Validation loss = 0.23519745469093323
Validation loss = 0.23425082862377167
Validation loss = 0.2357373684644699
Validation loss = 0.2373141497373581
Validation loss = 0.23701564967632294
Validation loss = 0.23771898448467255
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.27254152297973633
Validation loss = 0.23625683784484863
Validation loss = 0.23376932740211487
Validation loss = 0.23647984862327576
Validation loss = 0.24146142601966858
Validation loss = 0.2360110580921173
Validation loss = 0.23577634990215302
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.2671423852443695
Validation loss = 0.2399168312549591
Validation loss = 0.23552851378917694
Validation loss = 0.23461756110191345
Validation loss = 0.23426659405231476
Validation loss = 0.2368360459804535
Validation loss = 0.23584572970867157
Validation loss = 0.2357976883649826
Validation loss = 0.23460504412651062
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 162      |
| Iteration     | 2        |
| MaximumReturn | 194      |
| MinimumReturn | 114      |
| TotalSamples  | 13332    |
----------------------------
itr #3 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.28437694907188416
Validation loss = 0.274950236082077
Validation loss = 0.27198851108551025
Validation loss = 0.2761485278606415
Validation loss = 0.2724651098251343
Validation loss = 0.27197495102882385
Validation loss = 0.2726989984512329
Validation loss = 0.2760256230831146
Validation loss = 0.2723750174045563
Validation loss = 0.2706123888492584
Validation loss = 0.27419722080230713
Validation loss = 0.27655577659606934
Validation loss = 0.27616921067237854
Validation loss = 0.27402952313423157
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.2899523675441742
Validation loss = 0.27349141240119934
Validation loss = 0.27277347445487976
Validation loss = 0.2758837938308716
Validation loss = 0.2710338532924652
Validation loss = 0.27512189745903015
Validation loss = 0.2743385136127472
Validation loss = 0.27149397134780884
Validation loss = 0.27134189009666443
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.2876145839691162
Validation loss = 0.27422043681144714
Validation loss = 0.2742477357387543
Validation loss = 0.271793395280838
Validation loss = 0.2728165090084076
Validation loss = 0.2731233537197113
Validation loss = 0.2745591998100281
Validation loss = 0.2774599492549896
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.2841029465198517
Validation loss = 0.27022191882133484
Validation loss = 0.27218762040138245
Validation loss = 0.2782723903656006
Validation loss = 0.27309250831604004
Validation loss = 0.27476468682289124
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.2870039641857147
Validation loss = 0.2742302119731903
Validation loss = 0.27435269951820374
Validation loss = 0.27060267329216003
Validation loss = 0.27517271041870117
Validation loss = 0.2753343880176544
Validation loss = 0.2727596163749695
Validation loss = 0.27434220910072327
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 3        |
| MaximumReturn | 199      |
| MinimumReturn | 113      |
| TotalSamples  | 16665    |
----------------------------
itr #4 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.30743658542633057
Validation loss = 0.30645737051963806
Validation loss = 0.3131656050682068
Validation loss = 0.3009967505931854
Validation loss = 0.3040616512298584
Validation loss = 0.30458730459213257
Validation loss = 0.3029594123363495
Validation loss = 0.3026183843612671
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.30973708629608154
Validation loss = 0.3057670295238495
Validation loss = 0.30741047859191895
Validation loss = 0.30773061513900757
Validation loss = 0.3040224611759186
Validation loss = 0.30295106768608093
Validation loss = 0.3029329478740692
Validation loss = 0.3018043041229248
Validation loss = 0.30444949865341187
Validation loss = 0.3028358221054077
Validation loss = 0.3054382801055908
Validation loss = 0.30336523056030273
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.30732280015945435
Validation loss = 0.3040674924850464
Validation loss = 0.30623647570610046
Validation loss = 0.3057657778263092
Validation loss = 0.3039480149745941
Validation loss = 0.30339112877845764
Validation loss = 0.30199486017227173
Validation loss = 0.3057496249675751
Validation loss = 0.3023272156715393
Validation loss = 0.30149227380752563
Validation loss = 0.30217525362968445
Validation loss = 0.3025738000869751
Validation loss = 0.3022119104862213
Validation loss = 0.3042612671852112
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.306763619184494
Validation loss = 0.3016316592693329
Validation loss = 0.30540284514427185
Validation loss = 0.30343812704086304
Validation loss = 0.30387407541275024
Validation loss = 0.3059009313583374
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.30515438318252563
Validation loss = 0.3024851977825165
Validation loss = 0.3023516535758972
Validation loss = 0.3042125105857849
Validation loss = 0.3061599135398865
Validation loss = 0.3065333366394043
Validation loss = 0.3034052848815918
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 153      |
| Iteration     | 4        |
| MaximumReturn | 199      |
| MinimumReturn | 102      |
| TotalSamples  | 19998    |
----------------------------
itr #5 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3216095268726349
Validation loss = 0.3198390603065491
Validation loss = 0.31914958357810974
Validation loss = 0.3187843859195709
Validation loss = 0.31634148955345154
Validation loss = 0.3180784285068512
Validation loss = 0.3205416798591614
Validation loss = 0.31950944662094116
Validation loss = 0.31922033429145813
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.32071059942245483
Validation loss = 0.3191426992416382
Validation loss = 0.3170241713523865
Validation loss = 0.3178718686103821
Validation loss = 0.31724342703819275
Validation loss = 0.31963402032852173
Validation loss = 0.31845685839653015
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.32015058398246765
Validation loss = 0.3199554681777954
Validation loss = 0.31661874055862427
Validation loss = 0.31826406717300415
Validation loss = 0.32025736570358276
Validation loss = 0.31752243638038635
Validation loss = 0.3200775980949402
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.31737321615219116
Validation loss = 0.3189522922039032
Validation loss = 0.31666651368141174
Validation loss = 0.32063597440719604
Validation loss = 0.3209344744682312
Validation loss = 0.31681162118911743
Validation loss = 0.3179236948490143
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.32047680020332336
Validation loss = 0.3169279098510742
Validation loss = 0.317893922328949
Validation loss = 0.3200603127479553
Validation loss = 0.3212825357913971
Validation loss = 0.3201138973236084
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 163      |
| Iteration     | 5        |
| MaximumReturn | 198      |
| MinimumReturn | 115      |
| TotalSamples  | 23331    |
----------------------------
itr #6 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.33699947595596313
Validation loss = 0.33467110991477966
Validation loss = 0.3327489495277405
Validation loss = 0.3345518708229065
Validation loss = 0.33866727352142334
Validation loss = 0.33644258975982666
Validation loss = 0.3350432813167572
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.33319151401519775
Validation loss = 0.3401888310909271
Validation loss = 0.33497512340545654
Validation loss = 0.33570802211761475
Validation loss = 0.3339073061943054
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3353802561759949
Validation loss = 0.3362419903278351
Validation loss = 0.3333178758621216
Validation loss = 0.33242785930633545
Validation loss = 0.33499088883399963
Validation loss = 0.33418166637420654
Validation loss = 0.3348481357097626
Validation loss = 0.335563987493515
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3347551226615906
Validation loss = 0.3347206711769104
Validation loss = 0.3330428898334503
Validation loss = 0.33317190408706665
Validation loss = 0.3346753418445587
Validation loss = 0.33408334851264954
Validation loss = 0.3318597674369812
Validation loss = 0.33331334590911865
Validation loss = 0.33409738540649414
Validation loss = 0.3346862494945526
Validation loss = 0.3331660330295563
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.33762413263320923
Validation loss = 0.33587488532066345
Validation loss = 0.33586931228637695
Validation loss = 0.33816298842430115
Validation loss = 0.33370110392570496
Validation loss = 0.3337773382663727
Validation loss = 0.3348679840564728
Validation loss = 0.3345312774181366
Validation loss = 0.3348073661327362
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 6        |
| MaximumReturn | 198      |
| MinimumReturn | 100      |
| TotalSamples  | 26664    |
----------------------------
itr #7 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.34560251235961914
Validation loss = 0.34809204936027527
Validation loss = 0.34549039602279663
Validation loss = 0.3462054133415222
Validation loss = 0.34439370036125183
Validation loss = 0.34590911865234375
Validation loss = 0.3437715172767639
Validation loss = 0.34446510672569275
Validation loss = 0.34516042470932007
Validation loss = 0.3442927598953247
Validation loss = 0.34338852763175964
Validation loss = 0.34454840421676636
Validation loss = 0.3474080264568329
Validation loss = 0.3448595702648163
Validation loss = 0.3464210331439972
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3437076807022095
Validation loss = 0.3467487692832947
Validation loss = 0.34639760851860046
Validation loss = 0.34526634216308594
Validation loss = 0.34424734115600586
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.34505927562713623
Validation loss = 0.3468700647354126
Validation loss = 0.34505245089530945
Validation loss = 0.34694716334342957
Validation loss = 0.3462662100791931
Validation loss = 0.34414196014404297
Validation loss = 0.3452661335468292
Validation loss = 0.34732845425605774
Validation loss = 0.34476110339164734
Validation loss = 0.3471372723579407
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3464195430278778
Validation loss = 0.3465607762336731
Validation loss = 0.34637901186943054
Validation loss = 0.3454354405403137
Validation loss = 0.34432271122932434
Validation loss = 0.3459450900554657
Validation loss = 0.34562909603118896
Validation loss = 0.34479019045829773
Validation loss = 0.3460060954093933
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.34846940636634827
Validation loss = 0.34667083621025085
Validation loss = 0.34603404998779297
Validation loss = 0.34328049421310425
Validation loss = 0.3448892831802368
Validation loss = 0.34458810091018677
Validation loss = 0.34652525186538696
Validation loss = 0.3459448218345642
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 164      |
| Iteration     | 7        |
| MaximumReturn | 199      |
| MinimumReturn | 101      |
| TotalSamples  | 29997    |
----------------------------
itr #8 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3577839136123657
Validation loss = 0.35875481367111206
Validation loss = 0.36031389236450195
Validation loss = 0.36075034737586975
Validation loss = 0.3601534068584442
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3567742109298706
Validation loss = 0.35747215151786804
Validation loss = 0.3555493652820587
Validation loss = 0.3564240038394928
Validation loss = 0.3557991087436676
Validation loss = 0.3554261326789856
Validation loss = 0.3580760657787323
Validation loss = 0.35722967982292175
Validation loss = 0.3558186888694763
Validation loss = 0.357354074716568
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3567158281803131
Validation loss = 0.35773780941963196
Validation loss = 0.3576754927635193
Validation loss = 0.3583061993122101
Validation loss = 0.36011940240859985
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.35741275548934937
Validation loss = 0.356669545173645
Validation loss = 0.35784414410591125
Validation loss = 0.35642898082733154
Validation loss = 0.35899415612220764
Validation loss = 0.3578284978866577
Validation loss = 0.3588370084762573
Validation loss = 0.35770827531814575
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.35973984003067017
Validation loss = 0.35580185055732727
Validation loss = 0.35881417989730835
Validation loss = 0.3570358455181122
Validation loss = 0.35722097754478455
Validation loss = 0.3573544919490814
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.004310344827586207
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.004291845493562232
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.004273504273504274
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.00425531914893617
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.00423728813559322
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.004219409282700422
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.004201680672268907
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0041841004184100415
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.008333333333333333
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.008298755186721992
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.008264462809917356
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.00823045267489712
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.00819672131147541
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.00816326530612245
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.008130081300813009
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.008097165991902834
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.008064516129032258
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.008032128514056224
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.008
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 159      |
| Iteration     | 8        |
| MaximumReturn | 193      |
| MinimumReturn | 107      |
| TotalSamples  | 33330    |
----------------------------
itr #9 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3644663393497467
Validation loss = 0.3635063171386719
Validation loss = 0.3650783896446228
Validation loss = 0.3665982186794281
Validation loss = 0.3648754358291626
Validation loss = 0.36335763335227966
Validation loss = 0.3653222918510437
Validation loss = 0.36625659465789795
Validation loss = 0.36461153626441956
Validation loss = 0.36603444814682007
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3631449341773987
Validation loss = 0.3619791269302368
Validation loss = 0.36262378096580505
Validation loss = 0.3662482798099518
Validation loss = 0.3630286455154419
Validation loss = 0.36500832438468933
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3642534613609314
Validation loss = 0.36283767223358154
Validation loss = 0.3620584309101105
Validation loss = 0.36416536569595337
Validation loss = 0.3678864538669586
Validation loss = 0.364857941865921
Validation loss = 0.36316797137260437
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.36448732018470764
Validation loss = 0.36331483721733093
Validation loss = 0.36605897545814514
Validation loss = 0.3624225854873657
Validation loss = 0.36116454005241394
Validation loss = 0.362282395362854
Validation loss = 0.36287927627563477
Validation loss = 0.3647293448448181
Validation loss = 0.3651607632637024
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3639408349990845
Validation loss = 0.36149609088897705
Validation loss = 0.3630896210670471
Validation loss = 0.3639291524887085
Validation loss = 0.3629129230976105
Validation loss = 0.36788687109947205
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.00796812749003984
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.007936507936507936
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.011857707509881422
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.011811023622047244
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.011764705882352941
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.01171875
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.011673151750972763
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.011627906976744186
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.011583011583011582
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.011538461538461539
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.011494252873563218
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.011450381679389313
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.015209125475285171
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.015151515151515152
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.018867924528301886
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.018796992481203006
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.018726591760299626
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.018656716417910446
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.01858736059479554
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.022222222222222223
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.02214022140221402
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.022058823529411766
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.02197802197802198
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.021897810218978103
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.02181818181818182
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 9        |
| MaximumReturn | 197      |
| MinimumReturn | 120      |
| TotalSamples  | 36663    |
----------------------------
itr #10 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.37501776218414307
Validation loss = 0.3733268976211548
Validation loss = 0.37415894865989685
Validation loss = 0.37251657247543335
Validation loss = 0.3741046190261841
Validation loss = 0.3737081289291382
Validation loss = 0.37501537799835205
Validation loss = 0.37715595960617065
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3730631470680237
Validation loss = 0.3720152676105499
Validation loss = 0.3759571611881256
Validation loss = 0.37460702657699585
Validation loss = 0.37217065691947937
Validation loss = 0.37233859300613403
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3748602569103241
Validation loss = 0.3725164532661438
Validation loss = 0.37548696994781494
Validation loss = 0.37250497937202454
Validation loss = 0.375436931848526
Validation loss = 0.37607628107070923
Validation loss = 0.3744161128997803
Validation loss = 0.3740755617618561
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.37431052327156067
Validation loss = 0.3760620653629303
Validation loss = 0.37314358353614807
Validation loss = 0.37472453713417053
Validation loss = 0.37308135628700256
Validation loss = 0.37311220169067383
Validation loss = 0.372244268655777
Validation loss = 0.373515784740448
Validation loss = 0.3753345310688019
Validation loss = 0.374881774187088
Validation loss = 0.3751005530357361
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3732600808143616
Validation loss = 0.3719102144241333
Validation loss = 0.3744730055332184
Validation loss = 0.3729041814804077
Validation loss = 0.37135717272758484
Validation loss = 0.37301239371299744
Validation loss = 0.3747468888759613
Validation loss = 0.37394386529922485
Validation loss = 0.37528282403945923
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.021739130434782608
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.02527075812274368
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.025179856115107913
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.025089605734767026
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.025
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.02491103202846975
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.028368794326241134
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.028268551236749116
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.028169014084507043
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.028070175438596492
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.027972027972027972
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.027874564459930314
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.027777777777777776
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.02768166089965398
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.027586206896551724
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.027491408934707903
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0273972602739726
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.027303754266211604
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.027210884353741496
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.030508474576271188
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.030405405405405407
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.03367003367003367
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.03355704697986577
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.033444816053511704
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.03333333333333333
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 10       |
| MaximumReturn | 198      |
| MinimumReturn | 123      |
| TotalSamples  | 39996    |
----------------------------
itr #11 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3820919096469879
Validation loss = 0.38139864802360535
Validation loss = 0.3821697533130646
Validation loss = 0.3834940493106842
Validation loss = 0.3836575448513031
Validation loss = 0.3840276896953583
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3805687725543976
Validation loss = 0.3821200430393219
Validation loss = 0.3809391260147095
Validation loss = 0.3789943754673004
Validation loss = 0.3801473081111908
Validation loss = 0.3807106912136078
Validation loss = 0.37991470098495483
Validation loss = 0.37903276085853577
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3805553615093231
Validation loss = 0.38148242235183716
Validation loss = 0.3816625475883484
Validation loss = 0.38094452023506165
Validation loss = 0.38043731451034546
Validation loss = 0.3807058334350586
Validation loss = 0.3819614052772522
Validation loss = 0.3829542398452759
Validation loss = 0.38271066546440125
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.38469934463500977
Validation loss = 0.3830619156360626
Validation loss = 0.3813774585723877
Validation loss = 0.38346484303474426
Validation loss = 0.3835093379020691
Validation loss = 0.38254350423812866
Validation loss = 0.38295048475265503
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.38227516412734985
Validation loss = 0.37947720289230347
Validation loss = 0.3789559602737427
Validation loss = 0.38119933009147644
Validation loss = 0.37921765446662903
Validation loss = 0.3823483884334564
Validation loss = 0.3810265362262726
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.03322259136212625
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.033112582781456956
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.033003300330033
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.03289473684210526
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.03278688524590164
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.032679738562091505
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.03257328990228013
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.03571428571428571
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.038834951456310676
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.03870967741935484
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.03858520900321544
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.041666666666666664
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.04153354632587859
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.041401273885350316
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.04126984126984127
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.04113924050632911
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.04100946372239748
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.040880503144654086
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.04075235109717868
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.040625
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.040498442367601244
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.040372670807453416
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.04024767801857585
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.040123456790123455
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.043076923076923075
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 11       |
| MaximumReturn | 199      |
| MinimumReturn | 49.3     |
| TotalSamples  | 43329    |
----------------------------
itr #12 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.38819199800491333
Validation loss = 0.38889145851135254
Validation loss = 0.3900046646595001
Validation loss = 0.390491247177124
Validation loss = 0.38942286372184753
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.38660570979118347
Validation loss = 0.387456476688385
Validation loss = 0.3878490924835205
Validation loss = 0.38732972741127014
Validation loss = 0.3871256113052368
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3864687383174896
Validation loss = 0.3876517713069916
Validation loss = 0.3873685598373413
Validation loss = 0.38801664113998413
Validation loss = 0.38878005743026733
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.38809674978256226
Validation loss = 0.38760846853256226
Validation loss = 0.3867271840572357
Validation loss = 0.38825878500938416
Validation loss = 0.38782405853271484
Validation loss = 0.38813385367393494
Validation loss = 0.38796597719192505
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3881436288356781
Validation loss = 0.38773927092552185
Validation loss = 0.3889906406402588
Validation loss = 0.3869515061378479
Validation loss = 0.3855608403682709
Validation loss = 0.3868926763534546
Validation loss = 0.3857528269290924
Validation loss = 0.38697174191474915
Validation loss = 0.38988909125328064
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.046012269938650305
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.04892966360856269
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.04878048780487805
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0486322188449848
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.051515151515151514
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.054380664652567974
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05421686746987952
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05405405405405406
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05389221556886228
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05373134328358209
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05357142857142857
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05341246290801187
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05325443786982249
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05309734513274336
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.052941176470588235
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.05571847507331378
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.05847953216374269
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05830903790087463
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05813953488372093
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.057971014492753624
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.057803468208092484
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05763688760806916
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05747126436781609
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.06017191977077364
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.06
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 12       |
| MaximumReturn | 199      |
| MinimumReturn | 112      |
| TotalSamples  | 46662    |
----------------------------
itr #13 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3931449353694916
Validation loss = 0.39369669556617737
Validation loss = 0.3930927813053131
Validation loss = 0.3942212164402008
Validation loss = 0.3947570323944092
Validation loss = 0.3933340907096863
Validation loss = 0.39634060859680176
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3932364881038666
Validation loss = 0.3911968767642975
Validation loss = 0.3904910981655121
Validation loss = 0.39368876814842224
Validation loss = 0.39234498143196106
Validation loss = 0.3925801217556
Validation loss = 0.39304491877555847
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3935757875442505
Validation loss = 0.3925566077232361
Validation loss = 0.3928302228450775
Validation loss = 0.3958055377006531
Validation loss = 0.3934713304042816
Validation loss = 0.39432960748672485
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3954315483570099
Validation loss = 0.3937225341796875
Validation loss = 0.39644792675971985
Validation loss = 0.39510056376457214
Validation loss = 0.3950498104095459
Validation loss = 0.3947758376598358
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.39382460713386536
Validation loss = 0.3934900462627411
Validation loss = 0.3927110731601715
Validation loss = 0.3941207230091095
Validation loss = 0.39299675822257996
Validation loss = 0.3937680125236511
Validation loss = 0.39308038353919983
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05982905982905983
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05965909090909091
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.059490084985835696
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.059322033898305086
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.059154929577464786
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05898876404494382
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.058823529411764705
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05865921787709497
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.06128133704735376
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.06388888888888888
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.0664819944598338
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.06629834254143646
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.06887052341597796
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.07142857142857142
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07123287671232877
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07103825136612021
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07084468664850137
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07065217391304347
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07046070460704607
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07027027027027027
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07008086253369272
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.06989247311827956
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.06970509383378017
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.06951871657754011
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.07466666666666667
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 158      |
| Iteration     | 13       |
| MaximumReturn | 198      |
| MinimumReturn | 115      |
| TotalSamples  | 49995    |
----------------------------
itr #14 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.39878568053245544
Validation loss = 0.39734455943107605
Validation loss = 0.39952486753463745
Validation loss = 0.3994528651237488
Validation loss = 0.4011494815349579
Validation loss = 0.4005473852157593
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.39767923951148987
Validation loss = 0.3968220055103302
Validation loss = 0.39600247144699097
Validation loss = 0.3965541124343872
Validation loss = 0.4004054665565491
Validation loss = 0.39685094356536865
Validation loss = 0.39670607447624207
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.39967402815818787
Validation loss = 0.39734503626823425
Validation loss = 0.397851824760437
Validation loss = 0.4006098806858063
Validation loss = 0.39773938059806824
Validation loss = 0.3998984396457672
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.40046364068984985
Validation loss = 0.40054944157600403
Validation loss = 0.39820727705955505
Validation loss = 0.4024462103843689
Validation loss = 0.3979945480823517
Validation loss = 0.39921489357948303
Validation loss = 0.4001865088939667
Validation loss = 0.40235039591789246
Validation loss = 0.4015614986419678
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.39903223514556885
Validation loss = 0.3975593149662018
Validation loss = 0.3972404897212982
Validation loss = 0.39794018864631653
Validation loss = 0.3977500796318054
Validation loss = 0.39804646372795105
Validation loss = 0.398728609085083
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07446808510638298
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07427055702917772
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07407407407407407
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07387862796833773
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.07631578947368421
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.08136482939632546
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08115183246073299
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.08616187989556136
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.08854166666666667
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.09090909090909091
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.09067357512953368
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.09043927648578812
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.09020618556701031
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.08997429305912596
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.09230769230769231
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.09462915601023018
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.09948979591836735
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.09923664122137404
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.10152284263959391
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.10379746835443038
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.10606060606060606
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.10831234256926953
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.10804020100502512
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.10776942355889724
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.1075
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 164      |
| Iteration     | 14       |
| MaximumReturn | 198      |
| MinimumReturn | 102      |
| TotalSamples  | 53328    |
----------------------------
itr #15 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.40520110726356506
Validation loss = 0.4052422344684601
Validation loss = 0.40436267852783203
Validation loss = 0.4051089584827423
Validation loss = 0.4044698178768158
Validation loss = 0.404319167137146
Validation loss = 0.4067947566509247
Validation loss = 0.40802842378616333
Validation loss = 0.4063628911972046
Validation loss = 0.40703099966049194
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4017552137374878
Validation loss = 0.40050193667411804
Validation loss = 0.4036698341369629
Validation loss = 0.4044415056705475
Validation loss = 0.40245845913887024
Validation loss = 0.4041464030742645
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4023277163505554
Validation loss = 0.40184369683265686
Validation loss = 0.4053643047809601
Validation loss = 0.4051353335380554
Validation loss = 0.40446165204048157
Validation loss = 0.40511876344680786
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4052606225013733
Validation loss = 0.40523460507392883
Validation loss = 0.4054688811302185
Validation loss = 0.4055636525154114
Validation loss = 0.4057427644729614
Validation loss = 0.4081079661846161
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4035967290401459
Validation loss = 0.40246960520744324
Validation loss = 0.4027409255504608
Validation loss = 0.4034873843193054
Validation loss = 0.40316253900527954
Validation loss = 0.4033678472042084
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.10723192019950124
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.10696517412935323
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.11166253101736973
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.11386138613861387
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.11604938271604938
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.1206896551724138
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.12530712530712532
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.1323529411764706
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.13202933985330073
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.13414634146341464
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.13381995133819952
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.13349514563106796
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.13317191283292978
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.13526570048309178
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.14216867469879518
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.14423076923076922
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.14388489208633093
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.1507177033492823
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.1575178997613365
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.16666666666666666
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.17814726840855108
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.17772511848341233
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.1773049645390071
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.1792452830188679
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.17882352941176471
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 164      |
| Iteration     | 15       |
| MaximumReturn | 198      |
| MinimumReturn | 113      |
| TotalSamples  | 56661    |
----------------------------
itr #16 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4098135828971863
Validation loss = 0.410054475069046
Validation loss = 0.40907391905784607
Validation loss = 0.4085167348384857
Validation loss = 0.410145103931427
Validation loss = 0.4083690643310547
Validation loss = 0.41039299964904785
Validation loss = 0.41031262278556824
Validation loss = 0.41159096360206604
Validation loss = 0.4124322235584259
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4050483703613281
Validation loss = 0.4055872857570648
Validation loss = 0.4068809151649475
Validation loss = 0.40439867973327637
Validation loss = 0.4050120413303375
Validation loss = 0.40623238682746887
Validation loss = 0.4046189785003662
Validation loss = 0.40795016288757324
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4054616391658783
Validation loss = 0.40618202090263367
Validation loss = 0.4104977548122406
Validation loss = 0.4065350890159607
Validation loss = 0.4065365195274353
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4082697331905365
Validation loss = 0.409633606672287
Validation loss = 0.40844935178756714
Validation loss = 0.40709665417671204
Validation loss = 0.411372572183609
Validation loss = 0.40921953320503235
Validation loss = 0.4114375710487366
Validation loss = 0.41094353795051575
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.40674877166748047
Validation loss = 0.40476036071777344
Validation loss = 0.40600326657295227
Validation loss = 0.4064478874206543
Validation loss = 0.4071245491504669
Validation loss = 0.4052387773990631
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.18779342723004694
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.18969555035128804
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.19158878504672897
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.20046620046620048
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.20465116279069767
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.21345707656612528
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.21296296296296297
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.21478060046189376
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.21658986175115208
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.22528735632183908
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.23394495412844038
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.2334096109839817
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.23515981735159816
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.23690205011389523
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.23636363636363636
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.24489795918367346
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.248868778280543
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.2505643340857788
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.25
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.25617977528089886
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.257847533632287
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.26174496644295303
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.26339285714285715
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.267260579064588
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.2733333333333333
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 16       |
| MaximumReturn | 197      |
| MinimumReturn | 115      |
| TotalSamples  | 59994    |
----------------------------
itr #17 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.41238975524902344
Validation loss = 0.41274455189704895
Validation loss = 0.4148487150669098
Validation loss = 0.4143151640892029
Validation loss = 0.4154200255870819
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4082670211791992
Validation loss = 0.4099823534488678
Validation loss = 0.40913698077201843
Validation loss = 0.4094044864177704
Validation loss = 0.41042500734329224
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.40763047337532043
Validation loss = 0.40915459394454956
Validation loss = 0.4087788164615631
Validation loss = 0.4103595018386841
Validation loss = 0.4124104082584381
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4141506850719452
Validation loss = 0.4112061560153961
Validation loss = 0.4108225107192993
Validation loss = 0.41222083568573
Validation loss = 0.4130564332008362
Validation loss = 0.41472047567367554
Validation loss = 0.4130135178565979
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.40921857953071594
Validation loss = 0.4081811308860779
Validation loss = 0.411365807056427
Validation loss = 0.40972793102264404
Validation loss = 0.4088687002658844
Validation loss = 0.40994465351104736
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.2727272727272727
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.2831858407079646
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.282560706401766
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.28193832599118945
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.29010989010989013
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.2894736842105263
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.2888402625820569
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.28820960698689957
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.289760348583878
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.29347826086956524
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.3036876355748373
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.30303030303030304
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.3045356371490281
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.30387931034482757
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 0.31827956989247314
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.3240343347639485
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.3340471092077088
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.33974358974358976
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.34968017057569295
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.3553191489361702
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.3630573248407643
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.3665254237288136
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.37209302325581395
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.37763713080168776
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.37894736842105264
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 17       |
| MaximumReturn | 199      |
| MinimumReturn | 117      |
| TotalSamples  | 63327    |
----------------------------
itr #18 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4170553684234619
Validation loss = 0.4164198338985443
Validation loss = 0.41788485646247864
Validation loss = 0.41687092185020447
Validation loss = 0.4183322489261627
Validation loss = 0.4188042879104614
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4120666980743408
Validation loss = 0.4132367968559265
Validation loss = 0.4109598398208618
Validation loss = 0.4137950837612152
Validation loss = 0.41206198930740356
Validation loss = 0.4138626456260681
Validation loss = 0.41275936365127563
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4112056493759155
Validation loss = 0.411455363035202
Validation loss = 0.41282397508621216
Validation loss = 0.41325223445892334
Validation loss = 0.4145217537879944
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4157387614250183
Validation loss = 0.4186209440231323
Validation loss = 0.4168083369731903
Validation loss = 0.418948769569397
Validation loss = 0.4186025857925415
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4117341935634613
Validation loss = 0.4131799638271332
Validation loss = 0.41248807311058044
Validation loss = 0.4145987927913666
Validation loss = 0.413107693195343
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.38445378151260506
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.3836477987421384
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.3912133891213389
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.40083507306889354
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.40625
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.41372141372141374
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 0.42946058091286304
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.4389233954451346
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 0.4524793388429752
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.4556701030927835
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.4547325102880658
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.4620123203285421
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.4733606557377049
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.47443762781186094
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.4857142857142857
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.4847250509164969
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.49390243902439024
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.49898580121703856
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.5
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.5050505050505051
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 0.5181451612903226
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.5171026156941649
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.5220883534136547
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.5250501002004008
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.528
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 18       |
| MaximumReturn | 199      |
| MinimumReturn | 103      |
| TotalSamples  | 66660    |
----------------------------
itr #19 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.41943734884262085
Validation loss = 0.41947489976882935
Validation loss = 0.4183267652988434
Validation loss = 0.41995668411254883
Validation loss = 0.41810840368270874
Validation loss = 0.41918736696243286
Validation loss = 0.42164525389671326
Validation loss = 0.41940897703170776
Validation loss = 0.42179274559020996
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.41279977560043335
Validation loss = 0.4129891097545624
Validation loss = 0.41246724128723145
Validation loss = 0.41388389468193054
Validation loss = 0.4151631295681
Validation loss = 0.41351011395454407
Validation loss = 0.4172903001308441
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4128108620643616
Validation loss = 0.41145485639572144
Validation loss = 0.4124075770378113
Validation loss = 0.4129001796245575
Validation loss = 0.4140757918357849
Validation loss = 0.4122125506401062
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.41665416955947876
Validation loss = 0.41923433542251587
Validation loss = 0.42089295387268066
Validation loss = 0.41830921173095703
Validation loss = 0.41802409291267395
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.41275754570961
Validation loss = 0.41272422671318054
Validation loss = 0.4119930863380432
Validation loss = 0.41375112533569336
Validation loss = 0.41404226422309875
Validation loss = 0.41300633549690247
Validation loss = 0.4134314954280853
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.5329341317365269
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.5338645418326693
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.5387673956262425
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.5396825396825397
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.5445544554455446
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.5513833992094862
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.5601577909270217
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.5649606299212598
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.5736738703339882
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.5725490196078431
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.5733855185909981
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.583984375
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 0.5964912280701754
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.5972762645914397
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.6
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.6085271317829457
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.6170212765957447
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.6254826254826255
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.6242774566473989
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.6269230769230769
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.6333973128598849
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.6360153256704981
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.6347992351816444
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.6335877862595419
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.6419047619047619
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 19       |
| MaximumReturn | 199      |
| MinimumReturn | 102      |
| TotalSamples  | 69993    |
----------------------------
itr #20 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4207971394062042
Validation loss = 0.42170339822769165
Validation loss = 0.4217418432235718
Validation loss = 0.42225411534309387
Validation loss = 0.4214414954185486
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.41593506932258606
Validation loss = 0.41556236147880554
Validation loss = 0.4151998460292816
Validation loss = 0.4164046347141266
Validation loss = 0.4148792028427124
Validation loss = 0.4169894754886627
Validation loss = 0.417499303817749
Validation loss = 0.4171138107776642
Validation loss = 0.41651976108551025
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4143565893173218
Validation loss = 0.4147310256958008
Validation loss = 0.41377875208854675
Validation loss = 0.4172351062297821
Validation loss = 0.41721707582473755
Validation loss = 0.41680294275283813
Validation loss = 0.41626909375190735
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4190284311771393
Validation loss = 0.4173181653022766
Validation loss = 0.4198507070541382
Validation loss = 0.42024531960487366
Validation loss = 0.41927894949913025
Validation loss = 0.42091357707977295
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4129382073879242
Validation loss = 0.4136226773262024
Validation loss = 0.41451138257980347
Validation loss = 0.41434571146965027
Validation loss = 0.41451165080070496
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 0.6539923954372624
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.6527514231499051
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.6571969696969697
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 0.6710775047258979
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.6773584905660377
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 0.7005649717514124
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.7086466165413534
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.7166979362101313
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 0.7284644194756554
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.7271028037383177
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.7257462686567164
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.7337057728119181
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.741635687732342
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.7439703153988868
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.7518518518518519
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.7597042513863216
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.7638376383763837
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.7697974217311234
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 0.7849264705882353
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 0.7963302752293578
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 0.8076923076923077
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.8135283363802559
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.8120437956204379
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.8142076502732241
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.8181818181818182
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 162      |
| Iteration     | 20       |
| MaximumReturn | 199      |
| MinimumReturn | 106      |
| TotalSamples  | 73326    |
----------------------------
itr #21 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.42199209332466125
Validation loss = 0.42511335015296936
Validation loss = 0.42278045415878296
Validation loss = 0.4234377145767212
Validation loss = 0.4236205220222473
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4183551073074341
Validation loss = 0.417070597410202
Validation loss = 0.4196062684059143
Validation loss = 0.41862952709198
Validation loss = 0.41984331607818604
Validation loss = 0.41961154341697693
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4151679277420044
Validation loss = 0.4150875210762024
Validation loss = 0.4181560277938843
Validation loss = 0.41673219203948975
Validation loss = 0.41739580035209656
Validation loss = 0.41631948947906494
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.41879963874816895
Validation loss = 0.4205431640148163
Validation loss = 0.4196043908596039
Validation loss = 0.4209597110748291
Validation loss = 0.4213680922985077
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.41733649373054504
Validation loss = 0.4152849018573761
Validation loss = 0.41503745317459106
Validation loss = 0.41514575481414795
Validation loss = 0.41616857051849365
Validation loss = 0.41712990403175354
Validation loss = 0.4171234369277954
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.8239564428312159
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 0.8351449275362319
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 0.8499095840867993
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.8483754512635379
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.8468468468468469
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 0.8633093525179856
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.8671454219030521
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.8727598566308243
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.8819320214669052
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.8892857142857142
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.8877005347593583
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 0.8985765124555161
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.9058614564831261
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.9131205673758865
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.9132743362831859
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.9204946996466431
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 0.9312169312169312
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.9295774647887324
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.9384885764499121
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 0.9491228070175438
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.957968476357268
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.965034965034965
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.9720767888307156
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 0.9825783972125436
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 0.9930434782608696
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 159      |
| Iteration     | 21       |
| MaximumReturn | 199      |
| MinimumReturn | 121      |
| TotalSamples  | 76659    |
----------------------------
itr #22 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4227863550186157
Validation loss = 0.4221242666244507
Validation loss = 0.4220985174179077
Validation loss = 0.42192357778549194
Validation loss = 0.42414188385009766
Validation loss = 0.4259110987186432
Validation loss = 0.4250037670135498
Validation loss = 0.4246831238269806
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4191063940525055
Validation loss = 0.41847383975982666
Validation loss = 0.41919204592704773
Validation loss = 0.4188970625400543
Validation loss = 0.4188462197780609
Validation loss = 0.420524924993515
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4171279966831207
Validation loss = 0.4169151186943054
Validation loss = 0.418373703956604
Validation loss = 0.41588324308395386
Validation loss = 0.4185352027416229
Validation loss = 0.4185783565044403
Validation loss = 0.4197295606136322
Validation loss = 0.41939711570739746
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4213828146457672
Validation loss = 0.42076805233955383
Validation loss = 0.4199257791042328
Validation loss = 0.42070063948631287
Validation loss = 0.4207637906074524
Validation loss = 0.4220027029514313
Validation loss = 0.4246250092983246
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.41634130477905273
Validation loss = 0.41735967993736267
Validation loss = 0.4173949658870697
Validation loss = 0.4169052243232727
Validation loss = 0.4166478216648102
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.9982638888888888
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.0103986135181975
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.019031141868512
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.0259067357512954
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.0362068965517242
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.0430292598967297
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.0498281786941581
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.0566037735849056
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.0616438356164384
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.0735042735042735
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 1.075085324232082
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.080068143100511
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.0901360544217686
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.098471986417657
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 1.1
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 1.0981387478849407
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 1.097972972972973
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 1.0961214165261384
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 1.0976430976430978
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 1.1126050420168068
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 1.1275167785234899
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.137353433835846
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 1.1354515050167224
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.1402337228714525
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.1466666666666667
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 175      |
| Iteration     | 22       |
| MaximumReturn | 199      |
| MinimumReturn | 128      |
| TotalSamples  | 79992    |
----------------------------
itr #23 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4235214293003082
Validation loss = 0.4241894781589508
Validation loss = 0.4254975914955139
Validation loss = 0.4254789352416992
Validation loss = 0.4281254708766937
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4208797812461853
Validation loss = 0.4213709235191345
Validation loss = 0.4218228757381439
Validation loss = 0.42276573181152344
Validation loss = 0.4205588400363922
Validation loss = 0.42452412843704224
Validation loss = 0.4244216978549957
Validation loss = 0.4244315028190613
Validation loss = 0.42339539527893066
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.42010608315467834
Validation loss = 0.42049211263656616
Validation loss = 0.4209340214729309
Validation loss = 0.4217386245727539
Validation loss = 0.4209999144077301
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.42149752378463745
Validation loss = 0.421962171792984
Validation loss = 0.42353343963623047
Validation loss = 0.422900915145874
Validation loss = 0.42490506172180176
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.41794905066490173
Validation loss = 0.4183413088321686
Validation loss = 0.41707029938697815
Validation loss = 0.4183535575866699
Validation loss = 0.4185836911201477
Validation loss = 0.4193781316280365
Validation loss = 0.4197697043418884
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 1.1497504159733778
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.1578073089700998
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.1674958540630183
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 1.1688741721854305
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 1.171900826446281
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 1.1716171617161717
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 1.1696869851729819
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 1.1825657894736843
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.187192118226601
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 1.1901639344262296
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 1.2029459901800328
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 1.2026143790849673
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.2104404567699838
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 1.223127035830619
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.232520325203252
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 1.2337662337662338
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.2431118314424636
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 1.2588996763754046
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.2681744749596122
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.2774193548387096
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.2818035426731078
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.2861736334405145
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.2953451043338684
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 1.2932692307692308
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.2976
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 161      |
| Iteration     | 23       |
| MaximumReturn | 199      |
| MinimumReturn | 118      |
| TotalSamples  | 83325    |
----------------------------
itr #24 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4258500039577484
Validation loss = 0.4275680184364319
Validation loss = 0.42736101150512695
Validation loss = 0.4262634515762329
Validation loss = 0.42808663845062256
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4234863519668579
Validation loss = 0.42621371150016785
Validation loss = 0.4229722321033478
Validation loss = 0.4241010546684265
Validation loss = 0.42440497875213623
Validation loss = 0.4268358647823334
Validation loss = 0.4246833026409149
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4199984073638916
Validation loss = 0.42183926701545715
Validation loss = 0.4202328324317932
Validation loss = 0.4217362701892853
Validation loss = 0.42162221670150757
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.42413610219955444
Validation loss = 0.4259713292121887
Validation loss = 0.4246698319911957
Validation loss = 0.42361053824424744
Validation loss = 0.42585670948028564
Validation loss = 0.4252762198448181
Validation loss = 0.4265426993370056
Validation loss = 0.42914506793022156
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4184733033180237
Validation loss = 0.4193000793457031
Validation loss = 0.4208375811576843
Validation loss = 0.4206809401512146
Validation loss = 0.42067816853523254
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.305111821086262
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.3094098883572567
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 1.3073248407643312
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 1.3100158982511925
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 1.3126984126984127
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 1.3153724247226624
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.3212025316455696
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 1.339652448657188
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.3438485804416405
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.3480314960629922
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 1.3474842767295598
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 1.3626373626373627
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.371473354231975
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.380281690140845
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.3859375
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 1.4118564742589703
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.4221183800623054
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 1.4199066874027995
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.423913043478261
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.427906976744186
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 1.4303405572755419
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.4343122102009274
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.4429012345679013
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 1.4453004622496148
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.4523076923076923
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 162      |
| Iteration     | 24       |
| MaximumReturn | 199      |
| MinimumReturn | 117      |
| TotalSamples  | 86658    |
----------------------------
itr #25 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4252381920814514
Validation loss = 0.427751749753952
Validation loss = 0.42586132884025574
Validation loss = 0.42717471718788147
Validation loss = 0.42671385407447815
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.42385008931159973
Validation loss = 0.4245057702064514
Validation loss = 0.424657940864563
Validation loss = 0.4252989888191223
Validation loss = 0.4239555597305298
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4218129813671112
Validation loss = 0.42074501514434814
Validation loss = 0.421659380197525
Validation loss = 0.4202668070793152
Validation loss = 0.42230698466300964
Validation loss = 0.42292359471321106
Validation loss = 0.4222588539123535
Validation loss = 0.42373573780059814
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.42419251799583435
Validation loss = 0.4259878695011139
Validation loss = 0.4255400002002716
Validation loss = 0.42574501037597656
Validation loss = 0.4258391261100769
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4178003668785095
Validation loss = 0.41952306032180786
Validation loss = 0.4209371507167816
Validation loss = 0.41920995712280273
Validation loss = 0.42195674777030945
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 1.4531490015360984
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 1.4539877300613497
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 1.4532924961715161
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 1.45565749235474
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 1.4564885496183206
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.4634146341463414
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 1.461187214611872
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.4711246200607904
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 1.4688922610015174
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 1.4833333333333334
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.4901664145234492
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.4969788519637461
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.502262443438914
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 1.5180722891566265
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 1.5308270676691729
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.5345345345345345
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 1.535232383808096
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.5434131736526946
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.5515695067264574
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.5552238805970149
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 1.5573770491803278
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 1.5699404761904763
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 1.5824665676077265
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 1.5979228486646884
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.605925925925926
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 25       |
| MaximumReturn | 198      |
| MinimumReturn | 124      |
| TotalSamples  | 89991    |
----------------------------
itr #26 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4257318675518036
Validation loss = 0.42705684900283813
Validation loss = 0.428202748298645
Validation loss = 0.42687174677848816
Validation loss = 0.42827433347702026
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.42493781447410583
Validation loss = 0.4248233735561371
Validation loss = 0.4275628924369812
Validation loss = 0.42509782314300537
Validation loss = 0.4261109530925751
Validation loss = 0.4263848066329956
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4224366843700409
Validation loss = 0.42349767684936523
Validation loss = 0.4237715005874634
Validation loss = 0.4243007302284241
Validation loss = 0.4251197874546051
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.42506229877471924
Validation loss = 0.4287729561328888
Validation loss = 0.42758461833000183
Validation loss = 0.42652884125709534
Validation loss = 0.427287220954895
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4210415780544281
Validation loss = 0.4200359284877777
Validation loss = 0.4210834503173828
Validation loss = 0.42273038625717163
Validation loss = 0.4209410846233368
Validation loss = 0.42191144824028015
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 1.6183431952662721
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 1.6159527326440177
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 1.631268436578171
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 1.642120765832106
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.6470588235294117
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 1.6475770925110131
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.653958944281525
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 1.664714494875549
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.6710526315789473
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.6773722627737226
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.685131195335277
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 1.6826783114992723
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 1.6962209302325582
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.6995645863570392
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 1.7
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 1.7120115774240232
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.7196531791907514
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.7243867243867244
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 1.7363112391930835
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.7410071942446044
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 1.7528735632183907
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 1.7661406025824964
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.7736389684813754
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 1.7896995708154506
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.7985714285714285
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 163      |
| Iteration     | 26       |
| MaximumReturn | 198      |
| MinimumReturn | 119      |
| TotalSamples  | 93324    |
----------------------------
itr #27 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4267734885215759
Validation loss = 0.4269241988658905
Validation loss = 0.42708781361579895
Validation loss = 0.4279453456401825
Validation loss = 0.43018838763237
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.42599648237228394
Validation loss = 0.4260035753250122
Validation loss = 0.42534640431404114
Validation loss = 0.4278291165828705
Validation loss = 0.42805251479148865
Validation loss = 0.42631635069847107
Validation loss = 0.4272807538509369
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4241146445274353
Validation loss = 0.4235525131225586
Validation loss = 0.42430567741394043
Validation loss = 0.4237346351146698
Validation loss = 0.426901638507843
Validation loss = 0.4240610599517822
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4256795346736908
Validation loss = 0.4261704683303833
Validation loss = 0.42652732133865356
Validation loss = 0.427867591381073
Validation loss = 0.4277614653110504
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.42485499382019043
Validation loss = 0.4223218262195587
Validation loss = 0.42320653796195984
Validation loss = 0.4224593937397003
Validation loss = 0.42416128516197205
Validation loss = 0.42252278327941895
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 1.8145506419400856
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 1.8133903133903133
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.8165007112375533
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.8238636363636365
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 1.8382978723404255
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 1.8526912181303117
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 1.8670438472418671
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 1.8644067796610169
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.8730606488011283
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.880281690140845
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.8874824191279886
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.896067415730337
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.903225806451613
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.9061624649859943
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 1.916083916083916
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.9217877094972067
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 1.9232914923291493
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 1.9206128133704736
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.929068150208623
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 1.9416666666666667
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 1.9583911234396671
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 1.9695290858725762
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.9778699861687414
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 1.9875690607734806
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 1.9986206896551724
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 27       |
| MaximumReturn | 198      |
| MinimumReturn | 107      |
| TotalSamples  | 96657    |
----------------------------
itr #28 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4289555549621582
Validation loss = 0.4304597079753876
Validation loss = 0.43067267537117004
Validation loss = 0.4291780889034271
Validation loss = 0.4297346770763397
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4272179901599884
Validation loss = 0.42914867401123047
Validation loss = 0.4271458685398102
Validation loss = 0.42997288703918457
Validation loss = 0.42979851365089417
Validation loss = 0.42872750759124756
Validation loss = 0.42994365096092224
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.42493581771850586
Validation loss = 0.42593955993652344
Validation loss = 0.42588743567466736
Validation loss = 0.4243042469024658
Validation loss = 0.4283975660800934
Validation loss = 0.4264489710330963
Validation loss = 0.42789360880851746
Validation loss = 0.4292322099208832
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4284573793411255
Validation loss = 0.4283945858478546
Validation loss = 0.43000325560569763
Validation loss = 0.42733868956565857
Validation loss = 0.4299851953983307
Validation loss = 0.42953458428382874
Validation loss = 0.428815096616745
Validation loss = 0.4296301305294037
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4240892827510834
Validation loss = 0.424309641122818
Validation loss = 0.4241142272949219
Validation loss = 0.4243279993534088
Validation loss = 0.4263249337673187
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 2.0068870523415976
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 2.0178817056396148
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 2.0288461538461537
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 2.037037037037037
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 2.0465753424657533
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 2.0437756497948016
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 2.0628415300546448
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 2.068212824010914
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 2.0858310626703
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 2.11156462585034
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 2.1222826086956523
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 2.1275440976933515
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 2.1287262872628725
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 2.1339648173207038
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 2.1445945945945946
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 2.161943319838057
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 2.1805929919137466
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 2.2045760430686405
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 2.213709677419355
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 2.225503355704698
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 2.2439678284182305
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 2.248995983935743
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 2.2620320855614975
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 2.267022696929239
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 2.2693333333333334
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 163      |
| Iteration     | 28       |
| MaximumReturn | 198      |
| MinimumReturn | 120      |
| TotalSamples  | 99990    |
----------------------------
itr #29 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.428507000207901
Validation loss = 0.4277634024620056
Validation loss = 0.4286156892776489
Validation loss = 0.42929863929748535
Validation loss = 0.4295744001865387
Validation loss = 0.43031632900238037
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.429288774728775
Validation loss = 0.4280025362968445
Validation loss = 0.4303707480430603
Validation loss = 0.427971750497818
Validation loss = 0.42963624000549316
Validation loss = 0.43012428283691406
Validation loss = 0.42882010340690613
Validation loss = 0.43227291107177734
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4282750189304352
Validation loss = 0.42684605717658997
Validation loss = 0.4276546239852905
Validation loss = 0.42836058139801025
Validation loss = 0.4290590286254883
Validation loss = 0.42941227555274963
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.43026232719421387
Validation loss = 0.42814868688583374
Validation loss = 0.4319489002227783
Validation loss = 0.42870891094207764
Validation loss = 0.4325202703475952
Validation loss = 0.43095701932907104
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.42353007197380066
Validation loss = 0.4258562922477722
Validation loss = 0.42429879307746887
Validation loss = 0.42638659477233887
Validation loss = 0.42430248856544495
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 2.271637816245007
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 2.2726063829787235
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 2.2802124833997346
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 2.3037135278514587
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 2.3086092715231787
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 2.3134920634920637
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 2.3342140026420077
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 2.3535620052770447
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 2.357048748353096
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 2.3539473684210526
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 2.352168199737188
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 2.37007874015748
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 2.381389252948886
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 2.392670157068063
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 2.3973856209150326
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 2.406005221932115
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 2.4185136897001303
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 2.4270833333333335
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 2.439531859557867
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 2.4584415584415584
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 2.477302204928664
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 2.481865284974093
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 2.4902975420439843
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 2.5064599483204133
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 2.514838709677419
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 178      |
| Iteration     | 29       |
| MaximumReturn | 199      |
| MinimumReturn | 138      |
| TotalSamples  | 103323   |
----------------------------
itr #30 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.430655300617218
Validation loss = 0.42997121810913086
Validation loss = 0.4304664433002472
Validation loss = 0.43341711163520813
Validation loss = 0.431387722492218
Validation loss = 0.43125030398368835
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.430683970451355
Validation loss = 0.4320034682750702
Validation loss = 0.4315066933631897
Validation loss = 0.43133190274238586
Validation loss = 0.4326525926589966
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.42930641770362854
Validation loss = 0.4304507374763489
Validation loss = 0.4300689697265625
Validation loss = 0.4304984211921692
Validation loss = 0.431009441614151
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.43045881390571594
Validation loss = 0.43367868661880493
Validation loss = 0.43257415294647217
Validation loss = 0.4333491921424866
Validation loss = 0.4330211281776428
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4260775148868561
Validation loss = 0.4249616265296936
Validation loss = 0.4258430004119873
Validation loss = 0.42779073119163513
Validation loss = 0.4271794259548187
Validation loss = 0.4255397915840149
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 2.536082474226804
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 2.5444015444015444
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 2.5681233933161955
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 2.5853658536585367
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 2.598717948717949
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 2.6030729833546733
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 2.6112531969309463
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 2.6219667943805876
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 2.623724489795918
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 2.6369426751592355
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 2.6552162849872776
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 2.6658195679796695
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 2.684010152284264
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 2.688212927756654
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 2.688607594936709
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 2.692793931731985
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 2.698232323232323
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 2.707440100882724
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 2.7153652392947105
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 2.7358490566037736
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 2.7349246231155777
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 2.7603513174404015
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 2.7706766917293235
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 2.785982478097622
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 2.7875
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 163      |
| Iteration     | 30       |
| MaximumReturn | 199      |
| MinimumReturn | 115      |
| TotalSamples  | 106656   |
----------------------------
itr #31 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4295841157436371
Validation loss = 0.4317146837711334
Validation loss = 0.43157678842544556
Validation loss = 0.431004136800766
Validation loss = 0.43237239122390747
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.43165117502212524
Validation loss = 0.43159207701683044
Validation loss = 0.4330318868160248
Validation loss = 0.43176761269569397
Validation loss = 0.43263760209083557
Validation loss = 0.4320460259914398
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4294262230396271
Validation loss = 0.4306468367576599
Validation loss = 0.42993029952049255
Validation loss = 0.43204358220100403
Validation loss = 0.4289671778678894
Validation loss = 0.42995986342430115
Validation loss = 0.4311068058013916
Validation loss = 0.43122008442878723
Validation loss = 0.43025699257850647
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.43197229504585266
Validation loss = 0.4318269193172455
Validation loss = 0.43155550956726074
Validation loss = 0.43227383494377136
Validation loss = 0.43302810192108154
Validation loss = 0.43230512738227844
Validation loss = 0.43307679891586304
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.42712849378585815
Validation loss = 0.4257048964500427
Validation loss = 0.4251655340194702
Validation loss = 0.42712581157684326
Validation loss = 0.4264516234397888
Validation loss = 0.42756015062332153
Validation loss = 0.4263785779476166
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 2.7890137328339577
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 2.800498753117207
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 2.813200498132005
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 2.8196517412935322
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 2.8236024844720498
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 2.838709677419355
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 2.84634448574969
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 2.852722772277228
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 2.865265760197775
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 2.8753086419753084
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 2.8939580764488286
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 2.9002463054187193
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 2.910209102091021
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 2.92014742014742
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 2.92760736196319
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 2.9276960784313726
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 2.9424724602203183
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 2.952322738386308
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 2.970695970695971
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 2.984146341463415
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 2.9963459196102313
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 3.004866180048662
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 3.0182260024301337
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 3.0400485436893203
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 3.0484848484848484
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 31       |
| MaximumReturn | 199      |
| MinimumReturn | 122      |
| TotalSamples  | 109989   |
----------------------------
itr #32 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4293588101863861
Validation loss = 0.4308141767978668
Validation loss = 0.432270348072052
Validation loss = 0.43244537711143494
Validation loss = 0.4309813678264618
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.43036431074142456
Validation loss = 0.4314615726470947
Validation loss = 0.4328114688396454
Validation loss = 0.43023917078971863
Validation loss = 0.4317760765552521
Validation loss = 0.4310382902622223
Validation loss = 0.43170836567878723
Validation loss = 0.4320068359375
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.43217459321022034
Validation loss = 0.43089693784713745
Validation loss = 0.43402642011642456
Validation loss = 0.4305952191352844
Validation loss = 0.43096673488616943
Validation loss = 0.43266695737838745
Validation loss = 0.4319304823875427
Validation loss = 0.433002769947052
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4304008185863495
Validation loss = 0.43130388855934143
Validation loss = 0.429726243019104
Validation loss = 0.4306429922580719
Validation loss = 0.43217599391937256
Validation loss = 0.43138253688812256
Validation loss = 0.43305960297584534
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4264864921569824
Validation loss = 0.42589640617370605
Validation loss = 0.4259435832500458
Validation loss = 0.4265049993991852
Validation loss = 0.42509275674819946
Validation loss = 0.4267910420894623
Validation loss = 0.4264816641807556
Validation loss = 0.43013662099838257
Validation loss = 0.4270936846733093
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 3.056900726392252
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 3.0725513905683193
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 3.0845410628019323
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 3.100120627261761
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 3.116867469879518
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 3.1263537906137184
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 3.1334134615384617
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 3.142857142857143
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 3.1510791366906474
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 3.1664670658682637
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 3.1770334928229667
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 3.182795698924731
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 3.1897374701670644
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 3.1907032181168056
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 3.2
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 3.2104637336504163
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 3.2256532066508314
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 3.228944246737841
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 3.240521327014218
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 3.249704142011834
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 24
average number of affinization = 3.2742316784869976
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 3.2857142857142856
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 3.306603773584906
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 3.305064782096584
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 3.3129411764705883
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 178      |
| Iteration     | 32       |
| MaximumReturn | 198      |
| MinimumReturn | 128      |
| TotalSamples  | 113322   |
----------------------------
itr #33 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4291914999485016
Validation loss = 0.4311552345752716
Validation loss = 0.4309193789958954
Validation loss = 0.4317045211791992
Validation loss = 0.4340684711933136
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4309578537940979
Validation loss = 0.43472614884376526
Validation loss = 0.4350025951862335
Validation loss = 0.4328928589820862
Validation loss = 0.4343967139720917
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4319446384906769
Validation loss = 0.4329124093055725
Validation loss = 0.43148961663246155
Validation loss = 0.43260854482650757
Validation loss = 0.43258169293403625
Validation loss = 0.4326576888561249
Validation loss = 0.43440335988998413
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.43172433972358704
Validation loss = 0.43296173214912415
Validation loss = 0.43228790163993835
Validation loss = 0.4334265887737274
Validation loss = 0.43667104840278625
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4280067980289459
Validation loss = 0.4317505657672882
Validation loss = 0.42832010984420776
Validation loss = 0.4292469024658203
Validation loss = 0.42935702204704285
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 3.3219741480611047
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 3.335680751173709
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 3.3458382180539274
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 3.350117096018735
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 3.3637426900584795
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 3.3726635514018692
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 3.37806301050175
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 3.3916083916083917
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 3.4016298020954596
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 3.4046511627906977
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 3.4204413472706157
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 3.437354988399072
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 3.4484356894553883
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 3.462962962962963
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 3.469364161849711
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 3.489607390300231
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 3.506343713956171
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 3.52073732718894
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 3.529344073647871
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 3.5436781609195402
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 3.543053960964409
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 3.5573394495412844
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 3.5624284077892323
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 3.5720823798627004
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 3.6045714285714285
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 33       |
| MaximumReturn | 199      |
| MinimumReturn | 112      |
| TotalSamples  | 116655   |
----------------------------
itr #34 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.43093547224998474
Validation loss = 0.43071725964546204
Validation loss = 0.43154698610305786
Validation loss = 0.4317094683647156
Validation loss = 0.4319048523902893
Validation loss = 0.43252435326576233
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4339059591293335
Validation loss = 0.4358051121234894
Validation loss = 0.4344065487384796
Validation loss = 0.43157222867012024
Validation loss = 0.4339132606983185
Validation loss = 0.4335320293903351
Validation loss = 0.43323981761932373
Validation loss = 0.43429097533226013
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.43272310495376587
Validation loss = 0.43262621760368347
Validation loss = 0.4346243739128113
Validation loss = 0.4338058531284332
Validation loss = 0.432555615901947
Validation loss = 0.43359726667404175
Validation loss = 0.43495887517929077
Validation loss = 0.43397751450538635
Validation loss = 0.43652406334877014
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4322255849838257
Validation loss = 0.43236231803894043
Validation loss = 0.4311418831348419
Validation loss = 0.43428128957748413
Validation loss = 0.43362957239151
Validation loss = 0.43347856402397156
Validation loss = 0.43298983573913574
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4268031418323517
Validation loss = 0.4292396903038025
Validation loss = 0.4280351400375366
Validation loss = 0.42832303047180176
Validation loss = 0.429443359375
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 3.615296803652968
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 22
average number of affinization = 3.636259977194983
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 3.643507972665148
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 3.645051194539249
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 3.646590909090909
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 3.6617480136208855
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 3.6859410430839
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 3.699886749716874
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 3.7171945701357467
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 3.7242937853107345
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 3.730248306997743
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 3.7350620067643745
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 3.7533783783783785
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 3.760404949381327
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 3.7707865168539327
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 3.772166105499439
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 3.781390134529148
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 3.789473684210526
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 3.795302013422819
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 3.8033519553072628
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 3.8080357142857144
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 3.8260869565217392
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 3.8285077951002227
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 3.835372636262514
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 3.84
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 34       |
| MaximumReturn | 199      |
| MinimumReturn | 122      |
| TotalSamples  | 119988   |
----------------------------
itr #35 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4316527545452118
Validation loss = 0.431184858083725
Validation loss = 0.432072252035141
Validation loss = 0.43209755420684814
Validation loss = 0.4324694573879242
Validation loss = 0.4325677454471588
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4335920214653015
Validation loss = 0.43510371446609497
Validation loss = 0.4329763948917389
Validation loss = 0.43556562066078186
Validation loss = 0.4336467981338501
Validation loss = 0.43459275364875793
Validation loss = 0.4327818751335144
Validation loss = 0.43412211537361145
Validation loss = 0.4358372390270233
Validation loss = 0.43575990200042725
Validation loss = 0.4351135492324829
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4351487457752228
Validation loss = 0.4358738362789154
Validation loss = 0.43479010462760925
Validation loss = 0.4341622591018677
Validation loss = 0.4344087541103363
Validation loss = 0.4373455047607422
Validation loss = 0.43543338775634766
Validation loss = 0.4360426664352417
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4339146018028259
Validation loss = 0.4348658621311188
Validation loss = 0.43397119641304016
Validation loss = 0.4331996738910675
Validation loss = 0.4334608018398285
Validation loss = 0.4352763593196869
Validation loss = 0.4344439208507538
Validation loss = 0.43524205684661865
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4283394515514374
Validation loss = 0.42789676785469055
Validation loss = 0.4283754527568817
Validation loss = 0.42821216583251953
Validation loss = 0.428375244140625
Validation loss = 0.42817559838294983
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 3.8523862375138735
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 3.8569844789356984
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 3.8626799557032117
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 3.8716814159292037
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 3.881767955801105
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 3.9072847682119205
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 3.9162072767364937
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 3.9295154185022025
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 3.931793179317932
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 3.936263736263736
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 3.947310647639956
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 3.9660087719298245
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 3.9748083242059145
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 3.9857768052516414
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 3.9879781420765026
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 4.005458515283843
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 4.011995637949837
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 4.028322440087146
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 4.045701849836779
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 4.047826086956522
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 4.062975027144408
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 4.06941431670282
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 4.071505958829903
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 4.0811688311688314
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 4.085405405405405
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 35       |
| MaximumReturn | 198      |
| MinimumReturn | 119      |
| TotalSamples  | 123321   |
----------------------------
itr #36 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4336475431919098
Validation loss = 0.4339171350002289
Validation loss = 0.43398240208625793
Validation loss = 0.4341793358325958
Validation loss = 0.4362903833389282
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4355134665966034
Validation loss = 0.4382445514202118
Validation loss = 0.43620216846466064
Validation loss = 0.43487441539764404
Validation loss = 0.43519124388694763
Validation loss = 0.4371368885040283
Validation loss = 0.43828457593917847
Validation loss = 0.4390237331390381
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4351712763309479
Validation loss = 0.43645215034484863
Validation loss = 0.43650931119918823
Validation loss = 0.43682458996772766
Validation loss = 0.4361528754234314
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4339173436164856
Validation loss = 0.43446996808052063
Validation loss = 0.4358729124069214
Validation loss = 0.43628862500190735
Validation loss = 0.4360668957233429
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.42868417501449585
Validation loss = 0.4294634759426117
Validation loss = 0.42828044295310974
Validation loss = 0.42908424139022827
Validation loss = 0.4311526417732239
Validation loss = 0.43109652400016785
Validation loss = 0.42925670742988586
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 4.091792656587473
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 4.106796116504855
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 4.125
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 4.134553283100107
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 4.149462365591398
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 4.162191192266381
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 4.182403433476395
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 4.186495176848875
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 4.19271948608137
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 4.2
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 4.205128205128205
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 4.211312700106723
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 4.2153518123667375
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 4.225772097976571
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 4.245744680851064
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 4.2507970244420825
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 4.265392781316348
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 4.27677624602333
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 4.282838983050848
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 4.294179894179894
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 4.299154334038055
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 4.309398099260823
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 4.3238396624472575
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 4.334035827186512
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 4.3442105263157895
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 36       |
| MaximumReturn | 199      |
| MinimumReturn | 120      |
| TotalSamples  | 126654   |
----------------------------
itr #37 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4308743178844452
Validation loss = 0.43431124091148376
Validation loss = 0.43314090371131897
Validation loss = 0.43291154503822327
Validation loss = 0.43248745799064636
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.43594565987586975
Validation loss = 0.43736398220062256
Validation loss = 0.4353403151035309
Validation loss = 0.43589070439338684
Validation loss = 0.4392153322696686
Validation loss = 0.4367233216762543
Validation loss = 0.43775254487991333
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4364077150821686
Validation loss = 0.4354279339313507
Validation loss = 0.43582335114479065
Validation loss = 0.43603578209877014
Validation loss = 0.43737831711769104
Validation loss = 0.43653133511543274
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4344262182712555
Validation loss = 0.43414315581321716
Validation loss = 0.4365289807319641
Validation loss = 0.43444138765335083
Validation loss = 0.43450573086738586
Validation loss = 0.4360118508338928
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4307361841201782
Validation loss = 0.4286467730998993
Validation loss = 0.4292021691799164
Validation loss = 0.43028581142425537
Validation loss = 0.4290982186794281
Validation loss = 0.4294627010822296
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 4.351209253417455
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 4.360294117647059
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 4.374606505771249
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 4.379454926624738
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 4.375916230366492
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 4.380753138075314
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 4.387669801462905
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 4.38830897703549
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 4.3899895724713245
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 4.402083333333334
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 4.403746097814777
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 4.405405405405405
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 26
average number of affinization = 4.427829698857736
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 4.441908713692946
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 4.448704663212435
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 4.456521739130435
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 4.461220268872802
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 26
average number of affinization = 4.483471074380165
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 4.4943240454076365
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 4.5010309278350515
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 4.50669412976313
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 4.521604938271605
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 4.533401849948612
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 4.550308008213553
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 22
average number of affinization = 4.568205128205128
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 172      |
| Iteration     | 37       |
| MaximumReturn | 199      |
| MinimumReturn | 121      |
| TotalSamples  | 129987   |
----------------------------
itr #38 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4309796392917633
Validation loss = 0.433261513710022
Validation loss = 0.43223148584365845
Validation loss = 0.43169543147087097
Validation loss = 0.432438462972641
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4363313913345337
Validation loss = 0.4366389214992523
Validation loss = 0.4364946186542511
Validation loss = 0.4371486008167267
Validation loss = 0.4363134503364563
Validation loss = 0.43659040331840515
Validation loss = 0.4376292824745178
Validation loss = 0.44018033146858215
Validation loss = 0.43908363580703735
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.43551480770111084
Validation loss = 0.4361386299133301
Validation loss = 0.4376109838485718
Validation loss = 0.43670904636383057
Validation loss = 0.4362713098526001
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.43600618839263916
Validation loss = 0.43399152159690857
Validation loss = 0.4365288019180298
Validation loss = 0.4338582158088684
Validation loss = 0.4361361563205719
Validation loss = 0.4353603720664978
Validation loss = 0.43622100353240967
Validation loss = 0.43644338846206665
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.42859649658203125
Validation loss = 0.4288753569126129
Validation loss = 0.42900872230529785
Validation loss = 0.4307456612586975
Validation loss = 0.43019261956214905
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 4.578893442622951
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 4.597748208802456
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 4.6114519427402865
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 4.623084780388151
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 4.627551020408164
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 4.63506625891947
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 4.642566191446028
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 4.649033570701933
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 4.658536585365853
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 4.667005076142132
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 4.66632860040568
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 4.684903748733536
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 4.683198380566802
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 4.688574317492416
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 4.685858585858586
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 4.692230070635722
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 4.695564516129032
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 4.710976837865055
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 4.724346076458753
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 4.730653266331658
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 4.7379518072289155
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 4.736208625877633
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 4.744488977955911
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 4.741741741741742
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 4.745
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 178      |
| Iteration     | 38       |
| MaximumReturn | 198      |
| MinimumReturn | 127      |
| TotalSamples  | 133320   |
----------------------------
itr #39 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.43264836072921753
Validation loss = 0.432102233171463
Validation loss = 0.4324650168418884
Validation loss = 0.43239694833755493
Validation loss = 0.4335711896419525
Validation loss = 0.43348535895347595
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.43673720955848694
Validation loss = 0.4384951889514923
Validation loss = 0.43775346875190735
Validation loss = 0.4374518394470215
Validation loss = 0.43892496824264526
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4359581470489502
Validation loss = 0.4353333115577698
Validation loss = 0.43534818291664124
Validation loss = 0.43889474868774414
Validation loss = 0.43827980756759644
Validation loss = 0.4383256435394287
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.43754518032073975
Validation loss = 0.436421662569046
Validation loss = 0.43722963333129883
Validation loss = 0.43620699644088745
Validation loss = 0.4379553198814392
Validation loss = 0.4379887282848358
Validation loss = 0.43870192766189575
Validation loss = 0.4365529417991638
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.43051791191101074
Validation loss = 0.4299236536026001
Validation loss = 0.42951083183288574
Validation loss = 0.4299502968788147
Validation loss = 0.4299611747264862
Validation loss = 0.430511474609375
Validation loss = 0.4308909773826599
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 4.761238761238761
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 24
average number of affinization = 4.780439121756487
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 4.794616151545364
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 4.798804780876494
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 4.813930348258706
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 4.824055666003976
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 4.837140019860973
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 4.842261904761905
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 26
average number of affinization = 4.863230921704658
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 22
average number of affinization = 4.8801980198019805
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 22
average number of affinization = 4.897131552917903
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 4.909090909090909
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 4.913129318854886
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 4.92603550295858
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 4.935960591133005
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 4.947834645669292
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 4.9537856440511305
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 4.964636542239686
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 4.973503434739941
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 4.988235294117647
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 22
average number of affinization = 5.004897159647404
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 5.014677103718199
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 5.029325513196481
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 5.033203125
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 5.045853658536585
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 39       |
| MaximumReturn | 199      |
| MinimumReturn | 124      |
| TotalSamples  | 136653   |
----------------------------
