Logging to experiments/gym_pendulumO01/Pendulum_Observation_Noise/w100e0.5_seed2531
Print configuration .....
{'env_name': 'gym_pendulumO01', 'random_seeds': [3214, 2431, 2531, 2231], 'save_variables': False, 'model_save_dir': '/tmp/pendulumO01_models/', 'restore_variables': False, 'start_onpol_iter': 0, 'onpol_iters': 40, 'num_path_random': 25, 'num_path_onpol': 25, 'env_horizon': 200, 'max_train_data': 200000, 'max_val_data': 100000, 'discard_ratio': 0.0, 'dynamics': {'pre_training': {'mode': 'intrinsic_reward', 'itr': 0, 'policy_itr': 20}, 'model': 'nn', 'ensemble': True, 'ensemble_model_count': 5, 'enable_particle_ensemble': True, 'particles': 5, 'obs_var': 1.0, 'intrinsic_reward_coeff': 1.0, 'ita': 1.0, 'mode': 'random', 'val': True, 'n_layers': 4, 'hidden_size': 1000, 'activation': 'relu', 'batch_size': 1000, 'learning_rate': 0.001, 'reg_coeff': 0.0, 'epochs': 200, 'kfac_params': {'learning_rate': 0.1, 'damping': 0.001, 'momentum': 0.9, 'kl_clip': 0.0001, 'cov_ema_decay': 0.99}}, 'policy': {'network_shape': [64, 64], 'init_logstd': 0.0, 'activation': 'tanh', 'reinitialize_every_itr': False}, 'trpo': {'horizon': 200, 'gamma': 0.99, 'step_size': 0.01, 'iterations': 20, 'batch_size': 50000, 'gae': 0.95, 'visualization': False, 'visualize_iterations': [0]}, 'algo': 'trpo'}
Generating random rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating random rollouts.
Creating normalization for training data.
Done creating normalization for training data.
Particle ensemble enabled? True
An ensemble of 5 dynamics model <class 'model.dynamics.NNDynamicsModel'> initialized
Train dynamics model with intrinsic reward only? False
Pre-training enabled. Using only intrinsic reward.
Pre-training dynamics model for 0 iterations...
Done pre-training dynamics model.
Using external reward only.
itr #0 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3984387218952179
Validation loss = 0.22301490604877472
Validation loss = 0.20991063117980957
Validation loss = 0.20345015823841095
Validation loss = 0.2022189348936081
Validation loss = 0.20191097259521484
Validation loss = 0.2031804621219635
Validation loss = 0.19802357256412506
Validation loss = 0.1992790251970291
Validation loss = 0.1949058473110199
Validation loss = 0.21075944602489471
Validation loss = 0.19090232253074646
Validation loss = 0.18766841292381287
Validation loss = 0.18506789207458496
Validation loss = 0.20541170239448547
Validation loss = 0.18132449686527252
Validation loss = 0.192621111869812
Validation loss = 0.1770898848772049
Validation loss = 0.1803431212902069
Validation loss = 0.17742601037025452
Validation loss = 0.18382057547569275
Validation loss = 0.17468634247779846
Validation loss = 0.17522278428077698
Validation loss = 0.18537652492523193
Validation loss = 0.17451539635658264
Validation loss = 0.17409108579158783
Validation loss = 0.16805101931095123
Validation loss = 0.169640451669693
Validation loss = 0.1691819578409195
Validation loss = 0.1832510530948639
Validation loss = 0.1794099658727646
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4166509509086609
Validation loss = 0.2256656438112259
Validation loss = 0.20951028168201447
Validation loss = 0.2048352211713791
Validation loss = 0.20496590435504913
Validation loss = 0.2015618234872818
Validation loss = 0.2023119330406189
Validation loss = 0.2003246694803238
Validation loss = 0.19926229119300842
Validation loss = 0.20239001512527466
Validation loss = 0.20046544075012207
Validation loss = 0.1941758692264557
Validation loss = 0.19295820593833923
Validation loss = 0.18751178681850433
Validation loss = 0.19270172715187073
Validation loss = 0.18400250375270844
Validation loss = 0.17730143666267395
Validation loss = 0.17894317209720612
Validation loss = 0.18268869817256927
Validation loss = 0.1750527173280716
Validation loss = 0.19137972593307495
Validation loss = 0.17509382963180542
Validation loss = 0.17021703720092773
Validation loss = 0.16896113753318787
Validation loss = 0.1816803365945816
Validation loss = 0.1691952496767044
Validation loss = 0.17455576360225677
Validation loss = 0.17109347879886627
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4108014404773712
Validation loss = 0.22766156494617462
Validation loss = 0.20872803032398224
Validation loss = 0.20443513989448547
Validation loss = 0.20505644381046295
Validation loss = 0.2034057229757309
Validation loss = 0.20735034346580505
Validation loss = 0.20213791728019714
Validation loss = 0.19869889318943024
Validation loss = 0.20075488090515137
Validation loss = 0.20207741856575012
Validation loss = 0.19436372816562653
Validation loss = 0.1870671957731247
Validation loss = 0.1877908557653427
Validation loss = 0.18481457233428955
Validation loss = 0.19033469259738922
Validation loss = 0.19256317615509033
Validation loss = 0.18656319379806519
Validation loss = 0.17863956093788147
Validation loss = 0.17679844796657562
Validation loss = 0.18033990263938904
Validation loss = 0.18290981650352478
Validation loss = 0.17239156365394592
Validation loss = 0.17332281172275543
Validation loss = 0.1739400178194046
Validation loss = 0.1698649525642395
Validation loss = 0.17468233406543732
Validation loss = 0.1737305074930191
Validation loss = 0.1698409467935562
Validation loss = 0.1667247712612152
Validation loss = 0.1729632019996643
Validation loss = 0.17487896978855133
Validation loss = 0.17057542502880096
Validation loss = 0.16619601845741272
Validation loss = 0.16599445044994354
Validation loss = 0.17209351062774658
Validation loss = 0.17238466441631317
Validation loss = 0.17142531275749207
Validation loss = 0.16927140951156616
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4051538109779358
Validation loss = 0.2261686623096466
Validation loss = 0.21174584329128265
Validation loss = 0.2040838897228241
Validation loss = 0.20383891463279724
Validation loss = 0.2032087743282318
Validation loss = 0.20710442960262299
Validation loss = 0.20047108829021454
Validation loss = 0.19946855306625366
Validation loss = 0.20933349430561066
Validation loss = 0.19944049417972565
Validation loss = 0.1928911954164505
Validation loss = 0.18742471933364868
Validation loss = 0.19041849672794342
Validation loss = 0.19631411135196686
Validation loss = 0.18876977264881134
Validation loss = 0.1823282092809677
Validation loss = 0.18015450239181519
Validation loss = 0.1797598898410797
Validation loss = 0.18879085779190063
Validation loss = 0.17715075612068176
Validation loss = 0.17691074311733246
Validation loss = 0.17048464715480804
Validation loss = 0.17364495992660522
Validation loss = 0.17793992161750793
Validation loss = 0.17186279594898224
Validation loss = 0.17491154372692108
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.415863037109375
Validation loss = 0.21956279873847961
Validation loss = 0.20801928639411926
Validation loss = 0.20391109585762024
Validation loss = 0.20451830327510834
Validation loss = 0.20108327269554138
Validation loss = 0.20369146764278412
Validation loss = 0.1984265148639679
Validation loss = 0.1982763409614563
Validation loss = 0.19972294569015503
Validation loss = 0.20179970562458038
Validation loss = 0.20631876587867737
Validation loss = 0.19170139729976654
Validation loss = 0.18928973376750946
Validation loss = 0.18995416164398193
Validation loss = 0.18919457495212555
Validation loss = 0.18429146707057953
Validation loss = 0.1823212206363678
Validation loss = 0.17586277425289154
Validation loss = 0.18261651694774628
Validation loss = 0.18141582608222961
Validation loss = 0.18117155134677887
Validation loss = 0.1730232536792755
Validation loss = 0.18904699385166168
Validation loss = 0.17574426531791687
Validation loss = 0.1756109595298767
Validation loss = 0.16866245865821838
Validation loss = 0.17443795502185822
Validation loss = 0.17354370653629303
Validation loss = 0.1789035201072693
Validation loss = 0.17102235555648804
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 162      |
| Iteration     | 0        |
| MaximumReturn | 198      |
| MinimumReturn | 86.9     |
| TotalSamples  | 6666     |
----------------------------
itr #1 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.37123432755470276
Validation loss = 0.25300416350364685
Validation loss = 0.24285490810871124
Validation loss = 0.2404950112104416
Validation loss = 0.23754334449768066
Validation loss = 0.23573823273181915
Validation loss = 0.23414355516433716
Validation loss = 0.2381448596715927
Validation loss = 0.2393290400505066
Validation loss = 0.2334490418434143
Validation loss = 0.2367865890264511
Validation loss = 0.23829422891139984
Validation loss = 0.23427726328372955
Validation loss = 0.2354069948196411
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3621445596218109
Validation loss = 0.2514359652996063
Validation loss = 0.24609966576099396
Validation loss = 0.23844285309314728
Validation loss = 0.24198301136493683
Validation loss = 0.23928149044513702
Validation loss = 0.24258311092853546
Validation loss = 0.23973490297794342
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3505427837371826
Validation loss = 0.25152501463890076
Validation loss = 0.2400893121957779
Validation loss = 0.23640166223049164
Validation loss = 0.23463858664035797
Validation loss = 0.2344953566789627
Validation loss = 0.23395025730133057
Validation loss = 0.23602883517742157
Validation loss = 0.2345518320798874
Validation loss = 0.2379886358976364
Validation loss = 0.2366982251405716
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.33753499388694763
Validation loss = 0.24744725227355957
Validation loss = 0.23918206989765167
Validation loss = 0.23815985023975372
Validation loss = 0.23593872785568237
Validation loss = 0.23425322771072388
Validation loss = 0.23747354745864868
Validation loss = 0.23825226724147797
Validation loss = 0.23235392570495605
Validation loss = 0.23959791660308838
Validation loss = 0.2401127815246582
Validation loss = 0.233750581741333
Validation loss = 0.24355582892894745
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.36862802505493164
Validation loss = 0.24831509590148926
Validation loss = 0.23986005783081055
Validation loss = 0.23652447760105133
Validation loss = 0.23865269124507904
Validation loss = 0.23592031002044678
Validation loss = 0.23543918132781982
Validation loss = 0.23497498035430908
Validation loss = 0.23658324778079987
Validation loss = 0.23592597246170044
Validation loss = 0.23777993023395538
Validation loss = 0.2353190928697586
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 1        |
| MaximumReturn | 199      |
| MinimumReturn | 123      |
| TotalSamples  | 9999     |
----------------------------
itr #2 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.29178908467292786
Validation loss = 0.2655789256095886
Validation loss = 0.26539164781570435
Validation loss = 0.2655465304851532
Validation loss = 0.26089662313461304
Validation loss = 0.26420456171035767
Validation loss = 0.2607825994491577
Validation loss = 0.2591274082660675
Validation loss = 0.2647436559200287
Validation loss = 0.2585865259170532
Validation loss = 0.2581706643104553
Validation loss = 0.26146718859672546
Validation loss = 0.26492729783058167
Validation loss = 0.26357415318489075
Validation loss = 0.26160377264022827
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.2904844880104065
Validation loss = 0.26516371965408325
Validation loss = 0.26298967003822327
Validation loss = 0.2655717134475708
Validation loss = 0.264713853597641
Validation loss = 0.264059454202652
Validation loss = 0.2639557421207428
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.2956121563911438
Validation loss = 0.26149970293045044
Validation loss = 0.2644200921058655
Validation loss = 0.267597496509552
Validation loss = 0.2613983750343323
Validation loss = 0.2640543580055237
Validation loss = 0.2609710097312927
Validation loss = 0.26115211844444275
Validation loss = 0.26080217957496643
Validation loss = 0.26268497109413147
Validation loss = 0.26366257667541504
Validation loss = 0.26254981756210327
Validation loss = 0.260235071182251
Validation loss = 0.261199951171875
Validation loss = 0.2608124613761902
Validation loss = 0.25988951325416565
Validation loss = 0.2627350687980652
Validation loss = 0.2597300410270691
Validation loss = 0.2685820162296295
Validation loss = 0.2635105550289154
Validation loss = 0.26036250591278076
Validation loss = 0.2638869285583496
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.2799513638019562
Validation loss = 0.2643957734107971
Validation loss = 0.26308393478393555
Validation loss = 0.263263463973999
Validation loss = 0.2640972137451172
Validation loss = 0.26316314935684204
Validation loss = 0.26029738783836365
Validation loss = 0.26093775033950806
Validation loss = 0.2663416266441345
Validation loss = 0.26609185338020325
Validation loss = 0.26245078444480896
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.2911151051521301
Validation loss = 0.2606557011604309
Validation loss = 0.2627067267894745
Validation loss = 0.2601014971733093
Validation loss = 0.26470082998275757
Validation loss = 0.2640765607357025
Validation loss = 0.26220062375068665
Validation loss = 0.2611100375652313
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 169      |
| Iteration     | 2        |
| MaximumReturn | 199      |
| MinimumReturn | 71       |
| TotalSamples  | 13332    |
----------------------------
itr #3 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3016006052494049
Validation loss = 0.28613001108169556
Validation loss = 0.2857627868652344
Validation loss = 0.2860870659351349
Validation loss = 0.2866053283214569
Validation loss = 0.28536298871040344
Validation loss = 0.29189470410346985
Validation loss = 0.29002895951271057
Validation loss = 0.28415030241012573
Validation loss = 0.2892644703388214
Validation loss = 0.2857716381549835
Validation loss = 0.2890169620513916
Validation loss = 0.2948872148990631
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.29427602887153625
Validation loss = 0.2902325987815857
Validation loss = 0.2861526906490326
Validation loss = 0.2871027886867523
Validation loss = 0.2857225239276886
Validation loss = 0.28759416937828064
Validation loss = 0.29392367601394653
Validation loss = 0.29589006304740906
Validation loss = 0.2883938252925873
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3038899004459381
Validation loss = 0.28622379899024963
Validation loss = 0.28886792063713074
Validation loss = 0.2872019112110138
Validation loss = 0.2918550968170166
Validation loss = 0.28796103596687317
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.30636146664619446
Validation loss = 0.2879183888435364
Validation loss = 0.28683987259864807
Validation loss = 0.28684383630752563
Validation loss = 0.2868492305278778
Validation loss = 0.29100924730300903
Validation loss = 0.28809109330177307
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.2997431755065918
Validation loss = 0.287385493516922
Validation loss = 0.28745877742767334
Validation loss = 0.2936263680458069
Validation loss = 0.28818702697753906
Validation loss = 0.28813013434410095
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 3        |
| MaximumReturn | 197      |
| MinimumReturn | 130      |
| TotalSamples  | 16665    |
----------------------------
itr #4 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3262803554534912
Validation loss = 0.31840750575065613
Validation loss = 0.3129577934741974
Validation loss = 0.3172605335712433
Validation loss = 0.31334301829338074
Validation loss = 0.318549782037735
Validation loss = 0.3160014748573303
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.32336246967315674
Validation loss = 0.3212672472000122
Validation loss = 0.31622499227523804
Validation loss = 0.32110166549682617
Validation loss = 0.31459808349609375
Validation loss = 0.31601303815841675
Validation loss = 0.317433625459671
Validation loss = 0.31632575392723083
Validation loss = 0.3189592957496643
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3183766007423401
Validation loss = 0.3186854124069214
Validation loss = 0.31672605872154236
Validation loss = 0.31464362144470215
Validation loss = 0.3149881362915039
Validation loss = 0.3177117705345154
Validation loss = 0.3217560648918152
Validation loss = 0.3148421049118042
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3212672472000122
Validation loss = 0.31770652532577515
Validation loss = 0.3168080151081085
Validation loss = 0.319774329662323
Validation loss = 0.32567840814590454
Validation loss = 0.3212311267852783
Validation loss = 0.31368231773376465
Validation loss = 0.3147655427455902
Validation loss = 0.3181699812412262
Validation loss = 0.3153781592845917
Validation loss = 0.31763917207717896
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.31917011737823486
Validation loss = 0.3203365206718445
Validation loss = 0.31563276052474976
Validation loss = 0.3214958608150482
Validation loss = 0.3151628375053406
Validation loss = 0.31456050276756287
Validation loss = 0.31611573696136475
Validation loss = 0.31914255023002625
Validation loss = 0.31390219926834106
Validation loss = 0.31617918610572815
Validation loss = 0.3148793876171112
Validation loss = 0.320558100938797
Validation loss = 0.31885653734207153
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 4        |
| MaximumReturn | 198      |
| MinimumReturn | 120      |
| TotalSamples  | 19998    |
----------------------------
itr #5 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.32802289724349976
Validation loss = 0.3274689316749573
Validation loss = 0.3287302255630493
Validation loss = 0.32792505621910095
Validation loss = 0.33061841130256653
Validation loss = 0.3326147794723511
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.33100494742393494
Validation loss = 0.3291356861591339
Validation loss = 0.32773154973983765
Validation loss = 0.3262920379638672
Validation loss = 0.3267502188682556
Validation loss = 0.3259376585483551
Validation loss = 0.327584445476532
Validation loss = 0.32969701290130615
Validation loss = 0.32928216457366943
Validation loss = 0.327534943819046
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3293007016181946
Validation loss = 0.32807788252830505
Validation loss = 0.3282437324523926
Validation loss = 0.32988855242729187
Validation loss = 0.3271646797657013
Validation loss = 0.32663995027542114
Validation loss = 0.328816294670105
Validation loss = 0.32928788661956787
Validation loss = 0.33079203963279724
Validation loss = 0.33177095651626587
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.33804768323898315
Validation loss = 0.3280574679374695
Validation loss = 0.33322280645370483
Validation loss = 0.3262951970100403
Validation loss = 0.3283980190753937
Validation loss = 0.3302175998687744
Validation loss = 0.32925674319267273
Validation loss = 0.32918018102645874
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.33174794912338257
Validation loss = 0.33467209339141846
Validation loss = 0.33006900548934937
Validation loss = 0.3264036774635315
Validation loss = 0.3359518051147461
Validation loss = 0.3270764946937561
Validation loss = 0.33056750893592834
Validation loss = 0.32771793007850647
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 5        |
| MaximumReturn | 199      |
| MinimumReturn | 120      |
| TotalSamples  | 23331    |
----------------------------
itr #6 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.34337857365608215
Validation loss = 0.34113746881484985
Validation loss = 0.3384151756763458
Validation loss = 0.3405163288116455
Validation loss = 0.34323355555534363
Validation loss = 0.3455462157726288
Validation loss = 0.34214961528778076
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3397921621799469
Validation loss = 0.34164249897003174
Validation loss = 0.33818456530570984
Validation loss = 0.34061214327812195
Validation loss = 0.3411431610584259
Validation loss = 0.34400928020477295
Validation loss = 0.3385111093521118
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3424168527126312
Validation loss = 0.340364009141922
Validation loss = 0.3420601189136505
Validation loss = 0.34076401591300964
Validation loss = 0.3441256284713745
Validation loss = 0.3432717025279999
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.34145453572273254
Validation loss = 0.3416670262813568
Validation loss = 0.34063613414764404
Validation loss = 0.3411695659160614
Validation loss = 0.3434567451477051
Validation loss = 0.3428422808647156
Validation loss = 0.3390105068683624
Validation loss = 0.34312623739242554
Validation loss = 0.3412005603313446
Validation loss = 0.3404132127761841
Validation loss = 0.34065163135528564
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.34259629249572754
Validation loss = 0.3406807780265808
Validation loss = 0.33952298760414124
Validation loss = 0.3396332263946533
Validation loss = 0.3406943380832672
Validation loss = 0.33972838521003723
Validation loss = 0.3400140702724457
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 6        |
| MaximumReturn | 198      |
| MinimumReturn | 121      |
| TotalSamples  | 26664    |
----------------------------
itr #7 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.34927207231521606
Validation loss = 0.34926146268844604
Validation loss = 0.34931179881095886
Validation loss = 0.3495178818702698
Validation loss = 0.35180503129959106
Validation loss = 0.3491505980491638
Validation loss = 0.35118353366851807
Validation loss = 0.35108429193496704
Validation loss = 0.3514913320541382
Validation loss = 0.34903138875961304
Validation loss = 0.34939044713974
Validation loss = 0.3511422872543335
Validation loss = 0.35267990827560425
Validation loss = 0.3497012257575989
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3481704294681549
Validation loss = 0.34976544976234436
Validation loss = 0.34705427289009094
Validation loss = 0.34968164563179016
Validation loss = 0.34977832436561584
Validation loss = 0.34650278091430664
Validation loss = 0.3497999906539917
Validation loss = 0.3488421142101288
Validation loss = 0.34998035430908203
Validation loss = 0.34834912419319153
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.35032564401626587
Validation loss = 0.3490510582923889
Validation loss = 0.35106006264686584
Validation loss = 0.34935519099235535
Validation loss = 0.3505859673023224
Validation loss = 0.3523338735103607
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3508111834526062
Validation loss = 0.34785813093185425
Validation loss = 0.348694771528244
Validation loss = 0.34921595454216003
Validation loss = 0.3519202470779419
Validation loss = 0.34955576062202454
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3492194712162018
Validation loss = 0.34938254952430725
Validation loss = 0.3484395146369934
Validation loss = 0.34847304224967957
Validation loss = 0.3509007692337036
Validation loss = 0.34822553396224976
Validation loss = 0.35164308547973633
Validation loss = 0.34852394461631775
Validation loss = 0.352526992559433
Validation loss = 0.3475396931171417
Validation loss = 0.3496827781200409
Validation loss = 0.34832170605659485
Validation loss = 0.34973475337028503
Validation loss = 0.35154786705970764
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.004878048780487805
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0048543689320388345
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.004830917874396135
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.004807692307692308
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.004784688995215311
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.004761904761904762
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.004739336492890996
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0047169811320754715
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.004694835680751174
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.004672897196261682
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.004651162790697674
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.004629629629629629
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.004608294930875576
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0045871559633027525
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0045662100456621
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.004545454545454545
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.004524886877828055
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0045045045045045045
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.004484304932735426
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.004464285714285714
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0044444444444444444
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 7        |
| MaximumReturn | 198      |
| MinimumReturn | 101      |
| TotalSamples  | 29997    |
----------------------------
itr #8 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3609311878681183
Validation loss = 0.35986149311065674
Validation loss = 0.36114877462387085
Validation loss = 0.3613201081752777
Validation loss = 0.3636247217655182
Validation loss = 0.3619748055934906
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.36173170804977417
Validation loss = 0.36177337169647217
Validation loss = 0.36110368371009827
Validation loss = 0.35916975140571594
Validation loss = 0.35983172059059143
Validation loss = 0.35937997698783875
Validation loss = 0.3613097071647644
Validation loss = 0.35945236682891846
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.36580023169517517
Validation loss = 0.35953468084335327
Validation loss = 0.3624505400657654
Validation loss = 0.36414557695388794
Validation loss = 0.3602825105190277
Validation loss = 0.3612218201160431
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3621377646923065
Validation loss = 0.36088669300079346
Validation loss = 0.3593140244483948
Validation loss = 0.3588223457336426
Validation loss = 0.3608122766017914
Validation loss = 0.3614848554134369
Validation loss = 0.359032541513443
Validation loss = 0.3595125079154968
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3607427477836609
Validation loss = 0.3639296591281891
Validation loss = 0.35996487736701965
Validation loss = 0.36021289229393005
Validation loss = 0.36097997426986694
Validation loss = 0.359697550535202
Validation loss = 0.36053264141082764
Validation loss = 0.36124271154403687
Validation loss = 0.3618539869785309
Validation loss = 0.36016592383384705
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.004424778761061947
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.004405286343612335
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0043859649122807015
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.004366812227074236
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.004347826086956522
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.004329004329004329
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.004310344827586207
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.004291845493562232
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.004273504273504274
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.00425531914893617
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.00423728813559322
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.004219409282700422
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.004201680672268907
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.008368200836820083
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.008333333333333333
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.008298755186721992
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.008264462809917356
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.00823045267489712
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.00819672131147541
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.00816326530612245
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.012195121951219513
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.012145748987854251
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.016129032258064516
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.01606425702811245
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.016
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 8        |
| MaximumReturn | 197      |
| MinimumReturn | 122      |
| TotalSamples  | 33330    |
----------------------------
itr #9 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.36833012104034424
Validation loss = 0.37012532353401184
Validation loss = 0.36804506182670593
Validation loss = 0.3704662024974823
Validation loss = 0.37462759017944336
Validation loss = 0.3706233501434326
Validation loss = 0.3698852062225342
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3686056137084961
Validation loss = 0.37085771560668945
Validation loss = 0.3675301671028137
Validation loss = 0.369794100522995
Validation loss = 0.36684247851371765
Validation loss = 0.3671663999557495
Validation loss = 0.36805644631385803
Validation loss = 0.36745694279670715
Validation loss = 0.3683464527130127
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3698216676712036
Validation loss = 0.3695657253265381
Validation loss = 0.36839503049850464
Validation loss = 0.369176983833313
Validation loss = 0.3693462610244751
Validation loss = 0.369093656539917
Validation loss = 0.36907970905303955
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.36779433488845825
Validation loss = 0.36966532468795776
Validation loss = 0.3690232038497925
Validation loss = 0.36952704191207886
Validation loss = 0.3692854046821594
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.368794322013855
Validation loss = 0.36829233169555664
Validation loss = 0.3701441287994385
Validation loss = 0.3689935505390167
Validation loss = 0.37108170986175537
Validation loss = 0.37357932329177856
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.0199203187250996
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.023809523809523808
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.023715415019762844
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.023622047244094488
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.023529411764705882
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.0234375
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.023346303501945526
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.027131782945736434
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.03088803088803089
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.03076923076923077
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.034482758620689655
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.03435114503816794
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.034220532319391636
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.03409090909090909
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.033962264150943396
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.03383458646616541
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.033707865168539325
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.033582089552238806
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.03345724907063197
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.037037037037037035
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.03690036900369004
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.03676470588235294
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.03663003663003663
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.040145985401459854
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.04
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 177      |
| Iteration     | 9        |
| MaximumReturn | 198      |
| MinimumReturn | 135      |
| TotalSamples  | 36663    |
----------------------------
itr #10 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3817593455314636
Validation loss = 0.3811734616756439
Validation loss = 0.3793478310108185
Validation loss = 0.3821033239364624
Validation loss = 0.38337671756744385
Validation loss = 0.382793128490448
Validation loss = 0.38015368580818176
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.3798772096633911
Validation loss = 0.3809550404548645
Validation loss = 0.3783515989780426
Validation loss = 0.3798813819885254
Validation loss = 0.3812999725341797
Validation loss = 0.37834349274635315
Validation loss = 0.3796003758907318
Validation loss = 0.38023242354393005
Validation loss = 0.37899768352508545
Validation loss = 0.38225698471069336
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3800622820854187
Validation loss = 0.377145379781723
Validation loss = 0.3797905445098877
Validation loss = 0.37805110216140747
Validation loss = 0.37697169184684753
Validation loss = 0.3820299506187439
Validation loss = 0.37911301851272583
Validation loss = 0.3813001215457916
Validation loss = 0.3820731043815613
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3776547908782959
Validation loss = 0.3813170790672302
Validation loss = 0.37854406237602234
Validation loss = 0.3775952458381653
Validation loss = 0.3787841200828552
Validation loss = 0.3796256184577942
Validation loss = 0.3797735571861267
Validation loss = 0.3814188241958618
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3796568810939789
Validation loss = 0.38038671016693115
Validation loss = 0.3802967369556427
Validation loss = 0.37947186827659607
Validation loss = 0.38328102231025696
Validation loss = 0.38338711857795715
Validation loss = 0.38150304555892944
Validation loss = 0.3811556100845337
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.03985507246376811
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.04693140794223827
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.046762589928057555
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.04659498207885305
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.04642857142857143
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.046263345195729534
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.04964539007092199
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.05653710247349823
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.05985915492957746
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05964912280701754
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05944055944055944
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.059233449477351915
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.059027777777777776
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.058823529411764705
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05862068965517241
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.058419243986254296
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.05821917808219178
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.06143344709897611
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.06462585034013606
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.06779661016949153
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.06756756756756757
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.0707070707070707
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.0738255033557047
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07357859531772576
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07333333333333333
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 10       |
| MaximumReturn | 195      |
| MinimumReturn | 113      |
| TotalSamples  | 39996    |
----------------------------
itr #11 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3900936245918274
Validation loss = 0.3888973295688629
Validation loss = 0.3888550400733948
Validation loss = 0.38746675848960876
Validation loss = 0.38852638006210327
Validation loss = 0.3902912437915802
Validation loss = 0.38862481713294983
Validation loss = 0.3922850489616394
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.39427199959754944
Validation loss = 0.38535937666893005
Validation loss = 0.38829633593559265
Validation loss = 0.388606458902359
Validation loss = 0.38740232586860657
Validation loss = 0.38760560750961304
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.3888852596282959
Validation loss = 0.3905012607574463
Validation loss = 0.38676875829696655
Validation loss = 0.3890426754951477
Validation loss = 0.39169639348983765
Validation loss = 0.38731274008750916
Validation loss = 0.3888046145439148
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3888341188430786
Validation loss = 0.38906019926071167
Validation loss = 0.3868982493877411
Validation loss = 0.3880068361759186
Validation loss = 0.3885813057422638
Validation loss = 0.389177143573761
Validation loss = 0.38853225111961365
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3893964886665344
Validation loss = 0.38746124505996704
Validation loss = 0.38669613003730774
Validation loss = 0.3898676335811615
Validation loss = 0.3871653079986572
Validation loss = 0.3880763351917267
Validation loss = 0.3892834782600403
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.07308970099667775
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.076158940397351
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.0891089108910891
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.09210526315789473
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.09508196721311475
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.10784313725490197
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.11726384364820847
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.11688311688311688
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.11650485436893204
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.11612903225806452
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.1157556270096463
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.11858974358974358
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.1182108626198083
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.12101910828025478
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.12380952380952381
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.12341772151898735
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.12933753943217666
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.1320754716981132
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.13479623824451412
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.134375
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.13395638629283488
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.13975155279503104
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.1393188854489164
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.1388888888888889
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.14461538461538462
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 11       |
| MaximumReturn | 198      |
| MinimumReturn | 125      |
| TotalSamples  | 43329    |
----------------------------
itr #12 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.3958841860294342
Validation loss = 0.3936139643192291
Validation loss = 0.3958018720149994
Validation loss = 0.3952927887439728
Validation loss = 0.3963642120361328
Validation loss = 0.3959430158138275
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.39394354820251465
Validation loss = 0.39291292428970337
Validation loss = 0.3931175470352173
Validation loss = 0.39375486969947815
Validation loss = 0.39449605345726013
Validation loss = 0.3942984640598297
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.39542344212532043
Validation loss = 0.39503759145736694
Validation loss = 0.3915563225746155
Validation loss = 0.3934507966041565
Validation loss = 0.3942031264305115
Validation loss = 0.3938250243663788
Validation loss = 0.393934965133667
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.3919236361980438
Validation loss = 0.39257994294166565
Validation loss = 0.39352330565452576
Validation loss = 0.392286479473114
Validation loss = 0.3918280303478241
Validation loss = 0.3923431634902954
Validation loss = 0.39263594150543213
Validation loss = 0.39276930689811707
Validation loss = 0.3955197036266327
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.3926108777523041
Validation loss = 0.3932991027832031
Validation loss = 0.3938380777835846
Validation loss = 0.39259257912635803
Validation loss = 0.3942616879940033
Validation loss = 0.397995263338089
Validation loss = 0.39493516087532043
Validation loss = 0.3937523365020752
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.1441717791411043
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.1437308868501529
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.14634146341463414
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.1458966565349544
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.1606060606060606
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.16012084592145015
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.16265060240963855
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.16816816816816818
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.16766467065868262
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.1791044776119403
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.18154761904761904
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.18694362017804153
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.1952662721893491
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.20353982300884957
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.20588235294117646
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.20821114369501467
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.21637426900584794
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.22157434402332363
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.23255813953488372
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.24347826086956523
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.24566473988439305
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.25360230547550433
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.25862068965517243
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.2664756446991404
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.26857142857142857
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 164      |
| Iteration     | 12       |
| MaximumReturn | 196      |
| MinimumReturn | 119      |
| TotalSamples  | 46662    |
----------------------------
itr #13 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4027888774871826
Validation loss = 0.4049079716205597
Validation loss = 0.402627170085907
Validation loss = 0.40588146448135376
Validation loss = 0.40450045466423035
Validation loss = 0.40369635820388794
Validation loss = 0.40459156036376953
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4014430344104767
Validation loss = 0.40272122621536255
Validation loss = 0.40078186988830566
Validation loss = 0.4006492495536804
Validation loss = 0.3999100625514984
Validation loss = 0.4012646973133087
Validation loss = 0.4020483195781708
Validation loss = 0.40281033515930176
Validation loss = 0.4032917320728302
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.40272057056427
Validation loss = 0.4053117632865906
Validation loss = 0.4024036228656769
Validation loss = 0.40402930974960327
Validation loss = 0.4028611481189728
Validation loss = 0.4037420451641083
Validation loss = 0.4042322039604187
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.40128982067108154
Validation loss = 0.4009900987148285
Validation loss = 0.401921808719635
Validation loss = 0.4035373032093048
Validation loss = 0.4012732207775116
Validation loss = 0.40137070417404175
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.40235501527786255
Validation loss = 0.4014650881290436
Validation loss = 0.4022213816642761
Validation loss = 0.4027230143547058
Validation loss = 0.40438756346702576
Validation loss = 0.4032852053642273
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.2792022792022792
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.2784090909090909
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.2776203966005666
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.2796610169491525
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.27887323943661974
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.27808988764044945
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.28851540616246496
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.29329608938547486
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.30362116991643456
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.3138888888888889
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.3268698060941828
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.3259668508287293
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.325068870523416
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.3269230769230769
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.33972602739726027
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.33879781420765026
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.3460490463215259
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.358695652173913
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.35772357723577236
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.3621621621621622
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.3719676549865229
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.3763440860215054
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.3806970509383378
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.3850267379679144
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.3893333333333333
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 13       |
| MaximumReturn | 198      |
| MinimumReturn | 118      |
| TotalSamples  | 49995    |
----------------------------
itr #14 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4067964553833008
Validation loss = 0.4083646833896637
Validation loss = 0.41209402680397034
Validation loss = 0.4105505645275116
Validation loss = 0.41046562790870667
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.40768322348594666
Validation loss = 0.4092734456062317
Validation loss = 0.406100869178772
Validation loss = 0.40711379051208496
Validation loss = 0.40622657537460327
Validation loss = 0.407643586397171
Validation loss = 0.40721023082733154
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.40485939383506775
Validation loss = 0.4074317514896393
Validation loss = 0.4059509336948395
Validation loss = 0.4079822897911072
Validation loss = 0.40773046016693115
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4056786298751831
Validation loss = 0.4046682119369507
Validation loss = 0.40580904483795166
Validation loss = 0.4066486656665802
Validation loss = 0.40846794843673706
Validation loss = 0.4076506495475769
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4066867530345917
Validation loss = 0.40764331817626953
Validation loss = 0.40642911195755005
Validation loss = 0.4069925546646118
Validation loss = 0.40798455476760864
Validation loss = 0.4119175672531128
Validation loss = 0.4093775451183319
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.40425531914893614
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.41114058355437666
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.41798941798941797
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 0.44063324538258575
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.4394736842105263
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.44881889763779526
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.45287958115183247
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.4621409921671018
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.4713541666666667
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.4831168831168831
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.48704663212435234
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.49870801033591733
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.5
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.506426735218509
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.5102564102564102
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.5191815856777494
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.5306122448979592
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.5368956743002544
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.5355329949238579
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.5367088607594936
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.5429292929292929
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.5465994962216625
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.5452261306532663
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.5538847117794486
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.565
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 14       |
| MaximumReturn | 198      |
| MinimumReturn | 118      |
| TotalSamples  | 53328    |
----------------------------
itr #15 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.41281670331954956
Validation loss = 0.4148942232131958
Validation loss = 0.4145905077457428
Validation loss = 0.4137723743915558
Validation loss = 0.41232049465179443
Validation loss = 0.4132632911205292
Validation loss = 0.4134010672569275
Validation loss = 0.4153394401073456
Validation loss = 0.4160458445549011
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4129021465778351
Validation loss = 0.41402652859687805
Validation loss = 0.4147121012210846
Validation loss = 0.4125404953956604
Validation loss = 0.41439464688301086
Validation loss = 0.41262128949165344
Validation loss = 0.41339558362960815
Validation loss = 0.414481520652771
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4120073616504669
Validation loss = 0.41122907400131226
Validation loss = 0.41216084361076355
Validation loss = 0.41513317823410034
Validation loss = 0.4139609634876251
Validation loss = 0.4139726459980011
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4120894968509674
Validation loss = 0.41070860624313354
Validation loss = 0.4118002653121948
Validation loss = 0.4107292592525482
Validation loss = 0.41298672556877136
Validation loss = 0.4148353338241577
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4114963114261627
Validation loss = 0.412204384803772
Validation loss = 0.41230809688568115
Validation loss = 0.4138793349266052
Validation loss = 0.41509193181991577
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.57356608478803
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.5870646766169154
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.5905707196029777
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.599009900990099
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.6049382716049383
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 0.6206896551724138
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 0.6388206388206388
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.6397058823529411
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.6381418092909535
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.6463414634146342
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.6545012165450121
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.662621359223301
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.6658595641646489
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.6763285024154589
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.6746987951807228
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.6802884615384616
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.6906474820143885
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.7009569377990431
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.7040572792362768
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.7023809523809523
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.7102137767220903
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.7180094786729858
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.723404255319149
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.7216981132075472
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.7247058823529412
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 15       |
| MaximumReturn | 197      |
| MinimumReturn | 118      |
| TotalSamples  | 56661    |
----------------------------
itr #16 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4194068908691406
Validation loss = 0.4217808246612549
Validation loss = 0.4214913547039032
Validation loss = 0.42212799191474915
Validation loss = 0.41828402876853943
Validation loss = 0.42082729935646057
Validation loss = 0.4197748303413391
Validation loss = 0.4230627715587616
Validation loss = 0.42191794514656067
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.417124480009079
Validation loss = 0.4186529815196991
Validation loss = 0.4194043278694153
Validation loss = 0.41964295506477356
Validation loss = 0.4201011061668396
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.41658705472946167
Validation loss = 0.4161176085472107
Validation loss = 0.4178013801574707
Validation loss = 0.4185863435268402
Validation loss = 0.4183483123779297
Validation loss = 0.4180202782154083
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.41429394483566284
Validation loss = 0.41795825958251953
Validation loss = 0.41887444257736206
Validation loss = 0.41757360100746155
Validation loss = 0.41587358713150024
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.41918089985847473
Validation loss = 0.4185354709625244
Validation loss = 0.41874805092811584
Validation loss = 0.41853561997413635
Validation loss = 0.41651639342308044
Validation loss = 0.41931095719337463
Validation loss = 0.42215612530708313
Validation loss = 0.42267876863479614
Validation loss = 0.4209481179714203
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.7276995305164319
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.7259953161592506
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.7336448598130841
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 0.7482517482517482
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.7604651162790698
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.765661252900232
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.7638888888888888
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.76905311778291
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.7695852534562212
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.767816091954023
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 0.7821100917431193
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.782608695652174
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 0.7853881278538812
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 0.7995444191343963
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 0.7977272727272727
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.8027210884353742
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.8144796380090498
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.8261851015801355
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.8333333333333334
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 0.8471910112359551
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.8565022421524664
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.8680089485458613
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.875
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 0.8864142538975501
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 0.9022222222222223
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 179      |
| Iteration     | 16       |
| MaximumReturn | 197      |
| MinimumReturn | 130      |
| TotalSamples  | 59994    |
----------------------------
itr #17 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4272943139076233
Validation loss = 0.42687147855758667
Validation loss = 0.4265992343425751
Validation loss = 0.428791880607605
Validation loss = 0.430286705493927
Validation loss = 0.42904430627822876
Validation loss = 0.4277888238430023
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4246256649494171
Validation loss = 0.4253184199333191
Validation loss = 0.42459768056869507
Validation loss = 0.42417871952056885
Validation loss = 0.4259723722934723
Validation loss = 0.426324725151062
Validation loss = 0.4271980822086334
Validation loss = 0.42587414383888245
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.42382270097732544
Validation loss = 0.42322683334350586
Validation loss = 0.423639714717865
Validation loss = 0.4234747588634491
Validation loss = 0.42481935024261475
Validation loss = 0.42816251516342163
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.42261627316474915
Validation loss = 0.4248230755329132
Validation loss = 0.42433395981788635
Validation loss = 0.42206424474716187
Validation loss = 0.42289912700653076
Validation loss = 0.426742285490036
Validation loss = 0.4240458309650421
Validation loss = 0.4266085624694824
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4275653660297394
Validation loss = 0.4264751374721527
Validation loss = 0.4267726540565491
Validation loss = 0.4283233880996704
Validation loss = 0.426790714263916
Validation loss = 0.42885345220565796
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 0.9068736141906873
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 0.9203539823008849
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 0.9205298013245033
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 0.9295154185022027
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.9362637362637363
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 0.9429824561403509
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 0.9693654266958425
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 0.9956331877729258
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.0065359477124183
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.0152173913043478
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.0238611713665944
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.0367965367965368
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.0518358531317495
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.0581896551724137
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.0688172043010753
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.0815450643776825
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.0963597430406853
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 1.0940170940170941
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.1002132196162047
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 1.1212765957446809
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.127388535031847
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 1.1504237288135593
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 1.1522198731501057
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 1.1561181434599157
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.1621052631578948
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 174      |
| Iteration     | 17       |
| MaximumReturn | 199      |
| MinimumReturn | 120      |
| TotalSamples  | 63327    |
----------------------------
itr #18 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4297628700733185
Validation loss = 0.43317723274230957
Validation loss = 0.43571701645851135
Validation loss = 0.4327070415019989
Validation loss = 0.43303343653678894
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4294446110725403
Validation loss = 0.4294554591178894
Validation loss = 0.4299696683883667
Validation loss = 0.43096089363098145
Validation loss = 0.42974546551704407
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.42726704478263855
Validation loss = 0.4268917441368103
Validation loss = 0.42660075426101685
Validation loss = 0.4276586174964905
Validation loss = 0.4286835789680481
Validation loss = 0.4285491704940796
Validation loss = 0.4318491816520691
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.42902544140815735
Validation loss = 0.42814281582832336
Validation loss = 0.4298670291900635
Validation loss = 0.42937034368515015
Validation loss = 0.42799481749534607
Validation loss = 0.42859065532684326
Validation loss = 0.4312364161014557
Validation loss = 0.43186506628990173
Validation loss = 0.4302668273448944
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4290543794631958
Validation loss = 0.4311511814594269
Validation loss = 0.4296877682209015
Validation loss = 0.4308291971683502
Validation loss = 0.432523638010025
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 1.1869747899159664
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 1.1928721174004193
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 1
average number of affinization = 1.1924686192468619
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 1.196242171189979
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.2041666666666666
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.212058212058212
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.2199170124481329
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.2298136645962734
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.237603305785124
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 1.2350515463917526
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 1.2386831275720165
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.2525667351129364
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 1.2561475409836065
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 1.2760736196319018
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 1.273469387755102
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.2851323828920571
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.2967479674796747
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.304259634888438
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.3178137651821862
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 1.3151515151515152
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.3266129032258065
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.340040241448692
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.3473895582329318
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.3567134268537073
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 1.372
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 18       |
| MaximumReturn | 198      |
| MinimumReturn | 125      |
| TotalSamples  | 66660    |
----------------------------
itr #19 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.43306493759155273
Validation loss = 0.43421831727027893
Validation loss = 0.4335313141345978
Validation loss = 0.43434256315231323
Validation loss = 0.4337449073791504
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.43220558762550354
Validation loss = 0.43085575103759766
Validation loss = 0.43092966079711914
Validation loss = 0.43281877040863037
Validation loss = 0.4331580698490143
Validation loss = 0.4317641258239746
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4289070665836334
Validation loss = 0.4297597408294678
Validation loss = 0.4312305152416229
Validation loss = 0.431206613779068
Validation loss = 0.4318227171897888
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.43503743410110474
Validation loss = 0.4308491051197052
Validation loss = 0.4321182072162628
Validation loss = 0.4318704903125763
Validation loss = 0.43219614028930664
Validation loss = 0.433217853307724
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.43169090151786804
Validation loss = 0.43152952194213867
Validation loss = 0.4332062602043152
Validation loss = 0.43152475357055664
Validation loss = 0.4324493408203125
Validation loss = 0.43317171931266785
Validation loss = 0.4334971308708191
Validation loss = 0.4338492751121521
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.3852295409181636
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 1.408366533864542
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 1.4353876739562623
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.4484126984126984
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.4594059405940594
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 1.4782608695652173
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 1.495069033530572
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 1.5118110236220472
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 1.5147347740667976
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 1.5372549019607844
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 1.563600782778865
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.57421875
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 1.5867446393762183
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 1.5894941634241244
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 1.5980582524271845
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 1.6182170542635659
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 1.620889748549323
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 1.635135135135135
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 1.6493256262042388
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 1.6711538461538462
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 2
average number of affinization = 1.671785028790787
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 1.6896551724137931
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 1.6959847036328872
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 1.713740458015267
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 1.7295238095238095
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 19       |
| MaximumReturn | 198      |
| MinimumReturn | 121      |
| TotalSamples  | 69993    |
----------------------------
itr #20 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4337407648563385
Validation loss = 0.4334723949432373
Validation loss = 0.435714989900589
Validation loss = 0.434968501329422
Validation loss = 0.43630361557006836
Validation loss = 0.4356871545314789
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4343727231025696
Validation loss = 0.4327913820743561
Validation loss = 0.4328249990940094
Validation loss = 0.4348691403865814
Validation loss = 0.43452051281929016
Validation loss = 0.43550893664360046
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.43159255385398865
Validation loss = 0.4307834208011627
Validation loss = 0.43179959058761597
Validation loss = 0.43184104561805725
Validation loss = 0.4340583086013794
Validation loss = 0.43287336826324463
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4353080987930298
Validation loss = 0.43350112438201904
Validation loss = 0.4347418248653412
Validation loss = 0.4333491623401642
Validation loss = 0.4360106289386749
Validation loss = 0.43579575419425964
Validation loss = 0.436450332403183
Validation loss = 0.436358243227005
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4351511299610138
Validation loss = 0.4347761571407318
Validation loss = 0.4342326521873474
Validation loss = 0.4345189332962036
Validation loss = 0.4351411759853363
Validation loss = 0.43648311495780945
Validation loss = 0.43776845932006836
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 1.7699619771863118
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 7
average number of affinization = 1.77988614800759
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 1.8049242424242424
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 1.8525519848771266
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 1.8849056603773584
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 1.9001883239171375
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 1.9323308270676691
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 1.9662288930581613
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 1.9812734082397003
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 2.0
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 2.0279850746268657
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 2.0614525139664805
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 2.076208178438662
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 2.1001855287569575
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 2.1333333333333333
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 2.168207024029575
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 3
average number of affinization = 2.169741697416974
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 2.1915285451197053
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 24
average number of affinization = 2.2316176470588234
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 2.2605504587155965
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 2.2838827838827838
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 2.307129798903108
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 2.341240875912409
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 2.3788706739526413
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 2.403636363636364
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 165      |
| Iteration     | 20       |
| MaximumReturn | 198      |
| MinimumReturn | 95.3     |
| TotalSamples  | 73326    |
----------------------------
itr #21 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.43613168597221375
Validation loss = 0.43748539686203003
Validation loss = 0.4367384612560272
Validation loss = 0.43907272815704346
Validation loss = 0.4377093017101288
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4340682327747345
Validation loss = 0.4353269636631012
Validation loss = 0.43855395913124084
Validation loss = 0.43545785546302795
Validation loss = 0.4389533996582031
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.43316230177879333
Validation loss = 0.43370530009269714
Validation loss = 0.43302804231643677
Validation loss = 0.4349539279937744
Validation loss = 0.4343165159225464
Validation loss = 0.43467867374420166
Validation loss = 0.4356016516685486
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4355745017528534
Validation loss = 0.43690982460975647
Validation loss = 0.4358748495578766
Validation loss = 0.43747350573539734
Validation loss = 0.4374929666519165
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.43814733624458313
Validation loss = 0.4370744824409485
Validation loss = 0.43552884459495544
Validation loss = 0.4369857609272003
Validation loss = 0.43728840351104736
Validation loss = 0.4379523992538452
Validation loss = 0.43726426362991333
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 14
average number of affinization = 2.4246823956442833
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 2.4565217391304346
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 2.4864376130198913
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 2.527075812274368
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 2.554954954954955
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 2.5719424460431655
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 22
average number of affinization = 2.606822262118492
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 2.6379928315412187
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 2.667262969588551
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 2.7
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 2.730837789661319
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 12
average number of affinization = 2.7473309608540926
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 2.7602131438721136
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 5
average number of affinization = 2.7641843971631204
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 9
average number of affinization = 2.775221238938053
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 2.8056537102473498
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 2.8271604938271606
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 2.852112676056338
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 2.8769771528998245
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 4
average number of affinization = 2.8789473684210525
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 26
average number of affinization = 2.9194395796847634
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 2.937062937062937
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 2.9598603839441537
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 2.984320557491289
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 3.026086956521739
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 21       |
| MaximumReturn | 199      |
| MinimumReturn | 112      |
| TotalSamples  | 76659    |
----------------------------
itr #22 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4364016056060791
Validation loss = 0.4365125000476837
Validation loss = 0.4377872943878174
Validation loss = 0.43749162554740906
Validation loss = 0.4370483458042145
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.43628326058387756
Validation loss = 0.4339924454689026
Validation loss = 0.4362727105617523
Validation loss = 0.4350179433822632
Validation loss = 0.43610242009162903
Validation loss = 0.43691930174827576
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.43435990810394287
Validation loss = 0.4343511164188385
Validation loss = 0.43561020493507385
Validation loss = 0.4349173903465271
Validation loss = 0.4336984157562256
Validation loss = 0.4361704885959625
Validation loss = 0.43711891770362854
Validation loss = 0.43727436661720276
Validation loss = 0.43797704577445984
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.43519070744514465
Validation loss = 0.43633389472961426
Validation loss = 0.43711432814598083
Validation loss = 0.4391697943210602
Validation loss = 0.43877968192100525
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4372340738773346
Validation loss = 0.43878650665283203
Validation loss = 0.43600934743881226
Validation loss = 0.4370604455471039
Validation loss = 0.440395712852478
Validation loss = 0.4413401484489441
Validation loss = 0.44035083055496216
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 3.0503472222222223
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 26
average number of affinization = 3.0901213171577124
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 24
average number of affinization = 3.1262975778546713
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 3.150259067357513
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 30
average number of affinization = 3.196551724137931
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 22
average number of affinization = 3.2289156626506026
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 3.2577319587628866
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 3.2984562607204118
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 3.3321917808219177
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 3.3401709401709403
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 3.361774744027304
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 11
average number of affinization = 3.3747870528109027
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 3.4081632653061225
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 3.432937181663837
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 10
average number of affinization = 3.4440677966101694
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 3.451776649746193
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 3.4763513513513513
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 3.5025295109612142
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 8
average number of affinization = 3.51010101010101
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 26
average number of affinization = 3.5478991596638654
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 0
average number of affinization = 3.541946308724832
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 3.5812395309882747
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 21
average number of affinization = 3.6103678929765888
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 3.659432387312187
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 3.68
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 161      |
| Iteration     | 22       |
| MaximumReturn | 198      |
| MinimumReturn | 102      |
| TotalSamples  | 79992    |
----------------------------
itr #23 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.43298596143722534
Validation loss = 0.43311136960983276
Validation loss = 0.4331989288330078
Validation loss = 0.4347879886627197
Validation loss = 0.43533286452293396
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4338098466396332
Validation loss = 0.4326511323451996
Validation loss = 0.4324226379394531
Validation loss = 0.4335912764072418
Validation loss = 0.43444329500198364
Validation loss = 0.4342644214630127
Validation loss = 0.43634000420570374
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4334864616394043
Validation loss = 0.43496760725975037
Validation loss = 0.43451380729675293
Validation loss = 0.4347309172153473
Validation loss = 0.4352811872959137
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4342971742153168
Validation loss = 0.4351341128349304
Validation loss = 0.4339979290962219
Validation loss = 0.43574365973472595
Validation loss = 0.4348500370979309
Validation loss = 0.4370630383491516
Validation loss = 0.43785470724105835
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.43731489777565
Validation loss = 0.43692120909690857
Validation loss = 0.4381515383720398
Validation loss = 0.43634772300720215
Validation loss = 0.43630871176719666
Validation loss = 0.4382677972316742
Validation loss = 0.437502384185791
Validation loss = 0.43879491090774536
Validation loss = 0.4392772316932678
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 24
average number of affinization = 3.713810316139767
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 3.7607973421926912
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 3.7927031509121063
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 28
average number of affinization = 3.8327814569536423
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 3.852892561983471
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 6
average number of affinization = 3.8564356435643563
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 22
average number of affinization = 3.886326194398682
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 15
average number of affinization = 3.9046052631578947
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 31
average number of affinization = 3.949096880131363
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 17
average number of affinization = 3.9704918032786884
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 3.9950900163666123
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 4.042483660130719
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 24
average number of affinization = 4.075040783034257
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 4.130293159609121
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 28
average number of affinization = 4.16910569105691
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 24
average number of affinization = 4.201298701298701
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 22
average number of affinization = 4.230145867098866
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 4.252427184466019
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 4.289176090468498
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 22
average number of affinization = 4.317741935483871
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 13
average number of affinization = 4.331723027375201
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 4.356913183279743
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 30
average number of affinization = 4.398073836276083
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 30
average number of affinization = 4.439102564102564
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 4.4688
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 160      |
| Iteration     | 23       |
| MaximumReturn | 197      |
| MinimumReturn | 109      |
| TotalSamples  | 83325    |
----------------------------
itr #24 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.43289411067962646
Validation loss = 0.431942880153656
Validation loss = 0.4358116388320923
Validation loss = 0.4338524043560028
Validation loss = 0.4349331259727478
Validation loss = 0.43444791436195374
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.43423089385032654
Validation loss = 0.4342552721500397
Validation loss = 0.43337082862854004
Validation loss = 0.4340788722038269
Validation loss = 0.43189629912376404
Validation loss = 0.43622100353240967
Validation loss = 0.43250513076782227
Validation loss = 0.43614819645881653
Validation loss = 0.43760034441947937
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4330085515975952
Validation loss = 0.4339043200016022
Validation loss = 0.4346434473991394
Validation loss = 0.436381459236145
Validation loss = 0.43468666076660156
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4353314936161041
Validation loss = 0.43521058559417725
Validation loss = 0.4361107051372528
Validation loss = 0.4366663098335266
Validation loss = 0.4362490475177765
Validation loss = 0.43753981590270996
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4353775978088379
Validation loss = 0.4363684356212616
Validation loss = 0.4369617998600006
Validation loss = 0.4373849630355835
Validation loss = 0.43917781114578247
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 4.50479233226837
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 34
average number of affinization = 4.551834130781499
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 34
average number of affinization = 4.598726114649682
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 29
average number of affinization = 4.63751987281399
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 26
average number of affinization = 4.671428571428572
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 4.695721077654516
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 4.738924050632911
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 4.791469194312796
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 24
average number of affinization = 4.821766561514195
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 35
average number of affinization = 4.869291338582677
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 4.904088050314465
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 4.957613814756672
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 30
average number of affinization = 4.996865203761756
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 29
average number of affinization = 5.034428794992175
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 5.0875
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 5.118564742589704
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 19
average number of affinization = 5.140186915887851
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 22
average number of affinization = 5.166407465007776
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 5.1894409937888195
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 30
average number of affinization = 5.227906976744186
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 20
average number of affinization = 5.25077399380805
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 5.278207109737249
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 35
average number of affinization = 5.324074074074074
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 5.375963020030817
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 34
average number of affinization = 5.42
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 24       |
| MaximumReturn | 198      |
| MinimumReturn | 137      |
| TotalSamples  | 86658    |
----------------------------
itr #25 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4340308904647827
Validation loss = 0.43297234177589417
Validation loss = 0.43124425411224365
Validation loss = 0.43407049775123596
Validation loss = 0.4374880790710449
Validation loss = 0.4357280433177948
Validation loss = 0.43538013100624084
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.43406420946121216
Validation loss = 0.43215760588645935
Validation loss = 0.43516895174980164
Validation loss = 0.4348084628582001
Validation loss = 0.43471434712409973
Validation loss = 0.43582454323768616
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.43180686235427856
Validation loss = 0.4340584874153137
Validation loss = 0.43417367339134216
Validation loss = 0.4337299168109894
Validation loss = 0.43456795811653137
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.43386831879615784
Validation loss = 0.4365728199481964
Validation loss = 0.4360826313495636
Validation loss = 0.43857210874557495
Validation loss = 0.4370587468147278
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4342893958091736
Validation loss = 0.4370470643043518
Validation loss = 0.43530285358428955
Validation loss = 0.43654587864875793
Validation loss = 0.43994930386543274
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 43
average number of affinization = 5.477726574500768
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 24
average number of affinization = 5.506134969325154
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 30
average number of affinization = 5.54364471669219
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 44
average number of affinization = 5.602446483180428
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 35
average number of affinization = 5.647328244274809
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 5.679878048780488
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 5.719939117199391
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 5.7522796352583585
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 5.793626707132018
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 23
average number of affinization = 5.819696969696969
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 43
average number of affinization = 5.875945537065053
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 45
average number of affinization = 5.935045317220544
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 41
average number of affinization = 5.987933634992459
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 6.037650602409639
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 36
average number of affinization = 6.082706766917293
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 35
average number of affinization = 6.126126126126126
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 35
average number of affinization = 6.169415292353823
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 6.217065868263473
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 37
average number of affinization = 6.263079222720478
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 29
average number of affinization = 6.2970149253731345
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 16
average number of affinization = 6.311475409836065
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 30
average number of affinization = 6.346726190476191
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 6.384843982169391
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 6.41246290801187
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 28
average number of affinization = 6.444444444444445
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 25       |
| MaximumReturn | 197      |
| MinimumReturn | 126      |
| TotalSamples  | 89991    |
----------------------------
itr #26 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4360181987285614
Validation loss = 0.4356512725353241
Validation loss = 0.43411967158317566
Validation loss = 0.4356147050857544
Validation loss = 0.43364769220352173
Validation loss = 0.435439795255661
Validation loss = 0.4369427263736725
Validation loss = 0.4362213909626007
Validation loss = 0.43856778740882874
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4343714714050293
Validation loss = 0.43531444668769836
Validation loss = 0.4355507791042328
Validation loss = 0.4363909065723419
Validation loss = 0.4353645443916321
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.43303316831588745
Validation loss = 0.4336129426956177
Validation loss = 0.43378108739852905
Validation loss = 0.43161967396736145
Validation loss = 0.4342517554759979
Validation loss = 0.4334337115287781
Validation loss = 0.43426841497421265
Validation loss = 0.4354546368122101
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4366573691368103
Validation loss = 0.4362277090549469
Validation loss = 0.43709492683410645
Validation loss = 0.4374017119407654
Validation loss = 0.43892017006874084
Validation loss = 0.4370310604572296
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.43579941987991333
Validation loss = 0.43837878108024597
Validation loss = 0.4357450604438782
Validation loss = 0.43726521730422974
Validation loss = 0.43835175037384033
Validation loss = 0.4379696249961853
Validation loss = 0.4387831687927246
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 6.47189349112426
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 29
average number of affinization = 6.5051698670605616
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 6.54424778761062
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 44
average number of affinization = 6.59941089837997
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 6.647058823529412
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 61
average number of affinization = 6.726872246696035
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 43
average number of affinization = 6.780058651026393
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 46
average number of affinization = 6.837481698389459
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 6.875730994152047
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 50
average number of affinization = 6.938686131386861
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 6.967930029154519
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 35
average number of affinization = 7.008733624454148
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 31
average number of affinization = 7.0436046511627906
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 36
average number of affinization = 7.085631349782293
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 41
average number of affinization = 7.1347826086956525
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 56
average number of affinization = 7.205499276410999
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 43
average number of affinization = 7.257225433526012
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 43
average number of affinization = 7.308802308802309
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 34
average number of affinization = 7.347262247838617
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 41
average number of affinization = 7.39568345323741
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 36
average number of affinization = 7.436781609195402
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 7.486370157819225
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 7.530085959885387
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 7.55793991416309
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 47
average number of affinization = 7.614285714285714
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 166      |
| Iteration     | 26       |
| MaximumReturn | 198      |
| MinimumReturn | 120      |
| TotalSamples  | 93324    |
----------------------------
itr #27 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4372524917125702
Validation loss = 0.4347994029521942
Validation loss = 0.4369945526123047
Validation loss = 0.43723201751708984
Validation loss = 0.4398583173751831
Validation loss = 0.4419400095939636
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.43442419171333313
Validation loss = 0.433936208486557
Validation loss = 0.43517884612083435
Validation loss = 0.4353456497192383
Validation loss = 0.43551433086395264
Validation loss = 0.437883198261261
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.43513137102127075
Validation loss = 0.43426257371902466
Validation loss = 0.4353564381599426
Validation loss = 0.4357101619243622
Validation loss = 0.4342881441116333
Validation loss = 0.436333566904068
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4366401731967926
Validation loss = 0.43662816286087036
Validation loss = 0.43721622228622437
Validation loss = 0.4405474364757538
Validation loss = 0.4388379156589508
Validation loss = 0.4376828670501709
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4379476010799408
Validation loss = 0.43745657801628113
Validation loss = 0.436279833316803
Validation loss = 0.43663290143013
Validation loss = 0.4398804008960724
Validation loss = 0.4387785792350769
Validation loss = 0.4399375915527344
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 43
average number of affinization = 7.664764621968616
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 7.700854700854701
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 30
average number of affinization = 7.732574679943101
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 7.775568181818182
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 50
average number of affinization = 7.835460992907802
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 34
average number of affinization = 7.872521246458923
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 41
average number of affinization = 7.91937765205092
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 49
average number of affinization = 7.977401129943503
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 8.025387870239774
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 43
average number of affinization = 8.074647887323943
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 30
average number of affinization = 8.10548523206751
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 34
average number of affinization = 8.14185393258427
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 51
average number of affinization = 8.201963534361852
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 8.22549019607843
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 56
average number of affinization = 8.292307692307693
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 46
average number of affinization = 8.344972067039107
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 8.387726638772664
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 48
average number of affinization = 8.442896935933147
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 8.475660639777468
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 45
average number of affinization = 8.526388888888889
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 37
average number of affinization = 8.565880721220527
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 8.606648199445983
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 8.640387275242047
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 8.68646408839779
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 8.732413793103449
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 178      |
| Iteration     | 27       |
| MaximumReturn | 198      |
| MinimumReturn | 111      |
| TotalSamples  | 96657    |
----------------------------
itr #28 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4357561767101288
Validation loss = 0.43617764115333557
Validation loss = 0.4378734827041626
Validation loss = 0.4353054463863373
Validation loss = 0.4393198490142822
Validation loss = 0.43817201256752014
Validation loss = 0.4396815299987793
Validation loss = 0.438333660364151
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4349098205566406
Validation loss = 0.4349336624145508
Validation loss = 0.43471643328666687
Validation loss = 0.4341455399990082
Validation loss = 0.43590840697288513
Validation loss = 0.4353736937046051
Validation loss = 0.43595364689826965
Validation loss = 0.43587613105773926
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.43473020195961
Validation loss = 0.43587687611579895
Validation loss = 0.43400850892066956
Validation loss = 0.4335606098175049
Validation loss = 0.43561866879463196
Validation loss = 0.43380680680274963
Validation loss = 0.4348262846469879
Validation loss = 0.4362887144088745
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4372199475765228
Validation loss = 0.4359196424484253
Validation loss = 0.43813589215278625
Validation loss = 0.4391179084777832
Validation loss = 0.438442200422287
Validation loss = 0.4382157325744629
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.43660101294517517
Validation loss = 0.43587151169776917
Validation loss = 0.4383876323699951
Validation loss = 0.438596248626709
Validation loss = 0.4362741708755493
Validation loss = 0.4382840096950531
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 8.774104683195592
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 41
average number of affinization = 8.818431911966988
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 26
average number of affinization = 8.842032967032967
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 37
average number of affinization = 8.880658436213992
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 40
average number of affinization = 8.923287671232877
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 53
average number of affinization = 8.98358413132695
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 9.005464480874316
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 9.045020463847203
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 9.08991825613079
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 9.11156462585034
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 44
average number of affinization = 9.158967391304348
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 34
average number of affinization = 9.192672998643149
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 36
average number of affinization = 9.2289972899729
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 49
average number of affinization = 9.282814614343708
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 37
average number of affinization = 9.32027027027027
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 9.35897435897436
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 51
average number of affinization = 9.415094339622641
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 36
average number of affinization = 9.450874831763123
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 9.489247311827956
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 9.512751677852348
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 9.533512064343164
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 37
average number of affinization = 9.570281124497992
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 40
average number of affinization = 9.61096256684492
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 29
average number of affinization = 9.636849132176234
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 29
average number of affinization = 9.662666666666667
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 176      |
| Iteration     | 28       |
| MaximumReturn | 198      |
| MinimumReturn | 125      |
| TotalSamples  | 99990    |
----------------------------
itr #29 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.43760791420936584
Validation loss = 0.4379537105560303
Validation loss = 0.4391650855541229
Validation loss = 0.4385624825954437
Validation loss = 0.44072720408439636
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4371543824672699
Validation loss = 0.4350917935371399
Validation loss = 0.4348756670951843
Validation loss = 0.4351691007614136
Validation loss = 0.43657591938972473
Validation loss = 0.4364040493965149
Validation loss = 0.43820658326148987
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.43409132957458496
Validation loss = 0.4341083765029907
Validation loss = 0.4348051846027374
Validation loss = 0.4360746741294861
Validation loss = 0.43631231784820557
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4368330240249634
Validation loss = 0.43618133664131165
Validation loss = 0.43797358870506287
Validation loss = 0.4391111433506012
Validation loss = 0.43990352749824524
Validation loss = 0.4398318827152252
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.438483327627182
Validation loss = 0.4370197653770447
Validation loss = 0.4379398822784424
Validation loss = 0.4382721185684204
Validation loss = 0.43863171339035034
Validation loss = 0.43833449482917786
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 27
average number of affinization = 9.685752330226364
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 44
average number of affinization = 9.731382978723405
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 9.768924302788845
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 29
average number of affinization = 9.794429708222811
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 48
average number of affinization = 9.845033112582781
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 18
average number of affinization = 9.855820105820106
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 37
average number of affinization = 9.891677675033025
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 54
average number of affinization = 9.949868073878628
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 41
average number of affinization = 9.990777338603426
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 31
average number of affinization = 10.01842105263158
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 10.04862023653088
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 37
average number of affinization = 10.083989501312336
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 10.121887287024903
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 10.159685863874346
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 41
average number of affinization = 10.2
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 10.22845953002611
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 31
average number of affinization = 10.25554106910039
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 10.296875
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 41
average number of affinization = 10.336801040312094
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 10.355844155844157
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 50
average number of affinization = 10.407263294422828
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 52
average number of affinization = 10.461139896373057
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 43
average number of affinization = 10.503234152652006
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 36
average number of affinization = 10.536175710594316
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 41
average number of affinization = 10.575483870967743
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 164      |
| Iteration     | 29       |
| MaximumReturn | 198      |
| MinimumReturn | 128      |
| TotalSamples  | 103323   |
----------------------------
itr #30 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4368477165699005
Validation loss = 0.43663254380226135
Validation loss = 0.4372621774673462
Validation loss = 0.4392409920692444
Validation loss = 0.43970873951911926
Validation loss = 0.4413192868232727
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4343383014202118
Validation loss = 0.43358269333839417
Validation loss = 0.4359164535999298
Validation loss = 0.4360450208187103
Validation loss = 0.437979519367218
Validation loss = 0.435824453830719
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4344354569911957
Validation loss = 0.4327995479106903
Validation loss = 0.4331930875778198
Validation loss = 0.4351753294467926
Validation loss = 0.43560296297073364
Validation loss = 0.437332421541214
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.43695345520973206
Validation loss = 0.4362107515335083
Validation loss = 0.43869897723197937
Validation loss = 0.4356503486633301
Validation loss = 0.43834537267684937
Validation loss = 0.43856608867645264
Validation loss = 0.43916580080986023
Validation loss = 0.43928173184394836
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.43654119968414307
Validation loss = 0.43657931685447693
Validation loss = 0.43721020221710205
Validation loss = 0.4350084364414215
Validation loss = 0.4364943504333496
Validation loss = 0.43872159719467163
Validation loss = 0.43763086199760437
Validation loss = 0.43966028094291687
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 49
average number of affinization = 10.625
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 34
average number of affinization = 10.655083655083654
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 40
average number of affinization = 10.69280205655527
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 34
average number of affinization = 10.722721437740693
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 10.762820512820513
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 28
average number of affinization = 10.784891165172855
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 10.819693094629155
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 44
average number of affinization = 10.862068965517242
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 46
average number of affinization = 10.90688775510204
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 10.94140127388535
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 45
average number of affinization = 10.98473282442748
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 34
average number of affinization = 11.013977128335451
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 40
average number of affinization = 11.050761421319796
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 11.08998732572877
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 48
average number of affinization = 11.136708860759494
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 25
average number of affinization = 11.154235145385588
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 43
average number of affinization = 11.194444444444445
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 49
average number of affinization = 11.242118537200504
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 11.280856423173804
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 36
average number of affinization = 11.311949685534591
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 55
average number of affinization = 11.366834170854272
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 36
average number of affinization = 11.397741530740277
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 29
average number of affinization = 11.419799498746867
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 40
average number of affinization = 11.455569461827285
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 52
average number of affinization = 11.50625
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 173      |
| Iteration     | 30       |
| MaximumReturn | 199      |
| MinimumReturn | 123      |
| TotalSamples  | 106656   |
----------------------------
itr #31 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4379504323005676
Validation loss = 0.43882879614830017
Validation loss = 0.44017183780670166
Validation loss = 0.43682873249053955
Validation loss = 0.4383167028427124
Validation loss = 0.4382649064064026
Validation loss = 0.4370848834514618
Validation loss = 0.439421683549881
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4337528645992279
Validation loss = 0.43319106101989746
Validation loss = 0.435649037361145
Validation loss = 0.4352361857891083
Validation loss = 0.4366309940814972
Validation loss = 0.4378441870212555
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.43362224102020264
Validation loss = 0.4340229034423828
Validation loss = 0.43348249793052673
Validation loss = 0.43462175130844116
Validation loss = 0.43528854846954346
Validation loss = 0.4348616600036621
Validation loss = 0.43388795852661133
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4388805031776428
Validation loss = 0.43771830201148987
Validation loss = 0.4382207989692688
Validation loss = 0.4380779564380646
Validation loss = 0.43976891040802
Validation loss = 0.4405946135520935
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.43933260440826416
Validation loss = 0.43566039204597473
Validation loss = 0.43713048100471497
Validation loss = 0.4360523521900177
Validation loss = 0.43857541680336
Validation loss = 0.43704602122306824
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 40
average number of affinization = 11.541822721598002
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 37
average number of affinization = 11.57356608478803
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 11.607721046077211
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 48
average number of affinization = 11.652985074626866
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 44
average number of affinization = 11.693167701863354
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 35
average number of affinization = 11.722084367245657
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 45
average number of affinization = 11.763320941759604
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 32
average number of affinization = 11.788366336633663
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 35
average number of affinization = 11.817058096415328
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 37
average number of affinization = 11.848148148148148
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 53
average number of affinization = 11.89889025893958
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 55
average number of affinization = 11.951970443349754
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 51
average number of affinization = 12.0
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 29
average number of affinization = 12.02088452088452
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 48
average number of affinization = 12.065030674846625
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 43
average number of affinization = 12.102941176470589
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 57
average number of affinization = 12.157894736842104
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 54
average number of affinization = 12.209046454767726
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 57
average number of affinization = 12.263736263736265
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 40
average number of affinization = 12.297560975609755
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 55
average number of affinization = 12.349573690621193
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 47
average number of affinization = 12.391727493917275
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 45
average number of affinization = 12.43134872417983
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 12.467233009708737
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 46
average number of affinization = 12.507878787878788
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 164      |
| Iteration     | 31       |
| MaximumReturn | 198      |
| MinimumReturn | 125      |
| TotalSamples  | 109989   |
----------------------------
itr #32 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.43584832549095154
Validation loss = 0.43751710653305054
Validation loss = 0.4375172257423401
Validation loss = 0.4390890896320343
Validation loss = 0.4370929002761841
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4324425756931305
Validation loss = 0.4336685240268707
Validation loss = 0.4338308274745941
Validation loss = 0.43488770723342896
Validation loss = 0.43456584215164185
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.43303313851356506
Validation loss = 0.43233656883239746
Validation loss = 0.4332262873649597
Validation loss = 0.4352838099002838
Validation loss = 0.4344106912612915
Validation loss = 0.4340733289718628
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.43576934933662415
Validation loss = 0.43627533316612244
Validation loss = 0.43654343485832214
Validation loss = 0.43759945034980774
Validation loss = 0.4383164048194885
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.43484899401664734
Validation loss = 0.4358551800251007
Validation loss = 0.4374154210090637
Validation loss = 0.4352346658706665
Validation loss = 0.4352986216545105
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 53
average number of affinization = 12.556900726392252
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 51
average number of affinization = 12.603385731559856
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 12.63888888888889
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 46
average number of affinization = 12.67913148371532
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 12.710843373493976
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 47
average number of affinization = 12.752105896510228
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 54
average number of affinization = 12.801682692307692
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 48
average number of affinization = 12.843937575030012
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 47
average number of affinization = 12.884892086330936
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 50
average number of affinization = 12.92934131736527
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 56
average number of affinization = 12.980861244019138
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 13.01194743130227
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 13.042959427207638
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 30
average number of affinization = 13.063170441001192
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 48
average number of affinization = 13.104761904761904
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 13.135552913198573
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 49
average number of affinization = 13.178147268408551
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 47
average number of affinization = 13.218268090154211
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 40
average number of affinization = 13.25
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 37
average number of affinization = 13.278106508875739
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 60
average number of affinization = 13.333333333333334
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 53
average number of affinization = 13.380165289256198
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 34
average number of affinization = 13.404481132075471
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 56
average number of affinization = 13.454652532391048
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 48
average number of affinization = 13.49529411764706
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 171      |
| Iteration     | 32       |
| MaximumReturn | 198      |
| MinimumReturn | 115      |
| TotalSamples  | 113322   |
----------------------------
itr #33 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4371863901615143
Validation loss = 0.437883585691452
Validation loss = 0.436903715133667
Validation loss = 0.4356803596019745
Validation loss = 0.4399194121360779
Validation loss = 0.440964013338089
Validation loss = 0.4407772421836853
Validation loss = 0.4373663365840912
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4325335919857025
Validation loss = 0.433896541595459
Validation loss = 0.43239331245422363
Validation loss = 0.4351442754268646
Validation loss = 0.4353182315826416
Validation loss = 0.4339751601219177
Validation loss = 0.4357687532901764
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.43320849537849426
Validation loss = 0.4322546422481537
Validation loss = 0.43530163168907166
Validation loss = 0.43391963839530945
Validation loss = 0.4334416687488556
Validation loss = 0.4340957999229431
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.43751320242881775
Validation loss = 0.43598246574401855
Validation loss = 0.43713024258613586
Validation loss = 0.4359576404094696
Validation loss = 0.43681955337524414
Validation loss = 0.43782731890678406
Validation loss = 0.4379098117351532
Validation loss = 0.4389367699623108
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4361233711242676
Validation loss = 0.4350767731666565
Validation loss = 0.4369838535785675
Validation loss = 0.43633460998535156
Validation loss = 0.4364875257015228
Validation loss = 0.4354855418205261
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 64
average number of affinization = 13.55464159811986
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 41
average number of affinization = 13.586854460093896
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 53
average number of affinization = 13.63305978898007
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 40
average number of affinization = 13.663934426229508
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 13.697076023391812
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 54
average number of affinization = 13.744158878504672
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 41
average number of affinization = 13.775962660443406
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 48
average number of affinization = 13.815850815850816
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 55
average number of affinization = 13.863795110593713
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 55
average number of affinization = 13.911627906976744
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 59
average number of affinization = 13.963995354239257
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 47
average number of affinization = 14.002320185614849
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 35
average number of affinization = 14.026651216685979
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 55
average number of affinization = 14.074074074074074
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 37
average number of affinization = 14.100578034682082
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 14.128175519630485
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 53
average number of affinization = 14.173010380622838
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 33
average number of affinization = 14.194700460829493
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 50
average number of affinization = 14.23590333716916
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 52
average number of affinization = 14.279310344827586
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 71
average number of affinization = 14.34443168771527
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 14.376146788990825
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 54
average number of affinization = 14.421534936998855
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 49
average number of affinization = 14.461098398169336
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 14.492571428571429
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 33       |
| MaximumReturn | 198      |
| MinimumReturn | 122      |
| TotalSamples  | 116655   |
----------------------------
itr #34 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.43803513050079346
Validation loss = 0.4368099272251129
Validation loss = 0.436774343252182
Validation loss = 0.4378969073295593
Validation loss = 0.4376448690891266
Validation loss = 0.4383362829685211
Validation loss = 0.4390466511249542
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4323509931564331
Validation loss = 0.4354446530342102
Validation loss = 0.43196380138397217
Validation loss = 0.4326479136943817
Validation loss = 0.4332289397716522
Validation loss = 0.43367236852645874
Validation loss = 0.4361122250556946
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.43420225381851196
Validation loss = 0.4365558922290802
Validation loss = 0.432217001914978
Validation loss = 0.4355979561805725
Validation loss = 0.4323851466178894
Validation loss = 0.4347507655620575
Validation loss = 0.43445783853530884
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4359651207923889
Validation loss = 0.4362102746963501
Validation loss = 0.4378170371055603
Validation loss = 0.4377831518650055
Validation loss = 0.4382035732269287
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4345282316207886
Validation loss = 0.43739014863967896
Validation loss = 0.4344918727874756
Validation loss = 0.43380919098854065
Validation loss = 0.4358641803264618
Validation loss = 0.43609896302223206
Validation loss = 0.43626272678375244
Validation loss = 0.4370625615119934
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 40
average number of affinization = 14.521689497716896
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 38
average number of affinization = 14.548460661345496
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 51
average number of affinization = 14.58997722095672
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 48
average number of affinization = 14.627986348122867
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 37
average number of affinization = 14.653409090909092
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 49
average number of affinization = 14.692395005675369
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 46
average number of affinization = 14.727891156462585
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 41
average number of affinization = 14.757644394110985
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 40
average number of affinization = 14.786199095022624
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 45
average number of affinization = 14.820338983050847
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 41
average number of affinization = 14.849887133182845
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 54
average number of affinization = 14.894024802705749
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 14.92117117117117
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 53
average number of affinization = 14.96400449943757
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 55
average number of affinization = 15.008988764044943
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 52
average number of affinization = 15.05050505050505
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 54
average number of affinization = 15.094170403587444
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 57
average number of affinization = 15.141097424412093
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 42
average number of affinization = 15.171140939597315
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 57
average number of affinization = 15.217877094972067
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 50
average number of affinization = 15.256696428571429
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 41
average number of affinization = 15.285395763656632
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 56
average number of affinization = 15.330734966592427
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 47
average number of affinization = 15.365962180200222
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 51
average number of affinization = 15.405555555555555
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 34       |
| MaximumReturn | 198      |
| MinimumReturn | 124      |
| TotalSamples  | 119988   |
----------------------------
itr #35 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.439176470041275
Validation loss = 0.43640977144241333
Validation loss = 0.43870216608047485
Validation loss = 0.4374091923236847
Validation loss = 0.4370792508125305
Validation loss = 0.4372209310531616
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4302023649215698
Validation loss = 0.43126270174980164
Validation loss = 0.4341152310371399
Validation loss = 0.43153467774391174
Validation loss = 0.4333088994026184
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.43231716752052307
Validation loss = 0.4325495958328247
Validation loss = 0.43125906586647034
Validation loss = 0.43294376134872437
Validation loss = 0.4311304986476898
Validation loss = 0.43379321694374084
Validation loss = 0.4331536591053009
Validation loss = 0.4330888092517853
Validation loss = 0.43284931778907776
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.43507930636405945
Validation loss = 0.4344213306903839
Validation loss = 0.433868944644928
Validation loss = 0.4366718530654907
Validation loss = 0.43601635098457336
Validation loss = 0.43612438440322876
Validation loss = 0.4375268816947937
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4340628981590271
Validation loss = 0.4355008006095886
Validation loss = 0.4339262545108795
Validation loss = 0.4325999617576599
Validation loss = 0.4369300305843353
Validation loss = 0.4340449273586273
Validation loss = 0.4366127550601959
Validation loss = 0.43546372652053833
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 53
average number of affinization = 15.447280799112098
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 79
average number of affinization = 15.517738359201774
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 53
average number of affinization = 15.559246954595793
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 51
average number of affinization = 15.598451327433628
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 50
average number of affinization = 15.63646408839779
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 55
average number of affinization = 15.67991169977925
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 49
average number of affinization = 15.71664829106946
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 55
average number of affinization = 15.759911894273127
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 64
average number of affinization = 15.812981298129813
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 60
average number of affinization = 15.861538461538462
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 57
average number of affinization = 15.906695938529088
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 46
average number of affinization = 15.93969298245614
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 50
average number of affinization = 15.976998904709749
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 57
average number of affinization = 16.0218818380744
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 59
average number of affinization = 16.068852459016394
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 56
average number of affinization = 16.112445414847162
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 51
average number of affinization = 16.1504907306434
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 44
average number of affinization = 16.18082788671024
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 44
average number of affinization = 16.211099020674645
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 52
average number of affinization = 16.25
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 37
average number of affinization = 16.272529858849076
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 56
average number of affinization = 16.315618221258134
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 43
average number of affinization = 16.344528710725893
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 62
average number of affinization = 16.393939393939394
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 48
average number of affinization = 16.42810810810811
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 164      |
| Iteration     | 35       |
| MaximumReturn | 197      |
| MinimumReturn | 122      |
| TotalSamples  | 123321   |
----------------------------
itr #36 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4363711178302765
Validation loss = 0.4362013339996338
Validation loss = 0.4358731508255005
Validation loss = 0.43515557050704956
Validation loss = 0.4383365213871002
Validation loss = 0.4370558559894562
Validation loss = 0.4388514459133148
Validation loss = 0.4378841519355774
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4298177659511566
Validation loss = 0.43055471777915955
Validation loss = 0.4304336607456207
Validation loss = 0.43062683939933777
Validation loss = 0.430396169424057
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.43197575211524963
Validation loss = 0.4316106140613556
Validation loss = 0.43325909972190857
Validation loss = 0.4344078004360199
Validation loss = 0.4336562752723694
Validation loss = 0.435458779335022
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.43713152408599854
Validation loss = 0.43551474809646606
Validation loss = 0.43577972054481506
Validation loss = 0.4349516034126282
Validation loss = 0.4369145929813385
Validation loss = 0.43604856729507446
Validation loss = 0.4382283687591553
Validation loss = 0.43710851669311523
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.434119313955307
Validation loss = 0.43425196409225464
Validation loss = 0.4359724223613739
Validation loss = 0.43493086099624634
Validation loss = 0.43391942977905273
Validation loss = 0.43407389521598816
Validation loss = 0.4359183609485626
Validation loss = 0.43554335832595825
Validation loss = 0.4359162151813507
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 59
average number of affinization = 16.474082073434126
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 59
average number of affinization = 16.519956850053937
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 79
average number of affinization = 16.58728448275862
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 49
average number of affinization = 16.622174381054897
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 61
average number of affinization = 16.669892473118278
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 51
average number of affinization = 16.706766917293233
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 55
average number of affinization = 16.74785407725322
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 68
average number of affinization = 16.80278670953912
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 61
average number of affinization = 16.850107066381156
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 50
average number of affinization = 16.885561497326204
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 63
average number of affinization = 16.93482905982906
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 60
average number of affinization = 16.980789754535753
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 49
average number of affinization = 17.01492537313433
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 58
average number of affinization = 17.05857294994675
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 52
average number of affinization = 17.095744680851062
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 65
average number of affinization = 17.14665249734325
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 63
average number of affinization = 17.195329087048833
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 54
average number of affinization = 17.234358430540826
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 47
average number of affinization = 17.265889830508474
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 64
average number of affinization = 17.315343915343917
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 51
average number of affinization = 17.350951374207188
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 56
average number of affinization = 17.391763463569166
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 67
average number of affinization = 17.44409282700422
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 83
average number of affinization = 17.513171759747102
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 56
average number of affinization = 17.553684210526317
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 170      |
| Iteration     | 36       |
| MaximumReturn | 198      |
| MinimumReturn | 116      |
| TotalSamples  | 126654   |
----------------------------
itr #37 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.43562594056129456
Validation loss = 0.4380548298358917
Validation loss = 0.4354405105113983
Validation loss = 0.43730485439300537
Validation loss = 0.436885803937912
Validation loss = 0.43681758642196655
Validation loss = 0.4374373257160187
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.4306795299053192
Validation loss = 0.4294969439506531
Validation loss = 0.4299873411655426
Validation loss = 0.4305228590965271
Validation loss = 0.43074220418930054
Validation loss = 0.42991700768470764
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.430551677942276
Validation loss = 0.4309242367744446
Validation loss = 0.4304927587509155
Validation loss = 0.43118008971214294
Validation loss = 0.43270379304885864
Validation loss = 0.43325793743133545
Validation loss = 0.4328986406326294
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4344383478164673
Validation loss = 0.43408215045928955
Validation loss = 0.43266561627388
Validation loss = 0.4350655674934387
Validation loss = 0.4352070391178131
Validation loss = 0.4356895089149475
Validation loss = 0.4353298544883728
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.4368235766887665
Validation loss = 0.4345618784427643
Validation loss = 0.43429744243621826
Validation loss = 0.4340016841888428
Validation loss = 0.4355202913284302
Validation loss = 0.4344587028026581
Validation loss = 0.43401899933815
Validation loss = 0.4339042603969574
Validation loss = 0.4359307289123535
Validation loss = 0.43440189957618713
Validation loss = 0.4369833469390869
Validation loss = 0.43712693452835083
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 51
average number of affinization = 17.588853838065194
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 56
average number of affinization = 17.62920168067227
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 55
average number of affinization = 17.668415529905563
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 62
average number of affinization = 17.71488469601677
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 63
average number of affinization = 17.762303664921465
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 62
average number of affinization = 17.80857740585774
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 56
average number of affinization = 17.848484848484848
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 88
average number of affinization = 17.92171189979123
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 66
average number of affinization = 17.9718456725756
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 48
average number of affinization = 18.003125
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 49
average number of affinization = 18.03537981269511
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 39
average number of affinization = 18.057172557172557
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 63
average number of affinization = 18.103842159916926
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 53
average number of affinization = 18.140041493775932
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 40
average number of affinization = 18.162694300518133
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 63
average number of affinization = 18.209109730848862
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 72
average number of affinization = 18.264736297828335
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 78
average number of affinization = 18.326446280991735
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 69
average number of affinization = 18.37874097007224
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 71
average number of affinization = 18.432989690721648
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 54
average number of affinization = 18.46961894953656
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 51
average number of affinization = 18.503086419753085
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 74
average number of affinization = 18.5601233299075
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 52
average number of affinization = 18.594455852156056
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 70
average number of affinization = 18.647179487179486
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 167      |
| Iteration     | 37       |
| MaximumReturn | 199      |
| MinimumReturn | 109      |
| TotalSamples  | 129987   |
----------------------------
itr #38 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4357094466686249
Validation loss = 0.4355055093765259
Validation loss = 0.43506455421447754
Validation loss = 0.4355470836162567
Validation loss = 0.4346267580986023
Validation loss = 0.4360028803348541
Validation loss = 0.43572601675987244
Validation loss = 0.43608585000038147
Validation loss = 0.4364902675151825
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.42975905537605286
Validation loss = 0.4276852309703827
Validation loss = 0.43026262521743774
Validation loss = 0.42845457792282104
Validation loss = 0.42965516448020935
Validation loss = 0.42843690514564514
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.43033695220947266
Validation loss = 0.43126457929611206
Validation loss = 0.4303743839263916
Validation loss = 0.43371179699897766
Validation loss = 0.4315128028392792
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.4340246021747589
Validation loss = 0.4352880120277405
Validation loss = 0.4380975663661957
Validation loss = 0.43467479944229126
Validation loss = 0.4334985315799713
Validation loss = 0.4358731508255005
Validation loss = 0.4358940124511719
Validation loss = 0.43587952852249146
Validation loss = 0.43723148107528687
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.43451693654060364
Validation loss = 0.43440574407577515
Validation loss = 0.43432456254959106
Validation loss = 0.4364579916000366
Validation loss = 0.4370400905609131
Validation loss = 0.43534472584724426
Validation loss = 0.4359942078590393
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 40
average number of affinization = 18.66905737704918
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 45
average number of affinization = 18.69600818833163
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 58
average number of affinization = 18.736196319018404
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 66
average number of affinization = 18.78447395301328
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 68
average number of affinization = 18.83469387755102
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 59
average number of affinization = 18.875637104994905
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 48
average number of affinization = 18.90529531568228
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 68
average number of affinization = 18.955239064089522
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 48
average number of affinization = 18.984756097560975
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 48
average number of affinization = 19.014213197969543
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 48
average number of affinization = 19.043610547667342
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 49
average number of affinization = 19.073961499493414
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 56
average number of affinization = 19.111336032388664
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 69
average number of affinization = 19.161779575328616
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 58
average number of affinization = 19.2010101010101
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 75
average number of affinization = 19.25731584258325
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 58
average number of affinization = 19.296370967741936
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 70
average number of affinization = 19.347432024169184
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 59
average number of affinization = 19.387323943661972
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 62
average number of affinization = 19.430150753768846
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 63
average number of affinization = 19.473895582329316
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 66
average number of affinization = 19.520561685055167
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 65
average number of affinization = 19.56613226452906
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 56
average number of affinization = 19.6026026026026
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 78
average number of affinization = 19.661
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 163      |
| Iteration     | 38       |
| MaximumReturn | 198      |
| MinimumReturn | 122      |
| TotalSamples  | 133320   |
----------------------------
itr #39 | 
Fitting dynamics.
Fitting model 0 (0-based) in the ensemble of 5 models
Validation loss = 0.4358175992965698
Validation loss = 0.4349881112575531
Validation loss = 0.4382074177265167
Validation loss = 0.43613240122795105
Validation loss = 0.43476906418800354
Validation loss = 0.43549787998199463
Validation loss = 0.4356105923652649
Validation loss = 0.4355609118938446
Validation loss = 0.4375135600566864
Fitting model 1 (0-based) in the ensemble of 5 models
Validation loss = 0.42862647771835327
Validation loss = 0.42691853642463684
Validation loss = 0.42983102798461914
Validation loss = 0.42704302072525024
Validation loss = 0.42884501814842224
Validation loss = 0.4285995662212372
Fitting model 2 (0-based) in the ensemble of 5 models
Validation loss = 0.4300522804260254
Validation loss = 0.43127816915512085
Validation loss = 0.43047285079956055
Validation loss = 0.431595116853714
Validation loss = 0.4298383295536041
Validation loss = 0.4308422803878784
Validation loss = 0.4316779375076294
Validation loss = 0.4326431453227997
Validation loss = 0.43087857961654663
Fitting model 3 (0-based) in the ensemble of 5 models
Validation loss = 0.43462681770324707
Validation loss = 0.43408459424972534
Validation loss = 0.4344269931316376
Validation loss = 0.43555399775505066
Validation loss = 0.4357927143573761
Validation loss = 0.4358687400817871
Fitting model 4 (0-based) in the ensemble of 5 models
Validation loss = 0.43631866574287415
Validation loss = 0.4335802495479584
Validation loss = 0.43464016914367676
Validation loss = 0.4332789182662964
Validation loss = 0.4343041777610779
Validation loss = 0.43282121419906616
Validation loss = 0.43500813841819763
Validation loss = 0.43359148502349854
Validation loss = 0.4346188008785248
Validation loss = 0.4372723400592804
Done fitting dynamics.
Updating randomness.
Done updating randomness.
Training policy using TRPO.
Re-initialize init_std.
Obtaining samples for iteration 0...
Obtaining samples for iteration 1...
Obtaining samples for iteration 2...
Obtaining samples for iteration 3...
Obtaining samples for iteration 4...
Obtaining samples for iteration 5...
Obtaining samples for iteration 6...
Obtaining samples for iteration 7...
Obtaining samples for iteration 8...
Obtaining samples for iteration 9...
Obtaining samples for iteration 10...
Obtaining samples for iteration 11...
Obtaining samples for iteration 12...
Obtaining samples for iteration 13...
Obtaining samples for iteration 14...
Obtaining samples for iteration 15...
Obtaining samples for iteration 16...
Obtaining samples for iteration 17...
Obtaining samples for iteration 18...
Obtaining samples for iteration 19...
Done training policy.
Generating on-policy rollouts.
Path 0 | total_timesteps 0.
number of affinization with epsilon = 0.5 is 63
average number of affinization = 19.704295704295703
Path 1 | total_timesteps 200.
number of affinization with epsilon = 0.5 is 73
average number of affinization = 19.75748502994012
Path 2 | total_timesteps 400.
number of affinization with epsilon = 0.5 is 51
average number of affinization = 19.78863409770688
Path 3 | total_timesteps 600.
number of affinization with epsilon = 0.5 is 64
average number of affinization = 19.83266932270916
Path 4 | total_timesteps 800.
number of affinization with epsilon = 0.5 is 70
average number of affinization = 19.882587064676617
Path 5 | total_timesteps 1000.
number of affinization with epsilon = 0.5 is 63
average number of affinization = 19.92544731610338
Path 6 | total_timesteps 1200.
number of affinization with epsilon = 0.5 is 66
average number of affinization = 19.971201588877854
Path 7 | total_timesteps 1400.
number of affinization with epsilon = 0.5 is 65
average number of affinization = 20.015873015873016
Path 8 | total_timesteps 1600.
number of affinization with epsilon = 0.5 is 47
average number of affinization = 20.042616451932606
Path 9 | total_timesteps 1800.
number of affinization with epsilon = 0.5 is 64
average number of affinization = 20.086138613861387
Path 10 | total_timesteps 2000.
number of affinization with epsilon = 0.5 is 66
average number of affinization = 20.131552917903065
Path 11 | total_timesteps 2200.
number of affinization with epsilon = 0.5 is 46
average number of affinization = 20.15711462450593
Path 12 | total_timesteps 2400.
number of affinization with epsilon = 0.5 is 71
average number of affinization = 20.20730503455084
Path 13 | total_timesteps 2600.
number of affinization with epsilon = 0.5 is 58
average number of affinization = 20.244575936883628
Path 14 | total_timesteps 2800.
number of affinization with epsilon = 0.5 is 62
average number of affinization = 20.285714285714285
Path 15 | total_timesteps 3000.
number of affinization with epsilon = 0.5 is 68
average number of affinization = 20.33267716535433
Path 16 | total_timesteps 3200.
number of affinization with epsilon = 0.5 is 49
average number of affinization = 20.36086529006883
Path 17 | total_timesteps 3400.
number of affinization with epsilon = 0.5 is 58
average number of affinization = 20.397838899803535
Path 18 | total_timesteps 3600.
number of affinization with epsilon = 0.5 is 73
average number of affinization = 20.44946025515211
Path 19 | total_timesteps 3800.
number of affinization with epsilon = 0.5 is 78
average number of affinization = 20.50588235294118
Path 20 | total_timesteps 4000.
number of affinization with epsilon = 0.5 is 50
average number of affinization = 20.534769833496572
Path 21 | total_timesteps 4200.
number of affinization with epsilon = 0.5 is 65
average number of affinization = 20.578277886497066
Path 22 | total_timesteps 4400.
number of affinization with epsilon = 0.5 is 63
average number of affinization = 20.6197458455523
Path 23 | total_timesteps 4600.
number of affinization with epsilon = 0.5 is 56
average number of affinization = 20.654296875
Path 24 | total_timesteps 4800.
number of affinization with epsilon = 0.5 is 60
average number of affinization = 20.692682926829267
Done generating on-policy rollouts.
Updating normalization.
Done updating normalization.
----------------------------
| AverageReturn | 168      |
| Iteration     | 39       |
| MaximumReturn | 198      |
| MinimumReturn | 123      |
| TotalSamples  | 136653   |
----------------------------
